{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning and Evaluating RoBERTa base model for Binary Classification on a TensorFlow Augmented Dataset"
      ],
      "metadata": {
        "id": "vIxGpn52B0cs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Necessary Libraries"
      ],
      "metadata": {
        "id": "Xs3erR_A8U0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets scikit-learn huggingface_hub pandas nltk\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import random\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_CgV05X8ZlJ",
        "outputId": "3fe185d8-50e9-479e-84b3-7852fcc85c66"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/179.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Synonym Replacement Functions"
      ],
      "metadata": {
        "id": "vw2AF8OD8cTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to replace a random word in a sentence with its synonym\n",
        "def synonym_replacement(sentence, n=1):\n",
        "    if not sentence.strip():  # Handle empty sentences\n",
        "        return sentence\n",
        "\n",
        "    words = word_tokenize(sentence)\n",
        "    new_words = words.copy()\n",
        "    random_word_list = list(set(words))\n",
        "    random.shuffle(random_word_list)\n",
        "    num_replaced = 0\n",
        "\n",
        "    for random_word in random_word_list:\n",
        "        synonyms = get_synonyms(random_word)\n",
        "        if synonyms:\n",
        "            synonym = random.choice(synonyms)\n",
        "            new_words = [synonym if word == random_word else word for word in new_words]\n",
        "            num_replaced += 1\n",
        "        if num_replaced >= n:\n",
        "            break\n",
        "\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "# Function to get synonyms of a word\n",
        "def get_synonyms(word):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            if lemma.name().lower() != word.lower():\n",
        "                synonyms.add(lemma.name().replace('_', ' '))\n",
        "    return list(synonyms)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Mlr-Ss9a8bzx"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and Combine Datasets"
      ],
      "metadata": {
        "id": "I-xW8A3o8hCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load all datasets (ARB, BOH, NAM, UNK have bugs; nonbug.csv does not have bugs)\n",
        "datasets = ['ARB.csv', 'BOH.csv', 'NAM.csv', 'UNK.csv', 'nonbug.csv']\n",
        "dfs = []\n",
        "\n",
        "# Read and combine all datasets\n",
        "for dataset in datasets:\n",
        "    df = pd.read_csv(dataset)\n",
        "    # Assign bug label based on the dataset filename\n",
        "    label = 1 if dataset != 'nonbug.csv' else 0  # 1 for files with bugs, 0 for nonbug.csv\n",
        "    df['label'] = label\n",
        "    dfs.append(df)\n",
        "\n",
        "# Combine all datasets into a single DataFrame\n",
        "df_combined = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Check for missing values and handle them\n",
        "df_combined = df_combined.dropna(subset=['title', 'summary', 'comments'])\n",
        "\n",
        "# Combine summary and comments into a single text column for the model input\n",
        "df_combined['text'] = df_combined['summary'].astype(str) + \" \" + df_combined['comments'].astype(str)\n",
        "\n"
      ],
      "metadata": {
        "id": "JIGme8pB8n4S"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Dataset and Augment Training Data"
      ],
      "metadata": {
        "id": "MSivaZ7-8qsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into training (80%) and test (20%) sets\n",
        "train_data, test_data = train_test_split(df_combined, test_size=0.2, random_state=42, stratify=df_combined['label'])\n",
        "\n",
        "# Augment the training data by creating 5 variations for each row\n",
        "augmented_train_data = []\n",
        "for _, row in train_data.iterrows():\n",
        "    augmented_train_data.append(row)\n",
        "    for _ in range(5):  # Create 5 augmented versions\n",
        "        augmented_text = synonym_replacement(row['text'])\n",
        "        new_row = row.copy()\n",
        "        new_row['text'] = augmented_text\n",
        "        augmented_train_data.append(new_row)\n",
        "\n",
        "# Convert the augmented data to a DataFrame\n",
        "train_data_augmented = pd.DataFrame(augmented_train_data)\n",
        "\n",
        "# Check dataset sizes\n",
        "print(f\"\\nTraining Dataset Size: {len(train_data_augmented)}\")\n",
        "print(f\"Testing Dataset Size: {len(test_data)}\")\n",
        "\n",
        "# Check the distribution of labels (1 and 0) in the training dataset\n",
        "train_class_distribution = train_data_augmented['label'].value_counts()\n",
        "\n",
        "print(\"\\nClass Distribution in Training Dataset:\")\n",
        "print(train_class_distribution)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz5Gmv1u8vfG",
        "outputId": "a58360f3-b98b-42fd-ceac-5c4b4f1b68a2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Dataset Size: 9432\n",
            "Testing Dataset Size: 394\n",
            "\n",
            "Class Distribution in Training Dataset:\n",
            "label\n",
            "0    4908\n",
            "1    4524\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize Data"
      ],
      "metadata": {
        "id": "jEqC8xVt8xW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer\n",
        "model_name = \"roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize text and extract necessary fields\n",
        "train_encodings = tokenizer(list(train_data_augmented['text']), truncation=True, padding=True, max_length=128)\n",
        "test_encodings = tokenizer(list(test_data['text']), truncation=True, padding=True, max_length=128)\n",
        "\n",
        "# Add labels to the tokenized data\n",
        "train_encodings['labels'] = train_data_augmented['label'].values\n",
        "test_encodings['labels'] = test_data['label'].values\n",
        "\n",
        "# Convert to Hugging Face datasets\n",
        "train_dataset = Dataset.from_dict(train_encodings)\n",
        "test_dataset = Dataset.from_dict(test_encodings)\n",
        "\n",
        "# Compute class weights to handle class imbalance\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=np.array([0, 1]),\n",
        "    y=train_data_augmented['label']\n",
        ")\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
        "\n",
        "# Load the RoBERTa model for binary classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=2\n",
        ")\n",
        "\n",
        "# Prepare data collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8g-4nZwB80q_",
        "outputId": "b15bf241-87e3-4776-e29b-9671915e44ee"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Custom Dataset Class"
      ],
      "metadata": {
        "id": "P50betQG84sV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Trainer Class with class weights handling\n",
        "class CustomTrainer(Trainer):\n",
        "    def __init__(self, *args, class_weights=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
        "        return (loss, outputs) if return_outputs else loss\n"
      ],
      "metadata": {
        "id": "EwffxJh_89Ip"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Metrics Function"
      ],
      "metadata": {
        "id": "8fVBGZYU8_yP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom metric function\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predicted_labels = np.argmax(predictions, axis=1)\n",
        "    accuracy = accuracy_score(labels, predicted_labels)\n",
        "    f1 = f1_score(labels, predicted_labels, average='weighted')\n",
        "    return {'accuracy': accuracy, 'f1': f1}\n",
        "\n"
      ],
      "metadata": {
        "id": "FYBVA2VT9FtP"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Training Arguments"
      ],
      "metadata": {
        "id": "A52-iEtJ9Hhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=10,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l60AJJS19K1Q",
        "outputId": "8d3b007a-f7f3-45ea-9120-2ad91a4938bc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize and Train Model"
      ],
      "metadata": {
        "id": "i0gs0D9O9PhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Trainer\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    class_weights=class_weights.to(training_args.device)\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "dM1L13rL9aGK",
        "outputId": "890a9acb-f56c-4199-8fff-2e7ce164e819"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-d8246a4acfee>:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3537' max='3537' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3537/3537 14:25, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.490900</td>\n",
              "      <td>0.926281</td>\n",
              "      <td>0.621827</td>\n",
              "      <td>0.621100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.360800</td>\n",
              "      <td>1.737014</td>\n",
              "      <td>0.621827</td>\n",
              "      <td>0.621279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.208700</td>\n",
              "      <td>2.244069</td>\n",
              "      <td>0.609137</td>\n",
              "      <td>0.608935</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3537, training_loss=0.37406487693414536, metrics={'train_runtime': 865.9014, 'train_samples_per_second': 32.678, 'train_steps_per_second': 4.085, 'total_flos': 1861247605616640.0, 'train_loss': 0.37406487693414536, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Model"
      ],
      "metadata": {
        "id": "Z5TDk-8T9cH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluate the model after training\n",
        "def evaluate_model():\n",
        "    predictions = trainer.predict(test_dataset)\n",
        "    predicted_labels = predictions.predictions.argmax(axis=1)\n",
        "    true_labels = predictions.label_ids\n",
        "\n",
        "    # Print Accuracy and F1 score\n",
        "    print(f\"Accuracy: {accuracy_score(true_labels, predicted_labels) * 100:.2f}%\")\n",
        "    print(f\"Classification Report:\\n{classification_report(true_labels, predicted_labels)}\")\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "v5i5zPv_9fVy",
        "outputId": "8ac501d1-e1b7-4c11-bf5c-bc4bae4ed6ab"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 62.18%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.66      0.65       205\n",
            "           1       0.61      0.58      0.59       189\n",
            "\n",
            "    accuracy                           0.62       394\n",
            "   macro avg       0.62      0.62      0.62       394\n",
            "weighted avg       0.62      0.62      0.62       394\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}