ID,title,summary,comments
929,label image deadlocks,I am on commit 14cd77baeb0ee6f4b40816afc1f8c9c5d186bb8e. I ve added the following change that randomizes order of execution of tasks in thread pool Then run label image as after few iterations it deadlocks. All threads are blocked on condition variables but tasks they are waiting for can t run because all thread pool threads are busy This come up during testing of a faster thread pool implementation that has distributed queues and so does not preserve FIFO order. But I think it can come up with current pool as well maybe after some unrelated code changes or maybe just due to unlucky scheduling order . Computational tasks running on a fixed size thread pool must not ever block. Instead they should schedule continuations. Besides deadlocks blocking leads to serious CPU underutilization. I.e. on my machine label image utilizes only about half of cores because half of threads in pool are blocked waiting for other tasks. , jeremybarnes I wonder didn t you this with your pool I ve been submerged for the last couple of weeks and I didn t see the notification . I didn t see any deadlocks but I can see that they are definitely possible on any thread pool that doesn t know to schedule a new thread if a worker thread blocks. We need to be very careful in any bounded thread scenario as it s not possible to guarantee progress in the general case on a finite number of threads. In general the pattern I use to work around these kind of deadlocks with a fixed number of threads is to poll instead of block and perform work from the threadpool after un unsuccessful poll to guarantee forward progress. That would require some deeper changes to Eigen so that it knew it was in a worker thread and could adjust its behaviour accordingly. Thanks Jeremy. To clarify you run label image with your pool and did not see deadlocks or you did not run label image and did not see deadlocks Pulling for work while waiting can help to some degree but it still has problems when blocked tasks stack up on a single thread 1. Stack can grow without limit and cause stack overflow and you can t limit recursion at runtime because then you get back to deadlocks. 2. This limit available parallelism a task can be already ready to run but is sitting on a thread stack and the thread is busy executing another task. It is not possible to execute the readied task until all tasks on the same thread finish. 3. And more importantly it is subject to deadlocks as well. Consider that a task is waiting for completion of a task that is blocked on the very same stack below it. Oops. Some parallel computation libraries avoid the deadlocks by imposing a DAG requirement of work structure where all dependent tasks must be spawned by the task itself. This again requires fixing structure of all algorithms rather than just plugging polling into wait objects. And it makes 2 worse. Some other systems solve this by playing tricks with virtual memory http groups.csail.mit.edu sct wiki index.php title The Cilk M Project http supertech.csail.mit.edu papers stacks.pdf But I think this is out of question here. I would suggest to explicitly schedule continuations instead of blocking on worker thread stack. Continuations suck code wise. But that s what we have in C . Hopefully it s not that bad with closures. I ran label image and did not see any deadlocks although I wasn t looking for them. I don t think that continuations would be an issue code wise it s more enabling the synchronization methods to accept them that would take work. Also the reason for having work stealing across thread s work queues was to deal with your point 2 above. I agree it s a big deal. An issue I ve been having with multiple file and batch queues may be related. See this question http stackoverflow.com questions 35414009 multiple queues causing tf to lock up 35420394 on stack overflow. dvyukov rmlarsen Did the recent thread pool changes resolve this Yes it s fixed with the contraction changes. 
932,thread pool deadlocks on shutdown,ThreadPool dtor does not pop waiters from waiters list. As the result dead waiters are left on the list. If remaining tasks submit new tasks thread pool deadlocks because some notifications are consumed by the leftover dead waiters instead of alive threads that should receive the notifications. Here is a simple test that does classical parallel decomposition and reliably deadlocks , zheng xq Do you have thoughts on this The non blocking thread pool PR is problematic since it breaks Eigen s FIFO requirement but the deadlock issue seems like something we should fix. zheng xq girving On Tue Mar 8 2016 at 8 08 PM Zhifeng Chen wrote The thread calling delete p should have the sole ownership of p where p is generally any c object. So it s not valid if one thread is calling a threadpool p while another thread calling p Add . That s true. But the thread pool waits for the existing tasks to finish in destructor. If you combine these two points you conclude that tasks are not allowed to submit child tasks to the thread pool. Ever. That s just does not make sense. So you need to either 1 wait for all tasks to finish including subtasks of existing tasks or 2 don t wait in thread pool destructor at all stop threads as soon as possible assert that the queue is empty now and exit. I am working on a fix. dvyukov rmlarsen Did the recent thread pool changes resolve this rmlarsen Assigning you since dvyukov isn t part of the Github org. Should we add him Yes it s fixed. New thread pool waits for completion of all work. 
1363,Tests in tensorflow core distributed runtime ... sometimes time out,Some of the tests in this directory in particular master test and rpc grpc session test spawn subprocesses containing TensorFlow servers that listen on unused ports. There is a potential TOCTOU bug in this code because the process is 1. Pick unused ports in the parent. 2. Fork processes and instruct them to bind to the picked ports. At this point another process can bind the same ports. 3. Connect to subprocesses from the parent. Running these tests in exclusive mode is a workaround for the problem. ,Nobody has complained about this for some time so I m going to close this bug. Famous last words.... 
2957,Dequeueing immediately after starting threads fails,From a question on Stack Overflow http stackoverflow.com questions 37878696 dequeue immediately after starting threads fails the following code fails ...with the following error NotFoundError FetchOutputs node input producer Dequeue 0 not found It turns out that my fix for 2425 was incomplete and there is still a race between concurrent graph modification and Session.run calls. I m preparing a fix. ,Fixed by 91d65f6 
4135,Seg fault when computing gradient of 3D convolution filter with 1 1 1 kernel,Minimal example to reproduce the bug this leads to a seg fault tested on a Mac with CPU both with tensorflow 0.9.0 binary release and when compiled from source last commit https github.com tensorflow tensorflow commit ad4f02a69162abe5d242b7d94f62138849aec9ab . For certain values of x shape 1 f shape 1 for instance 2 4 or 8 32 the seg fault occurs but for other values 2 2 or 16 32 there is no seg fault. Inserting some std cout print statements in the code shows that the segmentation fault occurs during this function call in Conv3DBackpropFilterOp Compute https github.com tensorflow tensorflow blob master tensorflow core kernels conv grad ops 3d.cc L315 ,Thanks for the report. Internal bug opened. Also running into this on 0.10.0rc0 and nightly on ubuntu 14.04 and python 3.5 I m not seeing this with the latest nightly If I don t hear back I ll assume this was resolved. I was able to reproduce this on Mac OS X with the nightly. So this appears to be a Mac only bug. It does not cause crash on my Linux Ubuntu 14.04 for both CPU and GPU. Btw try not to use Mac OS because Apple developers tend to create their own standards which is different from the state of the art convention causing inconvenience and trouble. Last time when I was doing online translation service. I found that only for Mac OS if your URL ends with a multi byte UTF 8 character the system API will automatically append a NULL character at the end. This causes my web program to function incorrectly for Apple devices. kevin keraudren could you try with the latest version and build with config asan Closing due to inactivity. Feel free to re open if you would like us to look again. 
4521,Floating Point Exception with Conv3d ,I am getting a floating point exception with the following example I m currently using Cuda 8.0.27 and CUDNN 5.1.5 running on Ubuntu 16.04.1 LTS TensorFlow built from source commit a0d929df36ca7d6f72184371f6c3d6d877dded3e Bazel version 0.3.1 The code does not error out if you change the kernel height currently 16 to other numbers I ve tried 14 15 17 and 18 . Thanks in advanced. ,what was the error message I get the following error message Floating point exception core dumped It seems to work on CPU only for me does it work on CPU for you This does seem to be a GPU specific bug as the code works fine on the CPU. Passing this to Michal. Hi all I have a very similar problem. This MWE fails with a floating point exception only when w size 16 Tested with TensorFlow 0.12.0 CUDA 8.0.44 cuDNN 5.1 on the following systems Ubuntu 16.04.1 LTS Tesla K40c driver version 375.20 Ubuntu 14.04.5 LTS GeForce GTX 980 driver version 367.57 I can independently confirm this bug. Tried this with Tensorflow 1.0 CUDA 8.0.44 8.0.61 cuDNN 5.1 Ubuntu 16.04 LTS Tesla K80 GTX 1080 . This even crashes at w size 8 in some cases. Are there any updates I m having the same problem here with tf.nn.conv3d transpose on Ubuntu 16.04 TF1.0 Cuda 8.0.61 cuDNN 5.1 with a Titan X Maxwell It works with stride 8 and stride 8 tested 9 10 . mrajchl slundqui Could you test it on tensorflow 1.1 and see if this bug still exists Also mjanusz any update on this Seems to be working fine with TensorFlow 1.2.0 It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. Nagging Assignee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Verified with TensorFlow 1.4.1 and it has been fixed. 
5115,timeout breaks FIFOQueue,Ubuntu 14.04.5 LTS 0.10.0rc0 Using timeout with notebooks is very useful in case you dequeue an empty queue or enqueue a full one. The problem is that after a timeout occurs a enqueue or dequeueop throws an error here i get the usual timeout error. The problem is that when I then run sess.run enq feed dict ph 2 options timeout option i get the error ,I tried from a recent version and it doesn t happen there. Do you want to upgrade I can also reproduce this on 0.11rc0 If dequeue on empty queue is cancelled because of DeadlineExceededError the queue becomes unusable it s no longer possible to enqueue anything onto the queue. On Fri Oct 21 2016 at 6 48 AM cgel notifications github.com wrote Ubuntu 14.04.5 LTS 0.10.0rc0 Using timeout with notebooks is very useful in case you dequeue an empty queue or enqueue a full one. The problem is that after a timeout occurs a enqueue or dequeueop throws an error import tensorflow as tf with tf.device cpu 0 ph tf.placeholder tf.float32 q tf.FIFOQueue 2 tf.float32 enq q.enqueue ph deq q.dequeue timeout option tf.RunOptions timeout in ms 1000 sess tf.Session sess.run deq options timeout option here i get the usual timeout error. The problem is that when I then run sess.run enq feed dict ph 2 options timeout option i get the error CancelledError Traceback most recent call last ipython input 24 9067a9d62797 in module 1 sess.run Q options timeout option usr local lib python2.7 dist packages tensorflow python client session.pyc in run self fetches feed dict options run metadata 715 try 716 result self. run None fetches feed dict options ptr 717 run metadata ptr 718 if run metadata 719 proto data tf session.TF GetBuffer run metadata ptr usr local lib python2.7 dist packages tensorflow python client session.pyc in run self handle fetches feed dict options run metadata 913 if final fetches or final targets 914 results self. do run handle final targets final fetches 915 feed dict string options run metadata 916 else 917 results usr local lib python2.7 dist packages tensorflow python client session.pyc in do run self handle target list fetch list feed dict options run metadata 963 if handle is None 964 return self. do call run fn self. session feed dict fetch list 965 target list options run metadata 966 else 967 return self. do call prun fn self. session handle feed dict usr local lib python2.7 dist packages tensorflow python client session.pyc in do call self fn args 983 except KeyError 984 pass 985 raise type e node def op message 986 987 def extend graph self CancelledError Dequeue operation was cancelled Node fifo queue Dequeue QueueDequeue class loc fifo queue component types DT FLOAT DT INT64 DT FLOAT timeout ms 1 device job localhost replica 0 task 0 cpu 0 fifo queue Node PlaceholderWithDefault 25 Recv client terminated false recv device job localhost replica 0 task 0 gpu 0 send device job localhost replica 0 task 0 cpu 0 send device incarnation 1 tensor name edge 14 PlaceholderWithDefault tensor type DT FLOAT device job localhost replica 0 task 0 gpu 0 Caused by op u fifo queue Dequeue defined at File string line 1 in module File home cgel .local lib python2.7 site packages IPython kernel zmq kernelapp.py line 469 in main app.start File home cgel .local lib python2.7 site packages IPython kernel zmq kernelapp.py line 459 in start ioloop.IOLoop.instance .start File usr lib python2.7 dist packages zmq eventloop ioloop.py line 160 in start super ZMQIOLoop self .start File home cgel .local lib python2.7 site packages tornado ioloop.py line 883 in start handler func fd obj events File home cgel .local lib python2.7 site packages tornado stack context.py line 275 in null wrapper return fn args kwargs File usr lib python2.7 dist packages zmq eventloop zmqstream.py line 433 in handle events self. handle recv File usr lib python2.7 dist packages zmq eventloop zmqstream.py line 465 in handle recv self. run callback callback msg File usr lib python2.7 dist packages zmq eventloop zmqstream.py line 407 in run callback callback args kwargs File home cgel .local lib python2.7 site packages tornado stack context.py line 275 in null wrapper return fn args kwargs File home cgel .local lib python2.7 site packages IPython kernel zmq ipkernel.py line 281 in dispatcher return self.dispatch shell stream msg File home cgel .local lib python2.7 site packages IPython kernel zmq ipkernel.py line 245 in dispatch shell handler stream idents msg File home cgel .local lib python2.7 site packages IPython kernel zmq ipkernel.py line 389 in execute request shell.run cell code store history store history silent silent File home cgel .local lib python2.7 site packages IPython core interactiveshell.py line 2741 in run cell interactivity interactivity compiler compiler File home cgel .local lib python2.7 site packages IPython core interactiveshell.py line 2827 in run ast nodes if self.run code code File home cgel .local lib python2.7 site packages IPython core interactiveshell.py line 2883 in run code exec code obj self.user global ns self.user ns File ipython input 1 73bf93787995 line 76 in module input state action Y q.dequeue File usr local lib python2.7 dist packages tensorflow python ops data flow ops.py line 418 in dequeue self. queue ref self. dtypes name name File usr local lib python2.7 dist packages tensorflow python ops gen data flow ops.py line 863 in queue dequeue timeout ms timeout ms name name File usr local lib python2.7 dist packages tensorflow python framework op def library.py line 747 in apply op op def op def File usr local lib python2.7 dist packages tensorflow python framework ops.py line 2372 in create op original op self. default original op op def op def File usr local lib python2.7 dist packages tensorflow python framework ops.py line 1298 in init self. traceback extract stack You are receiving this because you are subscribed to this thread. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 5115 or mute the thread https github.com notifications unsubscribe auth AABaHEAbq2SwT4ovuH LMYQUf622LxEqks5q2MLEgaJpZM4KdPpU . gunan there seems to be a enqueue dequeue issue in 0.11 that does not appear in HEAD. A candidate for cherry picking Same behavior in 11rc1. To clarify the problem happens if you call enqueue after previous dequeue operation timed out conda create n tf11rc1 cpu python 3.5 source activate tf11rc1 cpu export TF BINARY URL https storage.googleapis.com tensorflow mac cpu tensorflow 0.11.0rc1 py3 none any.whl pip install upgrade TF BINARY URL export CUDA VISIBLE DEVICES python import tensorflow as tf print tf. version q tf.FIFOQueue 2 tf.float32 enq q.enqueue 1. deq q.dequeue timeout option tf.RunOptions timeout in ms 1000 sess tf.Session print sess.run enq works print sess.run deq works print sess.run deq options timeout option times out print sess.run enq doesn t work tf11rc1 cpu bash 3.2 export CUDA VISIBLE DEVICES tf11rc1 cpu bash 3.2 python Python 3.5.2 Continuum Analytics Inc. default Jul 2 2016 17 52 12 GCC 4.2.1 Compatible Apple LLVM 4.2 clang 425.0.28 on darwin Type help copyright credits or license for more information. import tensorflow as tf print tf. version 0.11.0rc1 q tf.FIFOQueue 2 tf.float32 enq q.enqueue 1. deq q.dequeue timeout option tf.RunOptions timeout in ms 1000 sess tf.Session print sess.run enq None print sess.run deq works 1.0 print sess.run deq options timeout option times out W tensorflow core kernels queue base.cc 302 0 fifo queue Skipping cancelled dequeue attempt with queue not closed Traceback most recent call last File Users yaroslav anaconda envs tf11rc1 cpu lib python3.5 site packages tensorflow python client session.py line 972 in do call return fn args File Users yaroslav anaconda envs tf11rc1 cpu lib python3.5 site packages tensorflow python client session.py line 954 in run fn status run metadata File Users yaroslav anaconda envs tf11rc1 cpu lib python3.5 contextlib.py line 66 in exit next self.gen File Users yaroslav anaconda envs tf11rc1 cpu lib python3.5 site packages tensorflow python framework errors.py line 463 in raise exception on not ok status pywrap tensorflow.TF GetCode status tensorflow.python.framework.errors.DeadlineExceededError Timed out waiting for notification During handling of the above exception another exception occurred Traceback most recent call last File stdin line 1 in module File Users yaroslav anaconda envs tf11rc1 cpu lib python3.5 site packages tensorflow python client session.py line 717 in run run metadata ptr File Users yaroslav anaconda envs tf11rc1 cpu lib python3.5 site packages tensorflow python client session.py line 915 in run feed dict string options run metadata File Users yaroslav anaconda envs tf11rc1 cpu lib python3.5 site packages tensorflow python client session.py line 965 in do run target list options run metadata File Users yaroslav anaconda envs tf11rc1 cpu lib python3.5 site packages tensorflow python client session.py line 985 in do call raise type e node def op message tensorflow.python.framework.errors.DeadlineExceededError Timed out waiting for notification print sess.run enq doesn t work Traceback most recent call last File Users yaroslav anaconda envs tf11rc1 cpu lib python3.5 site packages tensorflow python client session.py line 972 in do call return fn args File Users yaroslav anaconda envs tf11rc1 cpu lib python3.5 site packages tensorflow python client session.py line 954 in run fn status run metadata File Users yaroslav anaconda envs tf11rc1 cpu lib python3.5 contextlib.py line 66 in exit next self.gen File Users yaroslav anaconda envs tf11rc1 cpu lib python3.5 site packages tensorflow python framework errors.py line 463 in raise exception on not ok status pywrap tensorflow.TF GetCode status tensorflow.python.framework.errors.CancelledError Enqueue operation was cancelled Node fifo queue enqueue QueueEnqueue Tcomponents DT FLOAT class loc fifo queue timeout ms 1 device job localhost replica 0 task 0 cpu 0 fifo queue fifo queue enqueue component 0 During handling of the above exception another exception occurred Traceback most recent call last File stdin line 1 in module File Users yaroslav anaconda envs tf11rc1 cpu lib python3.5 site packages tensorflow python client session.py line 717 in run run metadata ptr File Users yaroslav anaconda envs tf11rc1 cpu lib python3.5 site packages tensorflow python client session.py line 915 in run feed dict string options run metadata File Users yaroslav anaconda envs tf11rc1 cpu lib python3.5 site packages tensorflow python client session.py line 965 in do run target list options run metadata File Users yaroslav anaconda envs tf11rc1 cpu lib python3.5 site packages tensorflow python client session.py line 985 in do call raise type e node def op message tensorflow.python.framework.errors.CancelledError Enqueue operation was cancelled Node fifo queue enqueue QueueEnqueue Tcomponents DT FLOAT class loc fifo queue timeout ms 1 device job localhost replica 0 task 0 cpu 0 fifo queue fifo queue enqueue component 0 Caused by op fifo queue enqueue defined at File stdin line 1 in module File Users yaroslav anaconda envs tf11rc1 cpu lib python3.5 site packages tensorflow python ops data flow ops.py line 329 in enqueue return gen data flow ops. queue enqueue self. queue ref vals name scope File Users yaroslav anaconda envs tf11rc1 cpu lib python3.5 site packages tensorflow python ops gen data flow ops.py line 982 in queue enqueue name name File Users yaroslav anaconda envs tf11rc1 cpu lib python3.5 site packages tensorflow python framework op def library.py line 756 in apply op op def op def File Users yaroslav anaconda envs tf11rc1 cpu lib python3.5 site packages tensorflow python framework ops.py line 2380 in create op original op self. default original op op def op def File Users yaroslav anaconda envs tf11rc1 cpu lib python3.5 site packages tensorflow python framework ops.py line 1298 in init self. traceback extract stack CancelledError see above for traceback Enqueue operation was cancelled Node fifo queue enqueue QueueEnqueue Tcomponents DT FLOAT class loc fifo queue timeout ms 1 device job localhost replica 0 task 0 cpu 0 fifo queue fifo queue enqueue component 0 On Mon Oct 24 2016 at 3 36 PM drpngx notifications github.com wrote I tried from a recent version and it doesn t happen there. Do you want to upgrade You are receiving this because you are subscribed to this thread. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 5115 issuecomment 255885879 or mute the thread https github.com notifications unsubscribe auth AABaHIyD8MLoO3AHyWZdJooYn2zy hphks5q3TL4gaJpZM4KdPpU . sure if we have a fix let s go ahead and cherrypick that into 0.11 I didn t test after timeout let me see. Yeah also broken at HEAD . Right. The dequeue has a closure which calls Cancel but that is ignored because the queue is not closed . ebrevdo any clue Looks like this https github.com tensorflow tensorflow blob 32d1dcc10e1fdf33dc6742337c6e0869f7b3c557 tensorflow core common runtime direct session.cc L1166 is the code responsible Do we have a fix for this We are still accepting cherry picks for the RC until EOD today. I have some time to look at this now so I ll attempt a quick fix. 
5221,tf.nn.softmax on GPU causes CUDA ERROR ILLEGAL ADDRESS okay on CPU ,CentOS 7 Tensorflow 0.10.0 TITAN X Pascal 367.44 I restore a model previously saved with tf.train.Saver and try to compute probabilities of outputs for a given input batch. Whenever I try to execute tf.nn.softmax on GPU I get an error but same computations work just fine on CPU produces gdb does not give any additional details Surprisingly this error occurred just recently. I have been working with this same codebase for months and before now nothing like this happened but now it is 100 reproducible on my machine with this specific code driver version etc so it is not something that effects everyone but rather pops up randomly. I might have changed operation device placement recently. code unfortunately large piece of code that required certain data failed to produce a minimal reproducing code link https gist.github.com MInner 4faa684a1d0d7eac6fafb9d6b08adfc3 full gdb output without prints link http pastebin.com q7ftuZcp environment Possibly related issues 2117 1450 2810 665 1060 4425 ,Could you help narrowing down the error with cuda memcheck from nvidia To make it more easier to trace probably setting CUDA LAUNCH BLOCK 1 and run with brain gpu sync every op . drpngx memcheck http pastebin.com KYV8rDwf with CUDA LAUNCH BLOCK 1 . Do I have to re build tensorflow from source to set brain gpu sync every op or there s an easier way Woops you re right. It looks like you have to tweak gpu device factory.cc but from the trace it s clear that it s in the eigen ReduceInitKernel in the dense softmax. zheng xq for any additional insight. We don t have access to the data but we have access to the source code. If that s an actual bug in tensorflow it would be great to get to the bottom of this. To sync after each op you ll have to build from source by modifying this line. https github.com tensorflow tensorflow blob master tensorflow core common runtime gpu gpu device factory.cc L34 If you still have this problem with the latest driver then we will need to know which kernel causes this problem. Combining that with memcheck and CUDA LAUNCH BLOCK 1 tends to give the best answer. zheng xq the memcheck above says it s in ReduceInitKernel from the eigen softmax. I wonder if there are diagnostics we could print dimensions etc of buffers vs expected op input here s the tf related part https gist.github.com MInner af316efe081dba2fc219391e12aa24ed of the code above if that is be helpful to understand device placement Would it be possible for you to create a repro that we could try locally If you had a build with debug on then we might be able to see what the intermediate stack frames are. ReductionInitKernel appears to have been passed nullptr offset maybe 300 or so so it s pulled from a struct which has about 300 bytes worth of stuff before the output and that struct instance pointer is nullptr presumably. benoitsteiner if you re interested. MInner Can you try with the release candidate for 0.11 There was a bug in 0.10 that could explain your CUDA ERROR ILLEGAL ADDRESS error and has been fixed since. benoitsteiner Which commit fixed this bug I encountered a same error in tensorflow 1.0.0 and try to reproduce it now. I also still encounter this problem when use large enough models with google seq2seq https github.com google seq2seq during evaluation on same hardware as mentioned in the topic heading. MInner to be clear you are getting CUDA ERROR ILLEGAL ADDRESS drpngx yes and other random errors like shape mismatch in the middle of training people in seq2seq issue tracker referenced above suggest that this might be related to race condition during concurrent training and evaluation in contrib.learn.experiment.train and evaluate . I also sometimes observe weird errors not related to memory allocation if I try running two processes concurrently even on different GPUs and often if I run two processes on same GPU. Please link to issue on seq2seq On Apr 13 2017 7 45 AM Ben Usman notifications github.com wrote drpngx https github.com drpngx yes and other random errors like shape mismatch in the middle of training people in seq2seq issue tracker referenced above suggest that this might be related to race condition during concurrent training and evaluation in contrib.learn.experiment. train and evaluate. You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 5221 issuecomment 293915757 or mute the thread https github.com notifications unsubscribe auth AT Sbbomikr2D 5AN2rwEKTxqtJbBbTKks5rvjTvgaJpZM4Khtms . I meant this https github.com google seq2seq issues 103 issuecomment 293796457 comment. Thanks Closing this one in favor of the other one so that we just have place to track. Have you figured it out 
5223,dynamic RNN fails on mac with unspecified batch Size,tl dr dynamic rnn fails on OS X with unspecified batch size but works fine on Ubuntu 14.04 Python3.4.3. It seems that on OS X the None evaluates poorly then at some point in ops rnn.py 918 this equality fails if const batch size got batch size i.e. None Dimension My best guess is that Dimension is hacked in such a way that any comparison using it returns some fake None which for some reason on OSX python 3.4 means true I tried explicitly if None as a sanity check it evaluates as false . For now I ve modified the offending code to be if const batch size got batch size if const batch size got batch size got batch size or const batch size got batch size const batch size print weird dimensions stuff in ops rnn.py what the shit python continue raise ValueError Batch size is not the same for all the elements in the input. This Environment info Operating System OS X 10.12 Python 3.4.1 Installed version of CUDA and cuDNN please attach the output of ls l path to cuda lib libcud N A CPU version If installed from binary pip package provide 0.11.0rc1 just installed wheel also fails on 0.10.0rc1 0.10.0 though it works on 0.10.0 on Linux If possible provide a minimal reproducible example We usually don t have time to read hundreds of lines of your code . It may work with less but this was a simpl import tensorflow as tf input size 256 length 100 num classes 64 data tf.placeholder tf.float32 None length input size target tf.placeholder tf.float32 None length num classes max length int target.get shape 1 num classes int target.get shape 2 network tf.nn.rnn cell.GRUCell 200 output new state tf.nn.dynamic rnn network data dtype tf.float32 What other attempted solutions have you tried Works fine on my Linux box with or without GPU Logs or other output that would be helpful If logs are large please upload as attachment or provide link . Traceback most recent call last File my directory tensorflow mac fail test.py line 18 in module output new state tf.nn.dynamic rnn network data dtype tf.float32 File Library Frameworks Python.framework Versions 3.4 lib python3.4 site packages tensorflow python ops rnn.py line 836 in dynamic rnn dtype dtype File Library Frameworks Python.framework Versions 3.4 lib python3.4 site packages tensorflow python ops rnn.py line 920 in dynamic rnn loop Batch size is not the same for all the elements in the input. ValueError Batch size is not the same for all the elements in the input. ,It looks like you pretty much know what the problem is. Do you have a idea for a fix you could send we could replace the existing comparison with if const batch size got batch size and not const batch size got batch size None if the dimensions aren t equal but also neither are unspecified but it seems quite awkward. I feel like a cleaner solution might be ensuring that None Dimension evaluates to false instead of None Dimension None and the negation becomes true. Edit If that change was made I would definitely advise putting a comment explaining the purpose because it seems quite weird otherwise . vrv possibly for any additional insight The fix seems reasonable to me perhaps send a PR if you have one. created patch https github.com tensorflow tensorflow compare master...pfaucon patch 1 Thanks Feel free to send the PR. 
5252,multigpu gradient averaging if grad op is IndexedSlices race condition in apply gradients ,If one follows the cifar10 multi gpu train.py https github.com tensorflow tensorflow blob master tensorflow models image cifar10 cifar10 multi gpu train.py example build two towers that share weights feed data independently collect gradients average them apply then and passes gradients to the average gradients https github.com tensorflow tensorflow blob master tensorflow models image cifar10 cifar10 multi gpu train.py L110 function but graph uses tf.gather consequently the resulting gradient op will be IndexedSlices that has many 1 values in indices field from my experience not sure about why this happens so when tensorflow will attempt to apply tensorflow.python.ops.gradients. IndexedSlicesToTensor to convert it to Tensor to apply tf.expand dims on it it will fail on unsorted segment sum https github.com tensorflow tensorflow blob master tensorflow python ops gradients.py L92 because 1 s are indeed not in the indexes range. I have explicitly printed w grad op.indices and they looked like so the average gradients indeed fails on these examples people have been reporting similar problems 1 http stackoverflow.com questions 37074077 fail to average indexedslices on porting ptb word lm py to multi gpu towers 2 http stackoverflow.com questions 39017896 tensorflow how to average several indexedslicesvalue .. 2 3 more around the web . One way of dealing with solution 1 it is just applying gradients consequentially with lower learning rate instead of averaging them first this works great and scales nicely but in my case led to some weird convergence issues eventually leading to nan s popping here and there possibly because of some sort of race conditions while applying gradients gd.apply gradients is probably not very atomic . One possible workaround solution 2 might be to use tf.control dependencies however this solution seems to somewhat significantly degenerate performance because of that blocking GPU load dropped from 90 to 60 . Several fundamental questions 1. What is officially recommended way of dealing with this multigpu IndexedSlices case could not find any people keep asking 2. Does solution 1 indeed might experience race condition or tensorflow handles it somehow and my convergences issues were not related to that and we all can just use solution 1 , mrry who might have better insight ebrevdo could you comment on this UPD after little more testing on relatively large language model it turned out that even though nvidia smi shows lower gpu utilization when I m using blocking solution 2 it scales great on 2 gpus the speedup is 1.83x and performs actually even better 10 faster then non blocking one 1 . MInner having 1s in the IndexedSlices object output of gather sounds like a bug. do you know if gather is being called with 1s for the indices input Automatically closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks I have the same problem when trying to implement a multigpu version model with embedding layer. What is officially recommended way of dealing with this multigpu IndexedSlices case could not find any people keep asking TomorrowIsAnOtherDay hi do you have any idea of average multigpu indexedslices I have the same problem I am wondering if this is solved in new versions of tensorflow or is there any solution for averaging IndexedSlices. yuefengz can maybe shed some light on this. Any ideas want to know the solutions tf.distribute.MirroredStrategy handles aggregating IndexedSlices already afaik. How are you doing multi GPU training 
5285, 0.11 parse single example doesn t play well with read file,minimal reproduction You ll notice that read file works just fine and outputs bytes as expected but then the process will completely lock up and does not respond to SIGINT responding only to SIGKILL or SIGTERM. Not sure what s going on here although I suspect that read file is reading some leading trailing bytes that TFRecordReader strips off. Still it seems like TFRecords should be a valid I O format without requiring a filename queue e.g. for reading in initial vocabulary files and I thought that read file parse single example would be the expected way to do this. Instead I guess I need to use make tensor proto or something Not sure why this is a separate serialization strategy... and completely missing from the docs. At the very least parse single example should fail and not livelock when it gets these strings. EDIT For people wondering what the correct pattern is it appears to be something like ,Let me repro this it looks like the example doesn t work. Which versions did you try 0.11.0rc0 Also. I think this may be related to system encodings for python since when running it on Cloud ML I get a utf8 encoding error... sys.getdefaultencoding is ascii the default . I m on Goobuntu 14.04. Yeah these parse errors have been appearing some of them are correct some aren t I haven t traced this down yet. drpngx What s the status of this bug It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. Nagging Awaiting TensorFlower It has been 14 days with no activityand the awaiting tensorflower label was assigned. Please update the label and or status accordingly. Is that still an issue with the new dataset api It doesn t appear to be a problem anymore I just reran the snippet code above and it now fails fast and with an InvalidArgumentError which is somewhat useful. Side Note tf.parse single example still exists so it doesn t really matter whether or not this is the preferable way of doing things it shouldn t fail this catastrophically. 
5446,Bus error in quantized network on Android,We have a tensorflow image processing network which executes fine on osX iOS and Android. Now that quantization has been pulled into core we wanted to use it to optimize our install size on phones. When we run the quantization process there are as expected small differences in the output on the Mac but nothing that seems unreasonable. Further when the quantized network is run on iOS it gives a warning that zero is outside the quantization range and it has to revert to a slow version to handle convolution padding. However it runs fine and the numbers are plausible. On Android however and running with precisely the same code that worked perfectly with the unquantized version it completely breaks. On one phone it gives outputs on the order of 1000 where we expect values on the order of 1. On another probably the more reasonable phone it just dies following a bus error. This has been true on the latest commits sampled over the last several days. If it s relevant the network was converted from a caffe predecessor using this tool https github.com ethereon caffe tensorflow Regrettably I m unable to upload the source code or network in question., petewarden do you have any thoughts on this or who can diagnose this I just hit a similar Bus error problem with another model so I ll take on debugging this. Hi there I m helping Pete with this bug. As a quick solution you can disable the new codepath call tensorflow meta SetEnabled false this will revert to Eigen. Or even better try building with TENSORFLOW DISABLE META defined petewarden maciekcc was there any resolution on this bug Is it still reproducible and relevant IIRC this bug has been fixed in https github.com tensorflow tensorflow commit 38aec869681e0ac964e33bf44aef57016c5960c5 
5652,cifar10 multi gpu train.py breaks with more than 1 GPU, Environment info Operating System Ubuntu Installed version of CUDA and cuDNN 8.0 and 5 1. The commit hash git rev parse HEAD 3d41cf77d624aeee0482f92121a9300b29db2809 2. The output of bazel version Build label 0.3.2 Build target bazel out local fastbuild bin src main java com google devtools build lib bazel BazelServer deploy.jar Build time Fri Oct 7 17 25 10 2016 1475861110 Build timestamp 1475861110 Build timestamp as int 1475861110 If possible provide a minimal reproducible example We usually don t have time to read hundreds of lines of your code python cifar10 multi gpu train.py num gpus 2 Both cifar10 train.py and cifar10 multi gpu train.py without specifying num gpus so running on a single GPU work. Logs or other output that would be helpful I tensorflow stream executor dso loader.cc 128 successfully opened CUDA library libcublas.so locally I tensorflow stream executor dso loader.cc 128 successfully opened CUDA library libcudnn.so locally I tensorflow stream executor dso loader.cc 128 successfully opened CUDA library libcufft.so locally I tensorflow stream executor dso loader.cc 128 successfully opened CUDA library libcuda.so.1 locally I tensorflow stream executor dso loader.cc 128 successfully opened CUDA library libcurand.so locally Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes. Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes. Traceback most recent call last File cifar10 multi gpu train.py line 280 in module tf.app.run File data github tensorflow python build tensorflow python platform app.py line 43 in run sys.exit main sys.argv 1 flags passthrough File cifar10 multi gpu train.py line 276 in main train File cifar10 multi gpu train.py line 180 in train loss tower loss scope File cifar10 multi gpu train.py line 92 in tower loss loss averages op loss averages.apply losses total loss File data github tensorflow python build tensorflow python training moving averages.py line 391 in apply self. averages var var decay zero debias zero debias File data github tensorflow python build tensorflow python training moving averages.py line 70 in assign moving average update delta zero debias variable value decay File data github tensorflow python build tensorflow python training moving averages.py line 177 in zero debias trainable False File data github tensorflow python build tensorflow python ops variable scope.py line 1024 in get variable custom getter custom getter File data github tensorflow python build tensorflow python ops variable scope.py line 850 in get variable custom getter custom getter File data github tensorflow python build tensorflow python ops variable scope.py line 346 in get variable validate shape validate shape File data github tensorflow python build tensorflow python ops variable scope.py line 331 in true getter caching device caching device validate shape validate shape File data github tensorflow python build tensorflow python ops variable scope.py line 650 in get single variable VarScope name ValueError Variable tower 1 tower 1 conv1 weight loss avg biased does not exist or was not created with tf.get variable . Did you mean to set reuse None in VarScope , shlens Could you take a look Scopes have been fiddled with recently and maybe this file didn t get updated This was fixed in November. 
6132,contrib.learn.Estimator does not work with multiple GPU,Attempting to assign ops to a GPU within model fn passed to an Estimator produces the following error This can be reproduced by running the example in examples learn multiple gpu.py,A quick bump on this am I making fundamentally incorrect assumptions on how this should work I m not eager to replicate the Estimator s functionality which is great overall but it s important for me to get my models working in TF w multiple GPUs. I d be happy to help w a PR if some work is needed to get this functioning. I had also posted on SO here http stackoverflow.com questions 41003989 tensorflow contrib learn estimator multi gpu . Thank you wonderful library. Thanks for filing this issue. We know about the problem and are in the process of fixing it. Specifically and as a stopgap we will allow you do optionally create a Saver yourself so you can control where its ops land. We ll update this thread once that has landed ispirmustafa FYI Thank you martinwicke. I ll keep an eye out. In the meanwhile were I to dig into the source and add allow soft placement True on the session creation is that likely to work around the problem or is there another blocking issue recognizing the can of worms I m opening w that option setting I believe if you did that you should be fine. Hi I get the same errors when using contrib.learn package Regressors where only the GPU memory is allocated but the GPU processing is at zero. I tried allow soft placement True but still get the same errors. Is this fixed yet We re making allow soft placement default in Estimator implementation. That should fix this issue. vvpreetham it s surprising that allow soft placement would not have fixed this. How did you set it Thanks for the quick response Wicke. I have done the following tensorflow v0.12 In the tensorflow lib python3.5 site packages tensorflow contrib learn python learn estimators run config.py Line 241 I have changed the code to self.tf config ConfigProto allow soft placement True .... Then in my files I am using a LinearRegressor as follows config run config.RunConfig m tf.contrib.learn.LinearRegressor model dir model dir config config feature columns wide columns optimizer tf.train.FtrlOptimizer learning rate 0.2 l1 regularization strength 1.0 l2 regularization strength 1.0 Which has a feed function def feed fn df with tf.device gpu 2 categorical cols k tf.SparseTensor indices i 0 for i in range df k .size values df k .values shape df k .size 1 for k in CATEGORICAL COLUMNS label tf.constant df REGRESSOR LABEL COLUMN .values return categorical cols label I get the error InvalidArgumentError see above for traceback Cannot assign a device to node SparseTensor 55 values Could not satisfy explicit device specification device GPU 2 because no supported kernel for GPU devices is available. Node SparseTensor 55 values Const dtype DT STRING value Tensor type string shape 149999 values 6b76ea8a9a6046649c021c2c26f9e2e7 bf9319d59ac94dd396de82601d3ccba0 4028cbe0384137e1013846732b4e009d... device device GPU 2 Should I use the scope for the variable Given that SparseTensor is a variable and not a constant Also I am using the estimator as follows estimator.fit input fn lambda feed fn batch df train steps train steps Also as an update the reason I am trying out the specific device allocation is that when I normally use the program without tf.device then I am seeing a strange behavior where only the Titan X GPU memory is allocated but the GPU processor is 0 used. I have 3 Titan X GPU on the same box vvpreetham I think what you re seeing is expected behavior and unrelated to Estimator. Tensorflow will automatically claim all available memory for all of the GPUs that it sees unless you tell it otherwise. It will do this whether you are using them for your model or not. Take a look at CUDA VISIBLE DEVICES environment setting to specify GPUs to use or you can change the memory settings see using GPUs in the docs ashern I am guessing your comment is specific to my last comment memory allocation . How do I ensure that tensorflow is actually using all the GPU processors I have Does CUDA VISIBLE DEVICE also ensure the GPU processor usage vvpreetham That s a pretty big topic. Take a look here https www.tensorflow.org how tos using gpu and I think you ll find more help if you post your questions to StackOverflow for a wider audience to answer. ashern Thanks. I shall post on StackOverflow meanwhile setting CUDA VISIBLE DEVICES still does not enable my processor usage. I have tried everything on the link you have specified and still no luck. Hence posted here martinwicke bump on my original question so that this thread is not lost The problem seems to be the SparseTensor. Note that the following piece of code works categorical cols k tf.SparseTensor indices i 0 for i in range df k .size values df k .values shape df k .size 1 for k in CATEGORICAL COLUMNS for d in gpu 0 gpu 1 gpu 2 with tf.device d Converts the label column into a constant Tensor. label tf.constant df REGRESSOR LABEL COLUMN .values Returns the feature columns and the label. BUT if I do the following that is assign the SparseTensor to the devices it fails for d in gpu 0 gpu 1 gpu 2 with tf.device d Creates a dictionary mapping from each categorical feature column name k to the values of that column stored in a tf.SparseTensor. categorical cols k tf.SparseTensor indices i 0 for i in range df k .size values df k .values shape df k .size 1 for k in CATEGORICAL COLUMNS Converts the label column into a constant Tensor. label tf.constant df REGRESSOR LABEL COLUMN .values Returns the feature columns and the label. If you re still getting the same error message cannot put string tensor on GPU then you still haven t enabled soft placement or soft placement does not consider colocation constraints vrv do you know . You cannot put string tensors on GPUs so that particular tensor has to live on the CPU. I am still getting the error. I am super certain that soft placement is enabled as I have dry run the code with log.info and also breakpoints. I also add the gpu fraction and soft placement directly in the session The GPU memory fraction works and soft placement works as I have stated for constants . My modified code for session is as follows I setup the session as follows The problem seems to be the tf.SparseTensor I see in the repo that Estimator has been promoted to core from contrib for 1.1 great news. Briefly checking in on this issue will the deployed implementation handle multiple GPU device assignment soft placement Many thanks for the hard work Yes the core estimator uses soft placement and multi GPU training should work. Excellent. Time to put my homespun solution to rest... Many thanks Martin. Appreciated your overview at the summit martinwicke any examples of multi gpu for tf.learn now that it will be on core besides the one provided in the example section The one from example section only does parallel model as opposed to data parallel https github.com tensorflow tensorflow blob master tensorflow examples learn multiple gpu.py Thanks. I don t think creating sparse string Tensors on the GPU will work since the GPU does not support strings. It s best to structure your model so the string to int conversion happens on the CPU and the GPU just processes the dense part of your model. That said the linear regression canned estimator will probably see no benefit from running on the GPU not enough matrix multiplications to offset the data transfer cost . 
6184,wide n deep Tutorial example not working,Windows 10 0.12.0 rc0 Encountered error AttributeError NoneType object has no attribute bucketize when running the example without any modification. Full working and error log as follows WARNING tensorflow The default value of combiner will change from sum to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from sum to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from sum to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from sum to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from sum to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from sum to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from sum to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from sum to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from sum to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from mean to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from mean to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from mean to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from mean to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from mean to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from mean to sqrtn after 2016 11 01. WARNING tensorflow From C Python35 lib site packages tensorflow contrib learn python learn estimators dnn linear combined.py 711 in fit. calling BaseEstimator.fit from tensorflow.contrib.learn.python.learn.estimators.estimator with x is deprecated and will be removed after 2016 12 01. Instructions for updating Estimator is decoupled from Scikit Learn interface by moving into separate class SKCompat. Arguments x y and batch size are only available in the SKCompat class Estimator will only accept input fn. Example conversion est Estimator ... est SKCompat Estimator ... WARNING tensorflow From C Python35 lib site packages tensorflow contrib learn python learn estimators dnn linear combined.py 711 in fit. calling BaseEstimator.fit from tensorflow.contrib.learn.python.learn.estimators.estimator with y is deprecated and will be removed after 2016 12 01. Instructions for updating Estimator is decoupled from Scikit Learn interface by moving into separate class SKCompat. Arguments x y and batch size are only available in the SKCompat class Estimator will only accept input fn. Example conversion est Estimator ... est SKCompat Estimator ... WARNING tensorflow From C Python35 lib site packages tensorflow contrib learn python learn estimators dnn linear combined.py 711 in fit. calling BaseEstimator.fit from tensorflow.contrib.learn.python.learn.estimators.estimator with batch size is deprecated and will be removed after 2016 12 01. Instructions for updating Estimator is decoupled from Scikit Learn interface by moving into separate class SKCompat. Arguments x y and batch size are only available in the SKCompat class Estimator will only accept input fn. Example conversion est Estimator ... est SKCompat Estimator ... WARNING tensorflow Rank of input Tensor 1 should be the same as output rank 2 for column. Will attempt to expand dims. It is highly recommended that you resize your input as this behavior may change. WARNING tensorflow Rank of input Tensor 1 should be the same as output rank 2 for column. Will attempt to expand dims. It is highly recommended that you resize your input as this behavior may change. WARNING tensorflow Rank of input Tensor 1 should be the same as output rank 2 for column. Will attempt to expand dims. It is highly recommended that you resize your input as this behavior may change. WARNING tensorflow Rank of input Tensor 1 should be the same as output rank 2 for column. Will attempt to expand dims. It is highly recommended that you resize your input as this behavior may change. WARNING tensorflow Rank of input Tensor 1 should be the same as output rank 2 for column. Will attempt to expand dims. It is highly recommended that you resize your input as this behavior may change. Traceback most recent call last File wide n deep tutorial.py line 207 in module tf.app.run File C Python35 lib site packages tensorflow python platform app.py line 43 in run sys.exit main sys.argv 1 flags passthrough File wide n deep tutorial.py line 203 in main train and eval File wide n deep tutorial.py line 196 in train and eval m.fit input fn lambda input fn df train steps FLAGS.train steps File C Python35 lib site packages tensorflow contrib learn python learn estimators dnn linear combined.py line 711 in fit max steps max steps File C Python35 lib site packages tensorflow python util deprecation.py line 191 in new func return func args kwargs File C Python35 lib site packages tensorflow contrib learn python learn estimators estimator.py line 355 in fit max steps max steps File C Python35 lib site packages tensorflow contrib learn python learn estimators estimator.py line 699 in train model train ops self. get train ops features labels File C Python35 lib site packages tensorflow contrib learn python learn estimators estimator.py line 1052 in get train ops return self. call model fn features labels model fn lib.ModeKeys.TRAIN File C Python35 lib site packages tensorflow contrib learn python learn estimators estimator.py line 1019 in call model fn params self.params File C Python35 lib site packages tensorflow contrib learn python learn estimators dnn linear combined.py line 504 in dnn linear combined model fn scope scope File C Python35 lib site packages tensorflow contrib layers python layers feature column ops.py line 526 in weighted sum from feature columns transformed tensor transformer.transform column File C Python35 lib site packages tensorflow contrib layers python layers feature column ops.py line 869 in transform feature column.insert transformed feature self. columns to tensors File C Python35 lib site packages tensorflow contrib layers python layers feature column.py line 1489 in insert transformed feature name bucketize File C Python35 lib site packages tensorflow contrib layers python ops bucketization op.py line 48 in bucketize return bucketization op.bucketize input tensor boundaries name name AttributeError NoneType object has no attribute bucketize ,I have traced back the error to bucketization op.py . There is a line of code that says bucketization op loader.load op library resource loader.get path to datafile bucketization op.so I could not find the file bucketization op.so in my TensorFlow installation for Windows 10. Is bucketization op.so supposed to be in the installation directory Thank you. I have a similar issue. My installation is tensorflow version 0.12.0rc1 on Python 3.5.2 Anaconda 4.2.0 64 bit on Windows 10. The function call in the bucketization op.py get path to datafile bucketization op.so just returns bucketization op.so object type str . The file itself does not seem to exist. The code runs fine removing the bucketization and the column crossing. Also I had to make some changes on the input fn definition from dense shape df k .size 1 to just shape df k .size 1 I m having a similar issue. Tensorflow 0.12.0rc1 Python 3.5.2 Windows 10 WARNING tensorflow The default value of combiner will change from sum to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from sum to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from sum to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from sum to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from sum to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from sum to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from sum to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from sum to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from sum to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from sum to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from mean to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from mean to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from mean to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from mean to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from mean to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from mean to sqrtn after 2016 11 01. WARNING tensorflow From C Program Files Python35 lib site packages tensorflow contrib learn python learn estimators dnn linear combined.py 711 in fit. calling BaseEstimator.fit from tensorflow.contrib.learn.python.learn.estimators.estimator with x is deprecated and will be removed after 2016 12 01. Instructions for updating Estimator is decoupled from Scikit Learn interface by moving into separate class SKCompat. Arguments x y and batch size are only available in the SKCompat class Estimator will only accept input fn. Example conversion est Estimator ... est SKCompat Estimator ... WARNING tensorflow From C Program Files Python35 lib site packages tensorflow contrib learn python learn estimators dnn linear combined.py 711 in fit. calling BaseEstimator.fit from tensorflow.contrib.learn.python.learn.estimators.estimator with y is deprecated and will be removed after 2016 12 01. Instructions for updating Estimator is decoupled from Scikit Learn interface by moving into separate class SKCompat. Arguments x y and batch size are only available in the SKCompat class Estimator will only accept input fn. Example conversion est Estimator ... est SKCompat Estimator ... WARNING tensorflow From C Program Files Python35 lib site packages tensorflow contrib learn python learn estimators dnn linear combined.py 711 in fit. calling BaseEstimator.fit from tensorflow.contrib.learn.python.learn.estimators.estimator with batch size is deprecated and will be removed after 2016 12 01. Instructions for updating Estimator is decoupled from Scikit Learn interface by moving into separate class SKCompat. Arguments x y and batch size are only available in the SKCompat class Estimator will only accept input fn. Example conversion est Estimator ... est SKCompat Estimator ... Traceback most recent call last File C Program Files Python35 lib site packages tensorflow python framework tensor shape.py line 578 in merge with self.assert same rank other File C Program Files Python35 lib site packages tensorflow python framework tensor shape.py line 624 in assert same rank Shapes s and s must have the same rank self other ValueError Shapes 0 and must have the same rank On windows load op library doesn t have any action just return None https github.com tensorflow tensorflow blob 5a5a25ea3ebef623e07fb9a46419a9df377a37a5 tensorflow contrib util loader.py I found the message from this url https github.com tensorflow tensorflow commit 45c838623c0df0489e1777af494ee4da0cf4e435 In tf.contrib only load external op libraries on non Windows platforms. This enables tf.contrib to be used on Windows which does not currently support the TensorFlow plugin mechanism. Side question any idea what s causing the warning WARNING tensorflow Rank of input Tensor 1 should be the same as output rank 2 for column. Will attempt to expand dims. It is highly recommended that you resize your input as this behavior may change. Is there a way to prevent this and or can it be ignored For someone who is searching as I was... The tutorial https www.tensorflow.org tutorials wide has two issues at least on windows installation 1. TypeError init got an unexpected keyword argument density shape 2. AttributeError NoneType object has no attribute bucketize Temporary SOLUTION as follows Issue 1 temp. solution Change density shape in following code categorical cols k tf.SparseTensor indices i 0 for i in range df k .size values df k .values density shape df k .size 1 for k in CATEGORICAL COLUMNS To just shape categorical cols k tf.SparseTensor indices i 0 for i in range df k .size values df k .values shape df k .size 1 for k in CATEGORICAL COLUMNS Issue 2 temp. solution Change model type from wide or wide n deep to deep caisq Would be great if someone from responsible persons could comment the problems. Is it just tutorial issue Is it windows installation issue Doe s it mean this is general issue therefore wide n deep model is unusable Thank you for your support As I see it from my attempts it is an windows version issue they do not ship the DLLs for some shared libraries like they do the SOs for the Linux version. Better switch to the Linux version it appears to work just fine. mcnarik I experience your problem 1 as well and I m using linux the parameter name on linux is dense shape and should be changed to shape . I guess they forget to update the tutorial. to be clear the issues with windows is on the point 2 bucketize has no definition the shared objects missing in windows. mcnarik shadofren I believe problem 1 is caused by the v1.0.0 alpha release https github.com tensorflow tensorflow releases tag v1.0.0 alpha in which they also changed the wide and deep tutorial to match the change of SparseTensor s argument from shape to dense shape see 056c0877ad . The wide and deep tutorial code in branch r0.12 https github.com tensorflow tensorflow blob r0.12 tensorflow examples learn wide n deep tutorial.py runs on my machine with Mac OS X 10.11.5 Python 2.7.12 and Tensorflow 0.12.1 worth having a look Will leave problem 2 for those with better understanding. liuchbryan The point 1 is clear but it makes sense to write it here together with solution because official tutorial is wrong and I spend a lot of time just searching for solution. Regarding point 2 . I know it works on different systems Linux Mac OS X and I also test the example you shared. However from my point of view installing a different system is not solution. I would like to use Windows because I know them much better and therefore can focus on neural networks and not on how to install and work with Linux. Since the tool should support Windows I think this is valid expectation. As mentioned by prodipta the issue on windows is in TensorFlow packages classes for windows and should be therefore fixed soon or later . But maybe I m wrong... Anyway I really appriciate work of the developers and love the tool mcnarik Apologies if I did not made clear in my previous comment the entire comment bar from the last line was solely dealing with issue problem 1 you have mentioned. That is you can also run the wide and deep tutorial code which lives in branch r0.12 if you only have access to the older Tensorflow 0.12 which should run without any error. On hindsight we are essentially suggesting the same fix and it is absolutely the right thing to do to suggest that temporary fix on here. I am inclined not to say the official tutorial here defined as wide n deep tutorial.py hosted on GitHub is wrong while it may be too ahead in time to be actually useful it is consistent with Tensorflow s latest version v1.0.0 alpha . On the other hand I think we will agree the code shown in this blog post https www.tensorflow.org tutorials wide and deep might need some verbose warning for the time being and some update on the use of shape dense shape argument keyword in the future. I totally agree your comment on the ability to use Tensorflow on Windows it will be a shame if half the community is denied use simply because some libraries are missing Sorry for getting off tangent will leave the thread for the actual working for issue problem 2. I got the same problem and doing two changes dense shape to shape and wide n deep to deep it worked. However I received a bunch of warnings. Is it due to windows or I need to update my system Is there an update on whether this will be addressed so the wide deep model can be run on Windows The issue still replicates on Windows 10 64bit Anaconda 4.3.8 virtual environment with Python 3.5.3 and TensorFlow 1.0.0. Running with the deep instead of the wide n deep flag does not actually run the model of interest based on tf.contrib.learn.DNNLinearCombinedClassifier . Thx in advance. tomwanzek I followed the build for https github.com tensorflow tensorflow issues 8217 but It can not solve the issue I m also having the same problem...Any updates on this I just pulled down the last stable nightly build for Windows GPU 114 the later builds up to 122 have failed for GPU and the Windows CPU build 122 . I can confirm that the original issue still persists. mrry Thanks for all the hard work on TF Are you in a position to pin down what remains to be done assuming that the now merged PR 8217 was one of the missing building blocks to a resolution tomwanzek I think the changes are fairly modest. There are two options 1. More principled. Convert the Wide Deep related ops to using guschmue s newly added tf.load library support which will require some additions to the CMake build similar to the ones Guenther did for tf.contrib.rnn in 8217. 2. Easier. Convert the Python code in the Wide Deep libraries to use statically generated Python wrappers rather than getting them from the return value of tf.load op library . We ve already done this for several libraries e.g. tf.contrib.tensor forest in 6908 . EDIT Interestingly an internal change is pending to do option 2 so it might be easier to wait a couple days and the HEAD should just work. mrry sounds great Whatever option is more appropriate in light of the TF Roadmap...your call Only speaking for myself here I d rather wait a little longer and have a solution that is robust and consistent beyond just wide deep than have a quick fix which outlives itself soon and implies different cognitive models for implementation of different models. Thx in advance. tomwanzek The change I mentioned is now in and I just sent a pull request https github.com tensorflow tensorflow pull 8808 to cherry pick it into the 1.1 release. Therefore if I m not mistaken you should be able to upgrade to a current nightly build or the next 1.1 release candidate which should be out soon and these ops will now work. Coincidentally I just posted an answer on Stack Overflow that explains the whole sorry story http stackoverflow.com a 43097772 3574081 in case you re interested mrry Sounds great I will give it a shot shortly as soon as they become available. I saw nightly 128 for Windows 10 GPU now references 1.1.0RC0. So I will give it a shot although based on you comment timing it might have to wait for the next stable nightly. In any case thanks a lot to yourself guschmue and everyone who chipped in on the windows load PR Just to confirm I created a conda environment based on Windows GPU Stable Build129 1.1.0rc0 nightly and the primary errors are indeed gone. There are some remaining warnings related to deprecated API usage but overall the wide and wide n deep models are runnable at this point So one more time Cudos to mrry and guschmue . Thanks for confirming this Tom 
6378,WhereOp Race condition between counting the number of true elements and writing them,I have the same problem as issue 4033 and tensorflow models 486 https github.com tensorflow models issues 486 happening to me in my own project with the nightly wheel. It is running fine on CPU the problem only happens on GPU titan x pascal . The code that s causing the problem is this pair idxs tf.where tf.greater equal iou params.thresh iou is a tensor params.thresh is a python float WIth the error message Cuda cuDNN tenserflow. version 0.12.head ,I managed to build a small reproducing example This also fails but commenting out the assignment makes it work. zheng xq do you have any insights on the concurrency issue here. I don t see the problem with the Titan X Maxwell edition on current nightly. On my system a similar error was produced as a result of the GPU running out of memory. I met the same issue turned out I already started a training instance so all my GPU memory are used up. After I killed that instance the issue is gone. hosang does this occur on a more recent version of TensorFlow 0.12 is quite old at this point. It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue Please update the label and or status accordingly. aselle updating to a newer version fixed the problem for me and I didn t encounter the problem again. hosang Maybe you could close the issue then. 
6509,Issue with tf.one hot in 0.12.0 in GPU mode CUDA ERROR ILLEGAL ADDRESS ,I m using a LeNet 5 mnist example from Udacity s course. Link to the source code is below. Training works ok on a CPU config tf.ConfigProto device count GPU 0 but fails in a GPU mode with the following CUDA ERROR ILLEGAL ADDRESS error I c tf jenkins home workspace release win device gpu os windows tensorflow core common runtime gpu gpu device.cc 885 Found device 0 with properties name GeForce GTX 1060 6GB major 6 minor 1 memoryClockRate GHz 1.7845 pciBusID 0000 01 00.0 Total memory 6.00GiB Free memory 5.01GiB I c tf jenkins home workspace release win device gpu os windows tensorflow core common runtime gpu gpu device.cc 906 DMA 0 I c tf jenkins home workspace release win device gpu os windows tensorflow core common runtime gpu gpu device.cc 916 0 Y I c tf jenkins home workspace release win device gpu os windows tensorflow core common runtime gpu gpu device.cc 975 Creating TensorFlow device gpu 0 device 0 name GeForce GTX 1060 6GB pci bus id 0000 01 00.0 Training... E c tf jenkins home workspace release win device gpu os windows tensorflow stream executor cuda cuda event.cc 49 Error polling for event status failed to query event CUDA ERROR ILLEGAL ADDRESS F c tf jenkins home workspace release win device gpu os windows tensorflow core common runtime gpu gpu event mgr.cc 198 Unexpected Event status 1 I have reproduced same error on two setups. Environment 1 Home PC windows 10 latest anaconda 4.2.0 python 3.5 cuda 8 cudnn 5.1 for win10 tensorflow 0.12.0 gpu https storage.googleapis.com tensorflow windows gpu tensorflow gpu 0.12.0 cp35 cp35m win amd64.whl GeForce GTX 1060 6Gb Environment 2 Work PC windows 7 latest anaconda 4.2.0 python 3.5 cuda 8 cudnn 5.1 for win7 tensorflow 0.12.0 gpu https storage.googleapis.com tensorflow windows gpu tensorflow gpu 0.12.0 cp35 cp35m win amd64.whl GeForce GTX 660 3Gb I m sharing two scripts with minor changes that allow a workaround https drive.google.com open id 0B6jkkqMOGy5cNHh3TVpxU283Ykk Main difference Script from example that crashes LabLenetBad.py uses raw mnist label data with the tf.one hot call. The workaround LabLenetGood.py reads mnist data with one hot True flag and does not use tf.one hot call on the Y placeholder. I think that tf.one hot does not work properly on the gpu.,https github.com vlfeat matconvnet issues 65 In this issue I see that some people were able to go around the problem by changing their drivers. Can it be the same issue here gunan I ve updated Nvidia s driver to the most recent version 376.33 on my Work PC. The issue still occurs. I d like to note that sometimes it triggers only CUDA ERROR ILLEGAL ADDRESS error. But sometimes it triggers CUDA ERROR ILLEGAL ADDRESS some cudnn errors. E c tf jenkins home workspace release win device gpu os windows tensorflow stre am executor cuda cuda event.cc 49 Error polling for event status failed to que ry event CUDA ERROR ILLEGAL ADDRESS F c tf jenkins home workspace release win device gpu os windows tensorflow core common runtime gpu gpu event mgr.cc 198 Unexpected Event status 1 E c tf jenkins home workspace release win device gpu os windows tensorflow stre am executor cuda cuda dnn.cc 385 could not create cudnn handle CUDNN STATUS IN TERNAL ERROR E c tf jenkins home workspace release win device gpu os windows tensorflow stre am executor cuda cuda dnn.cc 352 could not destroy cudnn handle CUDNN STATUS B AD PARAM F c tf jenkins home workspace release win device gpu os windows tensorflow core kernels conv ops.cc 532 Check failed stream parent GetConvolveAlgorithms algorithms P.S. I ve written a similar comment already but it has strangely disappeared from the topic. I know there is a problem with one hot T uint8 that results in the CUDA ERROR ILLEGAL ADDRESS. Looked at this in the past but found nothing obvious in the source. Let me attach a debugger and see if we find this. This happens only for T uint8 int32 will work fine. Thanks for sharing I had the same problem and when I replace the tf.one hot by the following hand written implementation it finally works. instead of one hot y tf.one hot y 43 num labels 43 sparse labels tf.reshape y 1 1 derived size tf.shape sparse labels 0 indices tf.reshape tf.range 0 derived size 1 1 1 concated tf.concat 1 indices sparse labels outshape tf.concat 0 tf.reshape derived size 1 tf.reshape num labels 1 one hot y tf.sparse to dense concated outshape 1.0 0.0 ebrevdo any idea what the problem is here a simple repro case attached only happens on windows gpu . I m think this showed only for uint8 in the past but now I see it also for int32. import tensorflow as tf import numpy as np data np.zeros shape 3 5 dtype int32 input tf.placeholder tf.int32 data.shape name input one hot op tf.one hot input depth 3 dtype tf.float32 init tf.initialize all variables with tf.Session as sess sess.run init print sess.run one hot op feed dict input data CC mrry for windows zheng xq for CUDA in case error message looks familiar Could be a bug in eigens generator API cuda impl that only shows in Windows. Do you get the same issue with tf.reverse sequence tf.reverse sequence works ok at least tensorflow python kernel tests reverse sequence op test.py does pass. tensorflow python kernel tests diag op test.py fails. I can collect some info with nvidia debugger ... takes me a little need to recompile with enough symbol info. Merging duplicate issues. weixsong aclaussen1 buaahsh Quoting some information from a duplicate buaahsh Here is my code with tf.Session as sess with tf.device cpu 0 x tf.ones shape 3 3 x diag tf.diag part x x diag matrix tf.matrix diag x diag print sess.run x diag matrix It works ok on a CPU but fails in a GPU mode with the following CUDA ERROR ILLEGAL ADDRESS error I c tf jenkins home workspace release win device gpu os windows tensorflow stream executor dso loader.cc 128 successfully opened CUDA library cublas64 80.dll locally I c tf jenkins home workspace release win device gpu os windows tensorflow stream executor dso loader.cc 128 successfully opened CUDA library cudnn64 5.dll locally I c tf jenkins home workspace release win device gpu os windows tensorflow stream executor dso loader.cc 128 successfully opened CUDA library cufft64 80.dll locally I c tf jenkins home workspace release win device gpu os windows tensorflow stream executor dso loader.cc 128 successfully opened CUDA library nvcuda.dll locally I c tf jenkins home workspace release win device gpu os windows tensorflow stream executor dso loader.cc 128 successfully opened CUDA library curand64 80.dll locally I c tf jenkins home workspace release win device gpu os windows tensorflow core common runtime gpu gpu device.cc 885 Found device 0 with properties name Tesla K40m major 3 minor 5 memoryClockRate GHz 0.745 pciBusID 0000 27 00.0 Total memory 11.16GiB Free memory 11.09GiB I c tf jenkins home workspace release win device gpu os windows tensorflow core common runtime gpu gpu device.cc 906 DMA 0 I c tf jenkins home workspace release win device gpu os windows tensorflow core common runtime gpu gpu device.cc 916 0 Y I c tf jenkins home workspace release win device gpu os windows tensorflow core common runtime gpu gpu device.cc 975 Creating TensorFlow device gpu 0 device 0 name Tesla K40m pci bus id 0000 27 00.0 E c tf jenkins home workspace release win device gpu os windows tensorflow core common runtime gpu gpu device.cc 586 Could not identify NUMA node of job localhost replica 0 task 0 gpu 0 defaulting to 0. Your kernel may not have been built with NUMA support. E c tf jenkins home workspace release win device gpu os windows tensorflow stream executor cuda cuda event.cc 49 Error polling for event status failed to query event CUDA ERROR ILLEGAL ADDRESS F c tf jenkins home workspace release win device gpu os windows tensorflow core common runtime gpu gpu event mgr.cc 198 Unexpected Event status 1 I have tried x tf.ones shape 3 3 x diag tf.diag part x and x diag matrix tf.matrix diag 1. 1. 1. Both work ok in a GPU mode. Maybe the tensor couldn t be input of tf.matrix diag in a Windows GPU mode not funny after recompiling the cuda kernels with G one hot works now. The diag op still dies CUmodule 1c133fcdc00 4 const operator Line 40 CUDA CUmodule 1c133fcdc00 3 const coeff Line 130 CUDA CUmodule 1c133fcdc00 2 evalScalar Line 136 CUDA CUmodule 1c133fcdc00 1 run Line 209 CUDA CUmodule 1c133fcdc00 0 EigenMetaKernel TensorEvaluator TensorAssignOp TensorMap Tensor double int 2 int 1 int64 int 16 MakePointer TensorGeneratorOp MatrixDiagPartGenerator double TensorMap Tensor double int 2 int 1 int64 int 16 MakePointer const const const GpuDevice int64 Line 244 CUDA I ll spend some more time on this later today. Derek looks like a Windows cuda eigen issue. Sergey can you run the job with cuda memcheck You may have to recompile c dbg. Not sure. I m facing the same issue I ran the cuda memcheck on the job and I see a lot of null pointer accesses from EigenMetaKernel. error log.txt https github.com tensorflow tensorflow files 695756 error log.txt Wondering if anyone is looking into this. An update would be good. I m looking at but have not had luck yet. My biggest issue is that when I compile the kernels with full debug info my repro case doesn t fail anymore. But I found that the one hot tests in tensorflow python kernel tests one hot op test.py.py can still repro a crash and before it crashes the results of tests are already wrong. When it crashes the arguments in EigenMetaKernel EigenMetaKernelEval are wrong like step size is some very high number. I ll spend some more time on it today and might need help if I don t find something. rmlarsen benoitsteiner Adding for possible issues in Eigen. A small update here I ve managed to reproduce the failures but I ll have to rely on people more experienced in Eigen CUDA MSVC to figure out why this op is being compiled to incorrect code on Windows. In the mean time I ve sent out 6822 which disables the broken GPU kernels for tf.one hot and some tf.matrix diag and tf.matrix diag part which seem to have the same failure mode and both also use the generator interface while we work on a fix. Hi guys what s the status on this I ve encountered the same bug on Windows GPU with a GTX Titan Black. When I replaced target one hot tf.onehot targets vocab size targets dtype tf.float32 with the workaround by name name it worked just fine. here s the function wrapper i used for the workaround my targets and vocab size targets are both tf.int32 Tensors. I m currently on r0.12 pip installation with cuda 8.0 and CuDNN 5.1 Pyrestone We disabled the broken GPU kernel for tf.one hot in the Windows build so if you upgrade to 1.0.0rc0 you should no longer need the patch. mrry Then the problems are now gone I am using win10 64bit tf gpu 0.12.1 with cuda 8.0 and CuDNN 5.1 I found that this problem InternalError Blas SGEMM launch failed remains even after upgrading tf version to 1.0.0.rc1. I encountered similar issue with 0.12.1 and tried suggested get arounds and found the results are strange. Here https github.com jaejun yoo Three ways to avoid tf.one hot function you can find three get arounds I have tried to avoid using tf.one hot . I found that the substitutes worked fine give the same output with one hot but the final results came out from the script were somewhat strange or wrong... I doubt that this is due to the other issue cuz I have only changed the part for one hot function in the code which worked fine before it. Maybe my fault somewhere ... I have the same issue with tf.one hot . Here is my settings Windows 10 NVIDIA GeForce GTX 1070 Cuda 8.0 CuDnn 5.1 TensorFlow 0.12 Error Error polling for event status failed to query event CUDA ERROR ILLEGAL ADDRESS Kernel died The problem is resolved in 1.0.0 rc0 1 or 2.. Please upgrade and try again. On Wed Feb 8 2017 at 9 23 PM smasoudn notifications github.com wrote I have the same issue with tf.one hot . Here is my settings Windows 10 NVIDIA GeForce GTX 1070 Cudda 8.0 CuDnn 5.1 TensorFlow 0.12 Error Error polling for event status failed to query event CUDA ERROR ILLEGAL ADDRESS Kernel died You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 6509 issuecomment 278550314 or mute the thread https github.com notifications unsubscribe auth AHlCOdbcNeg2bUAbLAbdDZ9D51 oEuGOks5raqK0gaJpZM4LV8wy . tested rc2 windows 10 gtx 1060 cuda 8 cudnn 5.1 The kernel diag op test fails I c tf jenkins home workspace release win device gpu os windows tensorflow core common runtime gpu gpu device.cc 975 Creating TensorFlow device gpu 0 device 0 name GeForce GTX 1060 3GB pci bus id 0000 03 00.0 E c tf jenkins home workspace release win device gpu os windows tensorflow stream executor cuda cuda event.cc 49 Error polling for event status failed to query event CUDA ERROR ILLEGAL ADDRESS F c tf jenkins home workspace release win device gpu os windows tensorflow core common runtime gpu gpu event mgr.cc 198 Unexpected Event status 1 I am still getting this error in some code I am running. Doesnt happen with the cpu version E c tf jenkins home workspace release win device gpu os windows tensorflow stream executor cuda cuda dnn.cc 397 could not create cudnn handle CUDNN STATUS NOT INITIALIZED E c tf jenkins home workspace release win device gpu os windows tensorflow stream executor cuda cuda dnn.cc 404 error retrieving driver version Permission denied could not open driver version path for reading proc driver nvidia version E c tf jenkins home workspace release win device gpu os windows tensorflow stream executor cuda cuda dnn.cc 364 could not destroy cudnn handle CUDNN STATUS BAD PARAM F c tf jenkins home workspace release win device gpu os windows tensorflow core kernels conv ops.cc 605 Check failed stream parent GetConvolveAlgorithms algorithms 6822 explicitly disables diag op test because of failures when using GPU. We are working on debugging those problems. So is there some official statement about what GPUs and versions of CUDA and cudnn work on windows 10 and for all platforms for that matter It is just Pascal that has the issue It would be helpful if there were some specific information about what features work and don t work rather than run code and get random errors. While I know that certain tests fail I am not familiar enough with tensor flow yet to fully understand what that means for what features of the api won t work. Thanks gunan I tested rc2 and the error message was gone which didn t in rc1 . Thank you for your comment I come up with the same issue. It can be solved by the solution provided by name name Here is my computer info Win 8.1 64bit GPU GT 750M 4G GK107 CUDA 8.0.60 cuDNN 5.1 tensorflow gpu 0.12.1 I have tried tensorflow gpu 1.0.0 not sure whether rc1 or 2 Install using pip install tensorflow gpu It actually make things worse the program will get error in both situations. 1. with the original code 2. replace the tf.one hot Here is the info I have got in tensorflow gpu 1.0.0 with tf.one hot The info I have got in tensorflow gpu 0.12.1 with tf.one hot is MikeZhu92 The error you re seeing seems to be from one of the breaking function signature changes that were made before the 1.0 release. Fortunately the solution to that problem is simple. Replace the following line in your code ...with the following line that names the arguments 
6586,Crash in Jupyter Notebook with conv2d stream parent GetConvolveAlgorithms algorithms , What related GitHub issues or StackOverflow threads have you found by searching the web for your problem I have googled for the issue on several fronts and found nothing close since the Windows release. Environment info Operating System Windows 7 64bit. Intel i3 6100 64 bit 8 gb DDR4 nVidia GTX 750 ti 2gb nVidia GPU driver 369.30 Installed version of CUDA and cuDNN please attach the output of ls l path to cuda lib libcud CUDA v8.0 cuDNN 5.1 jupyter notebook messages when importing tensorflow successfully loaded cublas64 80.dll cudnn64 5.dll cufft64 80.dll nvcuda.dll curand64 80.dll Found device 0 with properties name GeForce GTX 750 Ti major 5 minor 0 memoryClockRate GHz 1.1105 pciBusID 0000 01 00.0 Total memory 2.00GiB Free memory 1.65GiB DMA 0 0 Y Creating TensorFlow device ... If installed from binary pip package provide 1. A link to the pip package you installed I used the default pip install tensorflow as described on their website. 2. The output from python c import tensorflow print tensorflow. version . 0.12.0 rc0 If possible provide a minimal reproducible example We usually don t have time to read hundreds of lines of your code I built and tested this jupyter notebook WITHOUT gpu support. It already builds and runs correctly as in have trained and optimized a network . Now I m trying to get it to work with gpu to speed it up. Here s the step in the notebook that causes the issue. Prior to this tensorFlow is imported successfully as tf. This is the first session it s running. the training operation uses conv2d. run Training with tf.Session as sess sess.run tf.global variables initializer num examples len X train print Training... print for i in range EPOCHS X train y train shuffle X train y train for offset in range 0 num examples BATCH SIZE end offset BATCH SIZE batch x batch y X train offset end y train offset end sess.run training operation feed dict x batch x y batch y validation accuracy evaluate X validation y validation print EPOCH ... .format i 1 print Validation Accuracy .3f .format validation accuracy print saver.save sess . lenetTrafficSign print Model saved What other attempted solutions have you tried I saw a similar issue in a pre windows version that suggested it had something to do with memory. I tried adding the following before the tensorflow session config tf.ConfigProto config.gpu options.per process gpu memory fraction 0.4 with tf.Session as sess sess.run tf.global variables initializer It did not resolve the issue. Logs or other output that would be helpful If logs are large please upload as attachment or provide link . The error is as follows when trying to run the session image https cloud.githubusercontent.com assets 7866171 21579570 f25e4a0c cf7d 11e6 8136 1f8b681e3dcd.png The plain text at the last three lines are cuda event.cc 49 Error polling for event status failed to query event CUDA ERROR LAUNCH FAILED gpu event mgr.cc 190 Unexpected Event Status 1 cuda dnn.cc 305 could not create cudnn handle CUDNN STATUS INTERNAL ERROR cuda dnn.cc 352 could not destroy cudnn handle CUDNN STATUS BAD PARAM conv ops.cc 532 Check failed stream parent GetConvolveAlgorithms algorithms A few seconds after the script crashes the screen goes black and the video driver resets. ,Can you provide a reproducible example ie the model This error can happen when you give 0 dimensions to conv2d I m sure it s not the issue. The code works fine on Windows using CPU. Since I ve posted this I ve installed Ubuntu and the same code with no changes runs just fine on tensorflow with GPU support. I have the exact same problem on a MacBook using its internal GPU macOS 10.12 Xcode 8.2 overridden by Command Line Tools 7.3.1 . I have CUDA 8.0 cuDNN 5.1 and Tensorflow 0.12.1 installed via pip install tensorflow gpu . The first error is either CUDNN STATUS INTERNAL ERROR or CUDNN STATUS NOT INITIALIZED . In the latter case an additional message error retrieving driver version Invalid argument expected d. d or d. d. d form for driver version got is output. Tensorflow is imported normally but Python crashes when I run a session that uses convolutional neural networks CNN . Additionally I tried compiling Tensorflow from the source but the same error persists. I also tried uninstalling CUDA 8.0 and installing CUDA 7.5 and reinstalled cuDNN and Tensorflow. The problem persists. This is very annoying. mrry this seems to be Windows specific. Any ideas Can you try upgrading TensorFlow to the latest release version 0.12.1 and upgrading your drivers to the latest version 376.33 as far as I can tell then see if the problem persists ymfa It sounds like you might be experiencing a different problem with the Mac OS GPU support. Can you please open a separate issue Is it possible this is the same issue as 6509 This seems to be related to the issue as 6509 as gunan mentioned. 1 I had the same problem then I changed the tf.one hot to a custom solution as mentioned by name name in 6509 the problem went away. May be it might work for this. You can also upgrade your TF version to 1.0.0rc1 to resolve the problem. It s tied to the tf.one hot issue. It s tied to the tf.one hot issue. danbergeland did you solve the problem ricardobnjunior I believe it was resolved with release version 1.0. I no longer have this setup to verify as this was 2 years ago. 
6687,Cannot run a distributed training example with tensorflow v0.12.1,Hi I was trying to run a distributed tensorflow example https github.com tensorflow tensorflow blob master tensorflow tools dist test python mnist replica.py in the official repository with v0.12.1 current latest release . I can run asynchronous version without problems but when I turned on sync replicas tag some errors occurred. Please check the following logs in details This example code can be ran successfully with v0.12.0 so I guess there might be some modification from 0.12.0 to 0.12.1 Could someone check if that s the case Thanks. What related GitHub issues or StackOverflow threads have you found by searching the web for your problem I haven t found others reported this issue as tensorflow v0.12.1 just released Environment info Operating System Ubuntu 14.04 Installed version of CUDA and cuDNN CUDA 7.5 cuDNN 5.1 If installed from source provide 1. The commit hash git rev parse HEAD 4d924e796368163eff11a8151e8505715345f58d Release 0.12.1 2. The output of bazel version If possible provide a minimal reproducible example We usually don t have time to read hundreds of lines of your code I simply used this example mnist replica.py https github.com tensorflow tensorflow blob master tensorflow tools dist test python mnist replica.py in tensorflow repository. Logs or other output that would be helpful First I launched the parameter server and then launched the worker with sync replicas True tag but I got some errors here ,Hm can t reproduce...any idea what could be different about your setup Here s what I tried at the end it finishes with Thanks for your quick reply. Earlier I used the 0.12.1 version with gpu support which was built by myself since my cuda version is 7.5 . To make the situation simple I tried to use the pre built CPU version by the following instructions and then ran Unfortunately the same error occurred. However when I did the same things on another machine it turns out to behave normal... Now I have no ideas why it failed on a specific machine... I also found that someone faced similar errors https github.com tensorflow tensorflow issues 3208 but under different situations. What do you think of the possible reasons that cause the errors Thanks. sherrym there seems to be some issue that turns up in a variety of circumstances e.g. here and 3208 which results in this unhelpful error message. Any thoughts on making it easier to debug My reading of the error message is that self. replica id was None so the code executed array ops.reshape None ... which gives this error. jmchen g do you have any idea why replica id would be none in training sync replicas optimizer.py 751 Generally I think this would be more human readable if None checking occurred few levels up at op def lib.apply op ie here https github.com tensorflow tensorflow blob eb56a8af24695bf8258addf28b0c53fbabff72e6 tensorflow python framework op def library.py L289 . Since None is a Python only thing this could be a check for None at the top level of apply op which prints which value was none and which op. I m facing the same issue when trying to use the same example from here https github.com tensorflow tensorflow blob master tensorflow tools dist test python mnist replica.py . Asynchronous works but setting sync replicas to True results in the same error like this I am running in CPU only mode on Ubuntu 16.04 with tensorflow 0.12.1 and Python 2.7.12. I m facing the same issue with Python3 as well as on Ubuntu 14.04. Looks like in v 1.0 this issue was fixed Closing this issue. Having this issue in 1.3.0... 
6702,Deadlock when decoding TFRecords,I am storing my training examples as variable length TFRecords using the following function for encoding The data stored here is genomic but the details shouldn t matter. When I m training I use the following function to decode the TFRecords During training I ve been getting deadlocks with triple digit CPU loads during training running on a six core i7 and I ve isolated the problem to the above decoding function. A simple Tensorflow program like the following will reproduce the deadlock Now setting intra op threads 1 and inter op threads 1 will prevent the small script from deadlocking. However even when restricting the thread pool I have run into deadlocks when using these TFRecords in long running training sessions so I suspect there is a deeper issue. Environment info Operating System Ubuntu 14.04 Installed version of CUDA and cuDNN CUDA 8.0 cuDNN 5.1.5 If installed from binary pip package provide https storage.googleapis.com tensorflow linux gpu tensorflow gpu 0.12.1 cp27 none linux x86 64.whl If possible provide a minimal reproducible example We usually don t have time to read hundreds of lines of your code What other attempted solutions have you tried Logs or other output that would be helpful If logs are large please upload as attachment or provide link . ,Can you give a more complete reproducible example I tried running your code on some synthetic data and couldn t reproduce. In particular the function read single cossmo example executes to completion. Do you mean that the problem happens on session.run on tensors created in that function Also deadlock scenarios are typically low CPU usage you end up with every thread waiting on a mutex and those don t take any CPU. Some possibilities 1. Parse example is inefficient and if you wait long enough it ll produce the answer. One way to tease out inefficiencies is to use google profiling tools to get CPU profile like is done in https github.com tensorflow tensorflow issues 6116 2. There s some bug in parse example which causes things to go into an infinite loop. In this case it would be useful to get the example which causes this condition. Also perhaps you can gdb attach p pid to the hanging process and do bt to see which function it is hanging in. If you compile with c dbg it ll give you line numbers as well although it can make everything 10x slower and harder to reproduce Thanks yaroslavvb the behaviour I m seeing is kind of subtle so hopefully I can describe it a little better. The graph creation works fine the deadlock happens in session.run when running the graph. By deadlock I mean that CPU load gets extremely high such that the machine becomes unresponsive. This is quite certainly an issue with multi threading. If I don t supply a ConfigProto to session.run i.e. using the default parallelism settings then I run into the deadlock after very iterations of the loop. If I set the script above should run forever without a problem. However my problem is that even when I restricted parallelism executing a long running training script would still encounter the CPU load explosion situation which would crash my machine. I m not sure how I could create a standalone reproducible example of this issue so I m trying to reduce the issue to the root cause and it seems to me that the fact that decoding the TFRecords fails with default parallelism settings is the original problem here. I m aware that tf.parse single sequence example is inefficient but there is no equivalent to tf.parse example for sequence examples. I will use the Google profiling tools to generate profiles in both the multi threaded and single threaded case. Hopefully this will give some additional clues. Could it be that there s a bad tfrecords example which causes infinite loop If you use 1 thread it takes longer to get to that example so that explains the extra delay. You can fix seeds to help with determinism and print out example keys to see which example could be the problem I set a seed and printed every tf record key but the it always hangs at a different example. I did manage to run the CPU profiler though and created two profiles for the singled threaded configuration and the hanging multi threaded configuration. Single threaded https drive.google.com open id 0B AiZCOgaorxSXRaaUl3SGJCYUk Multi threaded hanging https drive.google.com open id 0B AiZCOgaorxWlVxaUFnS1YzWWs You can see that the hanging program spends all its time in sched yield in the Eigen thread pool. Hm from the profile it seems to get stuck in NonBlockingThreadPoolTempl WorkerLoop sched yield .... not really familiar with that part of the code but this vaguely seems like a bug in parse single sequence example blame says https github.com tensorflow tensorflow blame a4b9b3ad4d602621f2a81fe86d11a531aaa4701d tensorflow core ops parsing ops.cc vrv put that code in but it maybe misattribution Per last comment I m assigning to vrv to take this out of triage queue. I don t really know what parse single sequence example is I think the original author was ebrevdo so assigning to him. That op does not use multiple threads. Could you try running with config asan to see if there are any associated memory issues If you want me to try to debug you ll have to send me enough input data so i can reproduce by email. You can send it to my username the domain is either gmail.com or google.com. On Tue Jan 10 2017 at 4 58 PM Vijay Vasudevan notifications github.com wrote I don t really know what parse single sequence example is I think the original author was ebrevdo https github.com ebrevdo so assigning to him. You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 6702 issuecomment 271746489 or mute the thread https github.com notifications unsubscribe auth ABtimweFQ3p65c426AabmTtJ0Dg8oRGGks5rRCkrgaJpZM4LdLzp . ebrevdo Emailed you with some scripts that should hopefully reproduce the problem ebrevdo Have you been able to gain any insights ebrevdo yaroslavvb After additional investigation I believe that this problem is OS specific. I had a suspicion that the way the kernel queues tasks was related to the problem after I observed that the machine load would get extremely high e.g. 1000 on a six core machine but CPU memory and disk I O would all remain low. So I tried both my test script as well as my regular training task on Ubuntu 16.04 from 14.04 before and I haven t encountered the issue since. nice investigation Can you share more details for posterity IE what commands did you to use to query machine load how is that different from CPU I mean load average as reported by e.g. top or uptime and many other tools. For example here is an article that explains load and how it is different from CPU utilization http www.linuxjournal.com article 9001. You can find people reporting similar problems by googling high load average low cpu . Closing since this seems fixed by upgrading the OS. 
6780,crash when run distributed training,When I run distributed training on tensorflow 0.12 everything ok at first loss and global step was printed. But after thoudsands of steps following errors appear tensorflow.python.framework.errors impl.UnavailableError created 1484093728.844289839 description EOF file external grpc src core lib iomgr tcp posix.c file line 235 grpc status 14 and tensorflow.python.framework.errors impl.InvalidArgumentError Cannot assign a device to node save RestoreV2 26 Could not satisfy explicit device specification and E tensorflow core distributed runtime master session.cc 1372 Cleanup partition error Unavailable,Can you give complete backtrace I can t see what device it was trying to assign to. Also googling error code shows that this code can be returned when connection can t be established. Currently if connection is dropped TensorFlow session can become invalid and process needs to be restarted. Not sure if anything can be done about error messages mrry yaroslavvb Here s the complete backtrace So it looks like the crash happens in Estimator. This kind of UnavailableError can happen if your network connection is interrupted but a higher level framework should detect these and recreate session rather than crashing everything. cc ispirmustafa since he s the last person to modify this code i have this problem too but it seems that just restart it with supervisor is a trick solution So the issue is that network conditions can interrupt the connection which results in UnavailableError. Supervisor doesn t handle this and session may become invalid so you need to restart session yourself or just restart the whole program . MonitoredSession MonitoredTraining session improve on Supervisor by adding retries in presence of UnavailableError On Sat Jan 14 2017 at 12 05 AM Eric Yue notifications github.com wrote i have this problem too but it seems that just restart it with supervisor is a trick solution You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 6780 issuecomment 272609422 or mute the thread https github.com notifications unsubscribe auth AABaHBSlw3BqwbPpFKDg1T9025e HLEZks5rSIHGgaJpZM4LgItj . buptjz does restarting Supervisor moving to MonitoredTrainingSession solve the issue Agreed I ll add recoverable support to UnavailableError. Not sure if it s related to this issue but I observe UnavailableError with MonitoredTrainingSession if Parameter Server is restarted 7767 Sending out a fix for this right now. Fixed. help How to fix this daunting problem 
7051,Bug tfdbg multi gpu gives ValueError Duplicate node name n 0 ,Hello tensorflow team I have been starting to use your tensorflow debugger but have run into the issue that when I try and use it on a multi gpu model I get ValueError Duplicate node name n 0 . Inspecting things closer I saw that the issue originated from the run metadata whose partition graphs have many Send and HostRecv ops with names like n 0 . These ops are replicated with identical names across my towers which is what is causing the issue. Looking through the tensorflow code I believe I tracked where this name is set down to graph partition.cc 195 https github.com tensorflow tensorflow blob master tensorflow core graph graph partition.cc L195 where the edge s source name is used as the prefix n . Unfortunately I have not been able to figure out why the source s name is only n but that seems to be the root of the issue here. I should add that I never set any tensor name to n anywhere in my own code. Plus I see certain tests in your codebase rely on names such as n 0 which indicates to me the name is being set somewhere internally in the tensorflow code. Any help you can provide would be much appreciated What related GitHub issues or StackOverflow threads have you found by searching the web for your problem I didn t find any related issues. Environment info Operating System Ubuntu 14.04.5 LTS running in a singularity http singularity.lbl.gov container on a CentOS 6.7 host . Installed version of CUDA and cuDNN I am using CUDA 8.0 with NVIDIA driver 367.48 and cuDNN v5.1 . please attach the output of ls l path to cuda lib libcud 1. A link to the pip package you installed I installed tensorflow using pip install tensorflow gpu 0.12.1 2. The output from python c import tensorflow print tensorflow. version . What other attempted solutions have you tried The single GPU case works fine. Logs or other output that would be helpful Here is the dump of some of the problematic nodes. End of backtrace at crash point , raphtown a small code repro would be nice if you have one. caisq any idea Hello drpngx and thank you for your swift response. I can try and make a minimal code reproduction if you think it necessary but it may take a bit for me to do so. My hunch is that to fix this bug we just need to figure out where these nodes with names n are coming from in the library code. raphtown Thanks for reporting this bug. As drpngx said a short reproduction code will be very helpful. Even if the code can t show the error without a multiple GPU setup we may be able to suggest a quick workaround. FYI this bug about incompatibility between tfdbg and multiple GPUs should have been fixed by the commit above. Please let me know if you see any remaining issues. caisq aselle If you run transformer encoder model from tensor2tensor repo https github.com tensorflow tensor2tensor sentiment analysis with tfdbg enabled it will report multiple devices with nodes named n 78 but device name is not specified My node has 4 v100 gpus. The script I use attached here Hello raphtown did you solve this problem I meet with the same problem as you and I didn t find any useful information about this after looking through this page I wonder did you solve this problem and what did you do Hello spacegoing did you solve this problem I meet with the same problem as you and I didn t find any useful information about this after looking through this page I wonder did you solve this problem and what did you do Vamix Sorry I didn t. I will let you know if I did D 
7108,Tensorflow freezes on iOS during Session Run,Tensorflow hangs on iOS during Session Run. I have a deep LSTM model that requires running session.run many times. The program occasionally hangs after running a few sessions without consuming any cpu. Tensorflow seems to get stuck at DirectSession WaitForNotification. What related GitHub issues or StackOverflow threads have you found by searching the web for your problem https github.com tensorflow tensorflow issues 2121 https github.com tensorflow tensorflow issues 2788 Environment info Operating System iOS git rev parse HEAD e60e72435f0dfebe6424ab4c525523486006d47a Build label 0.2.3 Build target bazel out local fastbuild bin src main java com google devtools build lib bazel BazelServer deploy.jar Build time Tue May 17 14 22 21 2016 1463494941 Build timestamp 1463494941 Build timestamp as int 1463494941 If possible provide a minimal reproducible example We usually don t have time to read hundreds of lines of your code std vector tensorflow Tensor outputs for int t 0 t count t std vector std pair std string tensorflow Tensor feed .... auto status g session Run feed out layer names outputs if status.ok LOG ERROR status.ToString return Internal Error .... Logs or other output that would be helpful This is a stack trace of all of the threads when the program freezes thread 1 tid 0x206350 0x0000000183256e1c libsystem kernel.dylib psynch cvwait 8 queue com.apple.main thread stop reason signal SIGSTOP frame 0 0x0000000183256e1c libsystem kernel.dylib psynch cvwait 8 frame 1 0x000000018331c9c0 libsystem pthread.dylib pthread cond wait 640 frame 2 0x0000000182c453ec libc .1.dylib std 1 condition variable wait std 1 unique lock std 1 mutex 56 frame 3 0x00000001000ef6fc App tensorflow DirectSession WaitForNotification tensorflow Notification long long 176 frame 4 0x00000001000eb1cc App tensorflow DirectSession WaitForNotification tensorflow DirectSession RunState tensorflow CancellationManager long long 48 frame 5 0x00000001000e91b8 App tensorflow DirectSession Run tensorflow RunOptions const std 1 vector std 1 pair std 1 basic string char std 1 char traits char std 1 allocator char tensorflow Tensor std 1 allocator std 1 pair std 1 basic string char std 1 char traits char std 1 allocator char tensorflow Tensor const std 1 vector std 1 basic string char std 1 char traits char std 1 allocator char std 1 allocator std 1 basic string char std 1 char traits char std 1 allocator char const std 1 vector std 1 basic string char std 1 char traits char std 1 allocator char std 1 allocator std 1 basic string char std 1 char traits char std 1 allocator char const std 1 vector tensorflow Tensor std 1 allocator tensorflow Tensor tensorflow RunMetadata 1868 frame 6 0x00000001000e8a40 App tensorflow DirectSession Run std 1 vector std 1 pair std 1 basic string char std 1 char traits char std 1 allocator char tensorflow Tensor std 1 allocator std 1 pair std 1 basic string char std 1 char traits char std 1 allocator char tensorflow Tensor const std 1 vector std 1 basic string char std 1 char traits char std 1 allocator char std 1 allocator std 1 basic string char std 1 char traits char std 1 allocator char const std 1 vector std 1 basic string char std 1 char traits char std 1 allocator char std 1 allocator std 1 basic string char std 1 char traits char std 1 allocator char const std 1 vector tensorflow Tensor std 1 allocator tensorflow Tensor 112 frame 7 0x000000010052e01c App tensorflow internal AppendProtoDebugString tensorflow strings ProtoTextOutput tensorflow Feature const 6028 frame 8 0x000000010053c138 App main 24316 frame 9 0x0000000100535f00 App tensorflow internal AppendProtoDebugString tensorflow strings ProtoTextOutput tensorflow Feature const 38512 frame 10 0x0000000100535794 App tensorflow internal AppendProtoDebugString tensorflow strings ProtoTextOutput tensorflow Feature const 36612 frame 11 0x000000018a173d30 UIKit UIApplication sendAction to from forEvent 96 frame 12 0x000000018a2e7880 UIKit UIBarButtonItem UIInternal sendAction withEvent 168 frame 13 0x000000018a173d30 UIKit UIApplication sendAction to from forEvent 96 frame 14 0x000000018a173cb0 UIKit UIControl sendAction to forEvent 80 frame 15 0x000000018a15e128 UIKit UIControl sendActionsForEvents withEvent 452 frame 16 0x000000018a15e290 UIKit UIControl sendActionsForEvents withEvent 812 frame 17 0x000000018a17359c UIKit UIControl touchesEnded withEvent 584 frame 18 0x000000018a1730c4 UIKit UIWindow sendTouchesForEvent 2484 frame 19 0x000000018a16e328 UIKit UIWindow sendEvent 2988 frame 20 0x000000018a13eda0 UIKit UIApplication sendEvent 340 frame 21 0x000000018a92875c UIKit dispatchPreprocessedEventFromEventQueue 2736 frame 22 0x000000018a922130 UIKit handleEventQueue 784 frame 23 0x0000000184236b5c CoreFoundation CFRUNLOOP IS CALLING OUT TO A SOURCE0 PERFORM FUNCTION 24 frame 24 0x00000001842364a4 CoreFoundation CFRunLoopDoSources0 524 frame 25 0x00000001842340a4 CoreFoundation CFRunLoopRun 804 frame 26 0x00000001841622b8 CoreFoundation CFRunLoopRunSpecific 444 frame 27 0x0000000185c16198 GraphicsServices GSEventRunModal 180 frame 28 0x000000018a1a97fc UIKit UIApplication run 684 frame 29 0x000000018a1a4534 UIKit UIApplicationMain 208 frame 30 0x00000001005362b4 App main 120 frame 31 0x00000001831455b8 libdyld.dylib start 4 thread 4 tid 0x206397 0x000000018331ad88 libsystem pthread.dylib start wqthread frame 0 0x000000018331ad88 libsystem pthread.dylib start wqthread thread 8 tid 0x20639b 0x0000000183239188 libsystem kernel.dylib mach msg trap 8 name com.apple.uikit.eventfetch thread frame 0 0x0000000183239188 libsystem kernel.dylib mach msg trap 8 frame 1 0x0000000183238ff8 libsystem kernel.dylib mach msg 72 frame 2 0x00000001842365d0 CoreFoundation CFRunLoopServiceMachPort 192 frame 3 0x00000001842341ec CoreFoundation CFRunLoopRun 1132 frame 4 0x00000001841622b8 CoreFoundation CFRunLoopRunSpecific 444 frame 5 0x0000000184c9f26c Foundation NSRunLoop NSRunLoop runMode beforeDate 304 frame 6 0x0000000184cbfdd0 Foundation NSRunLoop NSRunLoop runUntilDate 96 frame 7 0x000000018ab1dc38 UIKit UIEventFetcher threadMain 136 frame 8 0x0000000184d9ce68 Foundation NSThread start 1024 frame 9 0x000000018331d850 libsystem pthread.dylib pthread body 240 frame 10 0x000000018331d760 libsystem pthread.dylib pthread start 284 frame 11 0x000000018331ad94 libsystem pthread.dylib thread start 4 thread 9 tid 0x2063c0 0x0000000183256e1c libsystem kernel.dylib psynch cvwait 8 frame 0 0x0000000183256e1c libsystem kernel.dylib psynch cvwait 8 frame 1 0x000000018331c9c0 libsystem pthread.dylib pthread cond wait 640 frame 2 0x0000000182c453ec libc .1.dylib std 1 condition variable wait std 1 unique lock std 1 mutex 56 frame 3 0x000000010019535c App tensorflow thread ThreadPool CurrentThreadId const 6296 frame 4 0x0000000100194e40 App tensorflow thread ThreadPool CurrentThreadId const 4988 frame 5 0x00000001001949dc App tensorflow thread ThreadPool CurrentThreadId const 3864 frame 6 0x00000001001946e4 App tensorflow thread ThreadPool CurrentThreadId const 3104 frame 7 0x00000001001a5828 App void std 1 thread proxy std 1 tuple std 1 function void void 100 frame 8 0x000000018331d850 libsystem pthread.dylib pthread body 240 frame 9 0x000000018331d760 libsystem pthread.dylib pthread start 284 frame 10 0x000000018331ad94 libsystem pthread.dylib thread start 4 thread 10 tid 0x2063c1 0x0000000183256e1c libsystem kernel.dylib psynch cvwait 8 frame 0 0x0000000183256e1c libsystem kernel.dylib psynch cvwait 8 frame 1 0x000000018331c9c0 libsystem pthread.dylib pthread cond wait 640 frame 2 0x0000000182c453ec libc .1.dylib std 1 condition variable wait std 1 unique lock std 1 mutex 56 frame 3 0x000000010019535c App tensorflow thread ThreadPool CurrentThreadId const 6296 frame 4 0x0000000100194e40 App tensorflow thread ThreadPool CurrentThreadId const 4988 frame 5 0x00000001001949dc App tensorflow thread ThreadPool CurrentThreadId const 3864 frame 6 0x00000001001946e4 App tensorflow thread ThreadPool CurrentThreadId const 3104 frame 7 0x00000001001a5828 App void std 1 thread proxy std 1 tuple std 1 function void void 100 frame 8 0x000000018331d850 libsystem pthread.dylib pthread body 240 frame 9 0x000000018331d760 libsystem pthread.dylib pthread start 284 frame 10 0x000000018331ad94 libsystem pthread.dylib thread start 4 thread 11 tid 0x2063c2 0x0000000183256e1c libsystem kernel.dylib psynch cvwait 8 frame 0 0x0000000183256e1c libsystem kernel.dylib psynch cvwait 8 frame 1 0x000000018331c9c0 libsystem pthread.dylib pthread cond wait 640 frame 2 0x0000000182c453ec libc .1.dylib std 1 condition variable wait std 1 unique lock std 1 mutex 56 frame 3 0x000000010019535c App tensorflow thread ThreadPool CurrentThreadId const 6296 frame 4 0x0000000100194e40 App tensorflow thread ThreadPool CurrentThreadId const 4988 frame 5 0x00000001001949dc App tensorflow thread ThreadPool CurrentThreadId const 3864 frame 6 0x00000001001946e4 App tensorflow thread ThreadPool CurrentThreadId const 3104 frame 7 0x00000001001a5828 App void std 1 thread proxy std 1 tuple std 1 function void void 100 frame 8 0x000000018331d850 libsystem pthread.dylib pthread body 240 frame 9 0x000000018331d760 libsystem pthread.dylib pthread start 284 frame 10 0x000000018331ad94 libsystem pthread.dylib thread start 4 thread 12 tid 0x2063c3 0x0000000183256e1c libsystem kernel.dylib psynch cvwait 8 frame 0 0x0000000183256e1c libsystem kernel.dylib psynch cvwait 8 frame 1 0x000000018331c9c0 libsystem pthread.dylib pthread cond wait 640 frame 2 0x0000000182c453ec libc .1.dylib std 1 condition variable wait std 1 unique lock std 1 mutex 56 frame 3 0x000000010019535c App tensorflow thread ThreadPool CurrentThreadId const 6296 frame 4 0x0000000100194e40 App tensorflow thread ThreadPool CurrentThreadId const 4988 frame 5 0x00000001001949dc App tensorflow thread ThreadPool CurrentThreadId const 3864 frame 6 0x00000001001946e4 App tensorflow thread ThreadPool CurrentThreadId const 3104 frame 7 0x00000001001a5828 App void std 1 thread proxy std 1 tuple std 1 function void void 100 frame 8 0x000000018331d850 libsystem pthread.dylib pthread body 240 frame 9 0x000000018331d760 libsystem pthread.dylib pthread start 284 frame 10 0x000000018331ad94 libsystem pthread.dylib thread start 4 ,I got some advice from mrry and he explained that DirectSession WaitForNotification blocking means that the run call is never signalling that it s finishing. On iOS we should only ever have one thread executing ops inter op parallelism in the arguments so it should be tough to get into this condition. From Derek my guess is i some op that we don t really support on that platform is running and failing in a bad way perhaps an AsyncOpKernel or ii some Session functionality that we haven t really tested on that platform is being used and failing in a bad way perhaps a run timeout To help understand this can you run tensorflow tools graph transforms summarize graph on your model file so we can see which ops are being used This is the result of running summarize graph Found 7 possible inputs name token type int32 3 shape 1 name lstm state 0 c in type float 1 shape 1 512 name lstm state 0 h in type float 1 shape 1 512 name lstm state 1 c in type float 1 shape 1 512 name lstm state 1 h in type float 1 shape 1 512 name lstm state 2 c in type float 1 shape 1 512 name lstm state 2 h in type float 1 shape 1 512 Found 7 possible outputs name lstm state 0 c out op Identity name lstm state 0 h out op Identity name lstm state 1 c out op Identity name lstm state 1 h out op Identity name lstm state 2 c out op Identity name lstm state 2 h out op Identity name logits op Identity Found 19 consts 0 variables 14 identities and 0 control edges Op types used 19 Const 14 Identity 9 Mul 9 Sigmoid 7 Placeholder 7 Tanh 6 Add 4 MatMul 3 BiasAdd 3 Concat 3 Split 2 Reshape 1 Gather 1 Pack 1 Unpack Any progress on this Same problem Is there any progress Thanks. I am having the same issue running the inceptionV3 network the one offered by tensorflow models . Problem still persists bad random freezes on iOS. AndreaPisoni I have solved my problem by setting inter op parallelism threads and intra op parallelism threads to 1. This would make Tensorflow not use parallelism at all. My session creation code looks like this Setting just inter op parallelism threads to 1 as suggested by petewarden was not enough for me. Brilliant I will give it a try thanks Just wondering does this make the inference any slower Thank you It works for me but it s still far from a solution. I m wondering why the official demo camera can work smoothly... Is someone still looking into the issue Do we have logs on what the notifier thread is doing I met the same problem. use LSTM and fixed by the upper method setting inter op parallelism threads and intra op parallelism threads to 1 . Is there any better methods to fix it Or tensorflow on IOS not support LSTM We are also seeing this issue on iOS. We believe the issue is caused by a where op which does in fact seem to have a fair bit of threading related code https github.com tensorflow tensorflow blob 2cfbd3347b943a389bb688125c2a90095b7735b5 tensorflow core kernels where op.cc . cancan101 use the lastest version still freeze. Has someone fixed gazzterran did you try what ratzinho87 suggested options.config.set inter op parallelism threads 1 options.config.set intra op parallelism threads 1 liamnaks I ve tried the inter op parallelism threads method and it works in most of time but it still occurs during testing in a very small probability like 1 100 From cancan101 said is there some bugs in code Why close the issue I believe it still exists on new builds of TF. A fix for general threading issues on iOS went in last month in https github.com tensorflow tensorflow pull 12573 Could you let me know if this solves your case also It appears to fix 12298. Did it make it into 1.3 It does not look like it is in 1.3 cancan101 did the fix from 12573 solve this issue for you I reported 12298 and I m using a version including that commit but I am also having this issue. For me it seems to only happen when in some other place in my iOS app I have an NSOperationQueue with a ton of operations and myQueue.maxConcurrentOperationCount NSOperationQueueDefaultMaxConcurrentOperationCount . Maybe that indicates the there s some thread contention between my operation queue and whatever thread Tensorflow is trying to use for its ops In any case setting maxConcurrentOperationCount to a smaller number for example 4 fixed the problem for me so that s a potential workaround. same issue. ios froze occasionally on LSTM and bad luck things worse after i did ios freezes every time i do inferece. greate news for me i resolved this issue ratzinho87 not only set inter op parallelism threads and set intra op parallelism threads but also add session inter op thread pool and set use per session threads here eigen2017 is there a PR you need to make or should this go int he docs somewhere That doesn t seem like a solution as much as a workaround. Nagging Awaiting Response It has been 14 days with no activityand the awaiting response label was assigned. Is this still an issue Nagging Awaiting Response It has been 14 days with no activityand the awaiting response label was assigned. Is this still an issue It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue Closing due to lack of activity but please reopen if there s some followup needed. 
7970,SyncReplicasOptimizer race condition strange behavior ,It seems there is a strange race condition in SyncReplicasOptimizer leading to strange behaviour. I include below an example code to reproduce what seems to be a bug hopefully in my code as well as the commands to reproduce it pretty much the same code as in mnist replica.py . I am trying to implement synchronized SGD using SyncReplicasOptimizer I also used the queue trick to make the parameter server stop gracefully when all workers are done. I have 4 workers and 1 parameter server. Worker 0 is the chief worker. Please bear with me for the long explanation of the different issues they depend on the order in which processes are launched First kind of issue launch the processes in this order python test.py job name ps python test.py job name worker taks index 0 python test.py job name worker taks index 1 python test.py job name worker taks index 2 python test.py job name worker taks index 3 The last worker throws the following error I tensorflow core distributed runtime master session.cc 909 DeregisterGraph error Unavailable created 1488366991.043859719 description OS Error errno 104 file external grpc src core lib iomgr tcp posix.c file line 229 grpc status 14 os error Connection reset by peer syscall recvmsg and quits and it happens also that it hangs not realising that the variable epoch is greater than 4 triggering the break from the training loop and the enqueue operation to let the ps stop gracefully . It also happen that all is fine and the execution terminates without any errors. Second kind of issue launch the processes in this order python test.py job name ps python test.py job name worker taks index 3 python test.py job name worker taks index 2 python test.py job name worker taks index 1 python test.py job name worker taks index 0 The chief here being launched at last. Strangely the chief completes the loop and quits I thought with SyncReplicasOptimizer it had to wait for the other workers to complete each step . As for the other workers I had all sort of results when doing the same experiment many times 1 Some workers simply hang and do not execute a single step in the while true training loop 2 Some execute some steps then simply hang apparently they lose contact with the chief and do not realise that the variable epoch is greater than 4 triggering the break from the training loop. Thank you for help with this issue. Below is the code of test.py import os import shutil import tempfile import numpy as np import pandas as pd import argparse from keras.models import Sequential from keras.layers.core import Dense from keras.regularizers import l2 import tensorflow as tf import keras nb samples 50 nb features 5 X train np.random.randn nb samples nb features .reshape nb samples nb features Y train np.random.randn nb samples .reshape nb samples 1 def build keras model input dim hidden dim 10 model Sequential model.add Dense input dim input dim output dim hidden dim activation tanh model.add Dense output dim 1 activation linear model.compile loss mse optimizer adam return model DISTRIBUTE parser argparse.ArgumentParser description tensorflow parser.add argument job name dest job name parser.add argument task index dest task index default 0 args parser.parse args ps hosts localhost 2222 worker hosts localhost 2223 localhost 2224 localhost 2225 localhost 2226 job name args.job name task index int args.task index Create a cluster from the parameter server and worker hosts. cluster tf.train.ClusterSpec ps ps hosts worker worker hosts server tf.train.Server cluster job name job name task index task index config tf.ConfigProto log device placement True inter op parallelism threads 1 intra op parallelism threads 1 if job name ps with tf.device job ps task 0 queue tf.FIFOQueue len worker hosts tf.int32 shared name done queue sess tf.Session server.target wait until all workers are done for i in range len worker hosts sess.run queue.dequeue else with tf.device tf.train.replica device setter worker device job worker task d task index cluster cluster keras.backend.set learning phase 1 keras.backend.manual variable initialization True model build keras model nb features preds model.output targets tf.placeholder tf.float32 None 1 total loss tf.reduce mean keras.objectives.mean squared error targets preds global step tf.Variable 0 name global step trainable False For early stopping management epoch tf.Variable 0 name epoch trainable False inc epoch op tf.assign add epoch 1 is chief task index 0 opt tf.train.AdamOptimizer num workers len worker hosts replicas to aggregate num workers opt tf.train.SyncReplicasOptimizer opt replicas to aggregate replicas to aggregate total num replicas num workers name sync replicas train op opt.minimize total loss global step global step local init op opt.local step init op if is chief local init op opt.chief init op ready for local init op opt.ready for local init op Initial token and chief queue runners required by the sync replicas mode chief queue runner opt.get chief queue runner sync init op opt.get init tokens op init op tf.global variables initializer with tf.device job ps task 0 queue tf.FIFOQueue len worker hosts tf.int32 shared name done queue enqueue op queue.enqueue 1 train dir tempfile.mkdtemp prefix worker d task index sv tf.train.Supervisor is chief is chief logdir train dir init op init op local init op local init op ready for local init op ready for local init op recovery wait secs 1 global step global step print ALL CREATED sess sv.prepare or wait for session server.target keras.backend.set session sess print SESSION OK if is chief sess.run sync init op sv.start queue runners sess chief queue runner local step 0 while True train feed model.input X train targets Y train step sess.run train op global step feed dict train feed loss sess.run total loss feed dict train feed if is chief sess.run inc epoch op local step 1 print epoch epoch.eval sess if epoch.eval sess 4 print TRYING TO LEAVE break shutil.rmtree train dir print WHILE LOOP LEFT sess.run enqueue op print ENQUEUE OP DONE , ispirmustafa might have some ideas. No charitable tensorflow soul to help with this cc jmchen g Could you please try with MonitoredSession. worker code should be as follows Thanks ispirmustafa I replaced my sess sv.prepare or wait for session server.target By the code you suggested. It resulted in the following error Traceback most recent call last File test.py line 127 in module sync replicas hook opt.make session run hook is chief File .. tensorflow 1.0.0 lib tensorflow python training sync replicas optimizer.py line 431 in make session run hook self.get init tokens op num tokens File .. tensorflow 1.0.0 lib tensorflow python training sync replicas optimizer.py line 418 in get init tokens op tokens array ops.fill num tokens self. global step File .. tensorflow 1.0.0 lib tensorflow python ops gen array ops.py line 1318 in fill result op def lib.apply op Fill dims dims value value name name File .. tensorflow 1.0.0 lib tensorflow python framework op def library.py line 491 in apply op preferred dtype default dtype File .. tensorflow 1.0.0 lib tensorflow python framework ops.py line 716 in internal convert to tensor ret conversion func value dtype dtype name name as ref as ref File .. tensorflow 1.0.0 lib tensorflow python framework constant op.py line 176 in constant tensor conversion function return constant v dtype dtype name name File .. tensorflow 1.0.0 lib tensorflow python framework constant op.py line 169 in constant attrs value tensor value dtype dtype value name name .outputs 0 File .. tensorflow 1.0.0 lib tensorflow python framework ops.py line 2354 in create op self. check not finalized File .. tensorflow 1.0.0 lib tensorflow python framework ops.py line 2077 in check not finalized raise RuntimeError Graph is finalized and cannot be modified. RuntimeError Graph is finalized and cannot be modified. This should not happen. Could you please show all the code after your change This is it.. the change is at the end. Thanks for your help. I am sure I am making a silly mistake import os import shutil import tempfile import numpy as np import pandas as pd import argparse from keras.models import Sequential from keras.layers.core import Dense from keras.regularizers import l2 import tensorflow as tf import keras nb samples 50 nb features 5 X train np.random.randn nb samples nb features .reshape nb samples nb features Y train np.random.randn nb samples .reshape nb samples 1 def build keras model input dim hidden dim 10 model Sequential model.add Dense input dim input dim output dim hidden dim activation tanh model.add Dense output dim 1 activation linear model.compile loss mse optimizer adam return model DISTRIBUTE parser argparse.ArgumentParser description tensorflow parser.add argument job name dest job name parser.add argument task index dest task index default 0 args parser.parse args ps hosts localhost 2222 worker hosts localhost 2223 localhost 2224 localhost 2225 localhost 2226 job name args.job name task index int args.task index Create a cluster from the parameter server and worker hosts. cluster tf.train.ClusterSpec ps ps hosts worker worker hosts server tf.train.Server cluster job name job name task index task index config tf.ConfigProto log device placement True inter op parallelism threads 1 intra op parallelism threads 1 if job name ps with tf.device job ps task 0 queue tf.FIFOQueue len worker hosts tf.int32 shared name done queue sess tf.Session server.target wait until all workers are done for i in range len worker hosts sess.run queue.dequeue else with tf.device tf.train.replica device setter worker device job worker task d task index cluster cluster keras.backend.set learning phase 1 keras.backend.manual variable initialization True model build keras model nb features preds model.output targets tf.placeholder tf.float32 None 1 total loss tf.reduce mean keras.objectives.mean squared error targets preds global step tf.Variable 0 name global step trainable False For stopping management epoch tf.Variable 0 name epoch trainable False inc epoch op tf.assign add epoch 1 is chief task index 0 opt tf.train.AdamOptimizer num workers len worker hosts replicas to aggregate num workers opt tf.train.SyncReplicasOptimizer opt replicas to aggregate replicas to aggregate total num replicas num workers name sync replicas train op opt.minimize total loss global step global step local init op opt.local step init op if is chief local init op opt.chief init op ready for local init op opt.ready for local init op Initial token and chief queue runners required by the sync replicas mode chief queue runner opt.get chief queue runner sync init op opt.get init tokens op init op tf.global variables initializer with tf.device job ps task 0 queue tf.FIFOQueue len worker hosts tf.int32 shared name done queue enqueue op queue.enqueue 1 train dir tempfile.mkdtemp prefix worker d task index sv tf.train.Supervisor is chief is chief logdir train dir init op init op local init op local init op ready for local init op ready for local init op recovery wait secs 1 global step global step print ALL CREATED sess sv.prepare or wait for session server.target sync replicas hook opt.make session run hook is chief with tf.train.MonitoredTrainingSession master server.target is chief is chief checkpoint dir train dir hooks sync replicas hook as sess keras.backend.set session sess print SESSION OK if is chief sess.run sync init op sv.start queue runners sess chief queue runner local step 0 while True train feed model.input X train targets Y train step sess.run train op global step feed dict train feed loss sess.run total loss feed dict train feed if is chief sess.run inc epoch op local step 1 print epoch epoch.eval sess if epoch.eval sess 4 print TRYING TO LEAVE break shutil.rmtree train dir print WHILE LOOP LEFT sess.run enqueue op print ENQUEUE OP DONE Please check the documentation of MonitoredSession. you need to remove following lines Also followings Also followings if is chief sess.run sync init op sv.start queue runners sess chief queue runner also can you delete train dir after running enqueue op. Thanks. It still does not work. Only the chief executes the train loop even if in the code replicas to aggregate nb workers and it ends with an exception I put below the new code as well as the output of the chief worker import os import shutil import tempfile import numpy as np import pandas as pd import argparse from keras.models import Sequential from keras.layers.core import Dense from keras.regularizers import l2 import tensorflow as tf import keras nb samples 50 nb features 5 X train np.random.randn nb samples nb features .reshape nb samples nb features Y train np.random.randn nb samples .reshape nb samples 1 def build keras model input dim hidden dim 10 model Sequential model.add Dense input dim input dim output dim hidden dim activation tanh model.add Dense output dim 1 activation linear model.compile loss mse optimizer adam return model DISTRIBUTE parser argparse.ArgumentParser description tensorflow parser.add argument job name dest job name parser.add argument task index dest task index default 0 args parser.parse args ps hosts localhost 2222 worker hosts localhost 2223 localhost 2224 localhost 2225 localhost 2226 job name args.job name task index int args.task index Create a cluster from the parameter server and worker hosts. cluster tf.train.ClusterSpec ps ps hosts worker worker hosts server tf.train.Server cluster job name job name task index task index config tf.ConfigProto log device placement True inter op parallelism threads 1 intra op parallelism threads 1 if job name ps with tf.device job ps task 0 queue tf.FIFOQueue len worker hosts tf.int32 shared name done queue sess tf.Session server.target wait until all workers are done for i in range len worker hosts sess.run queue.dequeue else with tf.device tf.train.replica device setter worker device job worker task d task index cluster cluster keras.backend.set learning phase 1 keras.backend.manual variable initialization True model build keras model nb features preds model.output targets tf.placeholder tf.float32 None 1 total loss tf.reduce mean keras.objectives.mean squared error targets preds global step tf.Variable 0 name global step trainable False For stopping management epoch tf.Variable 0 name epoch trainable False inc epoch op tf.assign add epoch 1 is chief task index 0 opt tf.train.AdamOptimizer num workers len worker hosts replicas to aggregate num workers opt tf.train.SyncReplicasOptimizer opt replicas to aggregate replicas to aggregate total num replicas num workers name sync replicas train op opt.minimize total loss global step global step with tf.device job ps task 0 queue tf.FIFOQueue len worker hosts tf.int32 shared name done queue enqueue op queue.enqueue 1 train dir tempfile.mkdtemp prefix worker d task index print ALL CREATED sync replicas hook opt.make session run hook is chief with tf.train.MonitoredTrainingSession master server.target is chief is chief checkpoint dir train dir hooks sync replicas hook as sess keras.backend.set session sess print SESSION OK local step 0 while True train feed model.input X train targets Y train step sess.run train op global step feed dict train feed loss sess.run total loss feed dict train feed if is chief sess.run inc epoch op local step 1 print epoch epoch.eval sess if epoch.eval sess 4 print TRYING TO LEAVE break print WHILE LOOP LEFT sess.run enqueue op print ENQUEUE OP DONE shutil.rmtree train dir and this is the output of the chief worker SESSION OK epoch 1 epoch 2 epoch 3 epoch 4 epoch 5 TRYING TO LEAVE Exception in thread Thread 2 Traceback most recent call last File usr lib64 python2.7 threading.py line 811 in bootstrap inner self.run File usr lib64 python2.7 threading.py line 764 in run self. target self. args self. kwargs File .. tensorflow 1.0.0 lib tensorflow python training queue runner impl.py line 250 in run coord.request stop e File .. tensorflow 1.0.0 lib tensorflow python training coordinator.py line 211 in request stop six.reraise sys.exc info File .. tensorflow 1.0.0 lib tensorflow python training queue runner impl.py line 234 in run sess.run enqueue op File .. tensorflow 1.0.0 lib tensorflow python client session.py line 767 in run run metadata ptr File .. tensorflow 1.0.0 lib tensorflow python client session.py line 965 in run feed dict string options run metadata File .. tensorflow 1.0.0 lib tensorflow python client session.py line 1015 in do run target list options run metadata File .. tensorflow 1.0.0 lib tensorflow python client session.py line 1035 in do call raise type e node def op message CancelledError RunManyGraphs Traceback most recent call last File test.py line 148 in module break File .. tensorflow 1.0.0 lib tensorflow python training monitored session.py line 478 in exit self. close internal exception type File .. tensorflow 1.0.0 lib tensorflow python training monitored session.py line 511 in close internal self. sess.close File .. tensorflow 1.0.0 lib tensorflow python training monitored session.py line 739 in close self. sess.close File .. tensorflow 1.0.0 lib tensorflow python training monitored session.py line 827 in close self. coord.join File .. tensorflow 1.0.0 lib tensorflow python training coordinator.py line 390 in join .join stragglers RuntimeError Coordinator stopped with threads still running Thread 2 jmchen g could you please review the code Looks like that the session trains well but couldn t go outside the loop because some threads are still running the queue runner thread in this case ... On Fri Mar 10 2017 at 2 29 PM ispirmustafa notifications github.com wrote jmchen g https github.com jmchen g could you please review the code You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 7970 issuecomment 285802619 or mute the thread https github.com notifications unsubscribe auth APD49jfjsRwXleF4iVdQfEeMNio2OIS0ks5rkc7egaJpZM4MPifr . Please confirm that the training went normal during these epocs. Thanks. Indeed training goes well and I see the loss decreasing when I print it in the chief worker. However the workers could not go outside the loop as you mentioned. Another issue is that it seems only the chief is doing the training even if in the constructor of SyncReplicasOptimizer I have the parameter replicas to aggregate num workers No good samaritan to help with this issue What do you mean by only the chief is doing the training Only the chief will update the variables through the chief queue runner but it should use the grads from all available workers... For the issue that when training finished successfully the other workers should at least exit via time out unless the time out is set to infinity I mean in the training loop the print statement print epoch epoch.eval sess got executed only by the chief. If I repeat my experiment several times it may get executed by the other workers from time to time. I thought in SyncReplicasOptimizer the chief was supposed to wait until all the workers are live before beginning the optimization indeed replicas to aggregate num workers in my code The chief is supposed to wait until enough grads are collected so not necessarily from all workers. Hmm... You guys are using the tf.train.replica device setter .. which doesn t tell global from local variables We only notice this recently ... This is not right and we are trying to fix it. Please wait until that is fixed. Thanks. Thank you When I use TF 1.1.0 The chief worker throws the following error Exception in thread Thread 7 Traceback most recent call last File export App anaconda3 envs py2 lib python2.7 threading.py line 801 in bootstrap inner self.run File export App anaconda3 envs py2 lib python2.7 threading.py line 754 in run self. target self. args self. kwargs File export App anaconda3 envs py2 lib python2.7 site packages tensorflow python training queue runner impl.py line 250 in run coord.request stop e File export App anaconda3 envs py2 lib python2.7 site packages tensorflow python training coordinator.py line 211 in request stop six.reraise sys.exc info File export App anaconda3 envs py2 lib python2.7 site packages tensorflow python training queue runner impl.py line 234 in run sess.run enqueue op File export App anaconda3 envs py2 lib python2.7 site packages tensorflow python client session.py line 778 in run run metadata ptr File export App anaconda3 envs py2 lib python2.7 site packages tensorflow python client session.py line 982 in run feed dict string options run metadata File export App anaconda3 envs py2 lib python2.7 site packages tensorflow python client session.py line 1032 in do run target list options run metadata File export App anaconda3 envs py2 lib python2.7 site packages tensorflow python client session.py line 1052 in do call raise type e node def op message CancelledError Step was cancelled by an explicit call to Session Close . Hi jmchen g I m facing the same kind of issues using tensorflow 1.0.0. In a distributed run for most of the runs the non chief workers get stuck and log from time to time INFO tensorflow Waiting for model to be ready. Ready for local init op None ready Variables not initialized In some runs they start working and print some logs as expected INFO tensorflow For task index 1 at step 88714 cost is 0.00616120453924 It might be related to the issue you mentionned as I m also using tf.train.replica device setter job worker task d FLAGS.task index I wonder if you could give any insights and or workarounds on the issue Many thanks ispirmustafa any further thoughts about this The device setter issue should have already been fixed in the latest branch. When did you pull Can you let me know the fix commit What was the problem with replica device setter. I experienced same problem but I just want to modify this solution instead of pulling latest branch. The setter issue should have already been fixed in the latest head. Do you still have this after syncing On Thu Jul 13 2017 at 8 01 AM sj6077 notifications github.com wrote Can you let me know the fix commit You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 7970 issuecomment 315104227 or mute the thread https github.com notifications unsubscribe auth APD49sZI jz2GZmFhMepvXG0FCM4P319ks5sNjFBgaJpZM4MPifr . I need to keep the version so I just want to modify that parts. When was it fixed The last commit in my repo is May 4. I also want to catch up the reason of the issue. I also tested with the latest head but I still got same error with fanlu npfp Hi I have encountered the same problem. Have you solved that INFO tensorflow Waiting for model to be ready. Ready for local init op None ready Variables not initialized It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. Nagging Awaiting TensorFlower It has been 14 days with no activityand the awaiting tensorflower label was assigned. Please update the label and or status accordingly. Nagging Awaiting TensorFlower It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. 
9125,KeyError in tf.contrib.graph editor.graph replace,When applying graph replace to graphs containing ops with the original op attribute it can fail with a KeyError . The error occurs in Transformer. copy ops when trying to copy an op whose original op has not yet been copied. The ordering of ops that are copied is not deterministic so this error pops up somewhat randomly. The original op attributes appear to be created by tf.gradients to point back to the op from the forward pass. Example code snippet note you may need to run this multiple times to get a failure Error I see three possible fixes 1. Remove original op attributes in the copied graph I don t see anywhere in the TF codebase where it is used 2. Move the creation of the original op attribute from the copy op handler function to the end of Transformer. copy ops after all ops have been copied. 3. Topologically sort the ops being copied so that ops that are original op attributes are created before their children. My implementation https github.com poolio tensorflow pull 1 files of option 2 seems to fix this problem but I might be missing something about the usage of original op ., purpledog Here s an interesting graph editor issue for you. If you need toposort I wrote a Tarjan implementation in Python I can give you. Also get walks intersection ops looks quadratic. The similar issue still persist when variable is matix import tensorflow as tf graph replace tf.contrib.graph editor.graph replace w tf.Variable tf.zeros 784 784 name w y tf. matmul tf.matmul w w name mul1 w name mul2 g tf.gradients y w 0 g new graph replace g w.value g Traceback most recent call last File scratch0 username Users Jaiusername Documents Study research pytorchExamples GAN keras adversarial examples testKeyErrorFailure.py line 9 in module g new graph replace g w.value g File scratch0 username installs tensorflow lib python2.7 site packages tensorflow contrib graph editor transform.py line 655 in graph replace ops replacement ts None dst scope src scope reuse dst scope File scratch0 username installs tensorflow lib python2.7 site packages tensorflow contrib graph editor transform.py line 617 in copy with input replacements sgv dst graph dst scope src scope reuse dst scope reuse dst scope File scratch0 username installs tensorflow lib python2.7 site packages tensorflow contrib graph editor transform.py line 434 in call self. copy ops info File scratch0 username installs tensorflow lib python2.7 site packages tensorflow contrib graph editor transform.py line 447 in copy ops op op outputs self.transform op handler info op File scratch0 username installs tensorflow lib python2.7 site packages tensorflow contrib graph editor transform.py line 171 in copy op handler original op info.transform original op handler info op. original op File scratch0 username installs tensorflow lib python2.7 site packages tensorflow contrib graph editor transform.py line 124 in transform op if inside handler return info.transformed ops op KeyError tf.Operation mul1 type MatMul Process finished with exit code 1 I m happy to submit a PR using my implementation https github.com poolio tensorflow pull 1 files of the proposed second option. Please let me know whether that would be helpful as graph replace was the only way of copying and replacing subgraphs. It would be great to have this working again especially given that other PRs for similar features have been rejected 5802 . Hi What s the current status of this bug as of r1.4 Thanks gokul uf it still exists. This is the same issue as https github.com tensorflow tensorflow issues 9978 . Current work around is to modify graph editor source locally although PR would be welcome It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. Please remove the assignee as this issue is inviting external contributions. Otherwise remove the contributions welcome label. Thank you. Closing out this issue as tf.contrib is no longer supported in TensorFlow 2.x. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 9125 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 9125 No a 
9136,Issues when using Queues tf.train.Server,NOTE Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow. You must complete this information or else your issue will be closed Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes TensorFlow installed from source or binary binary TensorFlow version 1.0.0 CPU 1.0.1 CPU and GPU enabled 1.1.0rc1 CPU Bazel version if compiling from source CUDA cuDNN version N A GPU Model and Memory N A Exact command to reproduce cf below. This problem has been reproduced on both Linux and various Mac OS machines. Describe the problem clearly We seem to experience issues when using both queues tf.train.Server . When executed in a simple python 3.5.3 console the following script hangs It outputs and then hangs forever. The problem vanishes when either commenting the input queue ... line or when writing session tf.Session instead of passing the server.target . The problems seems to happen not only with variable assignments but also saving the model using tf.train.Saver .save session my model for instance and possibly other operations . Note that reading a variable works fine. In the example script the time.sleep command simulates a pause between creating the session and running it to set a variable. The same effect is achieved for example when splitting session creation and running code across two Jupyter notebook cells. When executing the whole code in one cell it works fine. Source Code Logs The source code to reproduce the problem is displayed above. I have attached a traceback using gdb which shows that the program is hanging while trying to acquire a lock. tf issue gdb bt.txt https github.com tensorflow tensorflow files 913097 tf issue gdb bt.txt tf issue gdb stack threads.txt https github.com tensorflow tensorflow files 913102 tf issue gdb stack threads.txt ,Thanks for the detailed report and stacktraces this helps a lot and is much appreciated. mrry pointed out that we might have a bug when graphs are extended in a distributed session while some operations in this case the enqueue operation are in progress See master session.cc 1038 https github.com tensorflow tensorflow blob 87cdfafd44ff5e332fd820608783432fea83a4c9 tensorflow core distributed runtime master session.cc L1038 that code predates the queue runners . suharshs Would you have the bandwidth to look into that TODO jdonier In the mean time a workaround for you would be to ensure that the graph isn t modified after the queue runners are started. For example your snippet above could be rewritten as FYI jhseu saeta who might like to know about this too. asimshankar the reason why I want to modify the graph after the queue runners are started is to change some parameters during after training e.g. the training rate during training or dropout rates between training and inference so this needs to be done after the queue runners have been started. I guess I could define them as placeholders but it s a bit weird to have to feed these values for every computation... About the problem with model saving I was creating a tf.train.Saver at saving time which was causing the problem consistent with your explanation. It all works fine if I define it when I create the graph so thanks a lot I have a change coming soon that should fix this. Thanks mrry and asimshankar for flagging 
10226,Inconsistent Tensor Initialization on Multiple GPUs, The problem bug I m having trouble getting consistent initialization of variables across multiple GPUs and it appears to be a bug. Below is a test that replicates the bug. Basically the test just sets up a tensor on each GPU and an initializer for each. After running initialization and grabbing the initialized tensors they do not match despite the same initializer configurations. The problem exists across multiple platforms any number of GPUs 1 and multiple TF versions. Gory details The inconsistency is non deterministic occurring in about 50 of runs with 2 GPUs. Roughly 0.0002 of matrix values do not match. The matrix indices that do not match have significantly different values i.e. greater than FP rounding errors . Assuming row major tensor storage the incorrect indices are in contiguous groups of 4 floats 16B and the distance in memory addresses between these groups is consistent but platform dependent e.g. 512kB stride between groups on GTX980 vs. 480kB between groups on K40m I have written custom code System information OS Platform and Distribution Linux Ubuntu 14.04.2 TensorFlow installed from source TensorFlow version 1.0.1 tf.GIT VERSION b v1.0.1 0 ge895d5c tf.COMPILER VERSION b v1.0.1 0 ge895d5c protobuf 3.1.0 and 1.1.0 rc2 tf.GIT VERSION b v1.1.0 rc2 1164 g1d993dd tf.COMPILER VERSION b v1.1.0 rc2 1164 g1d993dd protobuf 3.3.0 Bazel version 0.45 Numpy version 1.12.1 CUDA cuDNN version cuda 8.0 cudnn 6 GPU model and memory GeForce GTX 980 TITAN X Maxwell Tesla K40m Tesla M40 24GB ,I have traced this bug back to a race condition in the PhiloxRandom GPU code. Taking a stab at a fix and will post a PR after testing. Thanks for looking into it looking forward to a PR. FYI ekelsen Created PR here that fixes the race condition 10298 Thanks for creating the fix jthestness 
10369,Deadlock in MapDataset, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow no OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 14.04 TensorFlow installed from source or binary source TensorFlow version use command below 95d90ab2e0994127ffc42b80e16a3f532895cf6d Bazel version if compiling from source 0.5.0 CUDA cuDNN version None GPU model and memory None Exact command to reproduce python map dataset op test.py Describe the problem The process hangs forever Source code logs Below is from map dataset op test.py Tests for the experimental input pipeline ops. from future import absolute import from future import division from future import print function import numpy as np from tensorflow.contrib.data.python.ops import dataset ops from tensorflow.python.framework import constant op from tensorflow.python.framework import dtypes from tensorflow.python.framework import errors from tensorflow.python.ops import array ops from tensorflow.python.ops import data flow ops from tensorflow.python.ops import lookup ops from tensorflow.python.ops import math ops from tensorflow.python.ops import random ops from tensorflow.python.ops import string ops from tensorflow.python.ops import variable scope from tensorflow.python.platform import test class MapDatasetTest test.TestCase def buildParallelMapDataset self components count num threads output buffer size def map fn x y z return math ops.square x math ops.square y math ops.square z return dataset ops.Dataset.from tensor slices components .map map fn num threads num threads output buffer size output buffer size .repeat count def testParallelMapDataset self Test an dataset that maps a TF function across its input elements. The pipeline is TensorSliceDataset ParallelMapDataset square 3 RepeatDataset count . components np.arange 7 np.array 1 2 3 np.arange 7 np.newaxis np.array 37.0 np.arange 7 count array ops.placeholder dtypes.int64 shape num threads array ops.placeholder dtypes.int32 shape output buffer size array ops.placeholder dtypes.int64 shape dataset self. buildParallelMapDataset components count num threads output buffer size iterator dataset.make initializable iterator init op iterator.initializer get next iterator.get next self.assertEqual c.shape 1 for c in components t.shape for t in get next with self.test session as sess def do test num threads val output buffer size val Test single threaded access to the iterator. sess.run init op feed dict count 14 num threads num threads val output buffer size output buffer size val for in range 14 for i in range 7 result sess.run get next for component result component in zip components result self.assertAllEqual component i 2 result component with self.assertRaises errors.OutOfRangeError sess.run get next Test multi threaded access to the same iterator. sess.run init op feed dict count 18 num threads num threads val output buffer size output buffer size val results def iterator thread while True try results.append sess.run get next except errors.OutOfRangeError return threads self.checkedThread target iterator thread for in range 8 for t in threads t.start for t in threads t.join results will contain the same elements components 2 repeated 18 times but in a non deterministic order. Sort the results and assert that each element of components 2 is produced 18 times. results.sort key lambda x x 0 for i in range 7 for j in range 18 for component result component in zip components results i 18 j self.assertAllEqual component i 2 result component for num threads val output buffer size val in 1 1 1 2 2 2 2 4 8 8 8 16 do test num threads val output buffer size val if name main test.main bt.txt https github.com tensorflow tensorflow files 1044353 s.txt , gdb thr 14 gdb f2 gdb p mutex 2 data lock 2 count 0 owner 28288 nusers 1 kind 0 spins 0 elision 0 list prev 0x0 next 0x0 Thread 12 is waiting on a mutex which is owned by LWP 28288. Thread 12 Thread 12 is waiting the output buffer to be non empty. snnn Could you reduce your sampler to something simpler Among other things seems like you re running multiple groups of threads if you can reduce it to the smallest example that demonstrates the failure that will be helpful CC mrry This is definitely a real bug which I suspect arises because you have 8 or fewer cores on the machine where you re running the test. The issue is that the current implementation of the IteratorGetNext op is a synchronous OpKernel but it can block an inter op threadpool thread and the unblocking action may require the use of another inter op threadpool thread. The default threadpool size is the number of cores in your machine. I m working on a fix but there are two short term workarounds Increase the size of the inter op threadpool when you create the session using tf.ConfigProto . Setting it to maximum number of concurrent get next ops 1 i.e. 9 in this case should address the deadlock. Reduce the number of concurrent get next calls to number of cores 1. The true fix will involve rewriting IteratorGetNext as an AsyncOpKernel which I m working on now.... 
10519,tf.contrib.data tf slim training pipeline gets stuck, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux leto28 3.16.0 4 amd64 1 SMP Debian 3.16.39 1 deb8u2 2017 03 07 x86 64 GNU Linux VERSION ID 8 VERSION 8 jessie TensorFlow installed from source or binary Binary TensorFlow version use command below tf.VERSION 1.2.0 rc2 tf.GIT VERSION v1.2.0 rc1 24 gce1d6ec tf.COMPILER VERSION v1.2.0 rc1 24 gce1d6ec Bazel version if compiling from source None CUDA cuDNN version 8.0 5.1 GPU model and memory TITAN X Pascal 12189MiB Exact command to reproduce python . mwe.py Describe the problem I recently ported my dataset handling to the new dataset API from tf.contrib.data . Now it seems that the tf slim training pipeline stalls if I request just 1 or 2 CPUs for my job it used to work just fine with the dataset API provided by tf slim . I does work if I grab 4 CPUs. I tried to come up with a MWE see below . The interesting thing is that it is not getting stuck if I remove one of the tf.summary.scalar s or .map at line 39. I suspect this issue is related to 10369. Source code logs ,Thanks for reporting this... it definitely looks like a bug. I think I ve tracked it down to the OneShotIterator op which internally blocks on this line while a function executes to build the dataset https github.com tensorflow tensorflow blob 91cb809bd6bc885458c583d46f4a322c30fa12cf tensorflow core kernels iterator ops.cc L248 That will block one of the inter op thread pool threads for the typically short execution time of the dataset construction function. The number of CPUs determines the default number of threads in that thread pool when you have only 1 CPU the system will deadlock as soon as you hit that line because no more thread pool threads are available to run the function that will unblock it . When you have 2 CPUs it can work but slim.learning.train uses tf.train.Supervisor which asynchronously runs a background thread... that runs the same OneShotIterator op. To ensure that the op only initializes once the initialization runs under a lock acquired here https github.com tensorflow tensorflow blob 91cb809bd6bc885458c583d46f4a322c30fa12cf tensorflow core kernels iterator ops.cc L194 The problem is probably starting to become clear two concurrent executions of the same OneShotIterator kernel will potentially block two inter op thread pool threads leading to deadlock in a 2 CPU system because there are no more threads available to run the function that will unblock them. Anyway mea culpa and thanks again for finding the bug. I ll be working on a fix although it might not make it into the final 1.2 release. In the mean time there are a couple of workarounds 1. Increase the number of threads to more than 2 in the inter op thread pool. You can do this by passing session config tf.ConfigProto inter op parallelism threads 3 to slim.learning.train . 2. Use dataset.make initializable iterator instead of dataset.make one shot iterator . This comes with the additional requirement that you have to run iterator.initializer which is not completely trivial with slim.learning.train because you don t have access to the tf.Session . One possibility is to pass local init op tf.group tf.local variables initializer tf.tables initializer iterator.initializer to slim.learning.train . Wow thanks for the swift response The first workaround seems to be working perfectly. On a side note I m quite happy with the tf.contrib.data . My code has become way cleaner. 
10954,Supervisor SummaryWriter and Saver stop after some time, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04 anaconda3 python 3.6 TensorFlow installed from source or binary binary pip install tensorflow gpu TensorFlow version use command below 1.2.0 Bazel version if compiling from source CUDA cuDNN version 5.1 GPU model and memory 1080 Titan X K80 Exact command to reproduce System information tf env.txt https github.com tensorflow tensorflow files 1091842 tf env.txt Describe the problem Hey guys I get the problem on 3 different machines all on ubuntu 16.04 and the tensorflow 1.2 . All experiments were executed on a single machine with a single GPU. To initialize a session I use a tf.Supervisor and supervisor.managed sessions with the default summary writer and saver . It works all well for up to 30mins 1h30mis. But after that time the summary writer stops to write events and the saver also stops to save the model parameters. However the model still runs and produces valid outputs. I also checked the python log for tensorflow with level DEBUG . But all I got was some information logs until it suddenly stops see below . Is there a way to track the SVSummaryThread SVTimerCheckpointThread Thanks in advance and keep up the good work Cheers Source code logs tensorflow python log ,It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. It didn t happen wit tf 1.4 anymore. So I ll close it. Thanks 
11132,Go SIGABRT when executing the same node more than once, Problem In Go when we pass the same node to the fetches list more then once SIGABRT is raised. Source code logs Here s the output The same logic in python works without any issue outputs as expected. System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Archlinux TensorFlow installed from source or binary source TensorFlow version use command below 1.2.0 Bazel version if compiling from source 0.5.1 CUDA cuDNN version cuda 8 cudnn 5.1 GPU model and memory GeForce GTX 1080 Exact command to reproduce go test ,Thanks for the report it certainly shouldn t fail with a SIGABRT. That said though am curious about the use case for fetching the same value multiple times . I was showing that in tfgo https github.com galeone tfgo when one assigns a go variable to another go variable it needs to clone it before the assignment in order to create a different and new node in the graph. Otherwise the assignment only exists in Go but the underlying reference points to the same node in the graph. In short I was showing how to use tf.assing and not the assignment operator of the lanuage used . To empathize this I d like to show that those 2 Go variables when evaluated contain the same value. But I can t because of that bug thus I fallback showing it in another way. The first example here https github.com galeone tfgo getting started 
11725,tf cnn benchmarks.py stuck when running with multiple GPUs and ImageNet data with protocol grpc verbs, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No running tf cnn benchmarks.py from benchmarks repo OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04.2 LTS TensorFlow installed from source or binary Unmodified source with RDMA Verbs enabled TensorFlow version use command below 1.3.0 rc0 Python version 2.7.12 Bazel version if compiling from source 0.5.1 CUDA cuDNN version 8.0 6 GPU model and memory NVIDIA Tesla P100 PCIe 16GB 8 per node Exact command to reproduce PS CUDA VISIBLE DEVICES python tf cnn benchmarks.py ps hosts 12.12.12.43 20000 worker hosts 12.12.12.44 20000 12.12.12.41 20000 batch size 64 model inception3 variable update parameter server local parameter device cpu job name ps task index 0 server protocol grpc verbs Worker0 CUDA VISIBLE DEVICES 0 1 2 3 4 5 6 7 python tf cnn benchmarks.py ps hosts 12.12.12.43 20000 worker hosts 12.12.12.44 20000 12.12.12.41 20000 batch size 64 model inception3 variable update parameter server local parameter device cpu job name worker task index 0 num gpus 8 data dir data imagenet data train dir data imagenet train server protocol grpc verbs Worker1 CUDA VISIBLE DEVICES 0 1 2 3 4 5 6 7 python tf cnn benchmarks.py ps hosts 12.12.12.43 20000 worker hosts 12.12.12.44 20000 12.12.12.41 20000 batch size 64 model inception3 variable update parameter server local parameter device cpu job name worker task index 1 num gpus 8 data dir data imagenet data train dir data imagenet train server protocol grpc verbs RDMA driver version MLNX OFED LINUX 4.1 1.0.2.0 Describe the problem When running the above commands Inception V3 synchronized data parallelism training with 2 workers and 1 external ps the tf cnn benchmarks application hangs forever after some iterations usually in warm up . It happens only when real data is involved ImageNet and with 4 GPUs. More GPUs less iterations before it hangs . Doesn t happen with grpc protocol or when running with synthetic data. The master service in the workers is stuck here https github.com tensorflow tensorflow blob master tensorflow core distributed runtime master session.cc L608 which I guess means some operations in the computation have not been completed. The RDMA protocol looks valid and clean all messages corresponds to the protocol see below logs . There some tensors requested by the workers which they don t receive but they are passed by the RDMA Verbs transport to the BaseRendezvoudMgr with RecvLocalAsync in a valid way and for some reason the higher level worker service doesn t trigger the Send kernel on those tensors. Any help is much appreciated If there are some debug mechanisms I can use to understand which tensors operations have not been completed it can greatly help. I was mostly debugging this from the RDMA Verbs layer till now without much success and I feel I don t have enough information there to understand what s missing. Also I feel we don t have enough knowledge on how the step id acts diving into this in the code now but there s some higher level documentation it can greatly help . My initial guess was an occurrence of a racy condition when loading the data since it creates a gap in execution time worker0 starts the first training step 30 60 seconds after worker1 since it does the preprocessing of the data twice for a reason I couldn t understand yet but after the first iteration which usually passes successfully the time is synchronized between workers. Source code logs Those are the logs of the runtime after moving the logging in rdma.cc https github.com tensorflow tensorflow blob master tensorflow contrib verbs rdma.cc to VLOG 0 also adding Tensor name and step id for all cases in some cases the step id doesn t mean anything like BUFFER REQUEST RESPONSE for example and also some VLOG in master session.cc https github.com tensorflow tensorflow blob master tensorflow core distributed runtime master session.cc worker0 https gist.github.com shamoya 15a42f421e088473b8f02bf00c16d0fc worker1 https gist.github.com shamoya dd3126c02c73990a6e28b534d9a9ddf6 ps https gist.github.com shamoya 0c856365802ae4d42b38baf988149574 Unfortunately they are fairly large but it s better then to cut the log files IMO. Example for analysis I did in the verbs layer comparing the Sent Tensor requests to the actual received tensors writes in both workers worker 0 job ps replica 0 task 0 cpu 0 f3c10d28b54074c0 job worker replica 0 task 0 gpu 0 edge 116943 group deps 2 NoOp 1 0 0 80661058974090965 job worker replica 0 task 1 cpu 0 1a50d5c51cd9c5d1 job worker replica 0 task 0 gpu 0 edge 116947 group deps 3 NoOp 1 0 0 80661058974090965 job worker replica 0 task 1 gpu 2 7f00fadabfe781f5 job worker replica 0 task 0 gpu 0 edge 111078 group deps 1 NoOp 2 0 0 80661058974090965 job worker replica 0 task 1 gpu 4 b07185dd19f62088 job worker replica 0 task 0 gpu 0 edge 111080 group deps 1 NoOp 4 0 0 80661058974090965 worker 1 job ps replica 0 task 0 cpu 0 f3c10d28b54074c0 job worker replica 0 task 1 cpu 0 edge 155113 AssignAdd 0 0 80661058974090965 job worker replica 0 task 0 gpu 0 f3df8abf03739fe8 job worker replica 0 task 1 cpu 0 edge 116948 group deps 3 0 0 80661058974090965 The tensors requests received well by the other side and passed to RecvLocalAsync but are not called later. Thanks a lot.,I was able to reproduce the issue. I also tried alexnet it hung as well. I will take a close look in the coming days. Thanks for reporting. Thank you for looking into this junshi15 mrry jhseu poxvoculi just a small question. I m trying to understand why execution hangs in here https github.com tensorflow tensorflow blob master tensorflow core distributed runtime master session.cc L608 . What I think I need is to get the Executor s which doesn t end in here https github.com tensorflow tensorflow blob master tensorflow core distributed runtime graph mgr.cc L541 . I tried passing tf.RunOptions trace level tf.RunOptions.FULL TRACE to sess.run but then I get a huge file I can t really understand. Is this the right way I have met stuck problem I think it may happen in warm up if I m right. when all workers finished all steps. I think global step should be protected by a lock.I raised an issue https github.com tensorflow benchmarks issues 38 . I m not sure if tensorflow has some special code for global step . When I m trying to debug a hung concurrent program the first step is usually to try to find a small case that exhibits the problem i.e. try to find the least number of workers and GPUs that still hangs. It s much easier to debug a small case than a large one. Then if it s a really small case you might be able to set the logging level high and read all the log files. Usually though I form some hypotheses about where the problem might be e.g. a missing lock a race that might result in deadlock logic error that always deadlocks and start putting in LOG INFO statements around the suspicious points to confirm or refute each hypothesis. On Tue Jul 25 2017 at 4 53 AM Ziming Dong notifications github.com wrote I think global step should be protected by a lock.I raised an issue https github.com tensorflow benchmarks issues 38 . I m not sure if tensorflow has some special code for global step. You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 11725 issuecomment 317713980 or mute the thread https github.com notifications unsubscribe auth AO818aHj9Ak6IS6emES wdAhL n1M4oUks5sRdcmgaJpZM4OhqyQ . tfboyd I m unassigning myself because I know very little about the RPC layer and know nothing about VERBs. I don t think this is a bug with tf cnn benchmarks because 11416 also has the same issue with a different model. suiyuan2009 I don t think tensorflow benchmarks 38 is the same issue since this issue only occurs with verbs and it occurs with another model with verbs. I ll take a look at tensorflow benchmarks 38 though. Thanks poxvoculi Sadly the smallest case which repro the issue is 3 nodes 2 workers 1 ps with InveptionV3 and ImageNet with the tf cnn benchmark.py haven t checked others . I had 15 hypotheses which turn out to be false. Added a lot of prints but still couldn t understand which tensor is the rebel. I feel the trace log mechanism tf.RunOptions trace level tf.RunOptions.FULL TRACE can help me here but I have no idea how to parse it. Is there a parsing tool or format for it after adding a lock to global step I didn t meet stuck problem. but I got error when running vgg benchmark code maybe my network environment is a little unstable. suiyuan2009 Your error might be caused by another process PS or worker in your job. We have met the same problem and it turns out when one process terminated the connection its peers will print out such odd error log. shamoya For a simpler model you could try model trivial the default one and set your number of batches to a smaller value. Sorry that I am busying working on migrating my own patch and have little time to reproduce this particular bug right now. shamoya Can you try what suiyuan2009 and byronyi suggested. 1 try the simple model to see if it hangs model trivial . 2 With inception3 add a lock around global step. something like some where in the init function you define the lock self.lock threading.Lock I was unable to get an infiniband setup yesterday. I will try again today. shamoya The tracing data is available in a pre parsed protobuf format. See the run metadata option to Session.run which returns tracing data in StepStats. junshi15 shamoya note that global step is also a part of computing graph assign add op has a use locking arg. byronyi you re the best it s reproduced on trivial case with ImageNet data . Now it will be much easier to debug going full VLOG now let s hope I ll have update soon. suiyuan2009 it s not related to the global step. I changed to have only the chief increment it and it doesn t have any affect. byronyi do your GDR patch have met same problem like stuck or failure No I have not met such kind of problems at least not during our internal testing. But my patch uses vastly different design compared to current verbs implementation so there is little I can tell about this particular issue. My observation agrees with shamoya s. 1 the program got stuck with the trivial model but it needed 8 GPUs per worker. Even 7 GPUs do not result in hang after running 1000 iterations . 2 the program got stuck either at warm up or early iterations 100 . 3 locking global step does not help. 4 At the very beginning some tensors changed size as shown below. This may not be an issue but it will be good to find out why. katyakats bkovalev Ok after reviewing the full logs this is what we think is the root cause A single GPU in worker1 doesn t complete loading of the model parameters from the CPU. For this GPU we don t see Async kernel done for the SEND RECV operation CPU 0 GPU x locally . The reason why it happens only with RDMA and not with gRPC is not known yet. Thought about possible interrupts issue due to excessive interrupts 8 GPUs NIC NVMe drive which holds the ImageNet data all on the same PCIe bus . however polling mode of Process CQ no interrupts from the NIC at all didn t resolve the issue. This https gist.github.com shamoya 824d452be527d95902f20b59f868b391 is the problematic GPU relevant logs. This is what I get from grepping one of the model parameters tensors in the problematic GPU log as above VS one of the other valid GPUs idos MTR IDOS cat gpu2.log grep affine1 biases 2017 07 27 16 04 48.740960 I tensorflow core common runtime executor.cc 1558 Process node 8 step 286 v affine1 biases read G109 Recv client terminated false recv device job worker replica 0 task 1 gpu 2 send device job worker replica 0 task 1 cpu 0 send device incarnation 5325932350616196133 tensor name edge 609 v affine1 biases read tensor type DT FLOAT device job worker replica 0 task 1 gpu 2 is dead 0 2017 07 27 16 04 48.748463 I tensorflow core common runtime executor.cc 1558 Process node 35506 step 286 v affine1 biases read G108 Send T DT FLOAT client terminated false recv device job worker replica 0 task 1 gpu 2 send device job worker replica 0 task 1 cpu 0 send device incarnation 5325932350616196133 tensor name edge 609 v affine1 biases read device job worker replica 0 task 1 cpu 0 v affine1 biases read is dead 0 idos MTR IDOS cat gpu4.log grep affine1 biases 2017 07 27 16 04 48.740332 I tensorflow core common runtime executor.cc 1558 Process node 8 step 286 v affine1 biases read G105 Recv client terminated false recv device job worker replica 0 task 1 gpu 4 send device job worker replica 0 task 1 cpu 0 send device incarnation 5325932350616196133 tensor name edge 605 v affine1 biases read tensor type DT FLOAT device job worker replica 0 task 1 gpu 4 is dead 0 2017 07 27 16 04 48.748482 I tensorflow core common runtime executor.cc 1558 Process node 35508 step 286 v affine1 biases read G104 Send T DT FLOAT client terminated false recv device job worker replica 0 task 1 gpu 4 send device job worker replica 0 task 1 cpu 0 send device incarnation 5325932350616196133 tensor name edge 605 v affine1 biases read device job worker replica 0 task 1 cpu 0 v affine1 biases read is dead 0 2017 07 27 16 04 48.764311 I tensorflow core common runtime executor.cc 1612 0x7f1fc4e67ab0 Async kernel done v affine1 biases read G105 Recv client terminated false recv device job worker replica 0 task 1 gpu 4 send device job worker replica 0 task 1 cpu 0 send device incarnation 5325932350616196133 tensor name edge 605 v affine1 biases read tensor type DT FLOAT device job worker replica 0 task 1 gpu 4 2017 07 27 16 04 48.765221 I tensorflow core common runtime executor.cc 1558 Process node 69 step 286 v 4 tower 4 gradients v 4 tower 4 L2Loss 3 grad mul Mul T DT FLOAT device job worker replica 0 task 1 gpu 4 v affine1 biase read G105 v 4 tower 4 gradients v 4 tower 4 mul 1 grad Reshape 1 is dead 0 2017 07 27 16 04 48.765199 I tensorflow core common runtime executor.cc 1558 Process node 9 step 286 v 4 tower 4 L2Loss 3 L2Loss T DT FLOAT device job worker replica 0 task 1 gpu 4 v affine1 biases read G105 is dead 0 2017 07 27 16 04 48.765338 I tensorflow core common runtime executor.cc 1558 Process node 38 step 286 v 4 tower 4 affine1 add Add T DT FLOAT device job worker replica 0 task 1 gpu 4 v 4 tower 4 affine1 MatMul v affine1 biase read G105 is dead 0 Just a wild guess if the callback passed into the local async recv kernel is an erroneous remote send would it appear to be stuck at the local recv What s the downstream operator of that local recv shamoya intriguing discovery. Local tensor transfer is handled by BaseRemoteRendezvous. There is one difference between gRPC and RDMA tolerate dup recv is set to false in the former https github.com tensorflow tensorflow blob r1.3 tensorflow core distributed runtime rpc rpc rendezvous mgr.cc L42 and true in the latter https github.com tensorflow tensorflow blob r1.3 tensorflow contrib verbs rdma rendezvous mgr.cc L34 . The reason gRPC only receives a tensor once with verbs we have multiple receive attempts since the tx rx buffer may not be ready at early attempts. I ran some experiments by changing that to true in gPRC model no hang was observed. However this patch https github.com tensorflow tensorflow commit cbfd50ff0f01e1825922230a8bc6e5766da98dd7 diff b9ae16e68ba80801fe243bb5e19bac51 totally broke the verbs code. I had to raise an issue here https github.com tensorflow tensorflow issues 11825 . Something to worry about after this debug. A correction to what I said early I finally saw hang with 7 GPUs per worker after 2000 steps. Thanks junshi15 for noticing this commit. Not sure I understand why you say BaseRemoteRendezvous is used in the case of local tensor transfer between GPU and CPU on the same worker. This https gist.github.com shamoya 36beb1d093d4b95a523e27d4deda16ea is a more detailed log of the all the occurences of affine1 biases in the last iteration in worker1. We can see the RdmaRemoteRendezvous has completed successfully from the PS. I m not so sure anymore this issue is even related to the Verbs code. shamoya RdmaRemoteRendezvous is a derived class of BaseRemoteRendezvous. The local send receive functions are defined in the base class. RdmaRemoteRendezvous only overrides RecvFromRemoteAsync . I noticed the date of that commit is 6 days ago but I remember if correctly the issue came out earlier than that. Please ignore me if I m wrong. Maybe you guys could revert that commit locally first and test if the issue could still be reproduced. That patch is unrelated to this bug. I am testing code prior to that. With that patch the RDMA path won t run and you will get an immediate crash. I digged a little deeper into the bug. One of the NoOps from a single GPU is never sent the thread is stuck in CopyGPUTensorToCPUSync method . It appears that this method is not called from the grpc but only from the verbs code. Reverting commit 626d8d905aa412aaca02d171e5e0b4a1c407656b Improve RDMA rendezvous speed that made changes in this code area did not resolve the issue. Does anyone have any ideas CopyGPUTensorToCPUSync is a synchronous wrapper for GPUUtil CopyGPUTensorToCPU. It was stuck only because CopyGPUTensorToCPU did not complete or did not send notification. But grpc uses CopyGPUTensorToCPU not sure why this is not an issue. thanks junshi15 Do you have any thoughts on why CopyGPUTensorToCPU does not complete or send notification I am looking at CopyGPUTensorToCPU https github.com tensorflow tensorflow blob r1.2 tensorflow core common runtime gpu gpu util.cc L304 L347 . It eventually calls ThenMemcpy https github.com tensorflow tensorflow blob r1.2 tensorflow stream executor stream.h L1492 L1497 . The comment says the host memory needs to be registered. I am not familiar with this piece of code but this is not done in the verbs code. If it is indeed required then it is what s missing. Did you check for the 3 odd edge cases mentioned here https github.com tensorflow tensorflow blob master tensorflow core distributed runtime rpc grpc worker service.cc L338 Just a wild guess if it is a NoOps it might be related to the is dead parameter or zero sized tensors. byronyi I don t see all 3 odd cases are checked in your link I don t see zero size and is dead is checked . Equivalent code in verbs is here https github.com tensorflow tensorflow blob r1.2 tensorflow contrib verbs rdma.cc L707 L719 pretty much follows that link. is dead is fed to SetProtoFromGPUSync but zero size is not checked in both cases. 
11742,Docker Restoring from a model outside the container returns FailedPreconditionError, System information OS Platform and Distribution Docker Tensorflow CPU Image TensorFlow version use command below 1.2.1 Python version Python 2.7.12 Source code logs The Issue I m binding a host models directory to data models inside the container. When trying to restore I got a Failed precondition Everything s okay when I m not using Docker Everything s okay when I m building a Docker Image with the models already copied into it but not flexible , ilchemla what are the versions of your tensorflow inside and outside the docker container caisq Same tensorflow 1.2.1 How are you mounting with v on the docker run Are they read only If you copy the checkpoint into the docker cotnainer rather than v mounted directory does it work slightly different than just pre loading it into the image Yes I m using v argument docker run ti v Users work data models data models v Users work tensorflow example opt tensorflow example tensorflow tensorflow python u opt tensorflow example train.py model data models OSVOS parent.ckpt 50000 The issue doesn t reproduce 100 but for most call it happen. EDIT Found that the issue happen only when I was mounting a directory located on an external hardisk. Closing the issue. Thanks for the support 
11882,conv2d transpose produce different results on GPU, System information cat etc issue Linux ST 4.2.0 42 generic 49 14.04.1 Ubuntu SMP Wed Jun 29 20 22 11 UTC 2016 x86 64 x86 64 x86 64 GNU Linux VERSION 14.04.4 LTS Trusty Tahr VERSION ID 14.04 are we in docker No compiler c Ubuntu 4.9.4 2ubuntu1 14.04.1 4.9.4 Copyright C 2015 Free Software Foundation Inc. This is free software see the source for copying conditions. There is NO warranty not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. uname a Linux ST 4.2.0 42 generic 49 14.04.1 Ubuntu SMP Wed Jun 29 20 22 11 UTC 2016 x86 64 x86 64 x86 64 GNU Linux check pips msgpack numpy 0.4.1 numpy 1.13.1 protobuf 3.2.0 tensorflow 0.10.0 tensorflow gpu 1.0.0 check for virtualenv False tensorflow import tf.VERSION 1.0.0 tf.GIT VERSION v1.0.0 rc2 15 g47bba63 dirty tf.COMPILER VERSION v1.0.0 rc2 15 g47bba63 dirty Sanity check array 1 dtype int32 I tensorflow stream executor dso loader.cc 135 successfully opened CUDA library libcublas.so.8.0 locally I tensorflow stream executor dso loader.cc 135 successfully opened CUDA library libcudnn.so.5 locally I tensorflow stream executor dso loader.cc 135 successfully opened CUDA library libcufft.so.8.0 locally I tensorflow stream executor dso loader.cc 135 successfully opened CUDA library libcuda.so.1 locally I tensorflow stream executor dso loader.cc 135 successfully opened CUDA library libcurand.so.8.0 locally env LD LIBRARY PATH home abc torch install lib usr lib x86 64 linux gnu home abc torch install lib usr local cuda lib64 usr local cuda lib64 home abc torch install lib home abc code torch torch install lib usr local cuda lib64 usr local computecpp lib data software gurobi652 linux64 lib DYLD LIBRARY PATH home abc torch install lib home abc torch install lib home abc code torch torch install lib nvidia smi Sun Jul 30 17 45 45 2017 NVIDIA SMI 367.48 Driver Version 367.48 GPU Name Persistence M Bus Id Disp.A Volatile Uncorr. ECC Fan Temp Perf Pwr Usage Cap Memory Usage GPU Util Compute M. 0 GeForce GTX 1080 Off 0000 01 00.0 On N A 0 53C P2 47W 260W 7909MiB 8112MiB 0 Default Processes GPU Memory GPU PID Type Process name Usage 0 1308 G usr bin X 357MiB 0 2590 G compiz 229MiB 0 3254 G ...el token CBAE43C38254E155E78826C3F38F0092 99MiB 0 9480 C python 1039MiB 0 10432 C usr bin python 5895MiB 0 20408 C usr bin python 283MiB 0 28024 G usr local MATLAB R2015a bin glnxa64 MATLAB 2MiB cuda libs usr local lib python2.7 dist packages torch lib libcudart.so.8.0 usr local lib python2.7 dist packages torch lib libcudart.so usr local cuda 8.0 lib64 libcudart.so.8.0.44 usr local cuda 8.0 lib64 libcudart static.a usr local cuda 8.0 doc man man7 libcudart.7 usr local cuda 8.0 doc man man7 libcudart.so.7 usr local MATLAB R2017a bin glnxa64 libcudart.so.8.0.44 usr local MATLAB R2015a bin glnxa64 libcudart.so.6.5.14 Describe the problem I am trying to use tf.nn.conv2d transpose but it produces different results every time on GPU. However the result would be the same when switching the device to CPU. It seems like a bug. Please check the toy model below for more details. Source code logs Out 2 False , zheng xq Do you have any thoughts on this. I made a slightly more elaborate test case that computes relative error between the gpu and cpu version and two runs of the cpu version. jnjaby s fear that gpu is not deterministic seems due to non determinism in reduction probably because I only see 1e 5 error between two runs on the gpu. But I see a 5770. relative error between the gpu vs cpu. Could you please take a look aselle because the weight initialized by CPU and GPU are different. I try to initialize weights with another tf.constant . That really confuses. I would agree with first making the initialization the same. One common way is to random initialize the weights in numpy and use that in both paths. This version shows no relative error between any of the implementations so I am thinking there is no bug aselle The last case is consistent. I make another test case output True and False respectively showing different result between tf.constant and tf.Variables . Commonly however the weights should remain the same after initialization. So let s clarify the questions Where is the non determinism produced Tensorflow or other libraries of GPU What is the difference between constant Tensor and variable Tensor zheng xq are you able to answer jnjaby s questions above Thanks There are some known differences between CPU and GPU convolutions. Yang is working with Benoit to resolve them. Any updates here I keep having to switch back and forth between an old version of TensorFlow and the current one. Is addressing this a priority for the team Please refer to https github.com tensorflow tensorflow issues 14601 issuecomment 361734250 for updates. Once we are using the updated version of eigen you can verify if the bug is fixed. I could be wrong but I as of today I don t think this issue has been addressed in the nightly wheel available over here http ci.tensorflow.org view tf nightly job tf nightly mac TF BUILD IS OPT OPT TF BUILD IS PIP PIP TF BUILD PYTHON VERSION PYTHON3 label mac slave lastSuccessfulBuild artifact pip test whl tf nightly 1.head py3 none any.whl Sorry for bothering you but are there any updates now Thanks very much yzhwang benoitsteiner Please check with the latest version of TensorFlow. Feel free to reopen if the issues still persists. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 11882 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 11882 No a 
11912,A potential null pointer deference bug in GraphProperties class,Dear developers I am studying the code of Tensorflow and found a potential null pointer dereference bug in tensorflow core grappler costs graph properties.cc The problem The function updateEnter at line 70 of tensorflow core grappler costs graph properties.cc calls the getContext function of the ShapeRefiner class which could return a null pointer line 79 of tensorflow core common runtime shape refiner.h and get stored in the variable enter ctx . At line 73 of the updateEnter function enter ctx is dereferenced in the for loop condition without being checked against null. If the null pointer dereference is triggered the program might crash. I noticed that for another call of the getContext function at line 255 of tensorflow core grappler costs graph properties.cc the function return value which gets stored in the variable qctx is checked against null indicating that getContext can indeed return null pointer. Source code I am analyzing the latest version of Tensorflow as of July 31 2017 and the two relevant files are 1 https github.com tensorflow tensorflow blob master tensorflow core grappler costs graph properties.cc commit 4432623 on 1 July 2 https github.com tensorflow tensorflow blob master tensorflow core common runtime shape refiner.h commit e85d3df on 30 June Hope my report helps ,Do you have a sense of how to fix it If so we d love if you could take the time to submit a pull request. Thanks. benoitsteiner could you take a quick look as well. aselle Hi Andrew I would love to help but I am not sure if wrapping the for loop with a null checker would be correct since the logic of updateEnter seems complicated. For the other two call sites of getContext I found that simple null checkers worked lines 256 258 and 345 347 of tensorflow core grappler costs graph properties.cc at commit 4432623 . internal tracking bug opened It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. Nagging Assignee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. This was fixed last August. 
12817,LMDB reader Error in Reading Data of multi thread queue based input pipeline,Hi I am now using LMDB reader to read and decode my custom data into tensorflow training pipeline based on the example Here I designed my own decoder and used LMDB reader here. But the problem is that when the num epochs is not 1 there will be ERROR Could you help check this Thanks ,The detailed code is like There should never be a SEGV. I m guessing the problem is on the LMDB reader. Do you have a stack trace I guess the reason is that there is lock file when we open the LMDB database this will prevent us to read it again when one epoch is finished. I got some explanation from other questions like When opening a database file LMDB creates a lock file lock.mdb in the same folder to prevent that database being opened more than once. This unfortunately prevents us to enqueue the database file multiple times. But how can solve this I mean how to use LMDB reader to read database file more than one epoch in the above pipeline Have you considered whether using DataFlow from tensorpack though independent of tensorpack dependency wise would address this There s an LMDB example here http tensorpack.readthedocs.io en latest tutorial efficient dataflow.html sequential read quaeler sorry I did not try. I already convert my data to TFRecords which is bug free and can use the pipeline I used for LMDB. And I am waiting bug fixation for LMDB. Thanks You mean the LMDB reader lacks a close I don t know what s causing the problem but just want to mention that lmdb database can be opened without a lock when the transaction is read only. We do currently open it in read only mode https github.com tensorflow tensorflow blob master tensorflow core kernels lmdb reader op.cc L39 but the documentation for the LMDB API http www.lmdb.tech doc group mdb.html ga32a193c6bf4d7d5c5d579e71f22e9340 states WRT read only mode If we also specified the MDB NOLOCK flag on the open drpngx perhaps this problem would go away i m unaware of other ramifications of not locking on the read only read. Thanks for reporting the bug. I just saw this issue today. This PR 13396 should fix it. Sorry for the bug. Hi all I was looking use my data which is present in LMDB form but i am unable to find any sample code or more usage documentation on using LMDB dataset in Tensorflow. If someone has tried LMDB could share the usage experience .it would be very helpful. A simple example to read a LMDB file and print the key value pair would be helpful. Thanks vishalghor It s pretty much the same as other readers like TFRecordReader. For sample code you may take a look at the test case. https github.com tensorflow tensorflow blob master tensorflow python kernel tests reader ops test.py L998 bowang thank you for fast reply.I was able to read an LMDB file and print the values by assigning the tensor key and value to normal variables. However Is there any sample implementation of LMDB dataset for any opensource models such as inception v3 or an implementation of LMDB dataset for the retraining inception v3 tutorial by tensorflow. It would be very helpful if a sample of usage in training a models is provided. Thanks and Regards Hi all I was able to read LMDB database using the following code But while researching more on LMDBReader i found tf.LMDBReader also. So what is the difference between io ps.LMDBReader and tf.LMDBReader . The Tensorflow documentation speaks very minimal about various other forms of dataset format such as LMDB HDF5 compared to TFRecord. How such datasets be included in existing code Please help me in clearing the doubt due to lack of adequate documentation. Thanks and Regards vishalghor io ps.LMDBReader and tf.LMDBReader are the same. But the former is for internal use only. Thus I d recommend to use tf.LMDBReader. LMDBReader shares the same interface as TFRecordsReader. So you may follow the documentation of the latter. Once you get both key value out of LMDB it s up to you and the LMDB you are using to determine how to decode it. For example if it s an image you can pass the value to tf.image.decode jpeg bowang So you fixed the error of loading LMDB multi epoch Segment Error Enigma li Yes. But it hasn t been merged yet. It d be great if you can bring it to the notice of TensorFlow maintainers by commenting on it. 13396 bowang Sure already commented Closing as the PR is merged. 
12824,Apparent Segmentation Violation with Go API,Hi I am trying to build a project that will take multiple URLs download their images and then use TensorFlow and the InceptionV3 pre trained model to perform image recognition. For this I am using the Go API. Most of the time this process occurs without an issue but every now and again an error occurs. I think this may be caused by a bug in TensorFlow as it appears to being thrown from the c code. A sample of the code that I am using is below. In case it is relevant I run ProcessImage from different workers concurrently. This obviously means that the tfSession variable is shared between different threads. However I don t think this is the problem as I have tried having each worker create and use its own session and the error still occurs. I m using macOS Sierra but this error also exists when I compile the project for Linux. I have included both of these Go environments below. On Sierra I installed TensorFlow using Homebrew and on Linux I installed it using the instructions here https www.tensorflow.org install install go . The error I am getting is below. For your information image.go line 99 is the line that contains output err tfSession.Run in the ProcessImage function above. , asimshankar do you have any hints to offer about how to figure this out That is troubling and from a cursory look at the stacktrace this seems like a NULL pointer dereference somewhere inside the C code but I can t tell. Is there any way you can reduce this to a smaller sample of code that reproduces the problem This may not be a smaller sample of code but they are individual files that can be run although you may need to run them a number of times as the errors aren t produced every time. In the below gist there is a file named image.go and image2.go these files are the same apart from the fact that image shares tfSession between 10 workers whereas image2 creates a new session for each worker. Due to that the memory used when running image is 750MB whereas for image2 it is 5GB. To run these files they need to be in a directory where there is an image named image.png and where the pre trained InceptionV3 model is in a directory named inception3 . This can be found gzipped here https storage.googleapis.com download.tensorflow.org models inception v3 2016 08 28 frozen.pb.tar.gz The log files with 2 in their name correspond to image2 and those that do not correspond to image . tf error.log and tf error2.log are errors that are caused by running tfSession in the ImageWorker function whereas tf error alt.log and tf error2 alt.log are errors that are caused by creating or running the session in the MakeTensorFromImage function. https gist.github.com jamiebaggott f540c2fbeadd89219127a8e0669bd462 If you require any more information please let me know. It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. Nagging Awaiting TensorFlower It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. A member of the TensorFlow organization has replied after the stat awaiting tensorflower label was applied. Is that still happening for you If so could you build with ASAN to see if you get more information Nagging Awaiting Response It has been 14 days with no activityand the awaiting response label was assigned. Is this still an issue Nagging Awaiting Response It has been 14 days with no activityand the awaiting response label was assigned. Is this still an issue Nagging Awaiting Response It has been 14 days with no activityand the awaiting response label was assigned. Is this still an issue Automatically closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks 
13558,segfaults in GPU tf.matrix inverse,I m running into segfaults in tf.matrix inverse I m adding identity 0.001 so matrices should be invertible and same procedure works fine in numpy and in TensorFlow CPU version. https github.com yaroslavvb stuff blob master inverse segfault.py python inverse segfault.py This non deterministically crashes after 1 2 seconds with various backtraces. IE or this TensorFlow commit https github.com tensorflow tensorflow commit 22a886b NVIDIA SMI 381.09 libcudart.so.8.0.44 libcudnn.so.6.0.21 Nvidia GTX 1080 , rmlarsen Would you have some cycles to look into this It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. From the comment on the PR it looks like the mutex was added. 
13764,Failure in TestNewTensor when running go test, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow no OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 TensorFlow installed from source or binary source branch 1.4 TensorFlow version use command below 1.4.0 dev Python version 3.5 Bazel version if compiling from source 5.4.0 CUDA cuDNN version 8.0 6.0 GPU model and memory nVidia 1080Ti 11G Exact command to reproduce go test v github.com tensorflow tensorflow tensorflow go Describe the problem I m trying to use the go bindings to the tensorflow c library. When I run the tests I get a nil pointer dereference and a segfault. The details are below. Note that I ve built the c library from source using the following options bazel build c opt config cuda config mkl c opt copt mavx copt mavx2 copt mfma copt mfpmath both copt msse4.2 c opt cxxopt D GLIBCXX USE CXX11 ABI 0 tensorflow libtensorflow.so Source code logs When I run go test v github.com tensorflow tensorflow tensorflow go I get the following error Adding some debugging it turns out that the TestNewTensor test fails when attempting to create the following tensor int64 2 0 int64 . If I comment out that line the tests pass.,It looks like this is due to the fact that TF TensorData returns nil if no data is allocated. Assuming this is correct behavior and nil needs to be checked for on the go side then the following patch fixes the problem Thanks for the report vishvananda. I m unable to reproduce the problem using the 1.3.0 release binary https storage.googleapis.com tensorflow libtensorflow libtensorflow gpu linux x86 64 1.3.0.tar.gz or 1.4.0 rc0 release binary https storage.googleapis.com tensorflow libtensorflow libtensorflow gpu linux x86 64 1.3.0.tar.gz will try rebuilding from source using the exact flags you re using . Do you see the same problem when using the release binaries of the C API Which version of go are you using Also is it possible that LD LIBRARY PATH is somehow bringing in an older version of the C API libraries that your go program ends up using It should be okay for TF TensorData to return nil . Any additional information in reproducing the environment will be helpful. I ll try to dig into this a bit more by rebuilding from source using the command you provided above Fascinating both the 1.3.0 and the 1.4.0 rc0 release binary return zero length from TF TensorData but they return a pointer to an actual buffer instead of nil. I m attempting my flags on the 1.3 branch to see if it is the flags that are causing it to return nil. Next I ll try removing the extra flags one at a time to see if I can narrow it down. I suspect AllocateTensor ends up with a nil buffer in certain cases. In the successful versions I don t see this error message In any case the nil check is probably good to have anyway. Ok I think I ve tracked down the issue to building with MKL. If I build without config mkl the tests pass fine. The issue is that building with MKL uses the bfc allocator to allocate memory. That allocator explicitly returns nil when an allocation of zero bytes is performed As a side note this probably should not be an error or warning if we expect it to happen when we request a zero length tensor . In the case of running without MKL the allocation eventually calls malloc or jemalloc or alloc aligned . The man tells me that malloc 0 is implementation defined and our version returns a non nil pointer so the code does not throw a null pointer exception. I suggest using something like the patch I included above for the go side and maybe downgrading the error and warning messages in bfc allocator.cc and allocator retry.cc to something a bit less scary maybe Info . Thanks for the detailed trackdown vishvananda much appreciated. Yes what you said makes sense. For the Go side would you like to contribute a pull request to make the fix If not let me know and I m happy to make the change as well. Thanks 
14177,Getting wrong value with placeholder, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No I have used code which is on the link https www.tensorflow.org get started get started . OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 LTS TensorFlow installed from source or binary Installing with native pip tensorflow gpu TensorFlow version use command below 1.3.0 Python version Python 2.7.12 v1.3.0 rc2 20 g0787eee 1.3.0 Bazel version if compiling from source GCC Compiler version if compiling from source gcc Ubuntu 5.4.0 6ubuntu1 16.04.5 5.4.0 20160609 CUDA cuDNN version CUDA release 8.0 V8.0.61 cuDNN CUDNN MAJOR 6 GPU model and memory GeForce 940MX Memory 2GB Driver Version 384.90 Describe the problem When I run at below code I think i am getting the wrong result. Source code logs My Output 3.0 1. 3. I think the true result should be 7.5 3. 7. Logs Also when i type sess tf.Session i am getting that output 2017 11 02 12 09 04.184601 W tensorflow core platform cpu feature guard.cc 45 The TensorFlow library wasn t compiled to use SSE4.1 instructions but these are available on your machine and could speed up CPU computations. 2017 11 02 12 09 04.184689 W tensorflow core platform cpu feature guard.cc 45 The TensorFlow library wasn t compiled to use SSE4.2 instructions but these are available on your machine and could speed up CPU computations. 2017 11 02 12 09 04.184717 W tensorflow core platform cpu feature guard.cc 45 The TensorFlow library wasn t compiled to use AVX instructions but these are available on your machine and could speed up CPU computations. 2017 11 02 12 09 04.184753 W tensorflow core platform cpu feature guard.cc 45 The TensorFlow library wasn t compiled to use AVX2 instructions but these are available on your machine and could speed up CPU computations. 2017 11 02 12 09 04.184805 W tensorflow core platform cpu feature guard.cc 45 The TensorFlow library wasn t compiled to use FMA instructions but these are available on your machine and could speed up CPU computations. 2017 11 02 12 09 04.368504 I tensorflow stream executor cuda cuda gpu executor.cc 893 successful NUMA node read from SysFS had negative value 1 but there must be at least one NUMA node so returning NUMA node zero 2017 11 02 12 09 04.368855 I tensorflow core common runtime gpu gpu device.cc 955 Found device 0 with properties name GeForce 940MX major 5 minor 0 memoryClockRate GHz 1.2415 pciBusID 0000 01 00.0 Total memory 1.96GiB Free memory 1.53GiB 2017 11 02 12 09 04.368870 I tensorflow core common runtime gpu gpu device.cc 976 DMA 0 2017 11 02 12 09 04.368874 I tensorflow core common runtime gpu gpu device.cc 986 0 Y 2017 11 02 12 09 04.368882 I tensorflow core common runtime gpu gpu device.cc 1045 Creating TensorFlow device gpu 0 device 0 name GeForce 940MX pci bus id 0000 01 00.0 ,Hmm....I m unable to reproduce the error using TensorFlow 1.3.0 I see the correct outputs of 7.5 and 3 7 . The outputs you see make sense if the line was adder node a instead of adder node a b I don t have the same driver GPU as you though I doubt those would be to blame. But just to be extra sure do you see the same errors when using just the CPU say with The log output you mentioned above is just informational logs. I didn t see any concerning notes in that. asimshankar Thank you for your interest. When i run over CPU with at above code i got the true result. 7.5 and 3 7 . Hmm...that s weird. Do you see the same problem with TensorFlow 1.4.0 Is it possible for you to try this on a different machine specifically one with a different driver CUDA version GPU or something Or does upgrading CUDA the NVIDIA driver make any difference asimshankar . I have also tried with Python 3.5.2 and Tensorflow 1.4.0 which is installed with pip3 tensorflow gpu. I got same wrong results. 3.0 1. 3. I think the problem is related to GPU it is old and has small memory. Thank you for your all effort. 
16179,ProfilerHook and loading libcupti.so cause Ubuntu to completely freeze,1. OS Platform and Distribution Ubuntu 14.04 LTE 2. TensorFlow version 1.14 3. Bazel version 0.9.0 4. CUDA cuDNN version 8.0 7.0.5 5. GPU model and memory GeForce GTX1060 6070MB 6. Exact command to reproduce python3.4 m music modeling I added ProfilerHook to Estimator for recording GPU memory consumption but it always causes my Ubuntu to freeze indefinitely and Ubuntu never makes away with it. Here is the source code I cannot attach the output of my console for it is impossible due to the indefinite freezing of my Ubuntu. All I can say is that it tells me that it loads libcupti.so and computes a step of training process. Then everything totally messes up. Thank you for your support in advance., ispirmustafa martinwicke it seems like there is a deadlock when using the ProfilerHook. On the other hand the documentation of ProfilerHook implies it was designed for monitored session so perhaps it is not supported. Please take a look. aselle I ve checked out the source code and it creates a MonitoredSession wrapped with 3 4 more different session wrappers in a nested fashion. Oh Btw. Upon request I can use my phone in order to capture a screenshot for you to see the output of execution. Nagging Awaiting TensorFlower It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. Hi I have the same problem in Tensorflow 1.5 when I try to use ProfilerHook and I launch my model.train ... my jupyter notebook kernel dies instantaneously. Is there a workaround I can use Thanks Nagging Awaiting TensorFlower It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. Nagging Awaiting TensorFlower It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. Nagging TensorFlower It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. Assigning to ispirmustafa who worked on ProfilerHook. Also iliTheFallen can you provide a short example that reproduces the problem It s very difficult to debug long examples so try making the example as short as possible. iliTheFallen it sounds like the machine might be running out of RAM. Do you know if that is happening Using profiling turns on FULL TRACE https github.com tensorflow tensorflow blob r1.7 tensorflow python training basic session run hooks.py L878 which will take some extra memory. You could try turning the trace level down by editing the code https github.com tensorflow tensorflow blob r1.7 tensorflow python training basic session run hooks.py L878 to one of the lower levels like SOFTWARE TRACE https www.tensorflow.org api docs python tf RunOptions . Nagging Assignee ispirmustafa It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue It has been 36 days with no activity and the awaiting response label was assigned. Is this still an issue We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks 
17116,Apparent thread safety issue in tensorflow core common runtime executor.cc, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 macOS 10.13.3 TensorFlow installed from source or binary source TensorFlow version use command below v1.6.0 rc1 277 g993006fa76 1.6.0 rc1 Python version Python 2.7.14 Bazel version if compiling from source 0.10.1 homebrew GCC Compiler version if compiling from source 4.2.1 CUDA cuDNN version 9.1 7.0.5 GPU model and memory NVIDIA GeForce GT 750M with 2 GB device memory CUDA compute capability 3.0 Exact command to reproduce N A Describe the problem Clang warns about a thread safety issue in tensorflow core common runtime executor.cc at line 2338 which warning appears to be valid. Here is the code around that line A lock is held on parent frame mu but not frame mu . If there is no thread safety issue I think that a comment should be added to explain why as it s not clear. Source code logs pre tensorflow core common runtime executor.cc 2338 27 warning reading variable dead exits requires holding mutex frame mu Wthread safety precise for const Node node frame dead exits tensorflow core common runtime executor.cc 2338 27 note found near match parent frame mu tensorflow core common runtime executor.cc 2338 27 warning reading variable dead exits requires holding mutex frame mu Wthread safety precise for const Node node frame dead exits tensorflow core common runtime executor.cc 2338 27 note found near match parent frame mu pre , skye Mind taking a look Since you might have been looking into FrameState lately mrry do you know if this is intentional I ll take a look when I get the chance if not. It looks like that code is only called here on L2027 https github.com tensorflow tensorflow blob a4f1478134cdbf73f0ad7eda3e73407135a54566 tensorflow core common runtime executor.cc L2024 L2027 I suspect that is frame done implies that there is only one thread operating on frame but it doesn t look like any annotations or lock acquisitions protect that invariant. Nagging Assignee skye It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee skye It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. I think that this was fixed by 3d9c27742693f9859e2fb75de57fe108520de712 
18266,Cache lockfile already exists,Using the train and evaluate method from estimator API and cache policy on filesystem I get an error because the evaluation starts before that all cache is written on the filesystem. So when the train runs the second time after the evaluation I get the error because it finds the lock file. If the cache is written before the first evaluation I don t get the error. Here a snippet runnable on colab and here the output , saeta Could this be an instance of the same bug as the ds.cache .take N .repeat one that you re looking at mrry I think that there is a conceptual problem with train and evaluate . Mainly you control train eval phases with EvalSpec with a default throttle secs 600 and or check point in RunConfig . If the dataset is quite big you cannot evaluate until the cache is done cause a new TrainSpec call will rebuild the input dataset pipeline and you will find a lock on the cache dir. There could be a not so trivial workaround with RunConfig save checkpoints steps to block throttle until you will not have covered the epoch but in the distributed setup I suppose that the dataset cache is working locally per node so is it still plausible Do you see any other solution that I suppose still require documentation cc isaprykin for suggestions about how to use train and evaluate . If I understand the problem correctly user defines the dataset with cache filename . For this input fn as the input fn is invoked multiple times train and evalaute will error out. Is this the issue Unfortunately train and evaluate does not specify the contract how many times train input fn and eval input fn will be invoked today. It will limit the implementation space we have. mrry can user change the code into following way ds tf.data.Dataset.range 100000 ds ds.map lambda x tf.py func func gen inp x Tout tf.float32 tf.float32 ds ds.map batch reshape ds ds.cache tmp mycache train ds ds.repeat 3 def my input fn train value ds.make one shot iterator .get next return input rgb value 0 softmax value 1 xiejw Unfortunately that won t work because ds will be bound to a graph that is different from the estimator s graph. I m not sure why train and evaluate is calling the input function multiple times it would be more efficient to build a single pipeline and split the elements across the device s in the process but the right solution would seem to be allowing concurrent iterators to build a cache with the same eventual filename. slightly offtopic too many issues with the high level APIs. I really hope that there is a concrete plan with https github.com tensorflow tensorflow issues 16182 issuecomment 372508885 What do you think about ispirmustafa comment https github.com tensorflow tensorflow issues 18016 issuecomment 380603186 current local run implementation is something like for ... estimator.train ... estimator.evaluate ... but making train and evaluate local mode to create this graphs only once is a good feature request. for now as a workaround you can do following ispirmustafa The workaround It will be useless cause will skip the lock but you will never match the cache. Also generally what about cache matching if the pipeline is rebuild and shuffled at every Train phase bhack could you please explain more The error message is about using same file as cache. this workaround resolves that error. what do you mean by skip the lock and never match ispirmustafa I.e. You have a large dataset over a network file system that needs to be cached over multiple train evaluate rounds. With this you will workaround the lock but you will never match the cache after the first evaluation. Also any temporary workaround is quite invalidated by limits bugs in https github.com tensorflow tensorflow issues 17650 Some news at https github.com tensorflow tensorflow issues 19062 issuecomment 400129963. But I think it will not solve this. Do you need to fix https github.com tensorflow tensorflow issues 19062 issuecomment 401736720 tensorflow 2.0.0 alpha0 cache lockfile already exists. When going for the 2nd epoch of evaluation the previous lock file isn t removed i can see it on the file system. Doesnt happen with training data hence it fires this error as it cant get hold of ds.cache file it seems Code def get ds tfdata cache self X train y train batch size load preprocess image filename import tensorflow as tf from Source.tfdata.PathConstants import Constants cachedir Constants .get tfcache dir path ds tf.data.Dataset.from tensor slices X train y train ds ds.map load preprocess image ds ds.cache filename cachedir filename ds ds.apply tf.data.experimental.shuffle and repeat buffer size batch size ds ds.batch batch size batch size ds ds.prefetch buffer size batch size return ds image ds train data.get ds tfdata cache X train X train y train y train batch size params list.get batch size load preprocess image helpermethods.load and preprocess image filename params list.get tf cache train filename image ds validate data.get ds tfdata cache X train X train y train y train batch size params list.get batch size load preprocess image helpermethods.load and preprocess image filename params list.get tf cache test filename early stop patience 5 training epochs 2 history model.fit x image ds train validation data image ds validate epochs training epochs steps per epoch steps per epoch validation steps validation steps callbacks early stop tensorboard Error 2019 05 09 11 12 53.961797 W tensorflow core framework op kernel.cc 1431 OP REQUIRES failed at iterator ops.cc 988 Already exists There appears to be a concurrent caching iterator running cache lockfile already exists . tfdata dir tfcache dir cache validate.tf data 0.lockfile . If you are sure no other running TF computations are using this cache prefix delete the lockfile and re initialize the iterator. Lockfile contents Created at 1557364206 Traceback most recent call last File input line 53 in module File C ML VirtualEnv tf2 alpha lib site packages tensorflow python keras engine training.py line 791 in fit initial epoch initial epoch File C ML VirtualEnv tf2 alpha lib site packages tensorflow python keras engine training.py line 1515 in fit generator steps name steps per epoch File C ML VirtualEnv tf2 alpha lib site packages tensorflow python keras engine training generator.py line 315 in model iteration steps name validation steps File C ML VirtualEnv tf2 alpha lib site packages tensorflow python keras engine training generator.py line 213 in model iteration batch data get next batch generator mode File C ML VirtualEnv tf2 alpha lib site packages tensorflow python keras engine training generator.py line 355 in get next batch generator output next generator File C ML VirtualEnv tf2 alpha lib site packages tensorflow python data ops iterator ops.py line 556 in next return self.next File C ML VirtualEnv tf2 alpha lib site packages tensorflow python data ops iterator ops.py line 585 in next return self. next internal File C ML VirtualEnv tf2 alpha lib site packages tensorflow python data ops iterator ops.py line 577 in next internal output shapes self. flat output shapes File C ML VirtualEnv tf2 alpha lib site packages tensorflow python ops gen dataset ops.py line 1984 in iterator get next sync six.raise from core. status to exception e.code message None File string line 3 in raise from tensorflow.python.framework.errors impl.AlreadyExistsError There appears to be a concurrent caching iterator running cache lockfile already exists . tfdata dir tfcache dir cache validate.tf data 0.lockfile . If you are sure no other running TF computations are using this cache prefix delete the lockfile and re initialize the iterator. Lockfile contents Created at 1557364206 Op IteratorGetNextSync 1 this is a bug in the design. cache files should support concurrent usage since one can easily get there via tf.data.experimental.parallel interleave . I wrote a little utility function to get rid of directories containing lock files Note that what you want to do is to create a single dataset iterator and keep using it I also find the .cache method of tf.data.Dataset a bit odd with the docs stating filename A tf.string scalar tf.Tensor representing the name of a directory on the filesystem to use for caching tensors in this Dataset. If a filename is not provided the dataset will be cached in memory. e.g. asking for filename when filename actually represents the name of a directory . Part of the cache method problem is that the cached file is not always auto cleaned up. I think adding a positional argument overwrite solves this issue by allowing users to cache to a known location like lenlen did you try the SumNeuron s workaround. Please let us know if that solves your problem. Thanks gadagashwini the overwrite kwarg does not exist it s just a proposal https github.com tensorflow tensorflow blob r1.14 tensorflow python data ops dataset ops.py L1738 L1740 overwrite would be helpful but wouldn t solve the concurrency problem. The op needs a do over. This is fixed with TF version 1.15.0 Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 18266 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 18266 No a 
19158,TFLite memory alignment errors with some armv7 Android devices, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Built on Mac OS X 10.13.4 and Linux Ubuntu 17.10 Android NDK revision 16.1.4479499 targeting various Android devices with TFLite. TensorFlow installed from source or binary Binary TensorFlow version use command below 1.7.0 Python version 2.7 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Exact command to reproduce See my repo tsob TFLite bug test https github.com tsob TFLite bug test for a demonstration of this issue. Clone the repo open in Android Studio and run the tests. Describe the problem When using TFLite to run inference on a saved .tflite model we see a memory alignment error on certain armv7 devices e.g. Galaxy S3 Galaxy Nexus which causes crashes pretty consistently. The same app runs fine on armv8 apparently as well as some armv7 devices Galaxy S4 . For example on Galaxy S3 we see On Galaxy Nexus we have Source code logs See tsob TFLite bug test https github.com tsob TFLite bug test for relevant info as well as the code to reproduce this issue.,Nagging Assignees petewarden aselle andrehentz It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignees petewarden aselle andrehentz It has been 15 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignees petewarden aselle andrehentz It has been 30 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignees petewarden aselle andrehentz It has been 45 days with no activity and this issue has an assignee. Please update the label and or status accordingly. I have similar problem when trying to run TensorFlow Lite model. I ve tested my own model inception v3 slim 2016 android 2017 11 10.tflite from sample. I use following version of TFLite in build.gradle org.tensorflow tensorflow android 1.8.0 When I try to run model on Nexus 4 Mako I get following error On Samsung Galaxy Alpha model runs successfully. Both devices has ARMv7 processors. tsob thank you for the repro app. We fixed an alignment issue recently on May 14 https github.com tensorflow tensorflow commit 88103d000add4ea7f8d1a34ee3c898fc79d9e3c7 I am going to verify if the issue reported by App is also fixed. Thanks shashishekhar I will also try to verify if it is fixed. I ran it on Nexus 5 and couldn t reproduce the issue please reopen if you can still reproduce it. Unfortunately the problem still exists I ran it on Nexus 4 6.0.1 and same issue occurs. shashishekhar Could you please reopen Crash comes when following line is executing tflite.run input output I tried updated version of TFLite org.tensorflow tensorflow android 1.10.0 rc0 Please take a look at the logcat spike07 Can you try with org.tensorflow tensorflow lite 0.0.0 nightly shashishekhar Just tried the org.tensorflow tensorflow lite 0.0.0 nightly still crashing with the same info in logcat on Nexus 4 with Qualcomm Snapdragon S4 Pro Krait However both org.tensorflow tensorflow lite 0.0.0 nightly and org.tensorflow tensorflow android 1.10.0 rc0 works fine on Samsung Galaxy Alpha SM G850F with Samsung Exynos Octa 5430 processor 4x ARM Cortex A7 4x ARM Cortex A15 . The previous version of TFLite which I tested org.tensorflow tensorflow android 1.8.0 crashed both on Samsung Galaxy Alpha and Nexus 4. So changes from version 1.8.0 to 1.10.0 helped fixing the problem on Samsung device. I ve just tried org.tensorflow tensorflow lite 1.9.0 and it works fine on Nexus 4 shashishekhar Thank you for your fast reply. However keep in mind that I ve tested following versions and all of them crashed on Nexus 4 org.tensorflow tensorflow lite 0.0.0 nightly org.tensorflow tensorflow android 1.8.0 org.tensorflow tensorflow android 1.9.0 rc0 org.tensorflow tensorflow android 1.10.0 rc1 
20346,Tensorflow Program Hangs on pthread cond wait, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04.4 LTS xenial TensorFlow installed from source or binary binary TensorFlow version use command below tensorflow gpu 1.8.0 Python version Python 3.5.2 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 9.0.176 7.1.3.16 GPU model and memory GeForce GTX 850M with 2GB of memory Exact command to reproduce Describe the problem I have a program using Tensorflow and it consistently hangs after a week or so arbitrarily. The GPU utilization goes to 0 and the program stops training. A stack trace shows that the program is stuck waiting on pthread cond wait. Source code logs I don t know that the source code will be of much use since it s not a concise example that reproduces the problem but the code is available here https github.com benbotto bsy dqn atari tree breakout best My code is single threaded. When last the program hung I took a stack trace. That s available here https pastebin.com LiPhz2CE This looks similar to this report https github.com tensorflow tensorflow issues 1947 Let me know if more information is needed.,the same problem no solution now. Looked at the pastebin and nothing obvious jumped out to me. mrry any ideas If the problem happened again could you paste a newer version of the stack trace The hang happens reliably on one of my machines. I have another machine that s running the same software on the same version of Tensorflow albeit different hardware and I haven t had any hangs on that one. The hang is sporadic though and often takes a week or more. If I really need to I can take another stack trace but that will take some time. Looking at this again and there s no obvious answer in the stack trace but I have a few suggestions 1. Can you try creating your tf.Session with config tf.ConfigProto inter op parallelism threads 1 intra op parallelism threads 1 This shouldn t fix anything... if anything it makes it more likely that buggy code will deadlock sooner... but it should give a more compact thread dump to analyze. 2. Are you using tf.py func in your code It looks like Thread 1 which appears twice in the pastebin link is blocked in some NumPy code in what appears to be OpenBLAS exec blas async wait . It s possible that if your other machine has a different version of NumPy then it might have different behavior for this method. 3. Related to 2 experimenting with the OMP NUM THREADS variable e.g. setting it to 1 might reveal something about the behavior. If you can reproduce the hang and capture another ideally smaller stack trace that would be useful It could be related to this issue in OpenBLAS https github.com xianyi OpenBLAS issues 660 This comment https github.com xianyi OpenBLAS issues 660 issuecomment 277033954 suggests that setting the environment variable OPENBLAS NUM THREADS 1 is a possible workaround but may slow things down. Thanks for the response mrry. 1. Right now I m not actually creating a session explicitly as I m using tf.keras which evidently creates a session behind the scenes. It looks like Keras has the option of using a manually defined session tf.keras.backend.set session so I ll try your config recommendation and see if I can get a better stack dump. 2. I m not using tf.py func anywhere no. As far as I know both machines are using identical version of everything Tensorflow Keras numpy and all other dependencies . I m using virtualenv and I set up both machines simultaneously. I ll double check the np version though. I wish I knew how to set up a smaller test case that would reproduce the issue as a programmer I certainly understand how important that is but I m not sure how in this instance. I don t think my code is doing anything out of the ordinary I m generating images doing some basic processing on those crop and grayscale and such then feeding them through a rather trivial network. I m not directly using any threading in my code or anything else that would cause a deadlock. All that is to say I wish I could be of more help and furthermore I wish the code would run reliably on all my machines . Anyway I ll try again with the session configured as recommended and see if I can get another stack trace. It may take a few weeks to lock up again. mrry The similar problem occurs at the end of an iteration when using DataSet api while the old Queue based data api is fine. TF version 1.12 build from source Linux kernel 3.10.0 Distributed running on yarn only cpu. Every time several random nodes will hang forever. formath That sounds like it could be a different problem. However there isn t enough information in that one stack to diagnose the cause it just tells us that one call to sess.run is hanging somewhere. Can you please open a new issue with a minimal reproducible example and or a full set of thread stacks Also to remove a potential source of complexity please try to reproduce it using tf.Session instead of tf.Session rpc server although if it only reproduces in an RPC based session then that would tell us something useful . We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks 
20696,ValueError when loading multiple tf.contrib.keras models in the same scope at different times, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution Linux Ubuntu 16.04 TensorFlow installed from binary TensorFlow version use command below 1.8.0 Python version 3.5.2 Bazel version N A CUDA cuDNN version CUDA 7.5 cuDNN 9.0 GPU model and memory GTX1080 8GB Exact command to reproduce See this snippet https gist.github.com Sergio0694 aa36c7ed94091ce5503ad908b142aaf0 Describe the problem TensorFlow throws a ValueError You are trying to load a weight file containing 16 layers into a model with 0 layers when trying to create multiple instances of a pretrained Keras model from within the same scope. This only happens if the instances are created one at a time reopening the same scope multiple times and works if all the instances are created consecutively after opening the scope a single time. Of course in a real world scenario these instances are created from different places and not all at the same time while building the model so one would actually need to reopen that scope multiple times. Hope this helps and that the snippet is clear enough Source code logs See this snippet https gist.github.com Sergio0694 aa36c7ed94091ce5503ad908b142aaf0 same one posted under system information. Here s the stack trace for the snippet above Traceback most recent call last File home sergio Documents code keras repro.py line 21 in module vgg2 load vgg19 t File home sergio Documents code keras repro.py line 12 in load vgg19 m tf.contrib.keras.applications.VGG19 weights imagenet include top False input tensor x File home sergio .local lib python3.5 site packages tensorflow python keras impl keras applications vgg19.py line 234 in VGG19 model.load weights weights path File home sergio .local lib python3.5 site packages tensorflow python keras impl keras engine network.py line 1190 in load weights saving.load weights from hdf5 group f self.layers File home sergio .local lib python3.5 site packages tensorflow python keras impl keras engine saving.py line 697 in load weights from hdf5 group layers. ValueError You are trying to load a weight file containing 16 layers into a model with 0 layers.,Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case or leave them as N A Thanks. Bazel version Done Not sure how to trigger the bot to re check the message or if it s just automatic tensorflowbutler Looks related to https github.com tensorflow tensorflow issues 20073 This is fixed with tf nightly version 1.15.0 dev20190726 . Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 20696 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 20696 No a 
21277,Using TensorFlow s Datasets API causes process to hang in session destructor, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOS High Sierra 10.13.1 though we ve also seen this happen on Linux as well we believe. TensorFlow installed from source or binary Source but happens with the binary version as well TensorFlow version use command below v1.8.0 0 g93bc2e2072 1.8.0 Python version Python 3.6.1 v3.6.1 69c0db5050 Mar 21 2017 01 21 04 Bazel version if compiling from source 0.10.1 GCC Compiler version if compiling from source Apple LLVM version 9.0.0 clang 900.0.39.2 CUDA cuDNN version N A GPU model and memory N A Exact command to reproduce Unfortunately the issue isn t that easy to reproduce without running our application I haven t managed to produce a smaller test case . Describe the problem Summary We are using TensorFlow s Datasets API. More specifically we re using tf.data.Dataset.from generator to create a dataset based on a generator function. When Python comes to garbage collect our tf.Session object its destructor makes a call into TensorFlow to delete the session tf session.TF DeleteSession . This call hangs because it s trying to execute a tf.py func function but cannot acquire Python s global interpreter lock. The function its trying to execute appears to be the finalize function from our dataset. This looks to me like a bug in TensorFlow as my understanding is that we shouldn t be able to write code that causes this to happen. Although it s clearly a consequence of our specific use of TensorFlow I can t see that we re doing anything in our application that we shouldn t be. More Details When our tf.Session object is garbage collected in Python its destructor del method hangs indefinitely. The problem appears to be this call in BaseSession tf session.TF DeleteSession self. session Running lldb shows the following stack trace thread 1 queue com.apple.main thread stop reason signal SIGSTOP frame 0 0x0000000101855e7e libsystem kernel.dylib psynch cvwait 10 frame 1 0x000000010188d662 libsystem pthread.dylib pthread cond wait 732 frame 2 0x00000001019b6cb0 libc .1.dylib std 1 condition variable wait std 1 unique lock std 1 mutex 18 frame 3 0x000000011279a63b libtensorflow framework.so nsync nsync mu semaphore p with deadline nsync nsync semaphore s timespec 283 frame 4 0x0000000112796eb7 libtensorflow framework.so nsync nsync cv wait with deadline generic nsync nsync cv s void void void void void timespec nsync nsync note s 423 frame 5 0x0000000112797621 libtensorflow framework.so nsync nsync cv wait nsync nsync cv s nsync nsync mu s 49 frame 6 0x00000001090810e3 pywrap tensorflow internal.so tensorflow Notification WaitForNotification 67 frame 7 0x0000000109d4d809 pywrap tensorflow internal.so tensorflow CapturedFunction RunInstantiated std 1 vector tensorflow Tensor std 1 allocator tensorflow Tensor const std 1 vector tensorflow Tensor std 1 allocator tensorflow Tensor 649 frame 8 0x0000000109cffa21 pywrap tensorflow internal.so tensorflow anonymous namespace GeneratorDatasetOp Dataset Iterator Iterator 97 frame 9 0x0000000109cffb8e pywrap tensorflow internal.so tensorflow anonymous namespace GeneratorDatasetOp Dataset Iterator Iterator 14 frame 10 0x0000000109cfd669 pywrap tensorflow internal.so tensorflow anonymous namespace FlatMapDatasetOp Dataset Iterator Iterator 105 frame 11 0x0000000109cfd6de pywrap tensorflow internal.so tensorflow anonymous namespace FlatMapDatasetOp Dataset Iterator Iterator 14 frame 12 0x00000001019e98fd libc .1.dylib std 1 shared weak count release shared 43 frame 13 0x0000000109d0a579 pywrap tensorflow internal.so tensorflow anonymous namespace IteratorResource IteratorResource 169 frame 14 0x0000000109d0a5fe pywrap tensorflow internal.so tensorflow anonymous namespace IteratorResource IteratorResource 14 frame 15 0x000000011226db4d libtensorflow framework.so tensorflow ResourceMgr DoDelete std 1 basic string char std 1 char traits char std 1 allocator char const unsigned long long std 1 basic string char std 1 char traits char std 1 allocator char const std 1 basic string char std 1 char traits char std 1 allocator char const 301 frame 16 0x000000011226dd50 libtensorflow framework.so tensorflow ResourceMgr DoDelete std 1 basic string char std 1 char traits char std 1 allocator char const std 1 type index std 1 basic string char std 1 char traits char std 1 allocator char const 192 frame 17 0x0000000109d0c558 pywrap tensorflow internal.so tensorflow anonymous namespace OneShotIteratorOp OneShotIteratorOp 104 frame 18 0x0000000109d0c71e pywrap tensorflow internal.so tensorflow anonymous namespace OneShotIteratorOp OneShotIteratorOp 14 frame 19 0x00000001122670ff libtensorflow framework.so tensorflow OpSegment Item Item 63 frame 20 0x0000000112267ffd libtensorflow framework.so tensorflow OpSegment RemoveHold std 1 basic string char std 1 char traits char std 1 allocator char const 205 frame 21 0x000000010b880b42 pywrap tensorflow internal.so tensorflow DirectSession DirectSession 546 frame 22 0x000000010b88108e pywrap tensorflow internal.so tensorflow DirectSession DirectSession 14 frame 23 0x000000010935dfd3 pywrap tensorflow internal.so TF DeleteSession 931 frame 24 0x0000000109006e5a pywrap tensorflow internal.so wrap TF DeleteSession object object 122 frame 25 0x00000001007bb688 Python PyCFunction FastCallDict 568 frame 26 0x00000001008443e4 Python call function 612 frame 27 0x0000000100849d84 Python PyEval EvalFrameDefault 21892 frame 28 0x00000001008447cc Python PyFunction FastCallDict 828 frame 29 0x000000010075f984 Python PyObject FastCallDict 356 frame 30 0x000000010075faa0 Python PyObject Call Prepend 208 frame 31 0x000000010075f8d4 Python PyObject FastCallDict 180 frame 32 0x00000001007d6579 Python slot tp finalize 121 frame 33 0x000000010089b18a Python collect 1418 frame 34 0x000000010089b8c3 Python PyGC CollectIfEnabled 99 frame 35 0x000000010087af57 Python Py FinalizeEx 119 frame 36 0x000000010087b0e0 Python Py Exit 16 frame 37 0x000000010087ef4c Python handle system exit 252 frame 38 0x000000010087f1a5 Python PyErr PrintEx 437 frame 39 0x0000000100880a1d Python PyRun SimpleStringFlags 125 frame 40 0x00000001008992a4 Python Py Main 1812 frame 41 0x0000000100000dfe Python frame 42 0x0000000100000c34 Python It appears that the session s destructor is waiting for an op to complete. The culprit seems to be PyFuncOp which doesn t get past this line py threadstate PyGILState Ensure So it looks like this op is trying to acquire the GIL but can t. My assumption is that this py func is the finalize function for the dataset from GeneratorDataset . My assumption is that when Python calls tf session.TF DeleteSession self. session that the GIL should be released and so PyFuncOp should then be able to acquire it again. Indeed when I write an isolated test to try and reproduce this I don t see this problem and the GIL is acquired successfully. Unfortunately as I mention I have been unsuccessful in writing an isolated test case to reproduce the problem. The problem only seems to happen when we use our application in a particular scenario but I haven t been able to isolate exactly what it is about this scenario that causes the problem.,Thanks for the report and for diving into the details... this definitely sounds like a bug although my understanding is the same as yours tf session.TF DeleteSession self. session should release the GIL because of this SWIG code block here https github.com tensorflow tensorflow blob 2826d123a017bc5f1a2cc7b969e275c5a63c326c tensorflow python client tf session.i L106 L111 In terms of a reproduction would you be able to capture a dump of all thread stacks when the problem occurs One thing I ve found useful in debugging this kind of problem is to set config tf.ConfigProto inter op parallelism 1 intra op parallelism 1 when creating the session. This makes the set of threads less unwieldy and can sometimes tease out deadlock bugs that are less likely to happen with larger threadpools. Sure I ve included a backtrace of all native threads below. This is with inter op parallelism threads 1 and intra op parallelism threads 1. I ve also included a thread dump from Python as well in case it s interesting. It doesn t look to me like any of the Python threads should be holding the GIL either. The only interesting one looks like 0x00007000025b4000 which is performing an IO operation consuming from a multiprocessing queue that I believe should also have caused the GIL to be released. Native threads Python threads Hmm I think the Python thread in multiprocessing connection.py 0x00007000025b4000 maps to native thread 7 which is in Py read and that drops the GIL as well. As far as I can tell though none of the native threads is blocked trying to acquire the GIL Thread 1 is waiting on a tensorflow Notification for the end of the finialize function. Thread 2 isn t TF related. Thread 3 is Thread 4 is a Python thread that s waiting on a Python lock. I don t think this is TF related. Threads 5 6 8 and 9 are idle TF Eigen threadpool threads that are waiting for work. I m a little surprised to see 4 of them rather than inter op parallelism intra op parallelism 2 threads but they don t seem to be doing anything concerning. Thread 7 is probably Python thread 0x00007000025b4000 blocked on a multiprocessing queue. It would be interesting to know if the PyFunc op in finalization actually started and or finished. One way to do this is to set the environment variable TF CPP MIN VLOG LEVEL 1 which triggers very verbose logging including each op invocation and completion . Could you try that and capture the part of the log that is produced after the tf.Session destructor begins Yes I was also confused by the fact that none of the native threads appear to be waiting to acquire the GIL. I haven t tried TF CPP MIN VLOG LEVEL 1 yet but if I surround the GIL lock acquisition with a couple of log statements as follows I can observe it printing About to ensure GIL but not Got GIL Which is why I mentioned that it was getting to PyGILState Ensure but no further. Anyway I will try with TF CPP MIN VLOG LEVEL 1 . I ve set TF CPP MIN VLOG LEVEL 1 and attached the resulting log from after the call to TF DeleteSession . Note that this includes my added log line About to ensure GIL at the end see my previous comment . And then the program hangs. It definitely looks like it s failing to acquire the lock . Is there some way you could log the thread ID on which the PyFunc op is trying to acquire the GIL so we can associate it with the stack trace Also the presence of multiprocessing is slightly suspicious because depending on when things are forked the TensorFlow runtime might end up in an illegal state essentially the process isn t forkable once a tf.Session has been created . Is it possible to reproduce the problem in a process that doesn t use multiprocessing I ve logged the thread that is executing the PyFunc op. Here s the output The native thread ID is the one we re interested in 4923929 . This corresponds to 0x4B2219 in hex. Here s what lldb tells us about our threads However 0x4B2219 is not there However if we look at our threads just before the session is destroyed we can see that this thread did previously exist thread 11 tid 0x4b2219 0x0000000101855e7e libsystem kernel.dylib psynch cvwait 10 As might be expected this is one of the Eigen threads So it looks like this thread has tried to get the GIL hasn t managed and then has stopped or been killed. Is it possible that the thread encounters an error when getting the GIL which is not logged for some reason and then is killed Regarding the multiprocessing stuff we are using Python s spawn multiprocessing context multiprocessing.get context spawn so as I understand it the limitations around spawning processes after sessions have been created do not apply but in any case we don t spawn any processes after session creation anyway . Having said that both of the scenarios where this happens in our application do use multiprocessing so I m not able to rule this out as a cause. I was also wondering why there are 4 Eigen threads in fact there are 6 before session destruction time so just to double check our config I dumped it out. Here it is It looks like an error may be occurring which is killing the thread. Stepping through the PyGILState Ensure function in lldb I can see the following happening This looks to me like the Py FatalError function is being called with Couldn t create thread state for new thread . Soon after this happens the thread dies. This seems to correspond to these lines from PyGILState Ensure looking at the code here https github.com python cpython blob e42b705188271da108de42b55d9344642170aa2b Python pystate.c But it s not obvious to me why this would be failing. Is the process terminating when you see the hang Looking the the PyGILState Ensure code as far as I can tell the only situation in which we d hit this path is if malloc returned null. I can t think why that might be happening but I d be less surprised if we were in some rarely hit exit path. Yes the process is terminating when we see the hang. In fact we can see from the original thread dump that the destructor appears to be called as a consequence of a garbage collection which happens as part of shut down The docs for Py FinalizeEx say Undo all initializations made by Py Initialize and subsequent use of Python C API functions so this could well be why PyThreadState New subsequently fails. I think this is may be why my simplified test case does not reproduce the problem it looks like the destructor is called as part of a regular GC in this test case. Adding a call to gc.collect just before the process starts shutting down seems to fix the problem for us so it does look like normal garbage collections are OK but when the Python VM is shutting down calling BaseSession s destructor is problematic. Aha that makes sense. Can you try patching the fix in 8cd2d6fe9389e93a4182ae9287f2f8325913fe6c and see if that fixes the problem without having to call gc.collect Yes that fixes the issue and I get a log message saying Thanks very much for your assistance on this. Thank you very much for digging into the details and providing such a useful report 
23195,Segfault reading dataset more than once make batched features dataset , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 macOS 10.14 Mojave Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device NA TensorFlow installed from source or binary Binary pip TensorFlow version use command below v1.11.0 rc2 4 gc19e29306c 1.11.0 Python version 2.7.10 Bazel version if compiling from source NA GCC Compiler version if compiling from source NA CUDA cuDNN version NA GPU model and memory NA Gist of full output of . tools tf env collect.sh https gist.github.com brianmartin 2b43dca69453478ed33b49f1029e03fe Exact command to reproduce Describe the current behavior Current behavior in 1.11.0 1.12.0rcX is that the included script segfaults 11 SIGSEGV . Describe the expected behavior Expected behavior is no segfault. Version 1.10.1 1.10.0 and 1.9.0 do not segfault. I have not checked lower versions. Code to reproduce the issue See a full script which reproduces this issue along two different code paths here https gist.github.com brianmartin 4f221eb838dce8099342f829fb13253d The segfault occurs when reading the dataset a second time. The first read works as expected. For reference the two code samples which produce datasets which cause a segfault on second read are I ve also dug into make batched features dataset and minified down to this repro Please let me know if there s anything else I can provide or help with This is blocking us from upgrading to the latest Tensorflow version in spotify spotify tensorflow https github.com spotify spotify tensorflow .,Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case or leave them as N A Thanks. Have I written custom code OS Platform and Distribution TensorFlow installed from Bazel version CUDA cuDNN version GPU model and memory Exact command to reproduce Mobile device Updated shivaniag As we discussed offline can you please take a look at this issue in case there s a bug in ParseExampleDataset Thanks I ve dug in a bit more. For a FixedLenFeature this test case still succeeds on the first read and fails on the second read no change there. But it fails with an exception rather than the segfault we see with VarLenFeature. I ve updated the gist to reproduce in tensorflow 1.11.0 here https gist.github.com brianmartin 4f221eb838dce8099342f829fb13253d The exception on the second failing read after seeing the first read succeed brianmartin Thank you very much for narrowing down the bug we have fixed it and are going to submit it soon will update you when that is done. 
24598,Potential tf.boolean mask bug when the mask array is empty, System information OS Platform and Distribution Windows 10 TensorFlow installed from source or binary Binary TensorFlow version use command below 1.11.0 Python version 3.6.6 CUDA cuDNN version V8.0.60 GPU model and memory Geforce GTX 1070 8GB Describe the current behavior I have actually experiencing almost the similar problem like in thread https github.com tensorflow tensorflow issues 24585 Again I want to partition a minibatch into different parts process them in parallel using different computation units and then stitch them back together. However this time I used tf.boolean mask instead of tf.dynamic partition for the partition operation since the latter runs into problems when one of the partitions is empty. This code is below it is copy paste reproducible To my disappointment tf.boolean mask runs into a similar problem when indices arr contains no references to at least one partition and it produces an empty array for that partition as the result. The for loop in the end runs correctly a few times but then the program crashes with the following error InternalError see above for traceback WhereOp Could not launch cub DeviceReduce Sum to count number of true nonzero indices. temp storage bytes 1 status invalid configuration argument node boolean mask Where Where T DT INT32 device job localhost replica 0 task 0 device GPU 0 boolean mask Reshape 1 9 node DynamicStitch 49 Recv client terminated false recv device job localhost replica 0 task 0 device CPU 0 send device job localhost replica 0 task 0 device GPU 0 send device incarnation 1 tensor name edge 259 DynamicStitch tensor type DT FLOAT device job localhost replica 0 task 0 device CPU 0 I think this is the same error underlying the problem in https github.com tensorflow tensorflow issues 24585 where it crashes when tf.dynamic partition receives an empty index array since they could be using the same mechanism in the cub library or whatever cub is . The tf.dynamic partition error also occurs after a few succesfull iterations like this one. What could be the reason here ,I was able to run your code snippet successfully on cpu however interestingly it failed computing on gpu. Indeed this is a bug fixed by pinning Where to the CPU. I m submitting a patch soon. Thank you alextp . How can I get the fix now 
25746,Object detection ppn tflite works wrong on mobile, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow N OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device HUAWEI MATE20 TensorFlow installed from source or binary binary TensorFlow version use command below 1.10 Python version 3.6 Bazel version if compiling from source 1.15 GCC Compiler version if compiling from source 1.9 CUDA cuDNN version 8.0 GPU model and memory 16GB I used object detection API the ssd mobilenetV2 is working well including the train eval inference on pc and .tflite on phone. Recently i try ppn mobilenetv1 the train and eval is well. The frozen inference graph.pb also work well on pc and the results are right. When i convert it to tflite graph.pb then convert it to detect.tflite. No errors occur. But when it runs on the mobile it shows totally wrong results. I can not find anything wrong here. Could anyone please help me find it out , holyhao Could you provide any code to reproduce the bug or could you provide more detailed steps commands you followed Thanks jvishnuvardhan hi thanks for your reply to reproduce the bug 1.Download the ppn mobilnetV1 http download.tensorflow.org models object detection ssd mobilenet v1 ppn shared box predictor 300x300 coco14 sync 2018 07 03.tar.gz 2.Conver it to tflite no errors occur 3. Run it on mobile i use the tensorflow lite from tensoflow 1.10 As I mentioned above ssd mobilenetV2 works well from the framework. The frozen inference graph.pb of ppn mobilenetv1 works well on pc. But when the tf.lite of ppn mobilenetv1 runs on the mobile it shows totally wrong results. I Look through the export tflite ssd graph.py and export inference graph.py . I wonder how it works well for ssd mobilenetV2 and wrong for ppn mobilenetv1 on mobile and how it works well for ppn mobilenetv1 on pc and wrong on the mobile. Looking forward to your help. Hi holyhao are you still experiencing a problem with the model Thanks. It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue Closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 25746 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 25746 No a 
26041,tf nightly gpu 2.0 preview dataset and tf.function error, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes TensorFlow installed from source or binary pip TensorFlow version use command below tf nightly gpu 2.0 preview Python version Python 3 in Google Colab CUDA cuDNN version 10 GPU model and memory Tesla K80 Describe the current behavior When using GPU runtime in Google Colab with tf nightly gpu 2.0 preview tensorflow version creating a dataset with tensorflow datasets inside a tf.function decorated function results in an exception regarding incompatible device types. Describe the expected behavior Expected behavior is not receiving an exception. Removing tf.function from the train method results in correct behavior. The code also works well without GPU support tf nightly 2.0 preview . Code to reproduce the issue Other info logs Link to the original Google Colab file https gist.github.com Mrpatekful 92f274756dffd6aab2993e401b7fb7af The encountered exception ,Any progress on this issue I m facing similar issue here https github.com tensorflow agents issues 19 I thought setting the device placement be the solution but I finally can t verify it. I believe jsimsa has fix for this in the works. Mrpatekful could you sync past https github.com tensorflow tensorflow commit 11f13a90545ff793762191da02e27669934c9e1c and see whether the problem goes away Thanks. ran into the same problem when run tf agents example code nightly build as of 2.0.0 dev20190322 still have the same issues I m too facing this. It triggers when moving to the second variable 1 in for i in tf.range 100 Commands under loop run correctly for i 0 for i in tf.range 100 is inside the function. Built 2.0 from source around 20th March or 2 days . Thanks I cannot reproduce the issue using Python 3 Google colab running pip install tf nightly gpu 2.0 preview this installs tf nightly gpu 2.0 preview 2.0.0.dev20190326 followed by bayesian and caissalover please create a separate issue with instructions on how to reproduce the issue you are seeing. I am closing this issue. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 26041 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 26041 No a I went into this error when using tf.keras.layers.GRU . Switching to tf.compat.v1.keras.layers.CuDNNGRU resolved the problem. 
26244,FailedPreconditionError when running Convolutional Keras model on CPU in TF 2.0, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 dev20190228 Python version 3.7 CUDA cuDNN version 10 7.4 GPU model and memory Tesla M60 on AWS g3.8xlarge Describe the current behavior Training a model defined on the CPU raises a FailedPreconditionError when using a machine with a GPU in TF 2.0 nightly. Describe the expected behavior No error is raised if I use one of the following fixes 1 use tensorflow.compat.v1.disable eager execution 2 remove the Conv2D layer 3 remove the batch size and epochs arguments from the .fit call However the context setter seems to have no effect and training is happening on the GPU anyways I can tell by how fast it s training the behavior seems weird can anyone explain what s going on Code to reproduce the issue Other info logs traceback , ezhulenev The presence of AnonymousVar here makes me think this is related to the arithmetic optimizer. It s just the name auto generated in https github.com tensorflow tensorflow blob abdbe6ee3d81d22d70e0181cea2a3bb261f7c09f tensorflow core framework resource mgr.cc L48 I don t see any artifact of arithmetic optimizer in the variable name. I think it s just the problem with Keras graph. robieta can you take a look at this Sure let me take a look. It looks like what s happening is that the model weights are initialized inside of the tf.device context but the RMSProp variables aren t. So when we try to call fit the resultant variables are then lazily created and assigned to the default device gpu 0 . I still need to dig into why this only happens in 2.0 though. Thanks robieta for looking into this. Keep in mind that 1 this only happens in eager mode if I use tensorflow.compat.v1.disable eager execution no error is shown 2 in this case the code executes but I suspect all variables are created on the GPU regardless of the context setter because training runs really fast robieta I think optimizer colocate its variable with model.trainable weights so it s likely that the model variables are lazily constructed which does not respect tf.device cpu 0 anymore ergo optimizer follows the lead and does the same thing. I m too facing this. I use tensorflow low level apis. Inside my while loop the message of AnonymousVar60 variable was uninitialized pops once the counter goes to 1 tf.function def func with tf.device device GPU 0 for i in tf.range 10 runs smoothly for i 0 I use cuda 10.0 and built tensorflow 2.0.0 from source around 20th March. Ubuntu 18.04 NVIDIA GTX 1060 import tensorflow as tf from tensorflow.keras.applications import Xception from tensorflow.keras.utils import multi gpu model import numpy as np num samples 32 height 224 width 224 num classes 32 Instantiate the base model or template model . We recommend doing this with under a CPU device scope so that the model s weights are hosted on CPU memory. Otherwise they may end up hosted on a GPU which would complicate weight sharing. with tf.device cpu 0 model Xception weights None input shape height width 3 classes num classes Replicates the model on 2 GPUs. This assumes that your machine has 2 available GPUs. parallel model multi gpu model model gpus 2 parallel model.compile loss categorical crossentropy optimizer rmsprop Generate dummy data. x np.random.random num samples height width 3 y np.random.random num samples num classes This fit call will be distributed on 8 GPUs. Since the batch size is 256 each GPU will process 32 samples. parallel model.fit x y epochs 20 batch size 256 Save model via the template model which shares the same weights Save model via the template model which shares the same weights model.save my model.h5 and the error is Epoch 1 20 FailedPreconditionError Traceback most recent call last ipython input 1 21adf99b6266 in module 31 This fit call will be distributed on 8 GPUs. 32 Since the batch size is 256 each GPU will process 32 samples. 33 parallel model.fit x y epochs 20 batch size 256 34 35 Save model via the template model which shares the same weights Anaconda3 envs py36 lib site packages tensorflow python keras engine training.py in fit self x y batch size epochs verbose callbacks validation split validation data shuffle class weight sample weight initial epoch steps per epoch validation steps validation freq max queue size workers use multiprocessing kwargs 871 validation steps validation steps 872 validation freq validation freq 873 steps name steps per epoch 874 875 def evaluate self Anaconda3 envs py36 lib site packages tensorflow python keras engine training arrays.py in model iteration model inputs targets sample weights batch size epochs verbose callbacks val inputs val targets val sample weights shuffle initial epoch steps per epoch validation steps validation freq mode validation in fit prepared feed values from dataset steps name kwargs 350 351 Get outputs. 352 batch outs f ins batch 353 if not isinstance batch outs list 354 batch outs batch outs Anaconda3 envs py36 lib site packages tensorflow python keras backend.py in call self inputs 3215 value math ops.cast value tensor.dtype 3216 converted inputs.append value 3217 outputs self. graph fn converted inputs 3218 return nest.pack sequence as self. outputs structure 3219 x.numpy for x in outputs Anaconda3 envs py36 lib site packages tensorflow python eager function.py in call self args kwargs 556 raise TypeError Keyword arguments unknown. Expected . .format 557 list kwargs.keys list self. arg keywords 558 return self. call flat args 559 560 def filtered call self args kwargs Anaconda3 envs py36 lib site packages tensorflow python eager function.py in call flat self args 625 Only need to override the gradient in graph mode and when we have outputs. 626 if context.executing eagerly or not self.outputs 627 outputs self. inference function.call ctx args 628 else 629 self. register gradient Anaconda3 envs py36 lib site packages tensorflow python eager function.py in call self ctx args 413 attrs executor type executor type 414 config proto config 415 ctx ctx 416 Replace empty list with None 417 outputs outputs or None Anaconda3 envs py36 lib site packages tensorflow python eager execute.py in quick execute op name num outputs inputs attrs ctx name 64 else 65 message e.message 66 six.raise from core. status to exception e.code message None 67 except TypeError as e 68 if any ops. is keras symbolic tensor x for x in inputs Anaconda3 envs py36 lib site packages six.py in raise from value from value FailedPreconditionError Error while reading resource variable AnonymousVar341 from Container localhost. This could mean that the variable was uninitialized. Invalid argument Trying to access resource AnonymousVar341 located in device job localhost replica 0 task 0 device GPU 0 from device job localhost replica 0 task 0 device CPU 0 node training RMSprop RMSprop update 99 mul ReadVariableOp training rmsprop rmsprop update 98 mul readvariableop resource 3 Op inference keras scratch graph 37943 I m too facing this. I use tensorflow low level apis. Inside my while loop the message of AnonymousVar60 variable was uninitialized pops once the counter goes to 1 tf.function def func with tf.device device GPU 0 for i in tf.range 10 runs smoothly for i 0 I use cuda 10.0 and built tensorflow 2.0.0 from source around 20th March. Ubuntu 18.04 NVIDIA GTX 1060 This is basically the exact error I am facing the first batch executes fine then one the second batch this error appears. How is this even possible Did you ever find a solution ghego This issue is fixed in Tf nightly version. Please find the gist https colab.sandbox.google.com gist gadagashwini ee6d90cb4fb5c3c9ea7c377afa9e327c untitled442.ipynb and let us know if you are happy with fix and close this issue. Thanks ghego Did you review the above attached gist. Please close the issue if it was already resolved for you. Thanks It works. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 26244 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 26244 No a 
26256,Contrib AdaMax implementation producing NaNs on GPU, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu TensorFlow installed from source or binary Source TensorFlow version use command below Tested on 1.12 and 1.13 Python version 3.6 CUDA cuDNN version 9 GPU model and memory verified on 1080ti and titan v Describe the current behavior On GPU AdaMax from tf.contrib.opt.AdaMaxOptimizer appears to apply NaNs to variables. Seems fine on CPU and bizarrely if any ops are put as a control dependency to the apply grads call then everything seems fine. Describe the expected behavior Not to produce NaNs. Code to reproduce the issue The above code produces on my machine Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,I can reproduce on GPU device on tf 1.11.0 as well. Thank you for pointing it out. I ll take a look at it. Hi could you take a try https www.tensorflow.org api docs python tf keras optimizers Adamax Because tf.contrib has been deprecated and tf.contrib.Adamax will be deleted replaced by tf.keras.AdaMax . Thank you In TF 2.0 tf.keras.optimizers.Adamax and I think Adam as well has the same problem. I had to use a different optimizer SDG in order to get mobilenet v2 to train and not produce a NaN loss benleetownsend I could not reproduce this with TF1.12.0 gist is here https colab.sandbox.google.com gist jvishnuvardhan aee69fe92d830ba63f1f8b9b706d28f8 untitled118.ipynb . But TF1.13.1 still has the issue gist is here https colab.sandbox.google.com gist jvishnuvardhan a38562b050931db33b09bddc34451ded untitled117.ipynb . Both the gists were created using Google colab with GPU. Thanks jvishnuvardhan Vishnuvardhan as maziello reported keras Adamax has the same problem could you take a try to reproduce it I think we should address it in keras optimizer module rather than contrib module. Here with tensorflow 2 tf.keras Adamax CPU is ok and GPU NaN Adam CPU is ok and GPU is ok Also have this problem. The other keras optimizers work just fine Adamax works on CPU but not on GPU using tensorflow 2. tanzhenyu case540 Can you take a look Thanks. Can you try Tensorflow 1.14 with tf.keras.optimizers.Adamax Nan loss there as well. If it helps I m using a network with an embedding layer and LSTM CuDNNLSTM so I assume it could be a problem with either the dense or the sparse update. Hmm...does the same thing happen on CPU as well Nope Adamax works just fine on the CPU. To elaborate it happens immediately and every time with Adamax on the GPU and never with Adamax on the CPU. Confirmed just now in colab with TF 1.14. On the second batch after updating the weights once the loss is nan. It s not a problem for me at the moment though. I switched to LazyAdam from tensorflow addons which also handles sparse updates well. Hello I have this issue on GPU AWS 1.14.0 using tf.keras.optimizers.Adamax recently which did not occur prior to the update when they were still using 1.13.1 1.13.1 probably doesn t have GPU kernel that s why it got relocated to CPU. The GPU kernel was probably introduced after 1.14 which had a major bug. This is fixed on a 09 11 code change and should be reflected in either 1.15 rc3 or 2.0. Please try that and let us know if that fixed your issue. I tried it on 2.0 on my local machine and it works on GPU. However AWS is still stuck on 1.14.0 for now which sucks and the bug is still there. Edwin Koh1 by it works on GPU you mean the NAN is gone correct Yes the NAN s are gone for Adamax optimizer on GPU training on tensorflow 2.0.0 but still persists on 1.14 Ok. I m gonna close this for now. AWS could update to 1.15 later since that should get fixed. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 26256 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 26256 No a Still having this problem on 1.15 is there any solution besides updating to 2.0.0 Still having this problem on 1.15 is there any solution besides updating to 2.0.0 Did you try tf.contrib optimizer or tf.keras optimizer Yes but for my specific use case I need the AdaMax optimizer to inherit tf.train.Optimizer which the tf.keras optimizer isn t doing. I faced the same inf validation loss using LSTMs with tf 1.15 and GPUs but not with CPUs. Once I switched from Adamax to Adam it runs great on GPUs. So perhaps the Adamax bug is only fixed on tf 2 BUG is in tensorflow core kernel training ops gpu.cc change var.device d lr beta1 power.constant one beta1 power .reshape single .broadcast bcast m v epsilon to var.device d lr beta1 power.constant one beta1 power .reshape single .broadcast bcast m v epsilon.reshape single .broadcast bcast and the problem will not appear 
26559,tensorflow1.12 hangs at LocalMaster RunStep with tf.train.MonitoredTrainingSession, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary source TensorFlow version use command below 1.12 Python version 3.6 Bazel version if compiling from source 0.18 GCC Compiler version if compiling from source 5.4 CUDA cuDNN version GPU model and memory You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION b v1.12.0 18 gd60b574 1.12.0 Describe the current behavior In distributed tensorflow some workers hang at the last batch of dataset when the dataset epoch is 1 and the step in StopAtStepHook is very large. What s more the number of samples in the last batch of dataset may less than the batch size. If we use tf.train.StopAtStepHook to stop training all workers can exit successfully. Describe the expected behavior Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. In local master.cc WaitForNotification function is as follows Workers hang at n WaitForNotification . We confuse that WaitForNotificationWithTimeout n timeout in us has been called why WaitForNotification is called again bt is as follows Using thread apply all bt in gdb we can see there are two threads wait at cond var .wait l in BackgroundWorker WorkerLoop method. ,Please assign to the owner of this code. I am not the owner. mrry any guesses here From the bug description all we know is that some Session.run call is blocked from the thread blocked in WaitForNotification and the session is still active from the two tf.data background threads . Without a way to reproduce the problem there s no way to tell why it s blocked. From previous experience it s possible that the termination logic in Estimator with or without StopAtStepHook has a race condition. One case that I ve seen is the past is that there might be an accidental dependency between worker tasks where when one worker exits cleanly another worker might start a concurrent Session.run call that depends on the exited worker and will block forever waiting for that worker to come back up. However I would have expected the default worker device filter to prevent it based on the code here https github.com tensorflow tensorflow blob 5b0c5251806c0fc8a704f5454a89d7c2b65c4e12 tensorflow python estimator run config.py L567 Are you using that device filter Which tasks worker or PS have exited if any when you observe the hang From the bug description all we know is that some Session.run call is blocked from the thread blocked in WaitForNotification and the session is still active from the two tf.data background threads . Without a way to reproduce the problem there s no way to tell why it s blocked. From previous experience it s possible that the termination logic in Estimator with or without StopAtStepHook has a race condition. One case that I ve seen is the past is that there might be an accidental dependency between worker tasks where when one worker exits cleanly another worker might start a concurrent Session.run call that depends on the exited worker and will block forever waiting for that worker to come back up. However I would have expected the default worker device filter to prevent it based on the code here tensorflow tensorflow python estimator run config.py https github.com tensorflow tensorflow blob 5b0c5251806c0fc8a704f5454a89d7c2b65c4e12 tensorflow python estimator run config.py L567 Line 567 in 5b0c525 tensorflow tensorflow commit 5b0c5251806c0fc8a704f5454a89d7c2b65c4e12 device filters job ps job worker task d self. task id Are you using that device filter Which tasks worker or PS have exited if any when you observe the hang It workers Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 26559 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 26559 No a From the bug description all we know is that some Session.run call is blocked from the thread blocked in WaitForNotification and the session is still active from the two tf.data background threads . Without a way to reproduce the problem there s no way to tell why it s blocked. From previous experience it s possible that the termination logic in Estimator with or without StopAtStepHook has a race condition. One case that I ve seen is the past is that there might be an accidental dependency between worker tasks where when one worker exits cleanly another worker might start a concurrent Session.run call that depends on the exited worker and will block forever waiting for that worker to come back up. However I would have expected the default worker device filter to prevent it based on the code here tensorflow tensorflow python estimator run config.py https github.com tensorflow tensorflow blob 5b0c5251806c0fc8a704f5454a89d7c2b65c4e12 tensorflow python estimator run config.py L567 Line 567 in 5b0c525 tensorflow tensorflow commit 5b0c5251806c0fc8a704f5454a89d7c2b65c4e12 device filters job ps job worker task d self. task id Are you using that device filter Which tasks worker or PS have exited if any when you observe the hang It workers Hi did you find why there is an accidental dependency between worker tasks 
28288,Race condition with keras model to estimator in distributed mode, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Centos7 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below 1.12.0 Python version 3.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Describe the current behavior We are using distributed tensorflow as described here with ParameterServerStrategy https www.tensorflow.org api docs python tf estimator train and evaluate Basically we are starting the tf server on every worker and then running train and evaluate on each worker. The estimator function is serialized sent to each worker and then executed to create the estimator create the graph and start the training. This works with standard estimator models but doesn t when using keras models with model to estimator doing this still seems the advised way to do distributed learning with keras https colab.research.google.com github lamberta models blob keras estimator tutorial samples core tutorials estimators keras estimator.ipynb we also tried new standalone mode without any success Some nodes are failing with an IO error when trying to save the first checkpoint concurrently When creating the estimator on each worker it calls model to estimator on each worker which calls save first checkpoint l457 https github.com tensorflow estimator blob 1d55f01d8af871a35ef83fc3354b9feaa671cbe1 tensorflow estimator python estimator keras.py Describe the expected behavior Being able to train Keras model in distributed mode. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Basically we are executing this code on each worker Full code example is here https github.com criteo tf yarn blob master examples keras example.py Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. Stacktrace of failure Traceback most recent call last File .. task commons.py line 59 in get experiment experiment dill.loads client.kv.wait KV EXPERIMENT FN File .. init .py line 233 in new experiment fn File keras example.py line 76 in experiment fn File tmp 347e8353 e113 45e6 bc3a a89be4f9788a install tensorflow 1.12.0 cp36 cp36m manylinux1 x86 64.whl.e0ceba8cc1b266d3356296be2708e12fb322668e tensorflow 1.12.0 cp36 cp36m manylinux1 x86 64.whl tensorflow python estimator keras.py line 484 in model to estimator config File tmp 347e8353 e113 45e6 bc3a a89be4f9788a install tensorflow 1.12.0 cp36 cp36m manylinux1 x86 64.whl.e0ceba8cc1b266d3356296be2708e12fb322668e tensorflow 1.12.0 cp36 cp36m manylinux1 x86 64.whl tensorflow python estimator keras.py line 367 in save first checkpoint saver.save sess latest path File tmp 347e8353 e113 45e6 bc3a a89be4f9788a install tensorflow 1.12.0 cp36 cp36m manylinux1 x86 64.whl.e0ceba8cc1b266d3356296be2708e12fb322668e tensorflow 1.12.0 cp36 cp36m manylinux1 x86 64.whl tensorflow python training saver.py line 1441 in save self.saver def.filename tensor name checkpoint file File tmp 347e8353 e113 45e6 bc3a a89be4f9788a install tensorflow 1.12.0 cp36 cp36m manylinux1 x86 64.whl.e0ceba8cc1b266d3356296be2708e12fb322668e tensorflow 1.12.0 cp36 cp36m manylinux1 x86 64.whl tensorflow python client session.py line 929 in run run metadata ptr File tmp 347e8353 e113 45e6 bc3a a89be4f9788a install tensorflow 1.12.0 cp36 cp36m manylinux1 x86 64.whl.e0ceba8cc1b266d3356296be2708e12fb322668e tensorflow 1.12.0 cp36 cp36m manylinux1 x86 64.whl tensorflow python client session.py line 1152 in run feed dict tensor options run metadata File tmp 347e8353 e113 45e6 bc3a a89be4f9788a install tensorflow 1.12.0 cp36 cp36m manylinux1 x86 64.whl.e0ceba8cc1b266d3356296be2708e12fb322668e tensorflow 1.12.0 cp36 cp36m manylinux1 x86 64.whl tensorflow python client session.py line 1328 in do run run metadata File tmp 347e8353 e113 45e6 bc3a a89be4f9788a install tensorflow 1.12.0 cp36 cp36m manylinux1 x86 64.whl.e0ceba8cc1b266d3356296be2708e12fb322668e tensorflow 1.12.0 cp36 cp36m manylinux1 x86 64.whl tensorflow python client session.py line 1348 in do call raise type e node def op message tensorflow.python.framework.errors impl.UnknownError viewfs root .. keras keras model.ckpt.index.tempstate16835974976294242898 Input output error node save SaveV2 defined at keras example.py 76 SaveV2 dtypes DT FLOAT DT INT64 DT FLOAT DT FLOAT DT FLOAT ... DT FLOAT DT FLOAT DT FLOAT DT FLOAT DT FLOAT device job localhost replica 0 task 0 device CPU 0 arg save Const 0 0 save SaveV2 tensor names save SaveV2 shape and slices SGD decay Read ReadVariableOp SGD iterations Read ReadVariableOp SGD lr Read ReadVariableOp SGD momentum Read ReadVariableOp dense bias Read ReadVariableOp dense kernel Read ReadVariableOp dense 1 bias Read ReadVariableOp dense 1 kernel Read ReadVariableOp dense 2 bias Read ReadVariableOp dense 2 kernel Read ReadVariableOp global step training SGD Variable Read ReadVariableOp training SGD Variable 1 Read ReadVariableOp training SGD Variable 2 Read ReadVariableOp training SGD Variable 3 Read ReadVariableOp training SGD Variable 4 Read ReadVariableOp training SGD Variable 5 Read ReadVariableOp ,omalleyt can you take a look This code seems to do model to estimator in each worker which will have the concurrent issue. Instead why not use model to estimator locally and then train and evaluate using the right cluster config i.e. let estimator do the right replication tanzhenyu I didn t manage to make keras model to estimator work with standalone mode nor only keras . Finally what I do is to call the model to estimator once locally to create the checkpoint and then start learning https github.com criteo tf yarn blob master examples keras example.py L93 It works. Workaround is good enough. So you can close this as won t fix. Yeah that seems to be the right way to do it. In general we only allow one machine to write checkpoint to prevent race condition that s probably true in synchronous training as well. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28288 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28288 No a 
28891,tf.contrib.distribute.CollectiveAllReduceStrategy always failed for OS error or socket closed,When using tf.contrib.distribute.CollectiveAllReduceStrategy to distribute train on 16 workers with 2 GPU each my job always failed with OS error or socket closed after running for several thousands of steps. I also tried 8 workers and different batch size the failure always reproduce. And when using parameter server strategy all my jobs can run successfully and I believe this was not caused by my cluster. Thank you so much for helping investigating this problem. System information Have I written custom code as opposed to using a stock example script provided in TensorFlow using estimator and train and evaluate OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device when training TensorFlow installed from source or binary source TensorFlow version use command below tf 1.13 Python version python 3.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version CUDA 10 GPU model and memory P100 16G Describe the current behavior When using tf.contrib.distribute.CollectiveAllReduceStrategy to distribute train on 16 workers with 2 GPU each my job always failed with OS error or socket closed after running for several thousands of steps. I also tried 8 workers and different batch size the failure always reproduce. Describe the expected behavior The job should keep training without failure Code to reproduce the issue Other info logs 2019 05 19T15 07 45.079Z 1 13 stderr INFO tensorflow loss 3.7297182 step 10300 50.597 sec 2019 05 19T15 07 45.080Z 1 14 stderr INFO tensorflow loss 3.7297182 step 10300 50.608 sec 2019 05 19T15 07 45.081Z 1 14 stderr INFO tensorflow global step sec 1.97598 2019 05 19T15 08 13.129Z 1 4 stderr 2019 05 19 15 08 13.128241 E tensorflow core common runtime ring reducer.cc 369 Aborting RingReduce with Unavailable Socket closed 2019 05 19T15 08 13.129Z 1 4 stderr 2019 05 19 15 08 13.128323 W tensorflow core common runtime base collective executor.cc 203 BaseCollectiveExecutor StartAbort Unavailable Socket closed 2019 05 19T15 08 14.047Z 1 1 stderr Segmentation fault core dumped or os error 2019 05 20T12 05 35.816Z 1 12 stderr INFO tensorflow loss 5.7242765 step 3400 57.129 sec 2019 05 20T12 05 35.817Z 1 14 stderr INFO tensorflow loss 5.7242765 step 3400 57.107 sec 2019 05 20T12 05 41.291Z 1 7 stderr 2019 05 20 12 05 41.289749 E tensorflow core common runtime ring reducer.cc 369 Aborting RingReduce with Unavailable OS Error 2019 05 20T12 05 41.291Z 1 7 stderr 2019 05 20 12 05 41.289831 W tensorflow core common runtime base collective executor.cc 203 BaseCollectiveExecutor StartAbort Unavailable OS Error 2019 05 20T12 05 41.774Z 1 1 stderr Segmentation fault core dumped ,Ping dubey There was a bug in grpc which has been fixed recently. Could you try a newer version of TF It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue Automatically closing this out since I understand it to be resolved but please let me know if I m mistaken.Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28891 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28891 No a yuefengz One question for your answer There was a bug in grpc which has been fixed recently. Could you try a newer version of TF Does TF 1.14 contains this bug fix 
29060, Cache iterator is in an invalid state error, System information OS Platform and Distribution macOS High Sierra 10.13.6 TensorFlow for CPU installed from PyPI TensorFlow version v1.13.0 rc2 5 g6612da8951 1.13.1 Python version 3.6.6 Describe the current behavior Minimal not working example The code above properly iterates through the dataset only the first run when a cache doesn t exist. But when it loads the dataset from the cache file it crashes with an error A workaround It happens because the map operation follows right after the cache . It starts working as expected if some other dataset operation is added between cache and map steps. For example , apls777 I have executed the first part of the code in TF 1.12.0 did not receive any error. Please try and let us know how it progresses. Thanks muddham It works in TF 1.12.0. Just changed the filename to . cache1 instead of cache1 otherwise it throws an error for the first run apls777 As it is working now are you happy for this issue to be closed muddham No clearly it s a bug that should be fixed in the latest stable version. apls777 I ran the code in TF 1.13 GPU the output I got was below. 0 1 2 3 4 5 6 7 8 9 out of range In TF 1.13 CPU I got the below error. AttributeError BatchDataset object has no attribute make one shot iterator muddham Did you run the code twice to load the dataset from a cache Are you sure you re running it on TF 1.13 .1 What version of Python do you use muddham Did you run the code twice to load the dataset from a cache Are you sure you re running it on TF 1.13 .1 What version of Python do you use I am able to reproduce the issue with TF 1.13.1 and Python 3.6.7. apls777 thank you for reporting the issue and providing a minimal reproducible example. I can confirm this is an issue. The problem is that the existing cache transformation produces an error if the dataset transformation that consumes its input asks for an input after the cache transformations has reached the end of its input. This happens in your repro because .map fn .batch batch size will get fused into .map and batch fn batch size which will ask for batch size worth of elements at once. Because the input cardinality 10 is not divisible evenly by the batch size 4 the invalid cache op kernel behavior is triggered. Inserting a transformation between map and batch will prevent the fusion from happening and so does disabling the fusion through tf.data options https www.tensorflow.org api docs python tf data experimental OptimizationOptions map and batch fusion . I have a fix for this and expect it to merged to master by the end of this week. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29060 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29060 No a apls777 I m running into this issue when caching before interleave tf 2.0.0 beta1 and python 3.6.8 devstein thank you for reporting the problem. This is a different issue so please create a new issue for it and reference it here. Thank you. jsimsa Will do 
29681,Cannot run a Process under a Thread when using tf.set random seed, System information OS Platform and Distribution Linux Ubuntu 16 and 18 TensorFlow version use command below 1.13.1 Python version 3.6.8 No issue on tf 1.5.0 Python 3.6.8 and Ubuntu 16 Issue When using tf.set random seed I cannot run a Process under a Thread Code This script ouput And the process block. Expected output ,I could able to reproduce the above issue with Tensorflow 1.13.1 on my environment. Thanks Apologies for the delay in response. This is fixed in TF 1.14.0 However if you select gpu accelerator the code will produce required result in TF 1.13.1 Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29681 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29681 No a 
29695,Scripts halts with make initializable iterator with non empty shared name, System information Custom code Linux Ubuntu 16.04 TensorFlow installed from source TensorFlow version 1.12.0 Python version 2.7.12 Bazel version 0.25.1 CUDA cuDNN version 9.0.176 7.0 GPU model and memory GeForce GTX 1080Ti Describe the current behavior The script doesn t finish. I discovered that it halts after TF CloseSession and even KeyboardInterrupt can t stop the script. I also discovered that it exits normally if I pass shared name . So it looks like that this place https github.com tensorflow tensorflow blob master tensorflow core kernels data iterator ops.cc L465 contains the bug. Possible deadlock or session can wait for iterator to free resources. I also can reproduce the problem with MacOS without Cuda. Describe the expected behavior I expect this script to finish normally. Code to reproduce the issue Other info logs Traceback from gdb Looking at the info threads we can see that all the threads are waiting for something ,I am able to reproduce the reported issue with Tensorflow 1.12.0 and 1.13.1. Thanks I can reproduce this issue on the latest master branch. PR 30102 is submitted to try to fix this issue. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29695 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29695 No a 
29911,Input signature of a tf.function decorator crashes when using multiple GPUs with MirroredStrategy, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 CentOS Linux 7.6.1810 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary pip binary TensorFlow version use command below tensorflow gpu 2.0.0 beta0 Python version 3.6.8 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 10.0.130 7.6.0 GPU model and memory Tesla P100 SXM2 16GB Describe the current behavior Tensorflow crashes when checking the input signature of a tf.function decorator when using multiple GPUs in a MirroredStrategy. A ValueError is generated cause a PerReplica object cannot be converted to a Tensor see the log below . Below you can find the minimum code needed to reproduce the error. The code runs just fine when I only utilize one GPU strategy tf.distribute.MirroredStrategy devices gpu 0 . Furthermore if the optional argument input signature is discarded only using tf.function the error disappears too again using multiple GPUs . Hence the specific combination of input signature and multiple GPUs causes the problem which I need for performance reasons in my work . Describe the expected behavior The code below won t generate any errors. Code to reproduce the issue Other info logs Traceback most recent call last File data gent gvo000 gvo00003 vsc41939 GENIUS miniconda3 envs tensorflow2 lib python3.6 site packages tensorflow python eager function.py line 1216 in convert inputs to signature value dtype hint spec.dtype File data gent gvo000 gvo00003 vsc41939 GENIUS miniconda3 envs tensorflow2 lib python3.6 site packages tensorflow python framework ops.py line 1100 in convert to tensor return convert to tensor v2 value dtype preferred dtype name File data gent gvo000 gvo00003 vsc41939 GENIUS miniconda3 envs tensorflow2 lib python3.6 site packages tensorflow python framework ops.py line 1158 in convert to tensor v2 as ref False File data gent gvo000 gvo00003 vsc41939 GENIUS miniconda3 envs tensorflow2 lib python3.6 site packages tensorflow python framework ops.py line 1237 in internal convert to tensor ret conversion func value dtype dtype name name as ref as ref File data gent gvo000 gvo00003 vsc41939 GENIUS miniconda3 envs tensorflow2 lib python3.6 site packages tensorflow python framework constant op.py line 305 in constant tensor conversion function return constant v dtype dtype name name File data gent gvo000 gvo00003 vsc41939 GENIUS miniconda3 envs tensorflow2 lib python3.6 site packages tensorflow python framework constant op.py line 246 in constant allow broadcast True File data gent gvo000 gvo00003 vsc41939 GENIUS miniconda3 envs tensorflow2 lib python3.6 site packages tensorflow python framework constant op.py line 254 in constant impl t convert to eager tensor value ctx dtype File data gent gvo000 gvo00003 vsc41939 GENIUS miniconda3 envs tensorflow2 lib python3.6 site packages tensorflow python framework constant op.py line 115 in convert to eager tensor return ops.EagerTensor value handle device dtype ValueError Attempt to convert a value PerReplica 0 job localhost replica 0 task 0 device GPU 0 tf.Tensor id 107 shape 2 12 dtype float32 numpy array 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. dtype float32 1 job localhost replica 0 task 0 device GPU 1 tf.Tensor id 108 shape 2 12 dtype float32 numpy array 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. dtype float32 with an unsupported type class tensorflow.python.distribute.values.PerReplica to a Tensor. During handling of the above exception another exception occurred Traceback most recent call last File issue2.py line 19 in module output distributed run x File data gent gvo000 gvo00003 vsc41939 GENIUS miniconda3 envs tensorflow2 lib python3.6 site packages tensorflow python eager def function.py line 432 in call args kwds File data gent gvo000 gvo00003 vsc41939 GENIUS miniconda3 envs tensorflow2 lib python3.6 site packages tensorflow python eager function.py line 1169 in canonicalize function inputs self. flat input signature File data gent gvo000 gvo00003 vsc41939 GENIUS miniconda3 envs tensorflow2 lib python3.6 site packages tensorflow python eager function.py line 1222 in convert inputs to signature str inputs str input signature ValueError When input signature is provided all inputs to the Python function must be convertible to tensors.Inputs PerReplica 0 job localhost replica 0 task 0 device GPU 0 tf.Tensor id 107 shape 2 12 dtype float32 numpy array 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. dtype float32 1 job localhost replica 0 task 0 device GPU 1 tf.Tensor id 108 shape 2 12 dtype float32 numpy array 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. dtype float32 input signature TensorSpec shape None 12 dtype tf.float32 name None .,Thank you for reporting we are going to look into fixing this issue. In the meantime I am wondering how you can get around this issue to get the performance you re seeking. I am assuming that your input data has varying dimensions in various steps and you don t want to tf.function to retrace everytime that happens. And you re trying to avoid that re tracing by providing an appropriate input signature. The 3 possible workarounds could be Try the experimental relax shapes argument to tf.function https www.tensorflow.org versions r2.0 api docs python tf function. This may not give you the most optimal but it could work. Instead of passing the actual inputs as argument to the tf.function pass something that can be used to get the inputs. For instance pass a callable that when called will return the data. This callable will never change so the function will not retrace. This is kind of hacky but something we have used sometimes to get good performance. You can also create the iterator on the dataset by iter dataset and pass that as argument to the function and do next inside the function to get the inputs. That will also get around this issue I believe. You could also consider passing the entire dataset into the tf.function and iterate inside it. This will arguably give you the best performance as it will convert the entire thing into a tf.while loop. Thanks for the reply and the suggestions. I will experiment with them in the following days. I had the same problem and am curious if the suggestions and which one specifically worked for you mcoolsce dsgupta I tried all the options and the second one seems to work fine The argument experimental relax shapes had no effect at all. This is the option I am using now. I create my dataset and make an iterable out of it which is the only argument going into the tf.function. Inside the function I call next dataset to get access to my data. Performance wise tested on 1 GPU it seems that is option is a little bit slower compared to my original code. I think the third option is theoretically the fastest but it throws some Out of Memory errors a few iterations in the converted tf.while loop. I just pass my entire dataset into the tf.function wrapper and used a python for loop to iterate over the data. This is very strange as I had no memory issues at all before where I iterate over the data using a python for loop outside the tf.function . mcoolsce Thanks for the response I faced the same sort of problem with the third approach but instead of Out of Memory I was getting Recursion Depth Exceeded if I tried using for input in dataset I ll try the second one and see how it goes. The first one doesn t work. I tried the second option but it turns out to be another problem BaseCollectiveExecutor StartAbort Out of range End of sequence . The third option is not suitable for me since I want to dynamically adjust the training strategy with the loss from the train step. Any other suggestions Slyne is the issue you encountered with second approach a bug perhaps you can file a separate issue for that and we can figure out how you can make progress with that the only other workaround I can think of is if you want to actually make up the distributed input signature yourself. I can provide the code for that but it requires using private TF APIs. Slyne is the issue you encountered with second approach a bug perhaps you can file a separate issue for that and we can figure out how you can make progress with that the only other workaround I can think of is if you want to actually make up the distributed input signature yourself. I can provide the code for that but it requires using private TF APIs. The second workaround gives an error like this issue https github.com tensorflow tensorflow issues 22484 unless I stop iterating the dataset before the last step. Slyne can you share the code for what you re doing when using the second workaround likely you need to change your code to catch the out of range error or i maybe able to suggest a slightly different API to get the next element Any info about when will this bug be fixed Any info about when this bug will be fixed. For dynamic batch size inputs this input signature is essential otherwise it will lead to tracing which is very expensive. This issue has now been fixed e1136256a791b02f793ff3f3e1f9ee7da4606807 . You can use the element spec property on the dataset or iterator to specify the tf.TypeSpec . For example Please reopen if you run into issues. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 29911 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 29911 No a This issue has now been fixed e1136256a791b02f793ff3f3e1f9ee7da4606807 . You can use the element spec property on the dataset or iterator to specify the tf.TypeSpec . For example Please reopen if you run into issues. This method only works for tf nightly gpu Tensorflow version 2.2.0 and not for TF 2.0 anj s alisaaalehi the provided solution doesn t work for me on 2.2.0.dev20200207 . can you clarify which function would need to be decorated the inner train step or the distributed train step my guess is that in my case the input signature doesn t match as I modified the input in the training step x encoder x . Is that correct What would be a suggested better way to handle thins Update removing the encoder thus making it part of the model does not yield into a crash and even works for 2.1.0 stable. That being said I would add the following question Is it possible and are there any caveats to have overwrite an input signature that does not match with dataset.element spec so that it works in the case mentioned above Of course using variable shape tensors as in the example by mcoolsce This is still not working even in TF 2.2.0 and 2.3 nightlly gpu versions. Any thoughts on this goldiegadde anj s s4sarath please provide code to repro your issue either here or in a new bug . Hi guptapriya . Thanks for the quick response. I have tested this both on TF 2.2.0 GPU and TF 2.3 nightly build also . On colab. a. Using MirroredStrategy experimental distribute TF 2.2.0 TypeError Traceback most recent call last ipython input 45 da85827572ff in module 1 train no tracing ds 12 frames usr local lib python3.6 dist packages tensorflow python framework func graph.py in wrapper args kwargs 966 except Exception as e pylint disable broad except 967 if hasattr e ag error metadata 968 raise e.ag error metadata.to exception e 969 else 970 raise TypeError in user code TypeError tf call for each replica missing 1 required positional argument kwargs b. Same code in TF nightly build 2.3.0 dev20200605 Now error is somewhat similar not exact TypeError Traceback most recent call last ipython input 6 679d965926a5 in module 12 break 13 14 train no tracing ds 13 frames usr local lib python3.6 dist packages tensorflow python framework func graph.py in wrapper args kwargs 966 except Exception as e pylint disable broad except 967 if hasattr e ag error metadata 968 raise e.ag error metadata.to exception e 969 else 970 raise TypeError in user code usr local lib python3.6 dist packages tensorflow python distribute mirrored run.py 96 call for each replica return call for each replica strategy fn args kwargs TypeError tf step fn takes 1 positional argument but 2 were given c. Using experimental distribute from dataset function TypeError tf call for each replica missing 1 required positional argument kwargs Here is the gist for TF 2.2.0 https colab.research.google.com gist s4sarath 899d93bd5381efd34dffe9f89c2688a6 dataset gpu.ipynb Here is the gist for TF 2.3 nightly https colab.research.google.com gist s4sarath f42fece3e4f217ee3c0b9a5873223160 dataset gpu nightly.ipynb To give some idea about dataset it is having same batch size but different inupt length in each batch. dynamic input to avoid unnecessary padding . Thats what tracing is precisely using for I guess . thanks s4sarath. anj s could you take a look at the attached colabs This looks like a bug to me it could likely be due to how we handle fn passed to run that are annotated with tf.functions. s4sarath one thing you could try right now is to put the call to run inside a tf.function and add the input signature to that. See this unit test here https github.com tensorflow tensorflow blob a00daa2f37954ed7d1fae09dfad81b3168b76715 tensorflow python distribute input lib type spec test.py L184 
30306, tf.keras predict generator stuck with using use multiprocessing True, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Debian 9.6 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device NA TensorFlow installed from source or binary Binary TensorFlow version use command below 1.14 Python version 3.5 Bazel version if compiling from source NA GCC Compiler version if compiling from source NA CUDA cuDNN version 10.0 GPU model and memory Tesla P100 16280MiB You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior When I use model.predict generator with use multiprocessing True the code gets stuck. Describe the expected behavior Ideally the code should not get stuck and all cores should be used for predictions. Code to reproduce the issue Other info logs NA, jashshah When trying to reproduce the issue NameError name model is not defined. Can you please share the complete reproducible code. rmothukuru I have shared the code to create the model. Do you want the code used to train the model as well jashshah Thank you for the code. When I tried executing it I am getting the error 79 model build model 128 128 50 3 False ValueError Input 0 of layer conv3d is incompatible with the layer expected ndim 5 found ndim 4. Full shape received None 128 128 50 . rmothukuru My apologies for the error. I have updated the code. jashshah Can you please confirm if you are getting below mentioned Exception when the execution is stuck. rmothukuru I am not getting any sort of error. The execution just remains stuck with 0 volatile GPU Utilization even though the GPU memory usage is around 15GB. rmothukuru is there any work around for now while the bug is fixed I am unable to speed up inference since I cannot use multiprocessing. jashshah The code snippet you provided is incomplete. Can you please update it and also as a sanity check for gpu config. Can you please try to execute your code in google colab https colab.sandbox.google.com notebooks welcome.ipynb recent true using gpu clcik edit notebook settings and select gpu from the drop down . Thanks ymodak can you tell me where the code is incomplete I believe rmothukuru was able to reproduce the issue. The only thing that I cannot provide you with is the data since that is proprietary. ymodak Sorry for the confusion. Reproduced the code in Google Colab with GPU as Runtime with TF Version 1.14 and with Dummy Data. This is the Gist https colab.sandbox.google.com drive 1BmaVlJHPX9dsJ1QNpkZKbbZkgWBvdaLR scrollTo VjoPXkL5SNbN . Execution was stuck as explained by jashshah with the exception shown below as I might have used X TEST and Y TEST as simple integers. Thanks. jashshah Apologies for the delay in response. I believe argument is irrelevant in this case since the code snippet fails with same error even after setting it to false. Were you able to execute it by setting it to false ymodak I am able to execute it when use multiprocessing argument is set to False . My guess is that a deadlock occurred within keras.utils.OrderedEnqueuer which is used when the generator is a sequence . Please confirm if you experience the same stall with fit generator or evaluate generator. Also please check if keras.experimental.terminate keras multiprocessing pools returns any useful errors https www.tensorflow.org api docs python tf keras experimental terminate keras multiprocessing pools Thanks Closing as there isn t enough information to reproduce the issue. Feel free to re open if you have a run able colab. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30306 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30306 No a rmothukuru did you solved the problem I m getting the same error 
30806,Inference on Larger Data Size on Edge , System information Have I written custom code Custom Code using resources and my own design OS Platform and Distribution Linux Ubuntu 16.04 Mobile device trying inference on google coral dev board TensorFlow installed from pip3 install tensorflow 2.0.0 beta1 TensorFlow version 2.0.0 beta1 Python version 3.6.9 Bazel version not used GCC Compiler version not used CUDA cuDNN version not used GPU model and memory not used Run On CPU Architecture x86 64 CPU op mode s 32 bit 64 bit Byte Order Little Endian CPU s 8 On line CPU s list 0 7 Thread s per core 2 Core s per socket 4 Socket s 1 NUMA node s 1 Vendor ID GenuineIntel CPU family 6 Model 94 Model name Intel R Core TM i7 6700HQ CPU 2.60GHz Stepping 3 CPU MHz 800.210 CPU max MHz 3500.0000 CPU min MHz 800.0000 BogoMIPS 5183.86 Virtualization VT x L1d cache 32K L1i cache 32K L2 cache 256K L3 cache 6144K 32 GB RAM Because of quantization issue for UpSampling2D I used a snippet from https colab.research.google.com gist ohtaman c1cf119c463fd94b0da50feea320ba1e edgetpu with keras.ipynb My issue is when I use image sizes from 28 till 256 I just used powers of 2 and used 28 just to check it worked fine for inferencing. I tested inferencing on the CPU itself using the interpreter package. But 512 or anything above freezes the machine and gives the following error. 2019 07 17 08 33 18.567609 W tensorflow core framework cpu allocator impl.cc 81 Allocation of 5368709120 exceeds 10 of system memory. 2019 07 17 08 33 20.626046 W tensorflow core framework cpu allocator impl.cc 81 Allocation of 5368709120 exceeds 10 of system memory. 2019 07 17 08 33 26.517300 W tensorflow core framework cpu allocator impl.cc 81 Allocation of 10737418240 exceeds 10 of system memory. 2019 07 17 08 33 27.836867 W tensorflow core framework cpu allocator impl.cc 81 Allocation of 21474836480 exceeds 10 of system memory. terminate called after throwing an instance of std bad alloc what std bad alloc Aborted core dumped Is there a limit for the array size that can be used at inferencing after making the input array flatten. Or am I doing something wrong here , ravikyram liyunlu0618 Were you able to follow this issue For now I use a workaround and it adds unnecessary complication to our system. I just need to clarify this before we design the production model. Is this a known issue or something else is wrong in my end vibhatha Is this still an issue Can you check with TF2.0 and let us know whether the issue persists with latest TF version. Thanks Yes it is. I checked with TF2.0rc versions. Couldn t check with the stable release. I will also check that. Is it possible for you to test this as well vibhatha Could you please let us know if this issue still persist. This is a 7 month old issue. At that time I solved this by feeding smaller images and stitching this up. I haven t looked into this after that time. Sorry I cannot confirm it now. I no longer work on this project. vibhatha please confirm if we can move this issue to closed status Yes please close the issue. Closing the issue since its resolved. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 30806 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 30806 No a 
31114,TFLite No implementation found for long org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter int , System information OS Platform and Distribution Android 5.1.1 API 22 Mobile device Xiaomi Redmi 3 TensorFlow installed from official binary TensorFlow version tensorflow lite 1.14.0 Describe the current behavior Tensorflow lite 1.13.1 works fine on all devices I tested. Whereas tensorflow lite 1.14.0 is broken for Xiaomi Redmi 3 Android 5.1.1 API 22 other devices are ok. I get a runtime error when Interpreter is created. Describe the expected behavior No error. Code to reproduce the issue Other info logs ,Did you solve it Firebase crashlytics reports this issue for Android 5 users of my app I added Tensorflow like this implementation org.tensorflow tensorflow lite 0.0.0 nightly implementation org.tensorflow tensorflow lite gpu 0.0.0 nightly Fatal Exception java.lang.UnsatisfiedLinkError No implementation found for long org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter int tried Java org tensorflow lite NativeInterpreterWrapper createErrorReporter and Java org tensorflow lite NativeInterpreterWrapper createErrorReporter I Did you solve it anonym24 nope I guess we can t do much about it without a fix in tf. BTW I guess you should not put both org.tensorflow tensorflow lite and org.tensorflow tensorflow lite gpu in your gradle file at the same time . But that s not the issue here anyway... From crashlitics does the crash affects only Android 5. . and all of them wosiu yes we need them both https www.tensorflow.org lite performance gpu image https user images.githubusercontent.com 8851301 62043870 2fa0df80 b20a 11e9 9c14 d56096189366.png wosiu in my case yes only Android 5 image https user images.githubusercontent.com 8851301 62052901 29692e00 b21f 11e9 8b3a fee666ab05f1.png liyunlu0618 are there any updates on that I also encounter this problem I resolve this by reconfigure my ndk abifilter settings. My project originally was only built for armeabi architecture. When I add any other abi options it works like a charm. However I m still confused because on the tutorial page of Tensorflow Lite it says that the library support all kinds of architecture which should include armeabi . Or is it because arm support was removed in ndk r17 Hope someone can help me to find a workaround for tflite to run on armeabi devices. Did anyone find solution for this I am also facing the same issue. I tested in Oneplus3. 2019 09 06 08 28 18.799 15233 15233 ai.fritz.tflitedemo E ritz.tflitedem No implementation found for long org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter int tried Java org tensorflow lite NativeInterpreterWrapper createErrorReporter and Java org tensorflow lite NativeInterpreterWrapper createErrorReporter I 2019 09 06 08 28 18.802 15233 15233 ai.fritz.tflitedemo E AndroidRuntime FATAL EXCEPTION main Process ai.fritz.tflitedemo PID 15233 java.lang.UnsatisfiedLinkError No implementation found for long org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter int tried Java org tensorflow lite NativeInterpreterWrapper createErrorReporter and Java org tensorflow lite NativeInterpreterWrapper createErrorReporter I at org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter Native Method Gradle implementation org.tensorflow tensorflow lite 0.1.2 nightly implementation org.tensorflow tensorflow lite 0.1 implementation org.tensorflow tensorflow lite gpu 0.0.0 nightly Any update on this I m also facing the same issue on 1.14.0 I m also got this crash I had a similar issue from NDK namely that arm64 Android 5.x devices failed to load the shared library libtensorflowlite.so because of register atfork being missing. The problem is that for at least arm64 register atfork usage is somehow generated in the shared library but this function is introduced in Android 6 https android.googlesource.com platform bionic master android changes for ndk developers.md . As suggested in the link I tried compiling with lower NDK target API level but this didn t seem to help. I also tried searching for atfork in tensorflow lite s source code but couldn t find any. Workaround for at least NDK users Disassembling libtensorflowlite.so left an impression that register atfork is only called while reporting errors so at least theoretically it shouldn t be called in production anyway. The workaround was thus to introduce fake register atfork declaration and definition in cpp and h files so that linker wouldn t complain anymore. So I put into tensorflow lite util.h and into tensorflow lite util.cc the exact files aren t important the definition just has to be compiled and linked to libtensorflowlite.so and recompiled. After that neural networks seem to work fine on all devices. I guess something similar should work for JNI users as well. I had the same issue. I fixed it by adding to the defaultConfig in app build.gradle file. kongaskristjan and FunmiKesa big thanks for investigating and sharing that Said that I feel propsed changes are more like workarounds and not solutions. It still would be nice to have a fix inside the tensorflow lib. Any updates on this It s still happening on 1.15.0 This seems like a critical bug to me tflite instantly crashes on some devices. I will bump my minApi to 23 for a while and later revert to 21 when this gets fixed EDIT Excluding 64bit architectures as some of you are doing is not a viable solution Google Play store enforces all apps to support 64bit since 1st of August 2019 https android developers.googleblog.com 2019 01 get your apps ready for 64 bit.html Hi all apologies for the delayed response just now seeing this issue. We set our Android NDK API level to 18 when building but it s possible there s something wrong with the config and it s assuming API 23. I ll take a look. This is fixed in the latest nightly and we ll try to pull the fix in to the 2.1 release https github.com tensorflow tensorflow pull 34419 . Thanks again for your patience. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 31114 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 31114 No a Hi jdduke thanks for the fix. Just to understand how we can resolve this issue on android would we be able to pull in the nightly build of TFL in gradle and have this fixed or do we have to also reconvert our models to TFL using the nightly build jdduke are you going to backport the fix to 1. version There won t be any backport however you can safely use the latest 2.x or nightly version with both 1.X and 2.X TF models. Hello jdduke please show me what I need to put in gradle.build to use latest fix runnableapps Answer or wait for 2.1 release. implementation org.tensorflow tensorflow lite 0.0.0 nightly does not fix crash You might need to clear your gradle cache https stackoverflow.com questions 23025433 how to clear gradle cache . I ve manually inspected the nightly build https bintray.com google tensorflow tensorflow lite files org 2Ftensorflow 2Ftensorflow lite 2F0.0.0 nightly and verified that register atfork is no longer referenced. There might be another issue with your usage in which case please attach the log from adb. Got the same problem with org.tensorflow tensorflow lite 2.0.0 Device T8 PLUS Android 5.1 alexeyvasilyev the fix did not make it into 2.0 but it will be in the upcoming 2.1 release expected to be finalized soon . In the meantime please try the nightly build. jdduke Thanks. Lets hope nightly build can sort out this issue in android 5. The 2.1 release is now available org.tensorflow tensorflow lite 2.1.0 please give it a try. I confirm 2.1.0 works on the device I originally issued the problem with Big thanks to all contributors 
31917,Intermittent crash with CUDA ERROR LAUNCH FAILED failure, System information OS Platform and Distribution on Linux based ML engine runtime version 1.12 which means it is using TensorFlow 1.12 with Python 3.6. CUDA cuDNN version CUDA Version 10.1 with cuDNN libcudnn.so.7.4.2 GPU model and memory Tesla K80 Exact command to reproduce running custom code on object segmentation task. The problem The problem is the training crash after reaching thousands of steps few hours . The crash is intermittent. The same code and dataset when rerun is OK on another occasion. This problems happened quite frequently 20x in my testings. One observation is that this does not happen for earlier TensorFlow 1.8. Current workaround for me is to downgrade to TF 1.8. This could be a bug on the library or CUDA driver. The same problem happened in workstation offline . The Logs Additional information logs The ML team support kindly provides machine debug information as followings FYI I did not succeed to reproduce the error with cuda memcheck or cuda gdb or CUDA DEVICE WAITS ON EXCEPTION 1. This issue is probably similar to 20356 ,Did you build TF from sources for cuda 10.1 support Also what is your nvidia driver version No I did not build the TF from the source code. I supposed the TF is preinstalled when running on the Google AI Platform . The nvidia driver version from running nvidia smi To add extra info this happens also for TF 1.13.1 although the test was not as extensive as TF 1.12. The specs for machine with TF 1.13.1 from nvidia smi is as follows jurneo could you provide a simple repro e.g. python scripts etc for the problem Also could you try with TF 1.15rc1 https pypi.org project tensorflow gpu 1.15.0rc1 or 2.0rc1 https pypi.org project tensorflow gpu 2.0.0rc1 Also nluehr are you familiar with the Graphics Exception error mentioned above The Graphics Exception is most likely a symptom of the unspecified launch failure which has left the application in a bad state. An unspecified launch failure often results from a Segfault within a GPU kernel. It is an asynchronous error in that it gets generated by a kernel executing on the GPU but isn t flagged on the CPU until the next CPU GPU sync point in this case the sync point appears to be an attempt to run a host to device memcpy . In addition to testing with the latest TF versions I would also recommend updating cudnn to the latest version 7.6.3. aaroey It is likely the driver issue on linux. To share additional recent finding this problem does not occur on TF 1.12 on Windows version. I will try your suggestion for later TF versions. Meanwhile related thread discussion is on issuetracker google 138398232 and the python script is similar to deeplab v3. I would close this issue with the workaround by upgrading to latest version of TF with latest update GPU on drivers but feel free to reopen. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31917 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31917 No a 
31952, TF 2.0 tf.gather doesn t work alongside tf.function, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Darwin Kernel Version 18.6.0 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 dev20190730 Python version Python 3.6.8 Anaconda Inc. Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior It seems that when tf.gather is called after a tf.function the gradient cannot be calculated. The example code blow shows the bug. The code itself raises the following error message AssertionError Expected all args to be Tensors or Variables but got CompositeTensor The code will work if we remove the tf.function decorator or put the tf.gather line inside the tf.funtion graph. Code to reproduce the issue ,I have tried on Colab with TF version 2.0.0 dev20190730 recent nightly version 2.0.0 dev20190825 and was able to reproduce the issue.Please find the gist https colab.research.google.com drive 1P4tejxmGIxKmXM4TFoesuaXTZUEsPpdx here.Thanks Hi We encountered the same bug which currently prevents our migration from TF1 to TF2. As David mao said it works perfectly well in Eager mode. The problem arises only when calling tf.gather on tensors returned from a tf.function and then calculating its gradients. diNatale I found a very ugly workaround for this. You wrap tf.gather into a graph function and use this gather instead of tf.gather in your code. I know it s absurd the most absurd part is to have 0 inside it which is necessary for reasons unclear to me but it works in my cases. Wow 0 of course how didn t we guess that It does work for me if I use both the wrapper and the 0 Is there any explanation for this behaviour thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 31952 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 31952 No a 
32017,Training stalls after saving checkpoint 0,Hello. I m trying to run the LibriSpeech problem using tensor2tensor on Google Colab s GPU runtime but the training stalls after saving checkpoint 0 and opening dynamic library libcublas.so.10.0. There is no error message it just stops there forever. I m posting it here because the stalling point happens on Tensorflow s packages. Python s version 3.6.8 Tensorflow s version 1.14.0 tensor2tensor s version 1.14.0 CUDA s version 10.1 OS Ubuntu 18.04 This is the code And here s the output ,Same problem. When setting TF CPP MIN VLOG LEVEL 2 it printing this Victor Almeida Will it possible to provide Google colab link to expedite the trouble shooting process. Thanks I encounter the same problem when executing tensor2tensor on my local machine. System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04.6 LTS TensorFlow installed from source or binary source TensorFlow version use command below 1.14.0 Python version Python 2.7.12 Bazel version if compiling from source 0.24.1 GCC Compiler version if compiling from source 5.4.0 CUDA cuDNN version CUDA 10.1 CUDNN 7.5 GPU model and memory Nvidia 1080Ti 11GB Describe the current behavior Code to reproduce the issue Install tensor2tensor Release v1.14.0 Here https github.com tensorflow tensorflow files 3551652 Training Speech Recognition Model with tensor2tensor.zip it is. I m experiencing the same issue. If I export TF CPP MIN VLOG LEVEL 2 before running t2t trainer I get the following error just prior to the stall. It might be unique to my custom librispeech like problem but curious if others are seeing that too Perhaps related I ve been fighting with a similar message when attempting to load an older t2t saved model into a current TFServing. Victor Almeida tenghaha Do you specifically need those versions of cuDNN and CUDA If not the official tested configuration for tf1.14 gpu looks like cuDNN 7.4 CUDA 10.0 see this https www.tensorflow.org install source tested build configurations . And you might have an easier time debugging it in comparison to something that works in CPU mode as well as in comparison to a trivially simple t2t problem and model. I m seeing the same issues running from the official 1.14.0 gpu docker image. As well as the nightly gpu. The training works fine with the simple models eg problem image mnist model shake shake . Seems to be some issue with the interaction between the T2T Librispeech problem and the new tensorflow versions. jsimsa could you triage I m guessing I was assigned due to the checkpoint keyword but it looks like checkpointing completes and this is either input pipeline optimization related or GPU related. Can you collect a stacktrace at the time your program hangs a share a link to it Can you clarify what you re looking for Since the process does not halt there is no stacktrace. A representative end of logging is shown in cantwbr post github.com tensorflow tensorflow issues 32017 issuecomment 525726395 . With TF CPP MIN VLOG LEVEL 2 enabled the logging is extremely verbose and probably not practical to post here. Here is a snip of the last 100 lines from a run of the basic problem run via t2t trainer problem librispeech clean small model transformer output dir models JUNK data dir data save checkpoints secs 1800 schedule train hparams set transformer tiny It does not halt but it does stalls and it would be good to understand what is each thread of the program doing e.g. if this is a deadlock . The log itself does not provide enough information to form a hypothesis about what could be going on and the first thing I would do to debug further is to use gdb to connect to running process to see what is each thread doing. As far as I know this will not be possible running the program in a colab though. ymodak could you please loop in someone from the trainer2trainer project as I am not familiar with that project. I ve attached a gdb to the stalled process would you want to see a bt Also FWIW the lead of the Tensor2Tensor team referred this to Tensorflow support in https github.com tensorflow tensor2tensor issues 1643 Could you run thread apply all bt and post the result in pastebin and share a link to it here Thanks. https pastebin.com w3ZevdSs The full trace was 1.6MB in size but most of the higher number threads were identical. I ve tried to edit out the mostly redundant traces. Thank you Michael. At least some of the threads seem to be waiting in tf.data. rachellim could you please try to reproduce and investigate this issue Thank you. Thank you rachellim please let me know if you have any trouble reproducing the issue on gpu . My setup is via Nvidia Docker Hub tensorflow tensorflow 1.14.0 gpu or tensorflow tensorflow nightly gpu and pip2 install tensorflow hub pip2 install tensor2tensor apt get update apt get install sox Note you ll need to include the generate data flag the first time you run to dl and prep the dataset. Did a little more testing to try to narrow down in which version of tensorflow this issue crops up... Testing with T2T 1.13.4 TF 1.13.2 works TF 1.14.0 hangs I m having trouble reproducing this with the following setup tensorflow gpu 1.14.0 tensor2tensor 1.14.0 t2t trainer problem librispeech clean small model transformer output dir tmp t2t output data dir tmp t2t data save checkpoints secs 1800 schedule train hparams set transformer librispeech It runs multiple steps without hanging Are you encountering this issue with non gpu tensorflow as well or just tensorflow gpu Hmm just setup a non gpu container and it is running w o issue there. Of course for production training we need to utilize the GPUs... Yup just trying to further narrow down the possible sources of this issue. Let me dig into this a little further. Thanks Thanks In case it is helpful the host NVIDIA GPU driver I currently have loaded is 430.34 I see the current version is 430.50 . UPDATE installed 430.50 gpu driver and got same stall behavior Could you guys check if you can resume training from checkpoint I am getting the checkpoint not valid error with this same setup on normal runtime. Em sex 13 de set de 2019 19 14 Michael Schonwetter notifications github.com escreveu Thanks In case it is helpful the host NVIDIA GPU driver I currently have loaded is 430.34 I see the current version is 430.50 . You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 32017 email source notifications email token AJ6VEEHJVI5AWQZ5M2QHVKLQJQGD5A5CNFSM4IQHWXEKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD6WJZ6A issuecomment 531406072 or mute the thread https github.com notifications unsubscribe auth AJ6VEEBDXBCVINGXZU2FAYTQJQGD5ANCNFSM4IQHWXEA . Victor Almeida just tested per your request with the non gpu version I was able to save a checkpoint and resume from that checkpoint w o error. TF1.14.0 T2T1.14.0 In case it helps I ran with TF CPP MIN VLOG LEVEL 2 enabled and here is the very end of the log where it hangs https pastebin.com xvxzyXkS rachellim When you are getting a working test Are you testing with the tensorflow tensorflow 1.14.0 gpu docker image What NVidia GPU are you using Do you have any other suggestions for how I could help isolate the cause of the hang I managed to get a repro now trying to get to the bottom of it I m using a GTX 1080 with driver version 430.34 Just adding some more findings here. Environments tensorflow 1.14.0 tensor2tensor tried 1.11 1.13 1.14 results were the same. When I use the default hparams set transformer librispeech the training will hang. I explored the problem specific hparams defined in tensor2tensor data generators speech recognition.py and tried the below changes which can avoid the hang set audio add delta deltas from True to False OR set audio preproc in bottom from False to True this moves the feature generation process from the preprocessing stage to the bottom of the transformer encoder It looks like the hang is due to some audio feature generation operations in preprocessing stage. When I removed them from preprocessing the hang is gone. Hope this helps in pinpointing the cause. In summary either of the below can make the training started Use tensorflow gpu 1.13.2. OR Use tensorflow gpu 1.14.0 with the above mentioned changes in hparams. Thanks for flagging it and the repro instructions. I found the issue in parallel interleave dataset op.cc . Fix https github.com tensorflow tensorflow commit 6274f037d4acc9d04cd4aafbda7547a3d89e5674 
33595, tf.summary.FileWriter overwrites existing events file if reopened in same logdir quickly., em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 TensorFlow installed from source or binary binary TensorFlow version use command below 1.15.0 Python version 3.6.3 CUDA cuDNN version 10.0.130 GPU model and memory GeForce 940MX 2GB Current behaviour If tf.summary.FileWriter is closed and then reopened in the same logdir FileWriter overwrites events file created right before closure. If I add a delay expr time.sleep 1 before reopening FileWriter reopened FileWriter creates new events file. Expected behaviour I expect FileWriter creating new events file regardless of how recently logdir was used. Code to reproduce the issue Other info logs ,Issue replicating for TF 1.15 kindly find the gist https colab.sandbox.google.com gist oanush 2fea3d9404fcfd69cf82dd87fe0e625f untitled.ipynb scrollTo IYPi FxzI R of colab.Thanks This is a known issue with FileWriter in TF 1.x and now that TF 2.0 is out we won t be making further changes to this API. But the new tf.summary.create file writer in TF 2.0 does a better job of avoiding this issue by including the PID and a per process UID in the filename as well as the timestamp. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33595 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33595 No a Thank you for the response 
34093, TF2.0 GradientTape.gradient raise SystemError when calling embedding column layer multiple times with tf.function, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 CentOS Linux release 7.4 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary pip install tf nightly gpu 2.0 preview TensorFlow version use command below v1.12.1 14959 g9663619 2.0.0 dev20191002 Python version 3.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version cudatoolkit 10.0.130 cudnn 7.6.4 GPU model and memory Describe the current behavior When calling a layer created from multiple times and calculate the gradients with using may cause raise depends on the number of times we call the same layer error raise if we call it more than 4 times . Without everything is fine. Describe the expected behavior The gradients should be calculated well regardless of how many times we call the layer within a . Code to reproduce the issue Other info logs ,Issue replicating for the given code for TF 2.0 please find the gist https colab.sandbox.google.com gist oanush 932cf9d8d1145a5c88ac8cecb84493e5 34093.ipynb of colab.Thanks alextp does GradientTape have an owner other than you I ve looked at most of the code but am probably not going to be giving random issues the attention they deserve... jaingaurav could you triage I m told the core team owns GradientTape. I believe this is resolved by https github.com tensorflow tensorflow pull 33912. I ll test it out and confirm. GoSz I tried with Tf nightly 2.2.0.dev20200318 but not getting any error. Please take a look at the colab gist https colab.sandbox.google.com gist gadagashwini 7d923164ab88f5e792d7abc1cacf529e untitled469.ipynb and confirm. Thanks Seems it will be fixed with TF2.2 thanks a lot. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34093 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34093 No a 
34119,TF2.0 Calling set session before fit generator causes training to freeze when using multiprocessing, System information Have I written custom code No OS Platform and Distribution Linux Ubuntu 16.04 Mobile device if the issue happens on mobile device N A TensorFlow installed from binary TensorFlow version v2.0.0 rc2 26 g64c3d38 2.0.0 Python version 3.6 Bazel version N A GCC Compiler version N A CUDA cuDNN version 10.0 7 GPU model and memory GTX 1080Ti Describe the current behavior I am trying to modify TF session parameters in combination with Keras e.g. the allowed GPU memory and use set session to store these parameters. However if set session is called before fit generator it causes the training to freeze when using multiprocessing. To reproduce the error use the following code. The Colab gist to reproduce the error can be found here https colab.research.google.com gist moberweger 2553560a5deeb2eaa5e3cfc23516ef34 untitled217.ipynb . The colab execution of the code in question is not responding so I stopped executing after 10 min which ultimately gives me the error. This problems seems to occur also in 1.14 but not 1.15 as indicated in 33973. The gist might work when executed the first time but the error can be reproduce after running the script a few times. Describe the expected behavior No freeze should occur. Code to reproduce the issue Other info logs , moberweger I think this was resolved in recent tf nightly . I ran your code with tf nightly and I cannot reproduce the error you had faced with TF2.0 . Please check the gist here https colab.research.google.com gist jvishnuvardhan 951378d5d7eabb359453dd5557d75dab untitled37.ipynb . There is a warning by the code that suggest you to update data pipelines with tf.data for better performance. Thanks WARNING tensorflow multiprocessing can interact badly with TensorFlow causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended. Please close the issue if this was resolved for you. Thanks Thanks jvishnuvardhan for checking. Although your notebook does not show the specific error it is still deadlocked ie stuck at Epoch 1 5 . Seems that there is now a valid warning and the Model.fit generator interface is deprecated. So assuming there is no impact of this problem after the interface got removed in the future I close this issue. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34119 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34119 No a moberweger Code was taking longer time. So I cancelled after some time. Please open a new bug if you face any issues in updating input pipeline. Thanks 