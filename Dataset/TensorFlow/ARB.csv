ID,title,summary,comments
1727,GPU resources not released when session is closed,As I understand from the documentation running sess.close is supposed to release the resources but it doesn t. I have been running the following test This allocates all the free memory of gpu0 but it is not released when sess is closed both using a context manager as in the code above but also when calling sess.close manually. The memory usage persists until that Python process is terminated. The way I have been checking memory usage is through nvidia smi but I have also confirmed that other processes can t allocate that GPU memory until the process terminates not the session closes. I would like to be able to free the resources and still keep the Python process running. Environment info I am running a 64 bit Linux CentOS with a computer that has two Tesla K40c driver 346.46 CUDA 7.0. I installed the 0.7.1 tensorflow for Linux and Python 3.4 through pip. The output of tf.version is 0.7.1. Steps to reproduce Simply running the code above should according to the document allocate and then release the memory. However the GPU memory is still allocated and thus unusable by other processes. However it can be re used by the same Python process meaning that I can re run the snippet over and over as long as I do it from the same Python process. Logs or other output that would be helpful Here is a log of the session. At the end the memory is still allocated. Note that another user is connected to both GPUs through Torch7 and is actively using gpu0.,I just wanted to add that I have also tested this on the most recent master https github.com tensorflow tensorflow commit de5101da4638ac469041575dadb4921ebb33eb6a now and it is still a problem. I m having this issue as well. Both opened and closed come back as false This is a bug. I am experiencing the same issue... thought I missed something... Please look into this ASAP if it is a bug. Same problem here. I have the same problem here with version 0.8.0. This is really anoying because I have to kill the python kernel to free the resources Same issue here with version 0.8.0rc0. In fact the GPU memory isn t even released after shutting down the Python kernel. Running 64 bit Linux CentOS with 4 nVidia GRID K520 GPUs Python 2.7. In lieue of fixing the issue a quick workaround could be to allow the user to free up the memory explicitly of course fixing the issue would be preferable EDIT this seems to only happen on GPU with ID 0. If I mask available GPUs through the env var CUDA VISIBLE DEVICES to not include 0 then all appears to go fine A negative side effect of this is that you can t run all the tests with bazel test c opt config cuda tensorflow ... A fraction of the tests 10 30 fail with CUDA ERROR OUT OF MEMORY on my 4GB GTX 980. But then if I rerun any of the failing tests using separate blaze test it works. Each bazel test invocation is a separate process so when the process exits it does release the memory. In this case bazel is running multiple tests in parallel. Use bazel test j 1 to only run one at a time. Thanks vrv that fixed all the out of memory errors I had As for the original problem currently the Allocator in the GPUDevice belongs to the ProcessState which is essentially a global singleton. The first session using GPU initializes it and frees itself when the process shuts down. Even if a second session chooses a different GPUOptions it would not take effect. Changing this would be a fairly large change. We need to rethink how the device is initialized and how it interacts with the session and therefore modify the current API. It is unlikely the TensorFlow team can get to this in the short term. Marking it as contribution welcome. If anyone is interested a design proposal could be discussed here before proceeding to implementations. Is it possible to provide a dedicated subroutine we can call at the end of our sessions that will free up GPU resources and release GPU control back to the OS On Windows 7 no GPU required programs can run after TensorFlow until the PC is restarted. recursionbane it s supposed to release it all although on Linux I ve seen cases that GPU was rendered unusable until restart and this was caused by NVidia driver problems Yes certain NVIDIA drivers had problems with releasing the memory. On Fri Mar 10 2017 at 3 38 PM Yaroslav Bulatov notifications github.com wrote recursionbane https github.com recursionbane it s supposed to release it all although on Linux I ve seen cases that GPU was rendered unusable until restart and this was caused by NVidia driver problems You are receiving this because you were assigned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 1727 issuecomment 285814765 or mute the thread https github.com notifications unsubscribe auth APAgTlGNS czunIHwsNxjXqGmUgsCu lks5rkd7hgaJpZM4H890g . I am on v369.30 for my Quadro M1000M. I will upgrade to v376.84 to check if the problem is resolved. I am using Ubuntu 16.04 with tensorflow 1.0 and NVIDIA Tesla K20Xm GPU CUDA 8.0 . I am facing similar problems. Memory is not released after the session is over As mentioned here the best idea anyone has is https github.com tensorflow tensorflow issues 1727 issuecomment 285815312 saying to upgrade your NVIDIA drivers. Closing this and locking to make sure the current conclusion is easily found if there is evidence that upgrading drivers does not solve the problem we can open up a new bug 
2233,protobuf message overflow on trying distributed ,I m trying to build an RNN on multi machines following the Distributed Tensorflow https github.com tensorflow tensorflow blob master tensorflow g3doc how tos distributed index.md . when I use with sv.managed session server.target as sess it shows error AttributeError Supervisor object has no attribute managed session So I follow the code of Inception with sv.prepare or wait for session server.target config sess config as sess Then it starts to run but hangs immediately after reporting the following error libprotobuf WARNING google protobuf src google protobuf io coded stream.cc 569 Reading dangerously large protocol message. If the message turns out to be larger than 67108864 bytes parsing will be halted for security reasons. To increase the limit or to disable these warnings see CodedInputStream SetTotalBytesLimit in google protobuf io coded stream.h. libprotobuf ERROR google protobuf src google protobuf io coded stream.cc 207 A protocol message was rejected because it was too big more than 67108864 bytes . To increase the limit or to disable these warnings see CodedInputStream SetTotalBytesLimit in google protobuf io coded stream.h. libprotobuf WARNING google protobuf src google protobuf io coded stream.cc 81 The total number of bytes read was 67108864 E tensorflow core framework tensor.cc 105 Input size was 67108839 and expected 72000800 Would you please help me on this Thanks a lot in advance ,Thanks for letting us know about this it sounds like a bug in the tensor serialization code but to figure out what s going wrong we re going to need more information 1. Can you pinpoint the line of Python on which it hangs Does it hang on the sv.prepare or wait for session or does it hang when you try to run something 2. Is there an obviously large tensor in your program Does it exist as a tf.constant a numpy array that is implicitly converted to a tf.constant a value that is fed into the graph or could it be a result that you re fetching Looking at the expected size 72000800 bytes I m guessing that it s a tensor with size 90001 in one dimension maybe a 200 x 90001 matrix of floats All of these cases should be handled but this will help to construct a minimal failing test case. 3. Can you try running the code in the following tests from server lib test.py https github.com tensorflow tensorflow blob master tensorflow python training server lib test.py in your environment testLargeConstant testLargeFetch and testLargeFeed . Thanks very much for your quick response 1. It passes sv.prepare or wait for session and hangs in session.run ... 2. Yes. I just follow the tutorial ptb word lm.py https www.tensorflow.org versions r0.8 tutorials recurrent index.html recurrent neural networks RNN model and made just a little modification to make it do classfication work. The embedding is 90001 200 floats. The embedding definition is just same as ptb word lm.py with tf.device cpu 0 embedding tf.get variable embedding vocab size size 90001 200 in my code. inputs tf.nn.embedding lookup embedding self. input data 3. I run your code with python . server lib test.py after showing a lot of log messages it shows the following message ....... Ran 12 tests in 4.028s OK When I try to decease the embedding matrix to smaller size such as 101 200 it never hangs and run correctly. I run the 90001 200 again today the warning message Input size was 67108839 and expected 72000800 still shows but it don t hang again. I don t known why... BTW is there any RNN sample code that could run on multi cards multi tower style or distributed multi machines I tried to port multi tower from CIFAR 10 example code to my RNN code after 1 week s work I failed I ll report the bugs in another issue . Then I gave up and turn to distributed style. The Inception example code needs BIG data set ILSVRC2012 and it s time consuming to download the data especially from China... . Since RNN is especially usefull in many application domains would you mind show me the multi cards or multi machines existing code if you know When I decrease the embedding size to 101 200 it starts running. But still it can t fully use my 4 GPU cards in the machine. There is only 1 card with more than 0 usage. Would you mind giving me an email then I can send my code to you Thanks a lot again for your time mrry In order to make you easier to reproduce the bug I just take ptb word lm.py https github.com tensorflow tensorflow blob master tensorflow models rnn ptb ptb word lm.py as example. I added Async Distributed tensorflow code to it. I run it by using 1 ps server and 2 workers all of them on the same node When I set vocab size to bigger than 83887 83887 200 4 67108864 it shows the following error job crash by most cases but not always crash When I set vocab size to less than but near to 83886 83886 200 4 67108864 it shows the warning In our large scale web mining job we need to increase the word embedding vocabulary size to millions of words. So we have no way but to solve this problem. My code is here reader.py https github.com tensorflow tensorflow blob master tensorflow models rnn ptb reader.py is just from github and unchanged ptb word lm.py.txt https github.com tensorflow tensorflow files 270704 ptb word lm.py.txt Did you solve this problem I have similar problem with my async distributed word2vec. 79840 vocab 300 dim I think the solution is in 582 the protobuf python package solution as the bottom of the thread doesn t seem to work for everyone as mentioned in 2046. Is it working for distributed version I have checked that the protobuf warning error message has gone for word2vec optimized example. But same problem occurs while running my own distributed word2vec optimized. Hi folks sorry for the delay on this one. We ve tracked down the issue the generated code for gRPC uses a protobuf parsing routine https github.com grpc grpc blob 305b0f4e2f99d2326bf1aaa881f857bb5fe1a817 include grpc 2B 2B impl codegen proto utils.h L209 that doesn t override the default 64MB limit. Generally speaking if your trainer is transferring large tensors in every step there might be a more efficient way to implement it at the Python level. For example instead of fetching a large fully connected layer to compute the logits you might use a sampled loss function https www.tensorflow.org versions r0.8 api docs python nn.html sampled loss functions and you can use tf.nn.embedding lookup https www.tensorflow.org versions r0.8 api docs python nn.html embedding lookup instead of tf.gather https www.tensorflow.org versions r0.8 api docs python array ops.html gather to more efficiently access sparse rows in an embedding matrix. Alternatively you can shard your variables manually to avoid the limit. Clearly this isn t ideal and we re working on a more robust fix. swlsw I use your codes ptb word lm.py.txt and run it on two machine. but i get the errror RuntimeError Graph is finalized and cannot be modified. I also run it by using 1 ps server and 2 workers. hope to get yours help 
4820,Thread created by SummaryWriter not killed,Hi I noticed that the EventLoggerThread created by summary writer does not get killed by the close method which will make the number of threads keep increasing until it exceeds the system capacity. Is there any particular reason to not kill these threads Best , wyx cornell Thanks for filing this issue Looking at the code I believe you re right and this is a bug. Have you run into a real world problem because of this or did you just notice via code inspection as well Would you like to file a pull request to fix this Contributions are welcome Hi I am working on this since haosdent is not working on it anymore. I ll try to reproduce this and I ll work on this if I can reproduce this. wyx cornell according to the latest code and I have tested number of threads will increase only when you create a new writer every time you want to add summary. Also if you kill the thread when close the writer there will be no thread serving you when you reopen this writer. how about kill the thread when close the writer and create a new thread when reopen the writer I was directed to this particular issue from 2788. I had been using tf.trainLoggingTensorHook which I am assuming uses the lower level summary writers under the hood. I am on tensorflow version 1.2 and upon seeing this I removed the logging and I went from intermittent hanging on start of run to now consistent successful training runs upon every start. I was seeing similar gdb traces on all my threads to the above linked issue and it seems all my threads were in lock. If this isn t the same issue I m more than willing to open a new one but it at least seemed related. GrandathePanda can you provide a minimal test case so we can reproduce suiyuan2009 Below is a test that can reproduce We want to make a call from C to python and TF. Thus an embedded python interpreter is used. However the Py EndInterpreter call fails with error message Fatal Python error Py EndInterpreter not the last thread This is because there is a daemon thread EventLoggerThread is still running after the python script finishes. The output of this test is 33 67.0 MainThread MainThread started 140735723737984 EventLoggerThread Thread 1 started daemon 123145450090496 Fatal Python error Py EndInterpreter not the last thread test.py file And the test cpp code I think this is still up to date. See here https github.com rwth i6 returnn blob a923504d7265214efb74ef67509db9ac72b84d1c tests test TFUtil.py L678 and here https github.com rwth i6 returnn blob 3f1e9a235e28f41e695686a345410f32e929a663 TFUtil.py L4466 . This test case still runs with latest TF e.g. here https travis ci.org rwth i6 returnn jobs 501581634 . Edit Just noticed that the test case actually does not check whether this is still really the case. whjiang it s a long time I ll take a look in this week. 
4981,FastGFile ResourceExhaustedError,In https github.com tensorflow models commit 2390974a03a62b1388a004173477418db267074a cshallue changed some tf.gfile.FastGFile calls to open because two RedHat users reported in 4685 that it causes ResourceExhaustedError even though resources don t appear to be exhausted. We ve also had the same issue reported in tensorflow models 531 tensorflow models 489 and tensorflow models 480 but we re still waiting to learn if they re using RedHat. Assigning to rohan100jain who has done work on this code in the past. ,Yeah there was a memory leak by which file handles weren t being closed. Should have been fixed with https github.com tensorflow tensorflow commit 0f605abad3b30594416dc627860102e3382d6dea diff 8dc0e318495c91370f8807ef81b589e0 Let me know if this is still an issue I meet the same issue but in a different file https github.com tensorflow tensorflow blob master tensorflow models image imagenet classify image.py L141 This example only predict 1 image. if we change this example by using for loop to predict a large number images it report the same ResourceExhaustedError error civilmanxx are you building TensorFlow from source with Bazel at HEAD yes but my local code is not the latest version which does not include a C fix 3 days ago for this issue. Now I use open instead of read to avoid this issue. If I get the lastest code rebuild at local and use read will be no issue anymore is that right As far as we know the issue goes away if you sync to HEAD and build. 
5401,Creating and fitting a Trainable leaks file descriptors ,Creating a Trainable and calling fit on it leaks file descriptors. Environment info Operating System Mac OS X Installed version of CUDA and cuDNN please attach the output of ls l path to cuda lib libcud None If installed from binary pip package provide 1. A link to the pip package you installed 2. The output from python c import tensorflow print tensorflow. version . 0.11.0rc0 If installed from source provide If possible provide a minimal reproducible example We usually don t have time to read hundreds of lines of your code Logs or other output that would be helpful Traceback most recent call last File tensor bug.py line 13 in module File Library Frameworks Python.framework Versions 3.5 lib python3.5 site packages tensorflow contrib learn python learn estimators estimator.py line 333 in fit File Library Frameworks Python.framework Versions 3.5 lib python3.5 site packages tensorflow contrib learn python learn estimators estimator.py line 708 in train model File Library Frameworks Python.framework Versions 3.5 lib python3.5 site packages tensorflow contrib learn python learn graph actions.py line 285 in monitored train File Library Frameworks Python.framework Versions 3.5 lib python3.5 site packages tensorflow contrib learn python learn monitored session.py line 368 in run File Library Frameworks Python.framework Versions 3.5 lib python3.5 site packages tensorflow contrib learn python learn monitored session.py line 521 in run File Library Frameworks Python.framework Versions 3.5 lib python3.5 site packages tensorflow contrib learn python learn monitored session.py line 488 in run File Library Frameworks Python.framework Versions 3.5 lib python3.5 site packages tensorflow contrib learn python learn monitored session.py line 612 in run File Library Frameworks Python.framework Versions 3.5 lib python3.5 site packages tensorflow contrib learn python learn monitored session.py line 634 in call hook before run File Library Frameworks Python.framework Versions 3.5 lib python3.5 site packages tensorflow contrib learn python learn basic session run hooks.py line 180 in before run File Library Frameworks Python.framework Versions 3.5 lib python3.5 site packages tensorflow python training training util.py line 76 in write graph File Library Frameworks Python.framework Versions 3.5 lib python3.5 site packages tensorflow python lib io file io.py line 211 in write string to file File Library Frameworks Python.framework Versions 3.5 lib python3.5 site packages tensorflow python lib io file io.py line 89 in write File Library Frameworks Python.framework Versions 3.5 lib python3.5 site packages tensorflow python lib io file io.py line 81 in prewrite check File Library Frameworks Python.framework Versions 3.5 lib python3.5 contextlib.py line 66 in exit File Library Frameworks Python.framework Versions 3.5 lib python3.5 site packages tensorflow python framework errors.py line 463 in raise exception on not ok status tensorflow.python.framework.errors.ResourceExhaustedError var folders 6c 3pzbp8jj3zd0lkhsszmvyd5h001cct T tmpzr7juea4 graph.pbtxt During the run lsof is indicating that the number of file descriptors used is increased on every loop iteration., martinwicke ilblackdragon could you take a look. ispirmustafa Maybe a summarywriter or similar object is not closing a file properly Leak can be anywhere in the code since this example is running all Estimator in an infinite loop. I ll look into it. I ll update the thread when I find the reason fix. 
5688,tensorflow crashes when using large image with 3d convolutional network,I m trying to implement a 3d fully convolutional network on my GPU. But for some reason I get a crash. Environment info Operating System Ubuntu 14.04 LTS GPU GeForce Titan X . Installed version of CUDA and cuDNN 8.0 and 5 attach the output of ls l path to cuda lib libcud cud.filelist.txt https github.com tensorflow tensorflow files 599236 cud.filelist.txt I installed tensorflow version 0.11.0rc2 and it also reproduce in docker installation gcr.io tensorflow tensorflow latest gpu Example code The following code reproduce the problem import numpy as np import tensorflow as tf graph tf.Graph with graph.as default tf dataset tf.placeholder tf.float32 shape 1 512 512 512 1 tf label tf.placeholder tf.float32 shape 1 512 512 512 1 layer1 weights tf.Variable tf.truncated normal 2 2 2 1 1 stddev 0.1 layer1 bias tf.Variable tf.zeros 1 conv tf.nn.conv3d tf dataset layer1 weights 1 1 1 1 1 padding SAME logits tf.nn.relu conv layer1 bias loss tf.reduce mean tf.nn.softmax cross entropy with logits logits tf label optimizer tf.train.GradientDescentOptimizer 0.05 .minimize loss with tf.Session graph graph as session tf.initialize all variables .run batchData np.random.rand 1 512 512 512 1 .astype np.float32 batchLabels np.random.rand 1 512 512 512 1 0.5 .astype np.float32 feed dict tf dataset batchData tf label batchLabels session.run optimizer feed dict feed dict with the following output I tensorflow stream executor dso loader.cc 111 successfully opened CUDA library libcublas.so locally I tensorflow stream executor dso loader.cc 111 successfully opened CUDA library libcudnn.so locally I tensorflow stream executor dso loader.cc 111 successfully opened CUDA library libcufft.so locally I tensorflow stream executor dso loader.cc 111 successfully opened CUDA library libcuda.so.1 locally I tensorflow stream executor dso loader.cc 111 successfully opened CUDA library libcurand.so locally I tensorflow core common runtime gpu gpu device.cc 951 Found device 0 with properties name GeForce GTX TITAN X major 5 minor 2 memoryClockRate GHz 1.076 pciBusID 0000 01 00.0 Total memory 11.92GiB Free memory 11.68GiB I tensorflow core common runtime gpu gpu device.cc 972 DMA 0 I tensorflow core common runtime gpu gpu device.cc 982 0 Y I tensorflow core common runtime gpu gpu device.cc 1041 Creating TensorFlow device gpu 0 device 0 name GeForce GTX TITAN X pci bus id 0000 01 00.0 F tensorflow stream executor cuda cuda dnn.cc 2440 failed to enqueue convolution on stream CUDNN STATUS NOT SUPPORTED ,Does it work with smaller images It vaguely sounds like running out of memory but it doesn t seem like it should given the numbers and batch size 1. Can it be a memory issue My GPU has 12GB. The size of the input is 0.25GB. Anyhow the following size works 32 32 512 512 1 which is larger in size but smaller in one convolutional direction. HggsHntr I m just a simple computer scientist. Your questions are reasonable but it s hard for me to think beyond testing whether a smaller image works or not. I guess it might be memory issue after all. I set config.gpu options.allow growth True and traced the memory consumption of the GPU. It raised to the maximum before it crashed. Also by changing the stride size to 4 in each direction or reducing the image size the crash disappear. So I wonder if this large memory consumption in done intentionally or is it a bug. zheng xq Is there any possible issue with GPU convolution scratch space e.g. can the autotuner selects an algorithm based on available memory and then fail later on I ve encountered the same issue on tensorflow 0.11.0rc2 with latest keras I was feeding the CNN 10k 200x200x3 images per fit . When I gradually lower the number to 8k the same issue no longer existed. On the other hand with the 10k images per fit the same CNN written in tflearn was just fine. can confirm a similar issue with large 3D convolutions will yield tensorflow stream executor cuda cuda dnn.cc 2674 failed to enqueue convolution on stream CUDNN STATUS EXECUTION FAILED . It runs fine if I set inshape 2 257 257 34 1 instead so there s some issue with the sizes. However I dont think its the usual GPU out of memory issue 1. the same convolution runs perfectly fine in theano there I can even increase to 2 300 300 34 1 without trouble 2. it doesnt respond to all dimension the same decreasing the batchsize from 2 1 will still crash even though we just halfed the input Turns out that it s somehow related to the gradient computations i.e. commenting the gradQ ... line just evaluating the result of the convolutions works Code was run on tensorflow 0.11.0rc2 same happens for 0.12.1 Titan X CUDA 8.0 cudnn 5 redst4r I have a similar issue. I too think there could be some issue when calculating the gradients. Any update on how you resolved it deepak09027 Unfortunately I dont have any solution yet. I was hoping that some googler picks it up here Can confirm this issue as well. I m having exactly the same problem Would be great if this could be resolved Similar problem here with deep 2d convolution network and huge batch size because my input is seq length batch size height width . Change batch size to a smaller one solves the issue. It d great if it can throw a python exception prb12 zheng xq any updates on this issue I am having the same issue as well Same here. I can reproduce this issue with the example code above. Can confirm CUDA 8 CUDNN 5.1 with tf 1.0. I ran into an interesting aspect when determining the maximum size of the tensor. It run when tensors with shape less than 1 256 256 256 1 are used. The example below will run and sz 256 will break. Is anyone looking into this I ve also run into the same issue where a P100 with 16GB RAM would get the same error but I know that my data is not even big enough to take up the entire GPU RAM. I m using TF 0.12 CUDA 8.0 CuDNN 5.1. Has this been fixed in the newest version of TF Mostly its not related to VRAM I am using latest TF with keras In first layer for a VGG like architecture Convoulution3D NumChannels 3 3 3 inputs if I set NumChannels 8 it crashes with the above message. This somehow contradicting with waTeim https github.com tensorflow tensorflow issues 2190 Hmm I am trying to run on tensors of shape 1 125 125 125 128 1 gig and getting the standard OOM error. I am wondering how much working memory the 3d conv needs. It seems like doing a conv of this size should work ok. This issue also goes away for me when I reduce my batch size. Seems like a memory issue that Tensorflow doesn t catch the way it does other OOM issues. Did anyone find any solution for this I am also hitting this issue when trying to train a CNN. When I reduce the batch size to 2 its runs and at this point the GPU VRAM consumption is 217 MB 3072 MB and on increasing the batch size to 3 it gives the failed to enqueue convolution on stream CUDNN STATUS EXECUTION FAILED error. I doubt if this is due to low VRAM. TensorFlow s response support has been very weak on the bugs being reported here. Really disappointing. tfboyd Any ideas same problem . name Tesla P100 PCIE 16GB major 6 minor 0 memoryClockRate GHz 1.3285 pciBusID 0000 05 00.0 Total memory 15.89GiB Free memory 15.61GiB I tensorflow core common runtime gpu gpu device.cc 906 DMA 0 I tensorflow core common runtime gpu gpu device.cc 916 0 Y I tensorflow core common runtime gpu gpu device.cc 975 Creating TensorFlow device gpu 0 device 0 name Tesla P100 PCIE 16GB pci bus id 0000 05 00.0 F tensorflow stream executor cuda cuda dnn.cc 1989 failed to enqueue convolution on stream CUDNN STATUS EXECUTION FAILED Sample data and label 50000 3072 array 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. Training data 35000 3072 35000 10 nValidation data 15000 3072 15000 10 Training started cm local apps slurm var spool job109000 slurm script line 14 38773 Aborted home ksrivastava tensorflow bin python home ksrivastava DL CIFAR10 scripts trainGPU.py I will try to escalate this. The issue has been open for a long time and reproduced many times based on my skimming of the thread. thanks tfboyd Internal email thread started looking for an owner to dig in . Sorry for the delays hopefully we get this moving forward. b 64718915 yzhwang is working on it and thank you to SrivastavaKshitij for following up via email and helping get this prioritized. Hi SrivastavaKshitij could you let me know which version of tensorflow you are using I tested with HggsHntr s example code on tf 1.2.1 with CUDA 8 and cudnn 5.1. This seems to be a duplicated issue with https github.com tensorflow tensorflow issues 11327 which I have already fixed in https github.com tensorflow tensorflow commit db596594b5653b43fcb558a4753b39904bb62cbd. Unfortunately the fix is still not included in 1.3rc2. If you believe this is a separate issue could you provide a reproducer Thank you Hi yzhwang I was able to solve the problem. I am working on a cluster and I was using an incompatible version of PGI OpenMPI with cuda and cudnn version which was causing the problem. But as soon as I corrected it everything worked fine. If you want I can give u the details. Thanks If it is not too complicated we would like you to provide a more detailed description and how you solved it. I think it would help other users with the same configurations. As for this issue I will close it for now as I have a fix for this https github.com tensorflow tensorflow commit db596594b5653b43fcb558a4753b39904bb62cbd and it has been verified both internally and on the head of OSS version. HggsHntr if you believe the fix didn t solve the issue please let us know so that we can reopen and investigate more. SrivastavaKshitij Please do put more details if you like even when the issue is closed. Much appreciated yzhwang I have upgraded to tensorflow 1.4 nightly build windows gpu and when I run the following import numpy as np from keras.engine import Input Model from keras.layers import Conv3D Activation dim 256 input shape dim dim dim 1 inputs Input input shape conv1 Conv3D filters 1 kernel size 2 2 2 strides 1 1 1 activation relu padding same inputs logits Activation softmax conv1 model Model inputs inputs outputs logits model.compile optimizer rmsprop loss binary crossentropy metrics accuracy batchData np.random.rand 1 dim dim dim 1 .astype np.float32 batchLabels np.random.rand 1 dim dim dim 1 0.5 .astype np.float32 model.fit x batchData y batchLabels it crashes with an error about Conv3DBackpropFilterV2 yzhwang I get the same thing as hadarpo . Is it the same bug or something else 
6111,Flawed memory management allow growth True consumes more memory causing out of memory,To prevent tensorflow TF from allocating the totality of graphic memory I always use the following options when creating sessions However doing so causes some experiments to run out of memory while not doing so will not cause memory overflow. For example when running experiments involving RNN such as translate.py or ptb word lm.py in the sample code if I specify allow growth True I always encounter the following However without specifying allow growth True I can run it successfully. Moreover the OOM occurs only after going through some epoches in the training data not right from the beginning. In principle for an ideal memory manager whether OOM will occur should not depends on whether memory is pre allocated in one go or allocated step by step dynamically. Thus Tensorflow s low level memory management code must be flawed in one way or another. Below are my system info ,Could you post the entire log somewhere We need to find out how much memory were requested when OOM happens Also we would need a small model to reproduce the problem. allow growth is expected to introduce some level of fragmentation. So if your model is running very close to GPU memory limit it is possible that allow growth can push you over the limit. The current allocator design is a trade off between performance and efficiency and sure it is not perfect. That s why the option exists. It will get better as we keep improving it. If the system still has a lot of free memory and the OOM still happens then it is a more serious problem than if the model is already running close to the limit where we would recommend not to use allow growth. In principle memory fragmentation should only affect the speed rather than the total available memory. You can reproduce the problem by running translate.py or ptb word lm.py in the sample code by setting a large enough vocabulary size for OOM to occur. In principle memory fragmentation should only affect the speed rather than the total available memory. I am not sure about that. https en.wikipedia.org wiki Fragmentation computing In computer storage fragmentation is a phenomenon in which storage space is used inefficiently reducing capacity or performance and often both. On Fri Dec 16 2016 at 12 07 AM xuancong84 notifications github.com wrote In principle memory fragmentation should only affect the speed rather than the total available memory. You can reproduce the problem by running translate.py or ptb word lm.py in the sample code by setting a large enough vocabulary size for OOM to occur. You are receiving this because you were assigned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 6111 issuecomment 267539412 or mute the thread https github.com notifications unsubscribe auth APAgTvY8fOvc7dxm mh7mW0VYhMYdcMYks5rIkbCgaJpZM4LE 5n . Hi I am testing the TF Slim package fine tuning the flowers dataset using the instructions in https github.com tensorflow models tree master slim To allow for memory growth I use the following code session config.gpu options.allow growth True session config.gpu options.allocator type BFC session config.gpu options.per process gpu memory fraction 0.4 gpu options tf.GPUOptions per process gpu memory fraction args.gpu memory fraction Run the training final loss slim.learning.train train op logdir train dir init fn get init fn number of steps 2000 session config session config There are mainly two issues. 1. As the GPU memory is not big enough I want to try to allow for GPU growth so that the process does not crash. The code I am using is the one below however it doesn t make any difference at all. Is allow growth and or per process gpu memory fraction really supported for TF slim or not I suppose this will be anyways an issue given the original message on the thread. So I guess all left to ask Is whether there any development on the issue to begin with is it really an issue 2. Is all the GPU needed for the allocation of the intermediate activation variables Because the inception model is about 100 Mb my total GPU memory after killing also the X server is 1.8 Gb so it feels somewhat weird that the model needs in the end 18X more GPU RAM than the inception model size. When using Caffe with Alexnet 350 Mb there was no problem at all no matter the batch size. I guess the way the gradients are computed is by allocating memory like MODEL PARAMETERS x BATCH SIZE since for smaller batch sizes the code works. Given that batch gradients are cumultative over the batch samples isn t this although arguably faster somehow redundant . I will try to get the RAM metadata log file if that is of any help. Automatically closing due to lack of recent activity. Since this issue is old at this point please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you. 
6599,memory leak in tensorflow gpu 0.12.1, Hi We use tensorflow for training our OCR system models. I simply train models in tensorflow 0.9 and former versions. after some upgrade in cuda and tensorflow I see large memory leak in our server with 32GB RAM. I have tried all of suggestions in How to debug a memory leak in TensorFlow http stackoverflow.com documentation tensorflow 3883 how to debug a memory leak in tensorflow t 201612280142239281993 and some other github issues and stackoverflow posts. No of them worked. train code images labels are queues from tf.train.shuffle batch function. I used Graph.finalize and I am sure no new operation added to graph in training because the size of stored models files are equal. I guess the reason of leak is tensorflow queues. Environment info Ubuntu Server 14.04.5 LTS Installed version of CUDA and cuDNN CUDA V8.0.44 cuDNN V5.1.5. installed tensorflow from PyPI. 1. sudo pip install upgrade tensorflow gpu. 2. tensorflow 0.12.1.,It can be useful to do a memory profile using google performance tools http goog perftools.sourceforge.net yaroslavvb I use google pprof and heap profile save in tmp folder. it takes too long time to train so the profile only contain near 150 itrations. the files uploaded here https www.dropbox.com s 2r2pqs4l0bjar9h tmp.zip dl 1 if you combine it with pprof you can get a graph and track down which function is responsible for allocating extra memory It doesn t show any function this is the output of top30 command in pprof graph file uploaded here https www.dropbox.com s mjo4kc17micbfw6 pprof3105.0.ps dl 1 unfortunately I cant find a function is responsible for allocating extra memory That usage doesn t seem very high why do you think there s a memory leak in queue Memory usage can increase between versions without having a leak because I see 32 GB memory usage after 700000 iterations. I think the cause is data I use tfrecord and tensorflow queue for data handling. I am sure that I hadn t leak in version 0.9. zffchen78 do you have any idea why google perf tools shows 29.5 00007f8a8afae90e instead of symbol name in the memory dump Does it mean this memory was allocated outside of tensorflow yaroslavvb the hex address shows up when the symbol lookup failed. Address to symbol name mappings are always very brittle. You can try to fiddle with build options to see if it helps. You need the frame pointers for instance IIRC. If you gdb attach to it then info symbol then you might be able to reverse map it. I got exactly the same problem as you I also use TFRecord and Queue to process data and my main memory keeps growing until the system goes down... By the way I use Tensorflow 0.12.1 CUDA 8.0.44 CUDNN 5.1 CentOS 7 Main memory as in CPU Could you build with tcmalloc then get a heap profile To enable tcmalloc Can this problem be solved in the next release carltonwang Could you give a reproducible example and profile as a picture graph cc ebrevdo in case he has ideas Are you using dynamic or static rnn I use static rnn. This is the simplified version of my code and data in TFRecord please run it on GPU... My system is CentOS 7 GTX 1070 CUDA 8.0.44 CUDNN 5.1 NVIDIA Driver 375.26 test.zip https github.com tensorflow tensorflow files 752806 test.zip Any idea about this problem carltonwang can you isolate your problem a bit more IE does it happen with latest version Does it happen with tcmalloc Can you run it with memory profiler and figure out who allocates all the extra memory It happens on the latest version with or without tcmalloc... I implemented an encoding algorithm in my code. Today I just encapsulated this encoding method as an RNN cell and then the memory leak disappeared Closing since the problem is gone. Happy to reopen if there is still an underlying bug to be fixed in TensorFlow but it sounds like everything so far is inconclusive. Mahdizade I met similar problem. My program is similar to yours and after several epoch 32G memory exhausted. I debugged the program and found each time the saver.save function is run the python process got a little bigger. I recommend you try this and see if the same thing happens. I have the same issue here. It goes overflowing after couple of iterations on version 1.0.1 My issue is already exists in TF 1.0. benwu232 Thanks for your comment. I will test your solution but I see the leak on my validation code. till now I cant find the problem. in my validation code leak happened only after the first load of graph parameters checkpoint after validate one checkpoint and load another checkpoint I don t see any leak in htop. I will investigate more on this problem. I don t thing the saver is the cause I see the steady leak on training I save the checkpoint every 5000 iteration but leak exists in this intervals. Look at this code result tf.contrib.layers.bias add inputs tf.matmul a left b right activation fn tf.nn.softmax If I use softmax as the activation function there will be a memory leak but if I change the activation function to relu or softplus or sigmoid the memory leak will disappear. By the way this softmax is not the one used at the last layer for classification. Platform CentOS 7 x86 64 TensorFlow version 1.0 gpu Could you provide a complete test case that exhibits this behavior so we can reproduce this After I update from TF 1.0.0 to TF 1.0.1 this problem disappear... looks like this was resolved with 1.0 release. Closing the issue. 
6766,softmax cross entropy with logits aborts the process if a tensor with zero first dimension is passed as an argument, Environment info Operating System Ubuntu 16.04 Installed version of CUDA and cuDNN CUDA 8.0 CUDNN 5.1.5 Tensorflow version 0.12.1 installed from https storage.googleapis.com tensorflow linux gpu tensorflow gpu 0.12.1 cp35 cp35m linux x86 64.whl Reproduced also using tf 0.11.0 CUDA 7.5 CUDNN 5.1.3 Minimal reproducible example Result on GPU Result on CPU , zheng xq Here s a GPU memory alloc issue. Reported both here and on Stack Overflow http stackoverflow.com questions 41530966 memory error with eigenallocator Hi jrosti Have this problem been solved I met the same problem in r1.01 I still have this problem which seems to arise when using tensorflow fold. Still getting this issue as of 06 02 2017 when using tf.gather nd ... and softmax cross entropy with logits ... together. Anyone looking into this I m also experiencing this issue using TensorFlow 1.2.0 v1.2.0 rc2 21 g12f033d . In my case a tf.while loop is being used in conjunction with tf.TensorArray however the same error occurs if I swap out the tf.while loop for a while statement. Backtrace abbr A bit further down I m also experiencing this issue and error message is same as j wilson . I am fine tuning object detction api on faster rcnn inception resnet v2 atrous coco 11 06 2017 model.ckpt. Also having the same problem in tf 1.3. Would be great if it threw a more descriptive exception. I hit this issue. After some investigating I realized I was feeding in an empty tensor. Not sure if that is the only thing that can cause it but that was my problem at least. It was easy to fix but it certainly would be nice if it threw a more descriptive error. Massive thanks. Empty tensor is the cause of my problem too. metachi It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. I m facing same problem even checking if tensor is empty or not filipetrocadoferreira Have you tried to wrap the conditional branches into lambda functions This should allow a lazy execution of the branches. see tf.cond https www.tensorflow.org api docs python tf cond wow this seems to work. Can you link to an explanation Added PR 16051 for a fix. filipetrocadoferreira I can t find any link with an extensive explanation I tought it was explained in tf.cond s documentation but actually it is not . In short if you don t define the branches as functions they will be both executed regardless of the condition. This explains why you still had the problem despite checking the tensor s size. 
7013,ResourceExhaustedError while converting ImageNet dataset to TFRecord,Determining list of input files and labels from run media root d9ecddfe d069 4252 aade b75fa707b3d2 imagenet data raw data train. Traceback most recent call last File cgpit models inception bazel bin inception build imagenet data.runfiles inception inception data build imagenet data.py line 704 in module File usr lib python2.7 site packages tensorflow python platform app.py line 30 in run File cgpit models inception bazel bin inception build imagenet data.runfiles inception inception data build imagenet data.py line 700 in main File cgpit models inception bazel bin inception build imagenet data.runfiles inception inception data build imagenet data.py line 597 in process dataset File cgpit models inception bazel bin inception build imagenet data.runfiles inception inception data build imagenet data.py line 513 in find image files File usr lib python2.7 site packages tensorflow python lib io file io.py line 251 in get matching files File usr lib64 python2.7 contextlib.py line 24 in exit File usr lib python2.7 site packages tensorflow python framework errors.py line 463 in raise exception on not ok status tensorflow.python.framework.errors.ResourceExhaustedError run media root d9ecddfe d069 4252 aade b75fa707b3d2 imagenet data raw data train n01440764 I have tried changing different number of threads different shard size for both training and validation. But still getting error. There is enough RAM is free while executing this command. Is it bug or it required some configuration changes Command used for converting bazel bin inception download and preprocess imagenet DATA DIR where DATA DIR run media root d9ecddfe d069 4252 aade b75fa707b3d2 imagenet data Environment info Operating System CentOS 7 64bit 32GB RAM Intel Xeon R CPU E5 2630 v2 2.60GHz 12 CPU only version of tensorflow Installed from source root cgpits inception bazel version Build label 0.3.1 Build target bazel out local fastbuild bin src main java com google devtools build lib bazel BazelServer deploy.jar Build time Fri Jul 29 09 09 52 2016 1469783392 Build timestamp 1469783392 Build timestamp as int 1469783392 head for tenserflow root cgpits tensorflow git rev parse HEAD 787ab22d490e79ea8c06511d60d6cddf1b2dd2c2 head version for tesorflow model root cgpits inception git rev parse HEAD 82c219f24d3a7c5c48a9550aef10bde3a031520d root cgpits inception python c import tensorflow print tensorflow. version 0.11.0rc0 ,I wonder what resource is getting exhausted. File Descriptors Someone got around a similar sounding problem by adding time.sleep statements to stagger their file opens in https github.com tensorflow tensorflow issues 6845 issuecomment 273946136 I think yaroslavvb is right. From reading the code it should be coming from GetChildren http google3 third party tensorflow core platform posix posix file system.cc l 201 which calls opendir . The only two that could arise in this situation are too many global process open files or too many links unlikely unless you have a loop of sorts . Could you try to debug Reboot reduce the number of threads insert time.sleep or trace down descriptor leak. First thing to do would be to put some code in GetChildren to see if that s what you re getting. can you please provide more details where I can add this statements I am not sure what to compare with this case statements. I have even tried to use different number of threads from 1 to 1024. Machine is dedicated server with single user and 16TB storage and 32GB RAM. I am quite sure that error is one of the following case case EMFILE Too many open files case EMLINK Too many links case ENFILE Too many open files in system case ENOBUFS No buffer space available case ENODATA No message is available on the STREAM read queue case ENOMEM Not enough space case ENOSR No STREAM resources Right which one Can you put a LOG INFO statement Closing due to lack of response. 
7321,Tensorflow retrained model compression fails with error terminate called after throwing an instance of std bad alloc what std bad alloc Aborted core dumped ,I am doing a college project on tensorflow. I have successfully retrained using rerain.py file to use that model in android program. I was trying to compress the model using commands in the following link https github.com tensorflow tensorflow blob master tensorflow tools graph transforms README.md shrinking file size But it is getting terminated throwing the below error ERROR terminate called after throwing an instance of std bad alloc what std bad alloc Aborted core dumped I am working on an Ubuntu 14.04 machine with 4GB RAM. Please help. Thanks Bruczzz,It sounds like you ran out of memory. However it s impossible for us to help with the level of detail you given. For example what command did you run that ran out of memory I am referring the Shrinking File Size section given at https github.com tensorflow tensorflow blob master tensorflow tools graph transforms README.md shrinking file size The command that failed was code bazel build tensorflow tools graph transforms transform graph bazel bin tensorflow tools graph transforms transform graph in graph tensorflow inception graph.pb out graph optimized inception graph.pb inputs Mul 0 outputs final result 0 transforms round weights num steps 256 code Bruczzz What version of TensorFlow are you using A request for this was in the instructions that you deleted when creating the issue. petewarden What would transform graph be doing that consumes a bunch of memory Are there huge constants in these graphs Apologies this is a known bug with the command line parser. If you remove the and newlines in the transform script it should work. I have a fix pending I m hoping to get it in soon. Thanks a lot petewarden and girving. Now I am able to compress my retrained model successfully. If you look at the Slim library there s a pretrained Inception v1 model that only has seven million parameters and so shrinks down to 7MB. You might want to try fine tuning that. https github.com tensorflow models blob master slim README.md Pretrained Thanks you so much petewarden for your information and time. I tried fine tuning on V3 model using flower data as provided on tensorflow page it is generating again a check point. Got stuck there with no clue of converting check point to .pb file. So that it can be compressed and used in android application. I have tried compression of inception v3 model using quantization in two ways but i feel quantizied model is getting corrupted or on classification something going wrong and producing incorrect results. Raised issue on that https github.com tensorflow tensorflow issues 7523 Please assist me. Thanks Automatically closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks I am runnning into the same error with the following code. As I am quite new to tensor flow I am trying to understand what is actually causing the abort paint2.txt https github.com tensorflow tensorflow files 1199967 paint2.txt I am sorry if it is a known bug. I am using python 3 tensorflow with pip install GPU still not working. Hi I encountered the same problem when working on Ubuntu 16.04 virtual machine with 1GB memory. I trained a neural network with weights of 84MB in total. Is it possible that the model ended up so huge to take up all memory Hi i have the same issue after running 4 epoch and i have tested in other machin but problem exists. I use tensorflow 1.3 ubuntu 16.04 gtx 1080 gpu system ram 128 and i am working on shared server with 3 gpus this is my trainig code github.txt https github.com tensorflow tensorflow files 1502929 github.txt 
7353,Session run timeout causes memory leak and saving stuck,I am using distributed tensorflow one of my machines was something wrong and caused almost all session run timeout eventually memory reached 80G and been killed. This also caused worker0 process failed to save model saver.save stuck forever. I am using 0.12.1 windows build. I searched and nobody ever reported issues like this. Any idea and advice to confirm and fix it ,I don t think we ll be able to debug this with the information provided. Can you reproduce the problem It may be worth checking if TensorFlow 1.0 solves it for you. I ran into this issue two times however I will try 1.0 version. But calling saving.save without shard True stuck forever each time when I have lots of workers for example 40 ps machines and 80 worker machines each worker machine starts 10 worker processes. here is the stack of master worker that calls saving maybe you guys could get some idea wow I ve never seen windows stack trace before cc mrry from my experience windows version is not very stable i also ran into session run stuck when I enabled full tracing option with one custom operator built in and setting timeout did not work sometimes the first step could get stuck but after i disabled full tracing everything is fine. but the script under linux is ok no stuck happened. and here is the stack Something weird is that many workers ran into session timeout just before ending. For example one cluster has 800 worker processes after 127 workers successfully ended training and no timeout error occurred all other workers started to ran into timeout and no worker could successfully finish even one step. After master worker finished reading data and started to save checkpoint it will hang forever and master worker memory usage is about 2G. Something important information 1 when building up cluster each worker will only see all parameter servers and is not aware of all other workers this could avoiding establishing too many tcp connections between workers 2 all non master workers will wait for exit signal variable using the same session for training when they finished processing their data. My guess is parameter server is something wrong and can not response any more so training will timeout and saver will hang forever without any timeout provided. Also 127 is very interesting could be any 128 limiting mrry Looks like all the threads in that stack trace are waiting so presumably the problem is elsewhere. Do you have recommendations for what extra information to get girving mrry I changed my script to write checkpoint to hdfs with sharded True and found one of the parameter server failed to output checkpoint so I am pretty sure worker timeout and saver stuck were caused by this one parameter server.here is the stack stack.txt https github.com tensorflow tensorflow files 768200 stack.txt girving mrry hey guys are you taking care of this issue if you have no time please give some advice and i will try my best to debug it. fesun Thanks for sending over the thread stacks. Unfortunately there doesn t seem to be anything informative in the parameter server stack... all threads appear to be blocked waiting for more work or in tf.Server.join . The most important observation seems to be that creating a tf.train.Saver with sharded True is a more stable configuration than sharded False especially when the total parameter size is large. You mentioned that you saw a failure when writing with sharded True to HDFS so it s possible that there s a bug in the HDFS layer and it might be worth instrumenting that code to see if an unhandled failure is occurring. Does failed to output checkpoint mean saver.save returned with no exception and yet it didn t write a file it should have As an aside it looks like I might have found a bug in SaveV2 which doesn t check the error status from writer.Add https github.com tensorflow tensorflow blob 9949045460ffb7bfc037f4b3a5396325791e0f09 tensorflow core kernels save restore v2 ops.cc L131 . This could potentially lead to a silent failure if the underlying write fails. concretevitamin can you take a look at this and confirm Beyond that I don t think we know enough about your program to know where the problem might lie. There are a lot of details mdash surrounding the setup of the parameters and the saver the coordination between the workers and the use of timeouts mdash that are unclear to me. Can you create a minimal example that exhibits the same problems and share the code for that With some code in black and white we should be able to provide more help with tracking down the problem. mrry thanks for your responses. First I used shared True and wrote checkpoint to hdfs and found only one parameter server had no output without any error occurred and as you said the ps stacks are all waiting for more work. Also before saving checkpoint all workers were timeout even with 100 seconds timeout. I am pretty sure that one parameter server can t response any more. Second each worker is not aware of other workers they only connect to ps. Each step used the same timeout 2 seconds Third when the total worker count is small like 200 it works fine with or without shard True but when the worker count is 800 or even 400 cluster will hang with or without shard True when the training is almost done. I am using windows version and even I provided code you may not reproduce the issue using very small cluster. But I will try to minimize it. First I used shared True and wrote checkpoint to hdfs and found only one parameter server had no output without any error occurred and as you said the ps stacks are all waiting for more work. Is it always the same PS task that fails to write a checkpoint Does it fail on the first attempt to write a checkpoint or does it fail non deterministically at some point during training Also before saving checkpoint all workers were timeout even with 100 seconds timeout. What were the workers doing that timed out I am pretty sure that one parameter server can t response any more. How did you confirm this Can you try running a step on that PS task e.g. to fetch a single variable that s held on that task Second each worker is not aware of other workers they only connect to ps. Which mechanism do you use for this By default all workers know about all PS tasks and all other worker tasks when you set up the tf.train.ClusterSpec . Each step used the same timeout 2 seconds Does this include steps that read from a queue based input pipeline Or steps that save a checkpoint It s possible that if the queues are mostly empty this timeout could be leaving behind a lot of cancelled attempts in the queue data structures which are more efficient in the case where timeouts are rare . Third when the total worker count is small like 200 it works fine with or without shard True but when the worker count is 800 or even 400 cluster will hang with or without shard True when the training is almost done. This is strange because I d expect the reliability of tf.train.Saver sharded True .save to be a function of the number of PS tasks and not affected by the number of workers. Out of curiosity why are you using multiple workers per machine I am using windows version and even I provided code you may not reproduce the issue using very small cluster. But I will try to minimize it. This is true... I don t have easy access to a dedicated cluster of hundreds of Windows machines . Hopefully we can work with you to figure out the problem and then patch it. mrry I am quite interested in the TensorFlow cluster scale in Google. What is the largest number of TensorFlow worker process have you Google guys used for model training And what is the upper bound of QPS can a parameter server handle Your experience may help our deciding machine count. shishaochen 1. Google uses stubby while open source version uses grpc which still has some obvious performance problems https github.com tensorflow tensorflow issues 6116 . I ve been told that Google plans to transition to gRPC internally so that may eliminate the gap eventually. 2. When I trained image models in Google and outside I have not seen communication minus the gRPC issues being the main bottleneck. The reason is that distributed training tends to be done using parameter server architecture which scales badly for reasons unrelated to hardware. You can actually hurt your accuracy by increasing hardware resources. For asynchronous updates imagine having 50 replicas updating parameters asynchronously. This means each replica sees parameters which are stale by 49 steps. As you increase number of workers the staleness increases even if network communication is instant. One ImageNet experiment I ve seen attained best accuracy for 50 replicas and then accuracy went down after that. Synchronous training does better for same replica count but exhibits similar diminishing returns as you increase number of replicas. You may have better scaling using model parallelism but this means using larger model and larger models take longer to train which is too long given that some of the smaller models already get trained for 6 months. For the largest image models data sizes I would say a few hundred GPU machines is an upper bound on the amount of resources you can efficiently utilize using parameter server. yaroslavvb Much thanks for your detailed explanation. As there are many idle machines without GPU we want to leverage them for distributed training with CPU only. mrry Second each worker is not aware of other workers they only connect to ps. Which mechanism do you use for this By default all workers know about all PS tasks and all other worker tasks when you set up the tf.train.ClusterSpec. A only provide all ps servers and itself when starting worker server cluster tf.train.ClusterSpec ps ps hosts worker task index worker hosts task index also tensorflow provide device filters when constructing ConfigProto tf.ConfigProto device filters job ps job worker task d LAGS.task index Does this include steps that read from a queue based input pipeline Or steps that save a checkpoint It s possible that if the queues are mostly empty this timeout could be leaving behind a lot of cancelled attempts in the queue data structures which are more efficient in the case where timeouts are rare . A the only queue I was using is file name queue and here is the data reading code So I am leveraging tfrecord to read data from file parse it using my own parser after getting raw sample data. I don t think it s queue issue. Out of curiosity why are you using multiple workers per machine A My neural network is very simple it s not CPU bottleneck increasing worker count will better utilize resources and increase throughput. I also tried python multi thread version by using the same session and different session each thread but didn t work well multi thread can not scale out well. I will reproduce this issue and dump tcp packet to see if I can find something. Will update later. mrry I changed the code in tensorflow core distributed runtime rpc grpc worker service.cc Line124 from to This change solved the hang issue. Now with total 400 workers no timeout error occurred and writing checkpoint finished successfully I verified it by four running. I don t understand why this could solve this problem is there any other performance issues when the cluster is large here is the skeleton of worker code fesun mrry great findings In response to mrry s previous hunch on the SaveV2 issue we recently pushed a patch to the master branch that OP REQUIRES OK the writer.Add Slice calls so hopefully the checkpoint writing path has become more robust or easier to debug . fesun Thanks for digging into the ENQUEUE REQUEST issue. This seems like a bug that only occurs at higher pending request counts e.g. when we have a larger cluster of workers all contacting the same PS task but it doesn t seem to trigger deterministically e.g. when the number of enqueued requests drops to zero . Perhaps somebody on the gRPC team has a hunch.... ctiller you were super helpful when we were writing the original async code for TensorFlow on gRPC and I recall you mentioning that there could be a negative impact on performance if the number of enqueued requests for a method ever dropped to zero. Can you think of a reason why a request might hang indefinitely in this regime It would be great to find a simple repro that doesn t involve running 800 processes.... Perhaps I made some fundamentally incorrect assumption about how the async code works mrry If changing the three constants to very small numbers maybe it s possible to reproduce the issue with dozens of workers I may try it when I get some time. mrry After increasing request object counts there is performance decay along with running time at first each step only takes 0.3 second after 4 5 hours each step takes 1 second. There must be some severe bug or design issues. fesun Thanks for looking into this further. Have you been able to reproduce this with some simple code that runs on a single machine If so we can try to reproduce it locally and take a look. Otherwise it would be great if you could try to attach some profiling tools such as pprof to see if i there is some slower path being taken later in the execution which would show up in a sample based profile or ii there is a memory leak that would show up in a heap profile of the running process. fesun any update Are you still experiencing this problem or can we close the issue skye My workaround works for me on v1.0.1. But I think the issue is still there I was told the workaround posted here didn t work for latest v1.2. fesun Maybe a small reproducing script would help us to identify the problem. As you mentioned if changing the three constants to very small numbers in ENQUEUE REQUEST could reproduce the issue in a smaller scale 2 3 nodes I am glad to run a customized build locally and see if I could reproduce that. Problem is I don t have a Windows box so I am not sure if it works. byronyi Sadly I can t reproduce it using small cluster. You mentioned the neural net involved in this case is simple. What is the largest size of tensors a worker need to fetch from PS and how many tensors are transmitted in a round gRPC is implemented in IOCP on Windows and it s beyond my knowledge if it only happens with a large number of concurrent connections and or requests per second. But I may have some idea if this issue occurs when transmitting large tensors. byronyi The tensor size is not large smaller than 1M what I want to highlight is the time when tensorflow hangs. It happens only when some workers finished training and waiting for others to finish using while loop by checking one finish count variable . The phenomenon is that sess.run didn t return within 20 seconds timeout when some workers finished without timeout exception at last I will call my custom saving operator master worker call sess.run my saving without timeout setting and all the parameter server wrote models successfully but sess.run never returned. Increasing those three magic numbers do solve my problem temporary but my scenario is this I have dozens of files to process I start parameter servers once restart workers for each file when I increase magic number to 3000 with 500 workers tensorflow hangs after about 6 7 restart increasing to 5000 hangs after about 10 restart. Seems that each worker will occupy one request object and not reusable. I believe this grpc implementation definitely has some wrong and probably platform independent. Could you elaborate on the way how you restart the workers Is it repeated call to session.run or you restart the whole worker process I remember there are some issues with current design such that a clean restart is not always possible within the same process lifetime there re some global resources that can t be released without exit of the process . Terminate all worker processes and create them again. byronyi If you want to know more details please contact me through email maybe we can have a quick talk so you can know exactly what I am doing. It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. 
7783,Segfault in runtime executor due to variable overflow,I m running Tensorflow 1.0 and I m encountering a segfault in tensorflow with the following output F tensorflow core common runtime executor.cc 484 Check failed e src output 32768 38774 vs. 32768 Aborted core dumped My program loads a fairly large sparse matrix 1879549 samples 556926 features 0.000038 of the entries in the matrix are nonzero into memory. I then create a tf.SparseTensor out of it x indices and x.data are the data for the sparse matrix in the correct format x ind tf.Variable initial value x indices.astype np.int64 trainable False x val tf.Variable initial value x.data dtype tf.float32 trainable False return tf.SparseTensor x ind x val dense shape x sparse.shape I then split this SparseTensor into minibatch sized splits. I have several queue runners feed that feed a Queue by taking a minibatch transforming it to dense and putting it into the queue. In code the process looks like this where self. input x sp is the SparseTensor self.x shape self. input x sp.get shape .as list self.y shape self. input y.get shape .as list n batches self.x shape 0 self.batch size self. x split tf.sparse split sp input self. input x sp num split n batches axis 0 self. y split tf.split self. input y num or size splits n batches axis 0 name y batch ... creating a Queue build a list of all possible enqueue OPs. We need to do this here while we re still single threaded as the Graph creation is not thread safe Later in the different threads we can just run the OPs self. op list for i in range n batches x batch tf.sparse tensor to dense self. x split i name x batch y batch self. y split i self. op list.append self. queue.enqueue many x batch y batch The queue runners randomly pick an operation from self. op list and execute it in a loop. I am confident that my code is not to blame as the program runs just fine on smaller input sizes a sparse matrix of 206208 samples and 133515 features of which 0.000135 nonzero entries but encounters the segfault on the larger matrix. Looking at the TF code where the error is generated here https github.com tensorflow tensorflow blob r1.0 tensorflow core common runtime executor.cc L484 it seems like the cause is a variable in tensorflow that is an unsigned short when it probably should be something larger than that.,Thanks for making a detailed analysis of the problem. You re welcome. I updated my post to remove some duplicated code and corrected the link to the runtime executor. If you need any further details let me know. Should be fixed in the next push. 
8231,memory leak when training complex neural networks,When training some complex seq to seq neural networks the memory cost of my program will keep growing and this only happens on GPU... On CPU everything is OK I used to report this problem in this issue https github.com tensorflow tensorflow issues 6599 After that I solved this problem by encapsulating my encoding method as an RNN cell so this issue was closed though no one knows the reason... But now this problem happens again because I changed the structure of my network... I do not think this is due to the bugs in my program because it runs very well on CPU...,Please provide details about what platform you are using operating system architecture . Also include your TensorFlow version. We ask for this in the issue submission template because it is really difficult to help without that information. Thanks In your previous issue report you did not describe how you measured or diagnosed the memory leak . What tool or statistic are you using Also if you believe there is a specific GPU operation which is somehow leaking tensors the this would be difficult to triage without a repro either code or a GraphDef which would allow us to indentify which ops you are running. Are you able to provide either of these carltonwang you can use https github.com yaroslavvb memory util to get a timeline of all tensor allocations deallocations determine which tensors are getting allocated that are taking so much space and then decide if this is intentional requested by your program or not Look at this code result tf.contrib.layers.bias add inputs tf.matmul a left b right activation fn tf.nn.softmax If I use softmax as the activation function there will be a memory leak but if I change the activation function to relu or softplus or sigmoid the memory leak will disappear. By the way this softmax is not the one used at the last layer for classification. Platform CentOS 7 x86 64 TensorFlow version 1.0 gpu carltonwang did you try yaroslavvb s memory analyzer. Perhaps with and without using tf.nn.softmax to get a baseline. After I update from TF 1.0.0 to TF 1.0.1 this problem disappear... Thanks closing since it seems to be resolved in newer version of software. I also find the same problem. After update TF from 1.0.0 to 1.0.1 the memory leak still exists. So we implement the softmax mannally use other TF operators such as tf.exp etc. The the memory leak disappear. we have submit a new open issue here https github.com tensorflow tensorflow issues 9779 I am training a deep neural network a CNN using out of memory data. As usual I am creating an imageDatastore and passing it to the trainNetwork function of the Deep Learning Toolbox. At the beginning of the training process the memory used by MATLAB is around 4GBs. Along the process the memory used increases constantly filling up the entire available amount which is 16GBs in my computer within a couple of epochs. Training terminates finally with the error message Out of memory . Searching for possible causes I wrote a small script which just reads data from https inloggenhulp.com the constructed datastores inside an infinite loop and found out that even this simple application increases the memory used by MATLAB gradually. Does anyone have an explanation for this issue 
8265,Memory leak when writing to Logfile with Tensorboard summarywriter, Issue Memory occupied after call to Filewriter not being freed till python termination. This causes accumulation of data and subsequent filling up of RAM which is freed only when the entire script completes execution and python is terminated. Environment info Operating System Ubuntu 16.04 CUDA cuDNN Not installed Link to the pip package you https github.com tensorflow tensorflow releases tag v1.0.0 Tensorflow version 1.0.0 A small script replicating the issue Logs or other output that would be helpful Python memory profiler output As can be seen above the additional 586.8 MB occupied after call to the write into log is never cleared. ,We are experiencing the same issue but with histograms. Eventually training script takes up 24G and is killed because of OOM. Digging a bit more into the first case falaktheoptimist shared above we found that the memory hogging is happening at two stages first when computing the image summary using session.run and second by the add summary function of Filewriter. Below are the profiler outputs showing half of the memory 285 MB occupied by sess.run and other half being occupied by file writer.add summary call. Case 1 Case 2 We have found the fix to this memory leak issue it s a minor tweak. We ll send in the pull request as soon as our CLA gets approved we re just waiting for that . This has been resolved with 8981 falaktheoptimist can you explain why it will cause memory leak when queue item is not passed directly The thread was not being killed till python termination and hence the reference to event queue item was never being freed. falaktheoptimist I get what you mean. The reference count of the last element in the queue will never decrease because self. queue.get will block forever. suiyuan2009 Exactly.. Thanks to your PR it s fixed now.. 
8494,Segfault Check failed num elements kMaxElements ,I m running the current nightly build as of today 2017 03 17 . I m working with very large sparse matrices and my program seems to have hit a variable overflow. It crashed with the following log entry 2017 03 17 10 48 34.554715 F tensorflow core framework tensor shape.cc 212 Check failed num elements kMaxElements 1141286132736 vs. 1099511627776 Aborted core dumped I m guessing this is because my sparse matrix is too large to handle for tensorflow. It would be great if this could somehow be fixed. Please refer to 7783 for more details about my code that was another overflow issue I encountered . ,Looks like we can just bump that limit up I ll work on getting that checked in. girving beat me to it. There s a patch in review one of us will update this issue once it s checked in. 
9462,numpy prod overflow during creating tensor,I am trying to allocate super large tensor using tensorflow but failed. https github.com tensorflow tensorflow blob master tensorflow python framework tensor util.py L417 Above code uses numpy.prod to calculate shape size and for numpy everything is typed say if the shape is 500000000 5 then numpy.prod returns 1794967296 it s very easy to reproduce it. So how about use int64 instead int64 should be large enough for any tensor. Changing shape size np.prod shape to shape size np.prod shape dtype np.int64 should fix it. Also about 100 lines of code using np.prod could we change them all to int64 , fesun Are you on a 32 bit machine I get np.prod 500000000 5 2500000000 np.prod 500000000 5 .dtype dtype int64 Explicitly setting dtype np.int64 does seem like the right solution but I d like to understand why it s breaking for you first. Maybe it s a Windows issue https docs.scipy.org doc numpy user basics.types.html id3 says that the default integer type is the same as a C long which is 32 bits in MSVC. girving I am on 64 bit windows machine I think mrry is right numpy default int type inside windows is 32 bit. fesun Would you be up for a pull request Adding dtype np.int64 sounds good. girving sure thanks. 
9487,terminate called after throwing an instance of std bad alloc not out of memory,Hi I m using Keras with tensorflow back end to train a LSTM network I was doing a grid search over the learning rate and dropout factor with the fixed batch size of 64 It ran perfectly but in the middle of it was interrupted by signal 6 SIGABRT with the following error terminate called after throwing an instance of std bad alloc what std bad alloc It should not be a memory allocation problem because it was running earlier for batch size of 64 which is not too much in my case you can find my system information tf env.txt from the following link https www.dropbox.com s wcv8y88fh659zck tf env.txt dl 0 ,Your memory usage may be on the edge. That is exactly the error given when running out of memory. Try running it again and monitor using ps or top the memory usage. I tried my training with batch size of 128 instead of 64 but it ran without any error I m wondering how it could be out of memory error while it can be run on a bigger batch size Because it could be memory fragmentation causing your memory usage to go up over run. Why don t you rerun your problem and graph the memory usage and see if that is the case thanks for the reply here is my memory usage free memory available in time image https cloud.githubusercontent.com assets 7490180 25541440 ab076890 2c1c 11e7 932d bbe868453f35.png I noticed those plateaus are when it reaches the last epoch number in the current training and moves to another training with different sets of parameters as i said earlier I m doing a parameter search I m wondering why it cannot free the memory after it reaches the end of the training and then moves to the next training with different parameter sets . Are you clearing your tensorflow graph between these trainings e.g. tf.reset default graph I used clear session in keras which is supposed to do the same thing. it fails to release the RAM that it was using. fchollet do you have any ideas clear session should definitely work here. It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. Nagging Awaiting TensorFlower It has been 14 days with no activityand the awaiting tensorflower label was assigned. Please update the label and or status accordingly. A member of the TensorFlow organization has replied after the stat awaiting tensorflower label was applied. sedghi is this still a problem for you Nagging Awaiting Response It has been 14 days with no activityand the awaiting response label was assigned. Is this still an issue tatatodd I was able to resolve it by sedghi did you call the K.clear session at the end of training or at the start of it ambodi at the end. I was doing 5 fold cross validation Hi I m using pi camera interfaced in raspberry pi3 b model. Kindly guide to resolve the error terminate called after throwing an instance of std bad alloc what std bad alloc I got same issue as KoushikRaghav . I have a Tensorflow 2.0 keras model that use tf.cast tf.exp and a division by 2. I noticed these operations idependently fail to run the TfLite file using TFLiteInterpreter. aselle Do you have some idea about why these operations are failing in tf nightly gpu 2.0 preview I m getting this error which I assume is from libtensorflow the C binary when using Tensorflow.js in a Node.js environment I can confirm that it is absolutely not using anywhere close to 10 of my system memory. In htop it caps out at well under 1GiB I have 24GiB to give it but it keeps crashing as above. 
9590,Memory Leak from Deep Learning Training Step Finalized Graph , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes although my code is somewhat based on the MNIST deep learning tutorial. OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 14.04 VERSION 14.04.5 LTS Trusty Tahr TensorFlow installed from source or binary binary I think not 100 sure since it s been a few months since install. How can I check TensorFlow version use command below v0.11.0 2614 g14aeb08 dirty 0.12.0 rc0 Bazel version if compiling from source CUDA cuDNN version CUDA Version 8.0.44 GPU model and memory GeForce GTX 780M 4GB Exact command to reproduce self.sess.run self.train step feed dict self.x trainingdata self.y true traininglabels self.keepratio self.training keep rate Describe the problem I apologize if this is not an actual bug I am relatively new to TensorFlow. I have posted on StackOverflow here http stackoverflow.com questions 43695085 tensorflow deep learning memory leak and the only response I have gotten suggests filing a bug report here. If I comment the sess.run line above and only that line out but still do all my pre processing and validation testing and such for a few thousand training batches the memory leak does not happen. The leak is on the order of a few GB per hour I am running Ubuntu and have 16GB RAM 16GB swap the system becomes very laggy and unresponsive after 1 3 hours of running when about 1 3 1 2 the RAM is used which is a bit weird to me since I still have lots of RAM and the CPU is mostly free when this happens... Here is some of the initializer code only run once at the beginning if it is relevant Notice that I have finalized the graph so nothing new should be added to it. Am I doing something wrong is this expected behavior or is this a real bug I have attached the source code as well two .py files in directory . Note I am happy put in the work to reduce the source to a minimal recreation of the bug but first I d like verification that 1 this would be helpful i.e. that the above info is not enough and that 2 this is probably a bug and not just an obvious beginner mistake on my part. Thank you in advance. source.zip https github.com tensorflow tensorflow files 969906 source.zip , jart could you take a look at the summary portion to see if you find anything obvious there I don t see obvious leak. Could you run with the env variable TF CPP MIN VLOG LEVEL 1 look at anything with LOG MEMORY and tf.RunOptions trace level tf.RunOptions.FULL TRACE https github.com tensorflow tensorflow issues 1824 is a good example of debugging. drpngx Every thousand steps instead of doing a normal training step I did Did I do it correctly The output is attached for steps 0 1000 2000 3000 4000 by which time a few GB have leaked . Search for step number to go quickly between them. I don t see anything about LOG MEMORY in there. If I did it wrong please let me know and I ll run it again. Thanks Output.txt https github.com tensorflow tensorflow files 982152 Output.txt It looks like it didn t work. You need to set the env variable something like this I tried again running it like you directed. I still do not see anything about LOG MEMORY in there. If it s still not correct the code is the same as above every 1000 training steps I run the following and the command I ran was Sorry if I m not doing it correctly I appreciate your help Output.txt https github.com tensorflow tensorflow files 987324 Output.txt Sorry try setting the min VLOG level TF CPP MIN VLOG LEVEL . There should be a lot of spew. Note that setting TF CPP MIN VLOG LEVEL if TF CPP MIN LOG LEVEL is set will have no effect. So you have to unset TF CPP MIN LOG LEVEL first. I wrote memory util https github.com yaroslavvb memory util to parse this log output. Save log to stderr.txt and then do something like this to get a human readable timeline memory util.print memory timeline open stderr.txt .read ignore less than bytes 10 6 No luck getting any other kinds of output. What am I looking for Will it print out in the console or is it going to be stored in my run metadata tf.RunMetadata variable I ve tried unsetting the env variables os.environ TF CPP MIN VLOG LEVEL 1 os.environ TF CPP MIN LOG LEVEL 1 restarting the computer Exporting both versions of the above. Doing it in the terminal instead of in the Python code Looking for results in both my output file and in my terminal What exactly am I looking for Any ideas on why I m not getting all the output Thank you again for the help. I just did this on my mac using Mac CPU nightly from last week and I saw a bunch of messages like this results only in Do I need to update my version or am I doing something else wrong I m using tf version 0.12.0 rc0 np version 1.12.0b1 Python version 2.7.6. Thanks Correct your TF version is too old you need at least 1.0 but perhaps a later version The issue is corrected in 1.1. Thank you both for the help The reason I was using the old version is because it says here that 0.12 is the latest stable version. https www.tensorflow.org versions If that is not the case I would suggest updating that page if possible to avoid other people making the same mistake. Thanks again for your time and help 
9779,memory leak when implement rnn attention decoder, System information cat etc issue Linux quad 4.4.0 72 generic 93 Ubuntu SMP Fri Mar 31 14 07 41 UTC 2017 x86 64 x86 64 x86 64 GNU Linux VERSION 16.04 LTS Xenial Xerus VERSION ID 16.04 are we in docker No compiler c Ubuntu 4.9.4 2ubuntu1 16.04 4.9.4 Copyright C 2015 Free Software Foundation Inc. This is free software see the source for copying conditions. There is NO warranty not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. uname a Linux quad 4.4.0 72 generic 93 Ubuntu SMP Fri Mar 31 14 07 41 UTC 2017 x86 64 x86 64 x86 64 GNU Linux check pips numpy 1.12.1 protobuf 3.3.0 tensorflow gpu 1.1.0 check for virtualenv True tensorflow import tf.VERSION 1.1.0 tf.GIT VERSION v1.1.0 rc0 61 g1ec6ed5 tf.COMPILER VERSION v1.1.0 rc0 61 g1ec6ed5 Sanity check array 1 dtype int32 env LD LIBRARY PATH is unset DYLD LIBRARY PATH is unset nvidia smi Tue May 9 12 16 54 2017 NVIDIA SMI 375.39 Driver Version 375.39 GPU Name Persistence M Bus Id Disp.A Volatile Uncorr. ECC Fan Temp Perf Pwr Usage Cap Memory Usage GPU Util Compute M. 0 GeForce GTX TIT... Off 0000 05 00.0 Off N A 22 47C P0 76W 250W 0MiB 12205MiB 0 Default 1 GeForce GTX TIT... Off 0000 06 00.0 Off N A 22 60C P2 129W 250W 11713MiB 12207MiB 80 Default 2 GeForce GTX TIT... Off 0000 09 00.0 Off N A 22 49C P0 83W 250W 0MiB 12207MiB 0 Default 3 GeForce GTX TIT... Off 0000 0A 00.0 Off N A 24 63C P2 117W 250W 11713MiB 12207MiB 70 Default Processes GPU Memory GPU PID Type Process name Usage 1 21558 C python3 11709MiB 3 21346 C python3 11709MiB cuda libs usr local cuda 7.5 doc man man7 libcudart.7 usr local cuda 7.5 doc man man7 libcudart.so.7 usr local cuda 7.5 lib64 libcudart static.a usr local cuda 7.5 lib64 libcudart.so.7.5.18 usr local cuda 7.5 lib libcudart static.a usr local cuda 7.5 lib libcudart.so.7.5.18 usr local cuda 8.0 doc man man7 libcudart.7 usr local cuda 8.0 doc man man7 libcudart.so.7 usr local cuda 8.0 lib64 libcudart static.a usr local cuda 8.0 lib64 libcudart.so.8.0.27 Describe the problem When we try to implement a complex network which contain a rnn attention decoder It will consume all the memory after several days. I extract the decoder in a test file the memory still grow in a slower speed. also found that if change softmax to sigmoid memory doesn t leak. Source code logs test code log ,by the way if we implement softmax like this the problem doesn t exists. original softmax in TF attention tf.nn.softmax tf.reshape attention T B dim 0 mannul softmax attention tf.matmul tf.reshape r attention 1 H1 w attention attention tf.reshape attention T B attention exp tf.exp attention T B attention sum tf.reduce sum tf.exp attention axis 0 B attention attention exp attention sum T B ebrevdo Can you take a look This seems like a reasonably minimized memory leak bug. Is this reproducible in the nightlies This is a stale issue. Please check the issue with latest TensorFlow. If the issue still persists in the newer version of TF please feel free to reopen it by providing details about the issue and a standalone code to reproduce the issue. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 9779 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 9779 No a 
9867,distributed runtime leaking memory on windows, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow no OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 TensorFlow installed from source or binary binary TensorFlow version use command below b unknown 1.1.0 Bazel version if compiling from source CUDA cuDNN version GPU model and memory Exact command to reproduce follow https www.tensorflow.org deploy distributed Describe the problem The memory footprint for parameter server keep growing. The cause is a memory leak in a windows specific path in grpc which is fixed here https github.com grpc grpc commit fa242cba900ece728d2910e7396d02ebab4ddb2c I filed the issue issue so others don t need to spend the time debugging it and as reason to update the grpc version. , mrry Looks like we should update grpc if that hasn t already happened. Thanks for figuring out the problem guschmue I think updating gRPC is blocked by 7466 which appears to be on hold because of a different bug in gRPC on Windows https github.com grpc grpc pull 9826. Handing this off to jhseu who is shepherding in 7466. https github.com tensorflow tensorflow pull 11768 closing. 
10408,Memory leak ,I have a memory leak with TensorFlow. I refered to https stackoverflow.com questions 35695183 tensorflow memory leak even while closing session to address my issue and I followed the advices of the answer that seemed to have solved the problem. However it does not work here. In order to recreate the memory leak I have created a simple example. First I use this function that I got here https stackoverflow.com questions 276052 how to get current cpu and ram usage in python to check the memory use of the python process def memory import os import psutil pid os.getpid py psutil.Process pid memoryUse py.memory info 0 2. 30 memory use in GB...I think print memory use memoryUse Then everytime I call the build model function the use of memory increases. Here is the build model function that has a memory leak def build model Model tf.reset default graph with tf.Graph .as default tf.Session as sess tf.contrib.keras.backend.set session sess labels tf.placeholder tf.float32 shape None 1 input tf.placeholder tf.float32 shape None 1 x tf.contrib.keras.layers.Dense 30 activation relu name dense1 input x1 tf.contrib.keras.layers.Dropout 0.5 x x2 tf.contrib.keras.layers.Dense 30 activation relu name dense2 x1 y tf.contrib.keras.layers.Dense 1 activation sigmoid name dense3 x2 loss tf.reduce mean tf.contrib.keras.losses.binary crossentropy labels y train step tf.train.AdamOptimizer 0.004 .minimize loss Initialize all variables init op tf.global variables initializer sess.run init op sess.close tf.reset default graph return I would have thought that using the block with tf.Graph .as default tf.Session as sess and then closing the session and calling tf.reset default graph would clear all the memory used by TensorFlow. Apparently it does not. The memory leak can be recreated as following memory build model memory build model memory The output of this is for my computer memory use 0.1794891357421875 memory use 0.184417724609375 memory use 0.18923568725585938 Clearly we can see that all the memory used by TensorFlow is not freed afterwards. Why I hope I made myself clear.,Would you be able to plot memory usage over a thousand iterations That will help us rule out the possibility that the memory usage is caused by modules being loaded or garbage collector fanciness. It would also be great if you could narrow down the number of APIs being called. image https cloud.githubusercontent.com assets 19774802 26753716 0e980254 486d 11e7 9ac8 4f57150b7815.png jart Here you go. As you can see the memory usage goes up in a linear way which is exactly the problem. About the number of APIs being called do you refer to Keras by saying that I only use tf.contrib.keras which is part of tensorflow. Hence I only use tensorflow here. Caselles Excellent. Thank you. fchollet There appears to be some type of memory leak in Keras. Early tests seems to show that GRAPH LEARNING PHASES is not cleared so there is tons of tensors that get kept. Changing to Seems to resolve the problem. memory use 0.13166046142578125 memory use 0.13190841674804688 memory use 0.13220977783203125 memory use 0.13220977783203125 memory use 0.13220977783203125 memory use 0.13220977783203125 memory use 0.13220977783203125 memory use 0.13220977783203125 Will do a PR on keras it will then get merged into TF I guess This is actually fixed in keras master branch. https github.com fchollet keras blob master keras backend tensorflow backend.py L82 So waiting for the merge will fix the problem. Dref360 this is fixed in TF master https github.com tensorflow tensorflow blob master tensorflow contrib keras python keras backend.py L288 Does this resolve the problem K.clear session is never called. maybe K.set session should do a clear session Before posting this issue I tried calling K.clear session but my tensorflow version was 1.1... Never lucky. By upgrading to 1.2 and calling K.clear session the problem was solved. Thank you very much Independently of clear session I would expect the use of graph scopes to prevent this behavior. What s the status of that The graph is not cleared. Maybe because we still have a reference to it so the GC doesn t collect If you print list GRAPH LEARNING PHASES.values 1 .graph.get operations you can see that the operations are still there. So the use of graph scopes will not prevent this behavior. Is this planned to be solved Dref360 Would you have a workaround to clear the operations still in the graph I would like not to clear all the operations in the graph but only those that are created within the with tf.Graph .as default tf.Session as sess block. fchollet jart Can you provide a solution for this problem The solution is to make learning phases part of a graph collection I believe. Potentially the same may also be true of layer name UIDs which also keeps graph references. Last time we tried this it caused a useless warning upon graph serialization which annoyed scared users. We will also have to figure out how to remove that warning. Ok thank you. Would you have a workaround for now In order to clear only the operations within the with tf.Graph .as default tf.Session as sess block. Add K.clear session at the end of your block inside the block . Problem is I m trying to run two model at the same time in a website. Hence calling K.clear session in the code clears operations that are used elsewhere. So this creates a bug. I really need to be able to specify the operations that I want to clear. Could you be a little more precise on how I could do that There are only two possibilities 1 you no longer use the graph. In that case clear session destroys it and that is what you want. 2 you are still using the graph. In that case you cannot garbage collect it. And you don t have a memory leak . On 13 June 2017 at 11 01 Syzygy notifications github.com wrote Problem is I m trying to run two model at the same time in a website. Hence calling K.clear session in the code clears operations that are used elsewhere. So this creates a bug. I really need to be able to specify the operations that I want to clear. Could you be a little more precise on how I could do that You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 10408 issuecomment 308197476 or mute the thread https github.com notifications unsubscribe auth AArWb8x5Gi6WMApi ZjfF8000k dLIhSks5sDs5mgaJpZM4Nu1dH . I m sorry for not saying that earlier but calling K.clear session within the tf.Graph .as default tf.Session as sess yields an error IndexError Traceback most recent call last ipython input 7 7793873c5c17 in module 1 build model ipython input 6 3626073d2002 in build model 31 32 K.set session sess 33 K.clear session 34 35 tf.reset default graph Users Syzygy anaconda lib python3.5 contextlib.py in exit self type value traceback 75 value type 76 try 77 self.gen.throw type value traceback 78 raise RuntimeError generator didn t stop after throw 79 except StopIteration as exc Users Syzygy anaconda lib python3.5 site packages tensorflow python framework ops.py in get controller self default 3626 finally 3627 if self. enforce nesting 3628 if self.stack 1 is not default 3629 raise AssertionError 3630 Nesting violated for default stack of s objects IndexError list index out of range has there been any progress on this issue I have the exact same problem and exact same error while calling clear session. I m using the latest keras and tensorflow version. I have a very similar issue causing memory leak but I m only using tensorflow without keras. Here s the minimal code import tensorflow as tf import numpy as np for i in range 30 tf.Session . enter tf.constant np.random.random 800 500 500 1 tf.get default session .close tf.reset default graph When executing the loop the memory used keeps going up. How can I actually delete the old large constants and free the memory I m using tensorflow 1.2 with python 3.4 on ubuntu 14.04 fchollet Perhaps everyone is in trouble with this bug. Do not forget. I am getting the same issue with multiple models prediction either in sequential or in parallel execution. The memory doesn t seem to free the memory after use. I recently resolved my model s memory leak issue it turns out to be a counter paradigm of how to construct and run the model i.e various TensorOps ... I mistakenly added cosine decay restarts https www.tensorflow.org api docs python tf train cosine decay restarts ops in every iteration of the training something like And the training python process would get killed by the OOM killer at some point. My instincts told me I shouldn t re construct the train.cosine decay restarts in every loop so after this simple remedy the memory leak issue was gone... So maybe everybody could make a coarse check of whether you are re constructing the model in every loop... Hello I am also having a problem with memory leak on running label image logic in a loop I use Tensorflow r1.0.1 and opencv 2.4.9 I am not sure yet if memory leak is opencv related or tensorflow related. In some posts seems that capture.read can lead to memory leak opencv related in other posts could be because of operators initialization inside loop tensorflow related Below part of my code I would appreciate if smo can check the code inside the loop in case there is smth obvious I should change to avoid memory leak thank you in advance with tf.Session graph graph config tf.ConfigProto inter op parallelism threads 1 intra op parallelism threads 1 as sess ret frame cap.read file name ...png if frame is not None cv2.imwrite file name frame input name file reader output name normalized file reader tf.read file file name input name if file name.endswith .png image reader tf.image.decode png file reader channels 3 name png reader float caster tf.cast image reader tf.float32 dims expander tf.expand dims float caster 0 resized tf.image.resize bilinear dims expander input height input width normalized tf.divide tf.subtract resized input mean input std t sess.run normalized results sess.run output operation.outputs 0 input operation.outputs 0 t results np.squeeze results top k results.argsort 1 1 labels load labels label file ... I am going to upgrade also opencv now in my system. Do you think I should upgrade also Tensorflow version. To be honest I would keep same version of tensorflow in order to use .pb graph already generated because I am not sure if my .pb will be compatible with newer versions. Any suggestions to overcome memory leak problem and find what really causes this will be very helpful. Thank you I have the same issue. Just by running the same code again the memory doesn t seem to get freed by clear session. Below is my code. Please help import keras from keras.applications.vgg16 import VGG16 vgg16 model VGG16 model Sequential for layer in vgg16 model.layers 1 model.add layer model.summary for layer in model.layers layer.trainable False model.add Dense 2 activation softmax model.summary K.clear session K.reset uids Problem is I m trying to run two model at the same time in a website. Hence calling K.clear session in the code clears operations that are used elsewhere. So this creates a bug. I really need to be able to specify the operations that I want to clear. Could you be a little more precise on how I could do that Dear sir. How did you resovle your issue I have the same issue. Thx. Caselles You might find some useful info here https stackoverflow.com questions 44327803 memory leak with tensorflow Dear sir. Thank you for your reply. As you can see calling K.clear session in the code clears all graphs however I just want to clear one of them. Thx. Caselles Leaving this here so I remember to come back Monday and write out a work around for the memory leak inside a loop issue using Keras. Caselles Is this still an issue I tried you code with tf nightly and could not reproduce the issue. I have used K.get session and K.clear session . The memory usage is almost constant. Output is Please let me know what you think Here is the gist https colab.sandbox.google.com gist jvishnuvardhan 1445ddf65ab642ffb7e502e6260f5482 tf 10408 keras runtime.ipynb . Thanks Closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks 
11334,Memory Overhead Leak in Android lib, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Nexus 6p Android v7.1.2 TensorFlow installed from source or binary source TensorFlow version use command below 1.2.0 rc2 Python version 2.7.10 Bazel version if compiling from source 0.4.5 homebrew CUDA cuDNN version N A GPU model and memory Exact command to reproduce Selective Headers bazel build c opt copt DSELECTIVE REGISTRATION copt DSUPPORT SELECTIVE REGISTRATION tensorflow contrib android libtensorflow inference.so crosstool top external android crosstool host crosstool top bazel tools tools cpp toolchain cpu armeabi v7a Added tensorflow core kernels random shuffle queue op.cc and tensorflow core kernels random shuffle op.cc to tf op files.txt file Removed unused nodes bazel build tensorflow tools graph transforms transform graph bazel bin tensorflow tools graph transforms transform graph in graph model.pb out graph optimized model.pb inputs input outputs output transforms strip unused nodes type float shape 1 299 299 3 Describe the problem The Tensorflow Android library is using a lot more memory than I expected. It almost seems like it s maintaining a reference to all input arrays as memory usage balloons the longer the model is used. Here is an example of the memory usage with feed run fetch commented out source code below no tensorflow https user images.githubusercontent.com 4616968 27930636 3342a548 624c 11e7 91ed 3f4f56b7cf5f.png Here is the same timeframe with the only difference being that feed run fetch is enabled tensorflow https user images.githubusercontent.com 4616968 27930738 954e125e 624c 11e7 82f2 cc21e67bc5d3.png Memory usage is over three times worse. The longer I leave the model running the more memory usage increases it eventually gets to 110 mb . The below method is being called at a rate of 4.419011933 per sec i.e. it s processing 4.412 input arrays per second where each input array is of size 96 96 3 27648 . This is being run on a Nexus 6p running stock 7.1.2. The model is a conv net with inception batch norm and dropout trained using tensorflow slim. Source code logs Commented out Enabled where float pixels is a float array of size 27648 denoting the pixels in an image of size 96x96. The custom code is an update to the InferenceInterface to accept boolean types during feeding Please let me know if there s any other information I can provide.,It looks to me like there s a related discussion about the overheads of feed run fetch in 8712. Does any of that help cy89 thanks for the response. I read that issue before posting mine and it was related. Unfortunately the discussion turned towards timing and performance as opposed to memory allocation. Makes sense. petewarden are there perhaps any updates about our Android TF ecosystem and its memory usage behavior faifai21 Thanks for the detailed report Are you able to determine exactly how much the memory use increases per inference pass If you change the size of your input array does the rate of increase vary proportionally I m just curious if something else in the run call could be allocating memory that never gets cleaned up. asimshankar Any idea why this might be happening given that closeFetches and closeFeeds are both guaranteed to be called every invocation of TensorFlowInferenceInterface.run There was a leak in 1.2.0 rc0 and prior versions fixed by https github.com tensorflow tensorflow commit 8304e197ea9eeb617f224a1ba0cc4068596098d1 faifai21 Since you re building from source could you confirm that your build includes the commit mentioned above asimshankar I just checked and that commit is included in the build. I can try rebuilding from the latest release and see if the issue goes away. Are you able to determine exactly how much the memory use increases per inference pass andrewharp I ll get you that information first thing tomorrow morning. asimshankar I pulled in the latest master and built against commit https github.com tensorflow tensorflow commit 99a38ffd9d77c55ca6d0c373c6d4b72686284ac5. The memory leak is still occurring though. andrewharp It seems like memory is increasing by 0.1 MB per inference pass which makes sense as each input array is around 110kb. As well increasing the size of the input array does cause the memory leak to increase With an input array of 96 96 3 each input array was 110kb. Memory usage was increasing at 0.5 mb per sec. With 4.41 inputs processed per sec it can be inferred that memory was increasing by 0.11 mb per run. With an input array of 120 120 3 each input array was 172kb. Memory usage was increasing at 0.7 mb per sec. Since input size was increased we were processing slightly less inputs per sec than before. Assuming we were processing 4.1 inputs per sec now we can infer that each run increased memory by 0.17mb. I was unable to increase input size by anymore as that caused BufferOverflow exceptions with FloatBuffer as the input array was copied over when creating the Tensor. I also tried decreasing the size of the input array to 30 30 3 which resulted in each input having a size of 10kb. The memory leak was still present but it was much less apparent. As expected though memory usage increased by 0.01 mb per run. andrewharp We are also experiencing the same issue with tensorflow library on android. The memory keeps on increasing as the number of inference runs keep on increasing. This is leading to lot of out of memory errors in our application. We tried creating a new TensorFlowInferenceInterface after every inference but even that did not solve the problem. It looks like a memory leak in tensorflow c library. We tried creating a new TensorFlowInferenceInterface after every inference but even that did not solve the problem. I tried the same thing and can confirm that the memory leak was still there. andrewharp asimshankar any updates on this We ve also found this issue in our iOS tensorflow library also built from source which makes sense given that the memory leak seems to be in C C code. Experiencing the same problem as above lots of memory leakage due to the feeding operation. While I m sure I m missing a lot of intricacies here the source seems obvious to me. During every call to feed we are allocating new memory for the input tensor float tensor in my case addFeed inputName Tensor.create dims FloatBuffer.wrap src Is there no way to overwrite the data in the existing FloatBuffer after the initial feed call Instead of allocating space for a new Tensor each time with the call to create Would that not solve the memory leak we are experiencing faifai21 No updates yet unfortunately. Since you have this in C land as well perhaps you could come up with a small C snippet that demonstrates this and it might make it easier to trace any problems by running the sample on a desktop laptop instead of a mobile device. Mr Grieves I m not following. During every call to feed a new Tensor is created but it is released with a call to closeFeeds https github.com tensorflow tensorflow blob r1.3 tensorflow contrib android java org tensorflow contrib android TensorFlowInferenceInterface.java L157 on every call to run . Are you experiencing leaks or is it that total memory usage is higher than you d want and waits for a garbage collection cycle to come down asimshankar The latter. I m new to Java and still not totally comfortable with the idea of leaving objects around to be cleaned up by the GC... My app runs as desired initially but slows down over time and eventually grinds to a halt. I initially thought this issue was being caused by a memory leak but perhaps the memory management is running as it should and the cause of the slowdown lies somewhere else... I ve attached a screenshot of my Memory and CPU monitors. GC events are being triggered every 10s or so with 99 of the allocations occurring during the addFeed inputName Tensor.create dims FloatBuffer.wrap src . If the memory is not the problem my issue is most likely CPU related in which case I am posting in the wrong thread. memorytrace https user images.githubusercontent.com 19175336 28688053 6c3455d4 72c5 11e7 981e 114d0cdf3d3d.png Mr Grieves It could be thermal throttling slowing down the CPU frequency after sustained load. I m not sure if you can view this in Android Studio but you could try printing the frequency https stackoverflow.com questions 3021054 how to read cpu frequency on android device manually via adb shell and see if it changes during a run. asimshankar Thanks for the response. I ve spoken to our iOS engineers a few more times and it seems like the memory leak does not actually happen in iOS land. This seems to be an issue on Android only perhaps in the JNI methods asimshankar andrewharp This memory leak is blocking us from shipping tensorflow on Android. Are there any updates for this Is there anything I can do to help I tried looking through the source code to find the memory leak but I m not well versed in c c . It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. A member of the TensorFlow organization has replied after the stat awaiting tensorflower label was applied. Sorry this fell through the cracks. Is that still a problem with the latest versions Our current recommended platform for mobile is tflite maybe that fits your needs better Nagging Awaiting Response It has been 14 days with no activityand the awaiting response label was assigned. Is this still an issue Nagging Awaiting Response It has been 14 days with no activityand the awaiting response label was assigned. Is this still an issue Nagging Awaiting Response It has been 14 days with no activityand the awaiting response label was assigned. Is this still an issue Automatically closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks drpngx I m facing the exact same issue on version 1.5.0 . As I keep performing inference on an Android device the memory consumed by the app keeps increasing. No. of Inference runs Memory usage 1 5.7 Mb 15 206 Mb 20 242 Mb 25 757 Mb 30 922 Mb Is there a fix for this issue deepaksuresh There isn t any known fixes. Could you file a new issue with the details particularly if there are any instructions to reproduce the problem Can you share the model perhaps asimshankar I m running the model from a .pb file. It is 70 Mb how can I share it with you You could upload it to some shared public storage and share from there. Though before that is there anything you can do to isolate the leak For example does a simple loop over the TensorFlowInferenceInterface object s method show this leak 
11721,Memory Leak from Training Step, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes although my code is somewhat based on the MNIST deep learning tutorial. OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 14.04 VERSION 14.04.5 LTS Trusty Tahr TensorFlow installed from source or binary Installed via the VirtualEnv method TensorFlow version use command below 1.1 and 1.2 Bazel version if compiling from source CUDA cuDNN version CUDA Version 8.0.44 GPU model and memory GeForce GTX 780M 4GB Exact command to reproduce self.sess.run self.train step feed dict self.x trainingdata self.y true traininglabels self.keepratio self.training keep rate Describe the problem This is very similar to the bug report I submitted here https github.com tensorflow tensorflow issues 9590 but is a bit of a slower leak and is present in both TF 1.1 and 1.2. I have finalized my graph. Using the architecture described by Zeiler et al. 2013 ZF Net https arxiv.org pdf 1311.2901v3.pdf batch sizes of 64 and 224x224 grayscale 1 channel input it leaks approximately 4GB after approximately 3000 batches. This makes it unworkable for say 80 epochs of ImageNet training. I have confirmed that the leak either does not occur or is much less severe hard to tell which if I comment out the training line i.e. still do all of my preprocessing and loading . As directed in that last linked issue I tried to call sess.run with options tf.RunOptions trace level tf.RunOptions.FULL TRACE run metadata run metadata and start the program with env TF CPP MIN VLOG LEVEL 1 python deep learning main.py but the amount of spew was enormous and it won t respond to keyboard interrupts I have to kill the job . If that info would be helpful how do I go about recording saving this information properly to upload and help you all debug 1 08.zip https github.com tensorflow tensorflow files 1170372 1 08.zip ,Write the log with TF CPP MIN VLOG LEVEL 1 to a file and upload it somewhere so that we can download it. Also see if you can reproduce the problem with a simpler program. The current program is over 1000 lines making it hard to debug a memory leak. Additionally I cannot run the program because you have some hardcoded paths and the matplotlib dependency makes it hard to run internally. See if synthetic data can be used to remove the dependency on the imagenet data. I apologize for not getting back to this and being able to help I ve run out of time and had to move on to a new project. However I have had a similar issue with PyTorch apparently it is an issue with CUDA and maybe my GPU. Perhaps the TF leak is caused by same issue See https discuss.pytorch.org t memory usage leak 5645 reedwm btw regarding using TF CPP MIN VLOG LEVEL to debug memory leaks it became less useful between 1.0.1 and 1.1 Some of the deallocation messages started missing allocation ids so you can t tell how much memory got reclaimed. ie for some of my scripts I see messages like this in TF 1.1 2017 05 09 15 31 16.865982 I tensorflow core framework log memory.cc 35 LOG MEMORY MemoryLogTensorDeallocation allocator name cpu instead of following in TF 1.0.1 I tensorflow core framework log memory.cc 35 LOG MEMORY MemoryLogTensorDeallocation allocation id 235 allocator name cpu JamesKostas That s ok. Respond to this post if you ever get time to upload the logs or reproduce the problem with a simpler program. yaroslavvb Can you file a separate bug allocation id might be only printed if its not 0 but I m not sure. reedwm filed under https github.com tensorflow tensorflow issues 13087 Without a reproducible test case it seems unlikely we are going to make much headway especially if you are working on a new project. So I am closing for now. zheng xq FYI. 
11948,Memory leak in Java API when using GPU, System information Custom code https github.com riklopfer TensorflowJavaGpuMemoryTest OS CentOS 7 TensorFlow installed from source or binary binary TensorFlow version use command below n a Python version n a Bazel version if compiling from source n a CUDA cuDNN version 8.0 GPU model and memory GeForce GTX 1080 Exact command to reproduce see https github.com riklopfer TensorflowJavaGpuMemoryTest Describe the problem Main memory on the machine is continuously consumed when running on the GPU. Memory consumption hovers around 600M when running on the CPU. Source code logs see https github.com riklopfer TensorflowJavaGpuMemoryTest, asimshankar could you please take a look at this. Sample log output fwiw I ve added valgrind http valgrind.org info tools.html memcheck output to the test repository https github.com riklopfer TensorflowJavaGpuMemoryTest blob master valgrind.out I m not really familiar with this tool but it seems like it would be useful. The summary makes me think that there definitely is a leak somewhere riklopfer Thanks very much for getting that information across. Unfortunately not a lot struck out to me. I did see 32 bytes of leaks from graph construction which I will fix but that happens once not in a loop so won t explain the increasing usage over time. asimshankar thanks for the fixes. Were you able to reproduce the issue of ever increasing memory consumption Any idea what the next steps might be Updating CUDA and Nvidia drivers seems to have greatly mitigated the problem for me. I added updated valgrind output https github.com riklopfer TensorflowJavaGpuMemoryTest blob master updated valgrind.out to the test repo. Thanks for the update riklopfer Sampling the latest output I m not sure if there are false positives or actual leaks e.g. many leaks are reported in CreateJavaVM which IIUC has nothing to do with TensorFlow it s just JVM initialization. When you say greatly mitigated are you still seeing a monotonic increase in memory usage over time or does it stabilize asimshankar I no longer see monotonic increase in memory consumption when running my the small test in the linked repo. However when I run a longer more complicated graph on the GPU it is killed by the OOM killer. I wasn t able to get a valgrind dump for that process. When I have time I will try increasing the complexity of the test graph until it shows the problem again or not . It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue Please update the label and or status accordingly. Running with 1.4.0 I still see a slow monotonic increase in memory consumption. I haven t had a chance to attempt to minimally reproduce the issue. It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue Please update the label and or status accordingly. The original poster has replied to this issue after the stat awaiting response label was applied. Closing since the original issue has been fixed. Please file another ticket with a repro if you can. Thanks 
13202,tf.InteractiveSession leaks sessions,The following works fine with tf.Session but will fail to release resources in tf.InteractiveSession The reason is that interactive session enters a context using enter and never quits it leaving a reference from a DefaultStack object. I found this when debugging why my notebook was hogging all GPU RAM. The two work arounds 1. Force C API to close the session using sess. del 2. Get rid of the dangling reference I think a better solution would be to have sess.close call both TF CloseSession and TF DeleteSession or have a method that will reset all sessions like session lib.Reset , mrry do you know if anyone s planning to make Session.Reset work for local sessions Derek could you please take a look. Thanks yaroslavvb I m confused by your diagnosis because the InteractiveSession.close method explicitly calls exit https github.com tensorflow tensorflow blob e9d5ee1ebffba25cef65f1f354b9e4ca9bcea10c tensorflow python client session.py L1627 on the context managers it enters in the constructor. Where is the dangling reference Session.reset is implemented https github.com tensorflow tensorflow blob e9d5ee1ebffba25cef65f1f354b9e4ca9bcea10c tensorflow core common runtime direct session.cc L183 for local sessions but it s not clear what you think it should do.... I m experiencing the same with the normal tf.Session and also while using the C API directly. The following code keeps the memory on the GPU allocated until it exits. I m running Ubuntu 16.04 cudnn 6.0 cuda 8.0 tensorflow master build is 2 hours old. PhilJd that s a different issue the memory allocator is global to the process so it will keep the memory allocated even if the session gets closed. mrry indeed that method gets called but somehow session doesn t get garbage collected. I assumed DefaultStack was to blame by looking at gc.get referrers sess CPython is supposed to call del immediately after ref count gets decremented to 0 and somehow it doesn t. Adding import gc gc.collect after exit seems to remove the leak as well Repro https github.com yaroslavvb stuff blob master resnet leak report.py When I run it I see I ll update this issue if I find why CPython doesn t call del OK looks like del isn t called because there s a cycle and ref count never goes to zero. sess. default session.args default points back to sess args are filled in here https github.com tensorflow tensorflow blob 03619fab3f4dd6f28b67418455a953b0fccdd9bf tensorflow python framework ops.py L4507 but there s no such cycle when using tf.Session Running gc.collect detects cycles and cleans them up. Ironically InteractiveSession only causes this problem during interactive use when you have a notebook open and create new graphs sessions without restarting the process. The underlying issue is that sess needs to keep a reference to the default context manager otherwise it gets garbage collected and closed whereas default context manager must link back to session. This seems hard to fix this issue without big refactoring of context manager logic. Perhaps it should be a docs issue and mentioned in docs of InteractiveSession that gc.collect must be called to reclaim resources Would it make sense to make tf.InteractiveSession a singleton and add a warnings.warn Use tf.Session instead if you intend to have multiple sessions upon the second call Singletons are typically bad but maybe very fitting in this particular case. I think that would be better than status quo which is 1. create multiple interactive sessions by accident. 2. run out of GPU memory 3. restart notebook server It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. Nagging Assignee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Fix in progress... Nagging Assignee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee mrry It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. So after implementing a couple of fixes for this it seems there s no way to stop the leak without breaking an existing use case. The fix will be a warning as carlthome suggested and we will investigate a more principled fix for TensorFlow 2.0. PS the work around is to do the following That will run cyclic garbage collector and remove all the dangling sessions Note that we ve had to roll back https github.com tensorflow tensorflow commit 0f508d4de379e800ad7f990de08959bbd6fcabb5 internally because it has a bug that produces spurious warnings. We will fix it after the weekend but in the mean time the nightly build will have the spurious messages. mrry maybe you can just do import gc gc.collect in the first call to .run of InteractiveSession yaroslavvb The import gc gc.collect trick doesn t solve the problem for me. Not sure why yet. Nagging Assignee mrry It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. The warning was re added in a80fb2b1cad1bb9c868222b8c25f162d69a509e6 so I m marking this as closed. 
13244,BUG Memory leak in tf.string split, profile 9734.0066.txt https github.com tensorflow tensorflow files 1325612 profile 9734.0066.txt profile 9734.0100.txt https github.com tensorflow tensorflow files 1325613 profile 9734.0100.txt profile 9734.0150.txt https github.com tensorflow tensorflow files 1325611 profile 9734.0150.txt Please go to Stack Overflow for help and support https stackoverflow.com questions tagged tensorflow If you open a GitHub issue here is our policy 1. It must be a bug or a feature request. 2. The form below must be filled out. 3. It shouldn t be a TensorBoard issue. Those go here https github.com tensorflow tensorboard issues . Here s why we have that policy TensorFlow developers respond to issues. We want to focus on work that benefits the whole community e.g. fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem rather than being redirected to Stack Overflow. System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Centos Linux version 3.10.0 229.4.2.el7.x86 64 gcc version 4.8.2 20140120 Red Hat 4.8.2 16 GCC TensorFlow installed from source or binary pip install tensorflow TensorFlow version use command below tensorflow 1.3.0 Python version Python 2.7.5 Bazel version if compiling from source N A CUDA cuDNN version N A CPU only GPU model and memory N A Exact command to reproduce You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION Describe the problem Describe the problem clearly here. Be sure to convey here why it s a bug in TensorFlow or a feature request. I am seeing noticeably large memory usage when I use tf.string split within the map function of a dataset API. I have attached a sample code below. I tried to do a heap analysis and I see std basic string Rep S create constantly growing in size and not freeing up its memory. If i remove the tf.string split and just return the line as is there is no memory held over. This issue is a blocker for us to scale up the tensorflow pipeline to large datasets. I have attached three output files of pprof over time . 6447.1 96.8 96.8 6447.1 96.8 std basic string Rep S create 9765.5 96.5 96.5 9765.5 96.5 std basic string Rep S create 14704.7 95.5 95.5 14704.7 95.5 std basic string Rep S create Source code logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem. import tensorflow as tf def mapper line line tf.identity line tokens tf.string split line delimiter t return tokens.indices def run filenames sample.txt Cluster spec cluster tf.train.ClusterSpec worker localhost 2223 server tf.train.Server cluster job name worker task index 0 dataset tf.contrib.data.TextLineDataset filenames dataset dataset.batch 1000 dataset dataset.map mapper 8 .repeat iterator dataset.make one shot iterator next element iterator.get next with tf.Session target server.target as session while True try session.run next element except tf.errors.OutOfRangeError break run You can run this script by LD PRELOAD usr lib64 libtcmalloc.so.4 HEAPPROFILE tmp profile nohup python u bug.py output.log , mrry do you have any insight into this. Does the dataset pipeline hold string data in some special way The dataset pipeline doesn t do anything special with strings and relies on tensorflow Tensor refcounting to release the data. sushanthku Can you try making a tf.constant string from a representative line in your file and running tf.string split on that constant in a loop to see if the memory growth is similar It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue Please update the label and or status accordingly. It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue Please update the label and or status accordingly. Closing due to lack of activity. sushanthku Feel free to reopen if you can pare this down to a self contained reproduction. 
13999,Memory Leak While Reading from TFRecord, Problem As I mentioned in my previous issue https github.com tensorflow tensorflow issues 6599 I have memory leak in my code. Finally I can write a sample code that can reproduce the problem. Source code logs I can t find the reason. Many thanks for your consideration. System information OS Platform and Distribution Linux Ubuntu 14.04 TensorFlow installed from binary TensorFlow version v1.3.0 rc2 20 g0787eee 1.3.0 Python version 2.7.6 CUDA cuDNN version NA GPU model and memory NA , mrry would you take a look or assign to whomever is supporting queue based inputs Interestingly the leak seems to be plugged if you replace the line ...with the following line I suspect it is not a coincidence that labels is a tf.SparseTensor because tf.train.shuffle batch handles these by storing them in a buffer replacing them with a key to pass through the batch queue and then removing them from the buffer when the key is emitted from the batch queue. Perhaps if you don t use the value the remove from buffer code doesn t get triggered Assigning to ebrevdo since he knows that part of the code best. Eugene perhaps we need all the returned tensors to carry a control dependency on the restore sparse ops here https github.com tensorflow tensorflow blob e9d2b60ed3d94eef7a3cf139cc27ec629f510681 tensorflow python training input.py L555 mrry great idea a long term fix is to use Variants here but I ll make the change you suggested. ebrevdo mrry is there any way to fix this without update tf code I use aliyun s pai running the code and it s tf version is 1.2 that i can t change it. You may be able to patch my pr into your tf installation On Tue Oct 31 2017 3 03 AM notifications github.com wrote ebrevdo https github.com ebrevdo mrry https github.com mrry is there any way to fix this without update tf code I use aliyun s pai running the code and it s tf version is 1.2 that i can t change it. You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 13999 issuecomment 340714531 or mute the thread https github.com notifications unsubscribe auth ABtim mJqZtSx5vg4w jc6FocCBOdolTks5sxvBqgaJpZM4QHqTE . Tried the fix in my app and it seems to solve the problem. 
14107,TensorFlow 1.4.0 takes more resources and is slower on GPU and CPU, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow https github.com tkuanlun350 Tensorflow SegNet OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 7 x64 TensorFlow installed from source or binary https pypi.python.org pypi tensorflow gpu 1.4.0rc1 TensorFlow version use command below 1.4.0 Python version 3.5 CUDA cuDNN version Cuda release 8.0 V8.0.60. cuDNN 6. GPU model and memory NVIDIA P4 Exact command to reproduce c python35 python3 main.py log dir . logs image dir image dir val dir validation dir batch size 15 training True Describe the problem Under 1.3.0 I was able to use a batch size of 15 put your max batch size here for training. Under 1.4.0 I get Resource Exhausted errors for that batch size. So use of GPU resources is going up. Not the right direction. For me here are the performance effects TensorFlow GPU 1.3.0 9.8 images sec for batch size 15 TensorFlow GPU 1.4.0 Can t do batch size 15. 7.8 images sec for batch size 12 Source code logs tf bug2.txt https github.com tensorflow tensorflow files 1428590 tf bug2.txt ,And it is slower than release 1.3 at least for the NMT model I am using. When I train it in 1.3 each epoch took about 600 seconds now it takes about 700 seconds. same here on linux performance drop but I tried the rc1 I ll evaluate the official release. Thank you very much for your feedback. It seems some op consumed more memory than before. If you have time can you please help figure out which op on your graph used more memory maybe by simplifying your code and finding out the bottleneck At the same time on our side we plan to add better debugging tool for gpu memory allocation and add memory regression tests. On my side I have a resnet in the same style as the examples in the official tensorflow models repository. Thanks a lot for looking into this. I have already provided the steps to reproduce the error. MNIST data works well. I have real issues with detailed performance profiling on TensorFlow so I ve stopped looking at detailed profiling on an op by op basis for now. A better profiling tool would be much appreciated. Have the same performance drop as mentioned above and a slightly increased use of memory . A batch takes 0.35 s on 1.4.0 used to take 0.25 s on 1.3. on cpu my model s peak RAM usages is larger by more than 300MB for 1.4 compared to 1.3 which is a 30 increase. Same here. Training a resnet against imagenet becomes slower by 30 with v1.4 compared to v1.3. With v1.4 I noticed GPU s starvation GPU usage is only 60 70 in average and is fluctuating a lot . should i open a separate issue for what i m seeing on CPU or should we change this bug the include it it has been a 50 increase in cpu for me and a 50 decrease in GPU. I mean utility. Due to the gpu starvation performance is therefore 50 of the tf 1.3. songgc btw I ve recently experimented https github.com yaroslavvb stuff tree master cluster imagenet64 with scripts from High Performance Models and have not observed CPU starvation speed degradation on TF 1.4 even when running on V100 GPUs which put much more pressure on CPU So the way to isolate the problem would be to see what the problematic script is doing that that tfboyd s reference scripts are not doing ie are they using probably will be deprecated Queue s to read data . Also the official resnet 50 performance has been stable since September although it doesn t rule out degradation from 1.3 which was last summer . It uses fixed image size with autotune enabled so that s another thing to try https benchmarks dot tensorflow testing.appspot.com test tf cnn benchmark resnet50 There is a performance regression that affects nmt decoders in tf 1.4 that we have solved in the nightlies. If you have a performance regression can you check if it persists with the tf nightlies hi after installing tf nightlies via pip command the problem persists. Im facing the same resource exhausted error when running predictions on validation and test sets changing batch size doesnt affect the outcome in tf 1.4 gpu. The same script worked flawlessly But slowly when i ran on the CPU tf 1.3 . The problem occured when i compiled tensorflow to run on the GPU. I have one GeForce 1080 ti 11gb. My conclusion are so far that either 1 the GPU cant offload the memory fast enough when all free memory is in use 2 or that tensorflow stores to much information without offloading 3 or that my setup is to weak for the dataset im running See details below . Training shape 430056 21 Validation shape 119042 21 Test shape 119043 21 Test 2 shape 892816 21 Hidden layer 1 shape 2000 hidden layer 2 shape 1000 Any help would be appreciated I can open a separate issue but as this discussion deals with tensorflow using to many resources which is my second conclusion i ve put it here yaroslavvb We do use queue and python threading for GPU feeding which works well with v1.3. I used queues and python threading and also StagingArea for feeding as well. And I haven t seen significant speed difference in ResNet 50 training in 1.4. The OOM issue reported by johnsrude is likely caused by fused batch norm. The documentation incorrectly states that tf.contrib.layers.batch norm with Fused None will use the default implementation. This is not true it will call the newer fused version which is more expensive in terms of gpu resources. johnsrude can you please replace batch norm layer in model.py with the following Let me know if this solves the OOM problem. bshao001 jmaye 11maxed11 eyaler songgc colmantse NicholaiStaalung please consider posting the code that reproduces the issue otherwise it is very hard to find the root cause. So i partially solved my problem by splitting the graph and run it on both the CPU and GPU with tf.device . What made most sense was running forward and backprop on the CPU while running the predictions on the GPU. Its a bit slower than running everything on the GPU but way faster than CPU only. So i guess my problem was a combination of my conclusions 1 and 2 and thus not a tensorflow issue. Problem seems to be solved by using Tensorflow 1.4.1. Might be related to this commit https github.com tensorflow tensorflow pull 15187 commits 03ef0a0c73eebabb4c6ff0faf16ecf9ab4665b3e Are you talking about the time performance issue I can reproduce the OOM problem with Tensorflow 1.4.1 and the code above. I tracked it down to fused batch norm which uses more memory because it transforms tensors internally from NHWC to NCHW. I m talking about time performance issue indeed. This was far worse with tf 1.4 as compared to 1.3. At the beginning it was starting good but then the global step sec was jittering like hell. Stopping and restarting made it stable again for a while. For information I m using tf.layers.batch normalization with fused True in my implementation without any problem. Edit it was a false hope it just lasted a bit more before seeing the performance drop i am running the code for tensorpack pix2pix on on windows 10 64 with gpu 1080ti cuda 8 cudnn 6 https github.com ppwwyyxx tensorpack blob master examples GAN Image2Image.py in tf 1.4 i am seeing an increase of 30 in runtime over 1.3 Yes. This should occur if you do not have a clean build of tf for win10 from source. I had the same prob with cpu utility goes up triple and gpu utility drops to the same extent. I decided to switch to linux and build from source much easier and now achieve full gpu utility. eyaler On a P100 I saw no performance difference between 1.3 and 1.4 also with Image2Image.py. My tensorflow was installed from pypi. It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. Some updates. I also had a similar issue like 14942 in which there were just data pipelines reading proto data and preprocessing rather than any training ops in the graph. I saw throughput degradation with v1.4.x. After I moved to v1.5rc0 with CUDA 9 CUDNN 7 all problems go away and both data pipelines and training process go back to normal and even slightly faster by 2 3 . I also noticed RAM usage reduction but no exact number . I have decided to skip v1.4. songgc question is if you should not expect better performance i.e. there may be still a problem but perhaps it is negated by cuda 9 etc eyaler I tried two cases. One was the whole training process and another was data reading and preprocessing only. I agree that the first one might be affected by different CUDA versions . However the second one used CPU only and had nothing to do with GPUs in which I saw a better throughput with v1.5. on my side 1.5 definitely solves the problem. A member of the TensorFlow organization has replied after the stat awaiting tensorflower label was applied. 
14169,Tensorflow predicts odd results with insufficient GPU memory,Hi all I am currently having a problem that when my code tries to initialize two or more predict instances in a GPU with insufficient memory instead of throwing an OOM exception the instances are initialized normally. However when I try to predict an image with these instances they produce weird results like 1.0 0.0 0.0 . Is this a bug when two or more sessions try to race for limited amount of GPU memory Please see below for my system information and the exact steps to reproduce the problem. I would very much appreciate it if you could take a look. Cheers Vincent System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution CentOS Linux release 7.2.1511 TensorFlow installed from source or binary official binaries TensorFlow version use command below 1.3.0 1.2.0 1.1.0 Python version 2.7.5 CUDA cuDNN version CUDA 8.0.61 CUDNN 6.0 GPU model and memory 1080 Ti 11172MB Exact steps to reproduce 1. Download and uncompress the file below with two scripts 2. Download the official inception v1 imagenet pretrained model from http download.tensorflow.org models inception v1 2016 08 28.tar.gz 3. Try to occupy most of the memory in the GPU to run on the exact amount of memory to occupy needs to be carefully tuned to reproduce the problem in my case I use pycuda to allocate 10720 out of the 11172MB of my 1080 Ti. 4. Run test monitor.py in two separate command prompts we should see weird results within a few minutes the scripts stop when encounter such results . reproduce code.zip https github.com tensorflow tensorflow files 1436321 reproduce code.zip ,Thanks for the instructions to reproduce. yzhwang and zheng xq have been investigating some issues with memory allocations and multiple processes and might have some more details here. Hi vincentfung13 . Thanks for reporting the issue. We have encountered similar issues internally when two or more processes are running TensorFlow with limited GPU memory. In our case a simple const tensor or random tensor creation will cause garbage values instead of any OOM error when one process has already taken most GPU memory. The short answer to the issue is that it is due to CUDA runtime silently failing to initialize with insufficient GPU memory. It will cause all the CUDA calls afterwards to produce incorrect results but not fail with errors. We currently have a workaround which is to initialize CUDA runtime before any of our large GPU memory allocation. It should go in within a week. I m also investigating the reason for the silent failure. I will provide update when I have something to share with you. Hi yzhwang zheng xq any updates on this issue Thanks. Hi vincentfung13 I ve made a commit that should solve the problem https github.com tensorflow tensorflow commit 4535bd5df4d077072a8f207146bf4cd051971237 Now each process that runs a TF program will try to initialize CUDA runtime before the giant device memory allocation and if the remaining memory is insufficient there should be an error for session creation. Please verify if this solves the problem. Thanks It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. I believe the problem has been resolved after the commit 4535bd5 closing this for now. 
14181,Tensorflow or python having memory cleanup issues when using multiple models in iterative loop, System information Have I written custom code yes OS Platform and Distribution Linux Ubuntu 17.04 TensorFlow installed from source or binary binary TensorFlow version v1.3.0 rc2 20 g0787eee 1.3.0 Python version Python 3.6.1 Anaconda 4.4.0 64 bit CUDA cuDNN version none GPU model and memory none cat etc issue Linux Bragi 4.10.0 37 generic 41 Ubuntu SMP Fri Oct 6 20 20 37 UTC 2017 x86 64 x86 64 x86 64 GNU Linux VERSION 17.04 Zesty Zapus VERSION ID 17.04 VERSION CODENAME zesty are we in docker No compiler c Ubuntu 6.3.0 12ubuntu2 6.3.0 20170406 Copyright C 2016 Free Software Foundation Inc. This is free software see the source for copying conditions. There is NO warranty not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. uname a Linux Bragi 4.10.0 37 generic 41 Ubuntu SMP Fri Oct 6 20 20 37 UTC 2017 x86 64 x86 64 x86 64 GNU Linux check pips numpy 1.12.1 numpydoc 0.6.0 protobuf 3.4.0 tensorflow 1.3.0 tensorflow tensorboard 0.1.8 check for virtualenv False tensorflow import tf.VERSION 1.3.0 tf.GIT VERSION v1.3.0 rc2 20 g0787eee tf.COMPILER VERSION v1.3.0 rc2 20 g0787eee Sanity check array 1 dtype int32 env LD LIBRARY PATH is unset DYLD LIBRARY PATH is unset nvidia smi . tf env collect.sh line 105 nvidia smi command not found Describe the problem I am working on a tensorflow model which takes pretty much RAM. It is executed iteratively to process given tasks. However with increasing time the whole process starts consuming more and more RAM although it should clean it up. This sounds like as if I d keep data of one graph over the iterations but I am almost sure that the graphs are cleanly separated. Problem I reduced the code to the following import tensorflow as tf import numpy as np reps 30 for i in range reps with tf.Graph .as default as graph with tf.Session graph graph as sess tf.constant np.random.random 1000 1000 200 1 I have 32GB RAM available working on a ubuntu 17.04 with CPU Tensorflow 1.3. This will give following error message after about the 25th or 27th iteration terminate called after throwing an instance of std bad alloc what std bad alloc Giving the process some time after each iteration results in no improvement import tensorflow as tf import numpy as np import time reps 30 for i in range reps with tf.Graph .as default as graph with tf.Session graph graph as sess tf.constant np.random.random 1000 1000 200 1 time.sleep 1 However it works if I force garbage collection invocation after each repetition import tensorflow as tf import numpy as np import gc reps 30 for i in range reps with tf.Graph .as default as graph with tf.Session graph graph as sess tf.constant np.random.random 1000 1000 200 1 gc.collect Question Now I wonder why I need to force garbage collection to run even though tensorflow should have closed the session and de referenced the graph object. Back to my original model I am not sure yet if the gc invocation actually helps. The memory usage grows pretty intense especially when I am about to persist the model to disk. Thanks for any insights. ,First of all thanks for filling in the the issue template with all the details and providing clear instructions to reproduce the problem. This is very helpful I modified your snippet a bit to reduce iteration time and dump out memory stats inline so avoid generating a new random numpy array on each iteration and I see issues even without creating a session The snippet above does show maxrss constantly increasing while if I add the gc as you suggested Then maxrss remains stable. Thanks for the report. allenlavoie has been looking at some other garbage generation and might have some thoughts. skye might also know a bit about graph construction code. allenlavoie skye Mind taking a look Thanks As long as we have reference cycles we re at the mercy of Python s garbage collection strategy. We could add some explicit gc.collect calls in places when popping the graph stack but for graph mode in TF 1.x I think that s the best we can do. graph.get operations 0 .graph is one definite reference cycle. I don t think this is something we can solve with weakref for graph mode while maintaining API compatibility we re pretty explicit about holding on to an operation being enough to execute it needs the graph and holding on to the graph being enough to execute it needs the operation . So eager mode may help where we will eventually try to avoid most reference cycles and so will hopefully not need the garbage collector much. But there are definitely reference cycles there too at the moment. It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. A member of the TensorFlow organization has replied after the stat awaiting tensorflower label was applied. Assigning to allenlavoie feel free to re assign. I don t think this bug has a reasonable solution so I m going to close. I don t want to run gc.collect manually. We make Graphs for Defuns and therefore Dataset map funcs and every time a ResourceVariable is created. It requires a sweep through all objects and could slow down graph building significantly. It is possible to allow a graph s memory to be freed https github.com tensorflow tensorflow blob 8a87518ae7074d5a0da779089e7024cd0920bba4 tensorflow python ops resource variable ops.py L77 without running the garbage collector but it s sufficiently niche that I don t think it warrants a new manual graph delete API but if someone wants to factor that into python.util as a non public API utility I m happy to review a pull request . Otherwise eager execution allows continually redefining what gets executed without creating work for the garbage collector. I ran into this issue when I tried to run a bunch of experiments. So I tried calling gc.collect manually with sess.close and printing out resource.getrusage resource.RUSAGE SELF .ru maxrss it just keeps increasing... 
14800,Potential memory leak from deleting array and closing file handler,Here are couple of minor memory leak for review. 1. https github.com tensorflow tensorflow blob 6c95675492aa8d25619f5e4ce1674582c051a7fe tensorflow c c api.cc L569 L593 delete base looks missing. 2. https github.com tensorflow tensorflow blob 6c95675492aa8d25619f5e4ce1674582c051a7fe tensorflow core lib io snappy snappy outputbuffer.cc L164 L173 delete compressed length array looks missing when macro TF RETURN IF ERROR fails. 3. https github.com tensorflow tensorflow blob 6c95675492aa8d25619f5e4ce1674582c051a7fe tensorflow core platform profile utils android armv7a cpu utils helper.cc L113 L123 Two potential problems a. There is no fclose being called after fscanf fails b. fclose could be called instead of pclose 4. https github.com tensorflow tensorflow blob 6c95675492aa8d25619f5e4ce1674582c051a7fe tensorflow tools proto text gen proto text functions.cc L132 L137 When fwrite fails fclose could be called before return 1 . PS I don t have handy working environment setup yet currently browsing code may be better fit for me.,Could you edit your post and wrap code in three backticks Markdown code highlighting please image https user images.githubusercontent.com 1595907 33139045 6ca77b38 cfac 11e7 8a29 233e0918a72d.png Or better yet paste links to the relevant lines and GitHub will insert code snippets for you https github.com tensorflow tensorflow blob 6c95675492aa8d25619f5e4ce1674582c051a7fe tensorflow c c api.cc L580 L583 carlthome thanks for the tip. Now I updated the description. orpillar I think those issues are true. In snappy outputbuffer.cc unique ptr probably could be used. Update Actually there are only 4 bytes so it could be placed into the stack instead. Would you like to create a PR for that Otherwise I could help create a PR for you. yongtang thanks for looking into the issues. You are right about snappy outputbuffer.cc. I am new to open source community just want to see there is any easy things I could contribute. Please feel free to help create a PR. Thanks orpillar Created PR 14816 for the fix. Thanks for your contribution to TensorFlow community yongtang. Thanks for the PR. It looks the sanity build had time out. orpillar bumped the build. Thanks 
15518,Possible memory leak with tf.py func with distributed Tensorflow ,When running Tensorflow as an distributed process to provide data with tf.data it gradually consumes more and more memory and finally consumes all memory of the system. Scripts to reproduce We use a dummy dataset which produce 128 28 28 1 tensors. Case1 Without distribute which works fine it will only consume 429Mb memory no matter how many batches we run. Codes in test1.py Case With distribute it will consumes more and more memory while running more and more batches. It consumes 10 Gb with less than 1M batches. Use the following two commands in two processes to run the test2.py Codes in test2.py Tensorflow v1.4.0 rc1 11 g130a514 1.4.0 OS ubuntu mate 16.04.1 Python 3.6.1 conda 4.3.30 ,I think this is an issue in tf.py func which Dataset.from generator uses internally . The following program exhibits the same memory leak Note that the memory leak is in the job dataset task 0 process which executes the tf.py func op and not the job test task 0 process which creates the session . Also note that this setup isn t intended to be supported but I suppose the note in the tf.py func docs https www.tensorflow.org api docs python tf py func could be read as allowing it The operation must run in the same address space as the Python program that calls tf.py func . If you are using distributed TensorFlow you must run a tf.train.Server in the same process as the program that calls tf.py func and you must pin the created operation to a device in that server e.g. using with tf.device . In a sense the program gets lucky because both processes call tf.py func in the same order and so the same identifier is used for the registered Python function in each process. I think the source of the leak is the decref cache which holds references to Python arrays that are passed without copying into the TensorFlow runtime and can only be cleared when the GIL is held. The ClearDecrefCache https github.com tensorflow tensorflow blob 810394550571c5feb333cb6da66afb4b20c3bd85 tensorflow python lib core ndarray tensor bridge.cc L52 function is only called in the session code and some TF Eager code for example at the beginning https github.com tensorflow tensorflow blob 810394550571c5feb333cb6da66afb4b20c3bd85 tensorflow python client tf session helper.cc L88 and end https github.com tensorflow tensorflow blob 810394550571c5feb333cb6da66afb4b20c3bd85 tensorflow python client tf session helper.cc L151 of a sess.run call. Since the cache is filling up in job dataset task 0 and it is only being cleared in job test task 0 we see a memory leak in job dataset task 0 . I ll assign this to alextp who added this mechanism and might be able to suggest a fix or workaround. It s easy to clear the decref cache while we hold the gil to call the py func so I ll add this and hopefully this bug will go away. Is tqdm necessary for this bug to happen or are you using it just for styling AFAICT it s just for styling 
16163,Dataset.from generator doesn t release memory after recreating the session, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 17.10 TensorFlow installed from source or binary binary TensorFlow version use command below 1.5.0 rc0 Python version Python 3.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Exact command to reproduce see below Describe the problem After closing the session and creating new one an iterator creates the generator instance but doesn t free the memory of the previous one. Every calling of the line session.run x see below increases memory consumption of the script 519 MiB after the first 600 MiB after the second 681 MiB after the third and so on. As you can see the delta is equal to 80 MiB N sizeof data.dtype . data.dtype is float64 here Source code logs ,Of course if session.run consumes all the elements produced by the generator it will be removed. mrry could you comment on this please Nagging Assignee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Yes this is a bug. I have a fix in preparation. Nagging Assignee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. The fix has been submitted internally but not yet merged into the Git master branch. It should be coming in the next push 
17698,Session JNI interface has memory leak,System information Have I written custom code as opposed to using a stock example script provided in TensorFlow NO OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu TensorFlow installed from source or binary source TensorFlow version use command below Master Python version 3.6 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version None GPU model and memory None Exact command to reproduce Run the snippet below. Describe the problem Read the tensorflow jni.cc code in the code section as blow. After throw exception it look like not release the status opts memory. ,Update the code in the github try to fix it. Please check it. https github.com raintung TensorflowPatch blob master v1.6 java src main native session jni.cc Nagging Assignee jart It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee jart It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Thank you for bringing this to our attention. That does appear to be a small memory leak. We d welcome a pull request fixing it. jart Can you please assign me this. I m new might need help but will submit a PR fixing this 
18540,Tensorflow leaks 1280 bytes with each session opened and closed Python API , Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes. OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 17.10 TensorFlow installed from source or binary Binary TensorFlow version use command below 1.6 also tested on 1.7 Python version 3.6 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version 9.0 7.0 GPU model and memory Titan XP 12GB Exact command to reproduce To reproduce save the following python script as memory test.py Then run it from the command line using different number of iterations python memory test.py 0 python memory test.py 1 python memory test.py 10 and so on. Describe the problem It seems that each Tensorflow session I open and close consumes 1280 bytes from the GPU memory which are not released until the python kernel is terminated. Running the script given above which simply opens and closes sessions without any further operation yields these results python memory test.py 0 yields bytes used 1280 python memory test.py 1 yields bytes used 2560 . python memory test.py 10 yields bytes used 14080 . python memory test.py 100 yields bytes used 129280 . python memory test.py 1000 yields bytes used 1281280 . The math is easy each session opened and closed leaks 1280 bytes. I tested this script on two different ubuntu 17.10 workstations with tensorflow gpu 1.6 and 1.7 and different NVIDIA GPUs. here s a related stackoverflow question https stackoverflow.com q 49735217 1500585 at least one user was able to reproduce .,Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case or leave them as N A Thanks. Bazel version Exact command to reproduce Thanks for the bug report. I was able to reproduce and diagnose. The missing memory belongs to scratch space allocated here https github.com tensorflow tensorflow blob master tensorflow core common runtime gpu gpu device.cc L292 I ll start working through a change to deallocate this memory in BaseGPUDevice but I m not yet 100 sure it s safe to do so. It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue The bug is still there trying the most recent nightly build. A fix went into the internal version on April 26. I see it in the github repository source here https github.com tensorflow tensorflow blob master tensorflow core common runtime gpu gpu device.cc L269 So I think it should be fixed at least for a build from source. 
18769,InvalidArgumentError for save restore of variables same version same OS same directory ,I get an InvalidArgumentError with no further information when I try to save and then restore parts of my model later to continue training it due to needing my laptop for class . Initialization saver tf.train.Saver embeddings embeddings weights nce weights biases nce biases Save saver.save sess model checkpoint path Load saver.restore sess model checkpoint path Clarification requested by tensorflowbutler Have I written custom code Yes I modified this code https github.com PacktPublishing TensorFlow Machine Learning Cookbook blob master Chapter 2007 doc2vec.py to work with TensorFlow 1.7 and to use the same embeddings variable for documents as for words with average instead of concatenation. I also updated the saved variables to include nce weights and nce biases so that training may be resumed. OS Platform and Distribution MacOS 10.13.4 17E199 TensorFlow installed from pip on VirtualEnv according to instructions https www.tensorflow.org install install mac TensorFlow version 1.7 Bazel version NA CUDA cuDNN version NA GPU model and memory NA Exact command to reproduce saver tf.train.Saver embeddings embeddings weights nce weights biases nce biases saver.restore sess .. trained model saved stuff ,Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case or leave them as N A Thanks. Have I written custom code OS Platform and Distribution TensorFlow installed from TensorFlow version Bazel version CUDA cuDNN version GPU model and memory Exact command to reproduce Ticket updated with requested information. Will use template in the future Possibly related to https github.com tensorflow tensorflow issues 18640 Update I tried this on Windows as well and I got rid of the stuff for loading the model graph and used the Saver to explicitly restore my variables. On Windows it fails for the same reason because I am loading an embedding larger than 2GB but with a better error message OutOfRangeError see above for traceback Read fewer bytes than requested Node save RestoreV2 RestoreV2 dtypes DT FLOAT DT FLOAT DT FLOAT DT FLOAT device job localhost replica 0 task 0 device CPU 0 arg save Const 0 0 save RestoreV2 tensor names save RestoreV2 shape and slices rohan100jain bump I am using tensorflow gpu 1.7.0 and meet exactly the same problem when restoring model in distributed tensorflow environment. when the model file is larger than 3GB the error below occurs InvalidArgumentError see above for traceback hdfs xxxx model.ckpt 5154361.data 00001 of 00008 Invalid argument Node save 1 RestoreV2 1 RestoreV2 dtypes DT FLOAT device job ps replica 0 task 0 device CPU 0 recv save 1 Const 0 S7 save 1 RestoreV2 1 tensor names save 1 RestoreV2 1 shape and slices Node save 1 restore all NoOp S10 Recv client terminated false recv device job worker replica 0 task 0 device GPU 0 send device job ps replica 0 task 0 device GPU 0 send device incarnation 1493071510865629599 tensor name edge 147 save 1 restore all NoOp tensor type DT FLOAT device job worker replica 0 task 0 device GPU 0 when I decrease the model size the error changed tensorflow.python.framework.errors impl.OutOfRangeError Read less bytes than requested Node save 1 RestoreV2 3 RestoreV2 dtypes DT FLOAT DT FLOAT device job ps replica 0 task 1 device CPU 0 recv save 1 Const 0 S1 save 1 RestoreV2 3 tensor names save 1 RestoreV2 3 shape and slices List the smaller models rw r r 3 root supergroup 11893084 2018 06 06 19 12 tmp xxx model.ckpt 446327.data 00000 of 00008 rw r r 3 root supergroup 330000440 2018 06 06 19 12 tmp xxx model.ckpt 446327.data 00001 of 00008 rw r r 3 root supergroup 106483864 2018 06 06 19 12 tmp xxx model.ckpt 446327.data 00002 of 00008 rw r r 3 root supergroup 5130000440 2018 06 06 19 13 tmp xxx model.ckpt 446327.data 00003 of 00008 rw r r 3 root supergroup 375688 2018 06 06 19 12 tmp xxx model.ckpt 446327.data 00004 of 00008 rw r r 3 root supergroup 330000440 2018 06 06 19 12 tmp xxx model.ckpt 446327.data 00005 of 00008 rw r r 3 root supergroup 232909600 2018 06 06 19 12 tmp xxx model.ckpt 446327.data 00006 of 00008 rw r r 3 root supergroup 330000440 2018 06 06 19 12 tmp xxx model.ckpt 446327.data 00007 of 00008 rw r r 3 root supergroup 1564 2018 06 06 19 13 tmp xxx model.ckpt 446327.index rw r r 3 root supergroup 571996 2018 06 06 19 13 tmp xxx model.ckpt 446327.meta Any updates rohan100jain I think the root cause may be in https github.com tensorflow tensorflow blob r1.7 tensorflow core platform hadoop hadoop file system.cc line 213 tSize r hdfs hdfsPread fs file static cast tOffset offset dst static cast tSize n the last param static cast tSize n means the number of bytes required. ie. the tensor s size. in https github.com tensorflow tensorflow blob r1.7 third party hadoop hdfs.h line 75 typedef int32 t tSize size of data for read write io ops tSize is int32 but tensor s size is int64 when the tensor is big enough overflow occurs. My solution is to use partitioned variables for large tensor then problem solved. But I m not using HDFS this is all on my local SSD. I am having a very similar issue as well. Following this tutorial https www.tensorflow.org tutorials estimators cnn but working with larger images. allenlavoie seems like we re having issues with large restore I thought the V2 version of the save restore ops was supposed to fix this. Any ideas We can put 2GB in checkpoints now. https github.com tensorflow tensorflow commit b8c86c3bbd8271ed968087f24e7fb704103bc733 diff f4340b63dcfb03e060905682f7471faa fixes an int32 size issue for string dtypes. I don t see a similar issue for Tensors and this error doesn t look like it s a length checksum issue. From OP REQUIRES failed at save restore v2 ops.cc 184 and reading the exceptions in the function it s calling it looks vaguely like this could be complaining about a dtype mismatch But that could just be the first thing that doesn t match if the whole file is corrupted for some reason. Can someone distill their issue into a snippet I can run e.g. with fill I ran the following and it seems to work Prints tf.Tensor 5e 09 shape dtype float32 We can put 2GB in checkpoints now. b8c86c3 diff f4340b63dcfb03e060905682f7471faa https github.com tensorflow tensorflow commit b8c86c3bbd8271ed968087f24e7fb704103bc733 diff f4340b63dcfb03e060905682f7471faa fixes an int32 size issue for string dtypes. I don t see a similar issue for Tensors and this error doesn t look like it s a length checksum issue. From OP REQUIRES failed at save restore v2 ops.cc 184 and reading the exceptions in the function it s calling it looks vaguely like this could be complaining about a dtype mismatch But that could just be the first thing that doesn t match if the whole file is corrupted for some reason. Can someone distill their issue into a snippet I can run e.g. with fill I ran the following and it seems to work Prints tf.Tensor 5e 09 shape dtype float32 I ve tried this snippet on Mac OS X 10.14.1 TF 1.12.0 Got such error restored io ops.restore v2 path a tf.float32 2018 11 08 17 27 05.767089 W tensorflow core framework op kernel.cc 1273 OP REQUIRES failed at save restore v2 ops.cc 184 Invalid argument Users alex HDD Develop tmp.data 00000 of 00001 Invalid argument Traceback most recent call last File stdin line 1 in module File usr local lib python3.6 site packages tensorflow python ops gen io ops.py line 1486 in restore v2 ctx ctx File usr local lib python3.6 site packages tensorflow python ops gen io ops.py line 1511 in restore v2 eager fallback attrs attrs ctx ctx name name File usr local lib python3.6 site packages tensorflow python eager execute.py line 66 in quick execute six.raise from core. status to exception e.code message None File string line 3 in raise from tensorflow.python.framework.errors impl.InvalidArgumentError Users alex HDD Develop tmp.data 00000 of 00001 Invalid argument Op RestoreV2 Interesting thank you for trying that out. So maybe it is a Mac specific issue. Just to check do you have 20GB of disk and 20GB of RAM free before running that snippet This could be a badly reported OOM or out of disk error I suppose. I have about 400Gb free space on disk and total 16Gb of RAM swapfile ON . Such issue occurs even if try to load 2.7Gb variable file. Interesting. I ll find a Mac next week and see if I can reproduce on it. Finally i found the way how to use save load embeddings. Variable partitioning is the answer. With tf.variable axis size partitioner and max shard bytes 2 30 large embeddings loading works well. The issue is still there Problem still exist when I restore a 18GB model. My OS is Windows Server 2016 Tensorflow 1.13.0 installed from pip 512GB memory and hard disk Only run on CPU. Please advise. I also put the below in my code tf.variable axis size partitioner max shard bytes 2 35 axis 0 bytes per string element 16 max shards None However it still shows below error when restoring the model 2019 03 16 08 12 32.430456 W tensorflow core framework op kernel.cc 1401 OP REQUIRES failed at save restore v2 ops.cc 184 Out of range Read fewer bytes than requested It has been 17 days with no activity and the awaiting response label was assigned. Is this still an issue Yes it is still an issue It has been 29 days with no activity and the awaiting response label was assigned. Is this still an issue Yes it s still an issue Finally got a chance to debug this on a mac. Apparently the pread system call despite taking an eight byte size t for its nbytes argument returns EINVAL if The sum of the iov len values in the iov array overflowed a 32 bit integer. And presumably pread is implemented in terms of readv so they have the same limitation. I have a change out for review which just limits reads to INT32 MAX on every platform. Seems to work if we do that. I checked that the checkpoints themselves were identical to what gets written on Linux so existing checkpoints will start working once that change is in. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 18769 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 18769 No a Uh this is still an issue I built the latest version of tensorflow from source and it s still happening I ve tried practically everything. One of the most arcane and infuriating errors I have dealt with... Sam DevZ operating system and version TF git version repro instructions The repro I posted https github.com tensorflow tensorflow issues 18769 issuecomment 430030868 works for me on 1.14.1 dev20190522 nightly and macOS 10.14.2. Hi Your snippet doesn t work either. Code Output allenlavoie Oh er I m guessing version 0.12.2 is not recent enough. I need 1.14.1 dev20190522 right Haha. 
19499,tf.data.Dataset iterators are not cleaned when the loop ends with a break, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 CentOS7 TensorFlow installed from source or binary binary TensorFlow version use command below 1.8.0 Python version 2.7.5 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Exact command to reproduce Describe the problem tf.data.Dataset iterators are not cleaned when the loop ends with a break . The code below opens one file per epoch. This eventually hits a system limit maximum number of open files . Replacing the break by a continue works better since the files are closed. However this is inefficient if we only need to iterate over a small fraction of the data Source code logs , allenlavoie It looks like you ve done something related to Eager resource garbage collection in 309e340619ab922f1ecb51b8f142283e09bda07d and this is still used in the current iterator ops.EagerIterator https github.com tensorflow tensorflow blob 2c9d129cb8f6e3f6a27d074664033e4fccd83a63 tensorflow python data ops iterator ops.py L480 L482 Can you please take a look and see if there s a reference leak here There is a reference leak but I don t think it s in Python. DestroyResourceOp gets run and Unrefs the resource but it looks like IteratorHandleOp is keeping a reference to the resource in its OpKernel https github.com tensorflow tensorflow blob f8b74d642420dcf2f5cab763b41884a05777ea45 tensorflow core kernels data iterator ops.cc L510 . AFAIK kernels are never deleted when executing eagerly they just sit around in the kernel cache. I ve verified that removing the reference from IteratorHandleOp fixes the files not closed issue they get closed when DestroyResourceOp runs . I can think of horrible hacks to get this to happen only when executing eagerly but maybe we should discuss tomorrow. Ugh yes whatever we do to that kernel implementation the current version will still leak the OpKernel object and related guff for each iterator. As a strawman we could solve it with i a new version of IteratorHandleOp that creates the handle doesn t retain a resource and ii some API for creating and running but not caching a kernel in eager mode. CCing asimshankar since kernel lifetimes in eager is something we ve talked about in the past. LionelCons In the meantime here s a workaround that should alleviate the file handle leak 
19837,freeze graph for inference memory leak frozen graph size 1.3GB takes 15GB memory when inference, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes details below OS Platform and Distribution e.g. Linux Ubuntu 16.04 macOS High Sierra 10.13.3 TensorFlow installed from source or binary binary via pip install TensorFlow version use command below 1.8.0 Python version 3.6 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Exact command to reproduce Describe the problem I want to train a graph save it then load it later for inference only purpose. The way I m doing it now During training I save the model with tf.graph util.convert variables to constants method and tf.train.write graph which produces a .pb file. Its size is around 1.3 GB. Then I load the frozen graph and do some inference with some toy data. This causes memory issue by a factor of 10 . In activity monitor Python takes 15GB on the first inference I ve also tried the saving loading inference procedure with saver.save saver.restore and it doesn t cause the memory issue. However for production purpose on my end loading a frozen graph would be strongly preferred to the Source code logs Saving graph for debugging purpose I SKIPPED training and save the graph once variables are initialized Load graph Try inference on some toy data Here in inference it goes through a loop with 10 iterations. In the first iteration the sess.run .. causes the memory leak issue 15 GB . In following iterations memory falls back to what I would expect with the size of the model loaded. Some details The issue persists whether I save the frozen graph on CPU machine macOS High Sierra 10.13.3 or a remote GPU machine Ubuntu 16.04.3 LTS . Again the graph is frozen once it s initialized. I skipped all training testing logic for debugging purpose. For freeze graph I also tried freeze graph in bazel bin in terminal. It still causes memory issue when loading and doing inference. I ve tried optimize for inference graph def ... in the graph loading section. It doesn t solve the issue. ,I have the same issue Freezing and optimizing does not help waiting Nagging Assignee jart It has been 46 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Hello Camuslu There should be some posts on Stack Overflow that address the memory problems the freeze graph and load model .pb file Stack Overflow http stackoverflow.com questions tagged tensorflow Automatically closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks 
20275, tf.profiler.profile outputs negative number of flops, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 N A TensorFlow installed from source or binary binary TensorFlow version use command below 1.8.0 Python version 3.6.5 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Exact command to reproduce See below Describe the problem tf.profiler.profile outputs negative number of flops for tf.nn.conv2d ops on large inputs because of np.int32 overflow. Source code logs The reason is https github.com tensorflow tensorflow blob cfebbbc94f3edd1622a9a42379dd2ccc956ea52c tensorflow python ops nn ops.py L2169 here np.prod is used which returns an np.int32 . This enforces the next product line https github.com tensorflow tensorflow blob cfebbbc94f3edd1622a9a42379dd2ccc956ea52c tensorflow python ops nn ops.py L2172 to compute everything as np.int32 . Actually the protobuf supports int64 https github.com tensorflow tensorflow blob cf375f06747b7be998f0df329772390f545577a1 tensorflow core profiler tfprof output.proto L105 so this line could be replaced by There are more ops affected by this issue as they also use np.prod . PS Sorry that I cannot submit the fix straight away everything needed to fix this issue is mentioned above.,Thanks for the clear problem diagnosis. I ll push a fix internally. Nagging Assignee poxvoculi It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee poxvoculi It has been 29 days with no activity and this issue has an assignee. Please update the label and or status accordingly. 
20436,Memory leaking in tf.data.Dataset in eager mode, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04 TensorFlow installed from source or binary Binary TensorFlow version use command below 1.8.0 Python version 3.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version CUDA 9.0 GPU model and memory Exact command to reproduce Describe the problem tf.data.Dataset doesn t seem to free its own memory usage even it is out of scope in eager execution mode. The memory I m mentioning is the main memory not GPU s memory. Source code logs The following codes are run in Jupyterlab. Block 1 Block 2 I repeat Block 2 many times and the memory usage grows each time. Note gc.collect doesn t have any effect., allenlavoie Are you familiar with this Probably the same issue as https github.com tensorflow tensorflow issues 19499. Worth trying on a more recent version that includes that fix https github.com tensorflow tensorflow commit 70674b950ab48f913ed1c99e48c4162287595d46 . Not going to have time to look into this before going on vacation feel free to reassign Nagging Assignee allenlavoie It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Closing for now. Feel free to re open if it s still an issue when synced after https github.com tensorflow tensorflow commit 70674b950ab48f913ed1c99e48c4162287595d46 FWIW I tried in a nightly and memory usage does fluctuate a bit which I assume is due to buffering but it doesn t seem to grow over a few minutes 
21081,Tensort RT Engine Object Detection,tensorflow.python.framework.errors impl.ResourceExhaustedError Requested batch size is not available and engine cache is full Node import my trt op 16 TRTEngineOp InT DT FLOAT DT FLOAT DT FLOAT DT FLOAT OutT DT FLOAT cached engine batches 1 calibration data fixed input size true input shapes 1 1 1 1 max cached engines count 1 output shapes 4 precision mode FP32 segment funcdef name my trt op 16 native segment serialized segment 2007 000 ...00 000 000 static engine true workspace size bytes 65075264 device job localhost replica 0 task 0 device GPU 0 import MultiClassNonMaxSuppression ClipToWindow split import MultiClassNonMaxSuppression ClipToWindow split 2 import MultiClassNonMaxSuppression ClipToWindow split 1 import MultiClassNonMaxSuppression ClipToWindow split 3 Hint If you want to see a list of allocated tensors when OOM happens add report tensor allocations upon oom to RunOptions for current allocation info. Node import SecondStagePostprocessor BatchMultiClassNonMaxSuppression MultiClassNonMaxSuppression SortByField strided slice 89 Recv client terminated false recv device job localhost replica 0 task 0 device CPU 0 send device job localhost replica 0 task 0 device GPU 0 send device incarnation 1 tensor name edge 1900 ...ided slice tensor type DT INT32 device job localhost replica 0 task 0 device CPU 0 Hint If you want to see a list of allocated tensors when OOM happens add report tensor allocations upon oom to RunOptions for current allocation info.,Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case or leave them as N A Thanks. Have I written custom code OS Platform and Distribution TensorFlow installed from TensorFlow version Bazel version CUDA cuDNN version GPU model and memory Exact command to reproduce Mobile device samikama not clear whether this is a bug or working as intended. Would you please take a look It looks like snownus created the engine with a batch size smaller than the execution time. Either a bigger batch size should be used or dynamic op should be generated. samikama in faster rcnn the second stage will have one more dimension. It seems tensorflow tensorrt see another dimension as batch size and in fact it isn t. Faster RCNN at the second stage have 300 boxes. I have same error. If no use tensorflow.contrib.tensorrt.create inference graph then no error. ResourceExhaustedError see above for traceback Requested batch size is not available and engine cache is full Node model 1 prior layer 1 my trt op 11 TRTEngineOp InT DT FLOAT OutT DT FLOAT cached engine batches 10 calibration data fixed input size true input shapes 2 max cached engines count 1 output shapes 2 precision mode FP32 segment funcdef name model 1 prior layer 1 my trt op 11 native segment serialized segment 270 020 0...00 000 000 static engine true workspace size bytes 18292682 device job localhost replica 0 task 0 device GPU 0 model 1 prior layer 1 ToFloat Hint If you want to see a list of allocated tensors when OOM happens add report tensor allocations upon oom to RunOptions for current allocation info. Node while Switch 2 85 Recv client terminated false recv device job localhost replica 0 task 0 device CPU 0 send device job localhost replica 0 task 0 device GPU 0 send device incarnation 1 tensor name edge 2429 while Switch 2 tensor type DT FLOAT device job localhost replica 0 task 0 device CPU 0 cloopwhile strided slice 5 stack 1 Hint If you want to see a list of allocated tensors when OOM happens add report tensor allocations upon oom to RunOptions for current allocation info. System ubuntu 16.04 tensorflow v1.10.1 from source cuda 9.0 cudnn 7.1 tensorrt 4.1.2 python 3.5 use create inference graph with maximum cached engines 3 no error snownus In your case it is a known issue of integration. Since TF doesn t have batch dimension concept and TRT requires a batch dimension our assumption of Rank0 being the batch dimension is failing in FasterRCNN. It is being worked on. There will be an NMS op implementation soon which should resolve this. trevor m pooyadavoodi Can you PTAL It has been 32 days with no activity and the awaiting response label was assigned. Is this still an issue Our somewhat recent improvements to the engine cache fixes this issue. It is available in the master branch of tensorflow tf nightly gpu . I think it was resolved. I am closing the issue. If you think I made a mistake please open another issue. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 21081 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 21081 No a 
22987,Segmentation Fault SIGSEGV in middle of Training due to Runtime statistics calculation ops., System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Custom code. The full code can be accessed here at GitHub https github.com grasseau HAhRD tree master GSOC18 OS Platform and Distribution e.g. Linux Ubuntu 16.04 CentOS Linux release 7.5.1804 Core Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary Binary TensorFlow version use command below v1.8.0 0 g93bc2e2072 1.8.0 Python version Python 2.7.5 default Apr 11 2018 07 36 10 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version cuda 9.0 cudnn 7 GPU model and memory 2 Tesla V100 Nvidia GPUs 15 Gb each Exact command to reproduce Please go through this instruction here https github.com BigBang0072 HAhRD wiki GSOC 18 Work Summary to run the code. The training runs without error on small dataset. The problem arises when number of minibatches are more per epoch keeping the batch size constant. i.e more number of sess.run call per epoch Describe the problem The training crashes with the error Segmentation Fault Core Dumped in the middle of the training after around 14 18 epochs with 800 minibatches in each even though I have sufficient RAM 12 20 utilization holding steadily before crash . screenshot from 2018 10 14 16 46 38 https user images.githubusercontent.com 17550410 46916113 c4b44e00 cfd3 11e8 842b 686a42b2c486.png Possible Memory Leak Checked Initially I suspected a memory leak due to the addition of new ops after each iteration so I finalized the graph in the training session. But this was not the source of the problem. No new nodes were added at each iteration. Attempt 2 Locating the problem After that I ran the training on gdb see the stack trace in logs section and along with some commenting of the code I have almost pinpointed the source of error. Currently every 30 minibatch training I am saving the summary here in the code. The full code can be found here https github.com grasseau HAhRD blob master GSOC18 train multi gpu.py L406 screenshot from 2018 10 14 17 13 59 https user images.githubusercontent.com 17550410 46916189 c29ebf00 cfd4 11e8 9a94 08f67c12198d.png The train track op in line number 415 is a list of which looks like this gradient update op loss GPU1 loss GPU2 merged summary op If I comment out the section in lines 406 438 the training runs without error. Attempt 3 Exact Location Now I comment out line 422 to 438 the part where I save timeline and summary . I have checked there is no problem due to these lines. Now if I run merged summary op along with rest of the training op and comment out line 418 and 419 i.e removing the run options and run metadata the training goes without error . And if I leave these two lines uncommented the Segfault comes back. screenshot from 2018 10 15 17 41 41 https user images.githubusercontent.com 17550410 46951781 93588280 d0a6 11e8 8173 f52dfce1c3c6.png So It seems some memory is being leaked when calculating the runtime statistics using run options and run metadata when doing the full trace of the graph. Source code logs Stack Trace Program received signal SIGSEGV Segmentation fault. Switching to Thread 0x7ff2d3fff700 LWP 138819 0x00007ff2b8b00d6f in from usr local cuda 9.0 extras CUPTI lib64 libcupti.so.9.0 gdb backtrace 0 0x00007ff2b8b00d6f in from usr local cuda 9.0 extras CUPTI lib64 libcupti.so.9.0 1 0x00007ff2b8b04fd0 in from usr local cuda 9.0 extras CUPTI lib64 libcupti.so.9.0 2 0x00007ff2b8d10c39 in from usr local cuda 9.0 extras CUPTI lib64 libcupti.so.9.0 3 0x00007ffff77fae25 in start thread from lib64 libpthread.so.0 4 0x00007ffff6e1bbad in clone from lib64 libc.so.6 ,CC nluehr This looks like an issue with CUPTI. Nathan could you help us triage this issue Thanks in advance azaks2 The bug is not related to CUPTI. The problem is that ExecutorState caches a raw pointer to the TraceCollector here https github.com tensorflow tensorflow blob master tensorflow core common runtime executor.cc L1369 . The cached copy then outlives the owning unique ptr created in DirectSession RunInternal https github.com tensorflow tensorflow blob master tensorflow core common runtime direct session.cc L550 . Either a shared ptr should be used if multiple TraceCollectors can coexist or a mutex is needed to keep direction session from destroying the TraceCollector while it is in use by ExecutorState. I m having the exact same problem with run options and run metadata on tensorflow gpu 1.12.0 pip CUDA 9.0 cuDNN 7.1 CentOS Linux release 7.4.1708 Python 3.6.2 . nluehr thanks for debugging this. I see your point that things are horribly broken when there are concurrent RunInternal calls. However with one RunInternal call things should work since RunInternal waits for all executors to finish. BigBang0072 do you have concurrent session runs Not sure I see synchronization of runners in RunInternal. In TF 1.12 I add two prints one of traceing TraceCollector this in DeviceTracerImpl after https github.com tensorflow tensorflow blob v1.12.0 tensorflow core platform default device tracer.cc L400 and the second of trace collector after https github.com tensorflow tensorflow blob v1.12.0 tensorflow core common runtime executor.cc L1578. Then I run train multi gpu.py provided above and the following output is generated before triggering a SEGFAULT at https github.com grasseau HAhRD blob master GSOC18 train multi gpu.py L443 L447. So the destructor seems to be called before the executor has completed. azaks2 No I just initiate one session while training. You can see this here https github.com grasseau HAhRD blob 7eacc002672da28a1f53015227497469eb2af69d GSOC18 train multi gpu.py L368 url I was able to reproduce the problem and have a fix. The problem was the data pipeline running concurrently with the training steps. Thanks for all your time. But we have to run the data pipeline concurrently with the training. Otherwise it will critically slow down our performance. Also how is data pipeline affecting the tracing of the tensorflow graph Apart from that if I just remove the tracing I don t get any Segmentation Fault even with the concurrent data pipeline running. Maybe I am missing something. Could you please elaborate on what you have found Thanks again for your help and time. I just fixed the lifetime of internal profiling related objects so they do not get GCed at the end of session.run I m encountering a really similar issue with multi gpu replicate model fn training and a parallel data pipeline. Several hours into the training loop and with no warning error message we get a segmentation fault from tf and the training halts. Is this a known issue and if so is it solved in a newer release Dont log metadata for now or log it very infrequently mostly when you want to debug . I dont know about the newer release but this will temporarily fix the problem. 
23733,Memory leak in tf.train.Example tf.train.Features tf.gfile, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow no OS Platform and Distribution e.g. Linux Ubuntu 16.04 Archlinux TensorFlow installed from source or binary repository TensorFlow version use command below 1.12 Python version 3.7 CUDA cuDNN version cuda 10 cudnn 7 GPU model and memory nvidia 1080ti Describe the current behavior The system goes OOM and I have 64 GiB of memory. Describe the expected behavior I should be able to create a dataset converting files I read from Google Cloud to local tfrecords. I m debugging the memory usage of python and as you can see below the usage is around 670K hence the memory allocation should be outside of python and the only library I m using is Tensorflow. Code to reproduce the issue Just use the gs2tfrecord function. I m using it to fetch data from Google Cloud Storage but since I m using tf.gfile a local path can be used too. My guess is that leak can be somewhere in tf.train.Example or in tf.train.Feature since those are the only 2 functions I call in a loop. UPDATE I ve created another function that just download the images from Google Cloud and store them locally. It goes out of memory in the same way. Maybe the leak is in tf.gfile Here s the function Other info logs This is the output of a display top invocation ,Update probably the leak is in tf.gfile since I can see that the memory grows every time I run the line In the loop. I ve also tried to change it to But every time I read the memory is never released Another update the problem is related to the tensorflow package I m using that s the one shipped by the Archlinux devs. If I create a virtualenv with python3.6 and I use the tensorflow package installed via pip it works correctly. galeone Thanks for all the details. I wanted to confirm the state after all your digging and investigation. Is it accurate to say that there is no problem observed in the official TensorFlow release binaries and you re only seeing this in a package built by the Archlinux folks asimshankar Yes I confirm. If I create a virtualenv with everything official inside there is no leak. However compiling tensorflow is straightforward hence probably is something is worth investigating since in future Tensorflow could be used with CUDA 10 and python 3.7 and those are the only differences between the official and the Archlinux version Update the same happen with Tensorflow installed from conda On Tue Nov 20 2018 07 57 Asim Shankar notifications github.com wrote galeone https github.com galeone Thanks for all the details. I wanted to confirm the state after all your digging and investigation. Is it accurate to say that there is no problem observed in the official TensorFlow release binaries and you re only seeing this in a package built by the Archlinux folks You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 23733 issuecomment 440164185 or mute the thread https github.com notifications unsubscribe auth AICZDHYP41NCNOScE9tk9eetFP2xRzqCks5uw6fRgaJpZM4YdiX7 . galeone With conda still Python 3.7 Just trying to understand the environment No with Python 3.6.7 and tensorflow gpu 1.12 installed using conda into the conda environement. Here s the detail cuda toolkit 9.2 cudnn 7.2.1 conda version 4.3.27 cupti 9.2.148 cuda version 9.0.176 The same problem exists on the official Cloud Deep Learning VM Image TF 1.12 CUDA 10 . It runs out of memory within a few minutes when training data on gs . Runs perfectly fine when the bucket is mounted to the local fs. Edit this bug isn t reproducible for some reason I m keeping it here so the rest of the thread still makes sense but feel free to skip ahead. Is there any update on this I m running out of memory with a simple of course in practice I iterate and fetch various elements e rather than serialize the same example over and over but this has nothing to do with the memory leak Observed in Ubuntu 16 both tensorflow 1.12 and 1.13 Thanks for the simple example. I ll take a look today. I tried to look into this problem today. urimend I cannot reproduce the memory leaks from your sample code using either tensorflow 1.12 installed from python3 virtualenv or anaconda. Can you please provide more details about this issue Does the size of e video matter I tried setting e video to numpy.zeros 10 5 . galeone I cannot reproduce the memory leaks you reported in the following code with tensorflow 1.12 with ananconda. I have some trouble making tensorflow gpu run in my machine. Can you please check to see if you can reproduce the memory leak using tensorflow instead of tensorflow gpu jing dong you meant urimerhav For me it is reproduced for almost every scenario. Just try to read a few thousands of files directly from gs using tf.gfile.GFile with the with statement. You will see a memory leak right away. For me the important part was is that it should be tf 1.12 built for CUDA 10. It did not repro for a CPU build installed using pip. This problem exists on the official Cloud Deep Learning VM Image TF 1.12 CUDA 10 . I will try to repro it on the official v1.13.1 to see if it is still an issue. jing dong I m sorry. I relaunched my aws instance from the same image and now I can t reproduce my error. I guess I m retracting my bug report since I m not able to reproduce For me it also didn t repro today with the official tensorflow gpu 1.13.1 Update So this problem is reproducible for me with ubuntu 16 python 2.7 and tflow both cpu and gpu 1.13.1. The only thing I can add that might be going on here is that I m fetching the data from an s3 path though I really can t imagine how that would effect anything. The problem is still present in the 2 different environments I reported python 3.7 cuda 10 archlinux build and python 3.6 tensorflow gpu 1.12 conda env . When I read a file using GFile from Google Cloud storage there s a memory leak that saturates all the system memory. It happens both when using context manager or when manually opening and closing the files I thought the leak could be caused by a wrong implementation of the context manager that doesn t close the file when exiting the context but it is not the case . galeone do you happen to know of a version with python 2.7 where the memory leak doesn t occur urimerhav no never used python 2.x with Tensorflow never tried urimerhav Can you please confirm that the memory leaks only occur when you try to read from s3 path by trying to read from local FS or gcs path gs if that is available to you If the memory leak only happens when reading the s3 paths it may be because the s3 file reader in TF contains a leak. galeone Would it be easy for you to try to experiment with tensorflow without gpu to see if you can reproduce the leaks there As I said earlier I have trouble running tensorflow gpu in my environment. Also just to confirm the code that produces the memory leak for you is the following right Any update on this bug We re seeing some sort of memory leak after moving from TF 1.1.9 to TF 1.1.3 on Python 2.7 I confirm that the bug still exists in TF 1.13. Env the latest NVIDIA TF 1.13 container running on Google Gloud python2 nvcr.io nvidia tensorflow 19.03 py2 ygoncharov quirkyllama can you confirm that python3 doesn t have that memory leak urimerhav with this setup https github.com tensorflow tensorflow issues 23733 issuecomment 444046925 the leak is still present and is Python 3 Thanks galeone I m willing to use just about any TF distribution that will rid me of this bug so far it seems like the only report on a working setup is 1.1.9 on python 2 Reading the comments looks like yes 1.1.9 py2 works but you can try with some new tensorflow release that support Python 2 to see if the leak is still there or is gone galeone ygoncharov urimend Can you please describe a python environment and a minimum code snippet that reproduces this memory leak I will try to see if I can reproduce this in a GCloud VM and go from there. Since I have not heard back regarding this issue I will close it for now. Please feel free to reopen the issue with a setup to reproduce the problem. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 23733 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 23733 No a jing dong this thread contains the exact environment see my post the latest NVIDIA TF 1.13 container running on Google Gloud docker image nvcr.io nvidia tensorflow 19.03 py2 and the code see urimerhav post to reproduce the issue. For easier repro please try the following 1. Env nvcr.io nvidia tensorflow 19.03 py2 running on GC v100 2. Create a GS bucket with 100k files 3. Read all these files one by one using tf.gfile.Open from gs and write them to the local FS your program will hit OOM This is a critical issue that appears across a few TF release builds in many environment not sure why it was closed. 
23904,New instances of iterator.make initializer do not release previously allocated memory, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device NA TensorFlow installed from source or binary binary TensorFlow version use command below v1.12.0 0 ga6d8ffae09 1.12.0 Python version 3.5.2 Bazel version if compiling from source NA GCC Compiler version if compiling from source NA CUDA cuDNN version 9.0.176 7.3.0.29 GPU model and memory GeForce GTX 1080 Ti Describe the current behavior Consider the following setup see the code below 1. Create a dataset using tf.data.Dataset.from tensor slices 2. Use it to initialize a tf.data.Iterator.from structure iterator 3. Repeat the previous step several times in a single session Observed behavior memory consumption grows linearly with number of iterations Describe the expected behavior Memory consumption should be bounded for any number of iterations. Code to reproduce the issue Output , artsobolev I think the root cause here is lower level than the Iterator.make initializer implementation. For example the following simpler program exhibits similar growth ...and gives me this output There are two issues that conspire to create the problem 1. Each time we call iterator.make initializer dataset or tf.add in the loop some new ops are created. These new ops aren t found in the session s cache of previously executed subgraphs so it reoptimizes the computation and stores a new optimized subgraph inside the session. This new subgraph accounts for the memory growth. 2. Every time we use the same large constant in a different subgraph a copy of that constant s data is made and stored in the subgraph. That accounts for why the memory growth is so severe in this case because the raw data tensor is approximately 400MB in size. Issue 1 is pretty fundamental and with the shift to eager execution and away from sessions in TF 2.0 it s unlikely to be fixed for existing code new eager code won t have this problem since there s no graph that will accumulate this state . Issue 2 is potentially fixable but will require some internal changes to the Session code to intern the values of large constants. I m assigning this to asimshankar to make the call on whether the latter is worth doing. Thanks for the report artsobolev As mrry pointed out here we are adding operations to the graph in each iteration of the loop which is causing memory growth. Interning the constants as mrry suggested would help significantly in this case due to the large constants but we ll still be constantly adding nodes to the graph so there will still be an increase in memory footprint. For the example in https github.com tensorflow tensorflow issues 23904 issue 383237718 I think what you want is to create the initializer operation outside the loop so That way there is no growth in the graph. I m tempted to close this out since adding nodes to a graph is bound to increase memory. With TF 2.0 it won t be easy to run into this situation. Interning constants will help and is something worth looking into in general but not specific to this issue. Please feel free to reopen if I have misunderstood. 
23924,Memory leak when using tf.contrib.data.unbatch , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below v1.12.0 0 ga6d8ffae09 1.12.0 Python version Python 3.6.7 Anaconda Inc. Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 9.0 GPU model and memory 1080ti Describe the current behavior Memory usage continuously increase when using tf.contrib.data.unbatch . Describe the expected behavior Memory usage should not increase. Code to reproduce the issue Memory usage will increase with use unbatch while with nouse unbatch memory usage does not increase. Other info logs It seems like input Unref call is missing in UnbatchDatasetOp https github.com tensorflow tensorflow blob master tensorflow core kernels data unbatch dataset op.cc L42 .,Possibly related to 23904 Thanks for tracking down the issue indeed the missing input Unref seems to be the culprit. artsobolev I think the root cause for this bug is different. I ll comment on the other thread. 
24196,Possibly buffer overflow in tf.nn.conv2d on GPU, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux TensorFlow installed from source or binary binary TensorFlow version use command below 1.12 Python version 3.5.3 CUDA cuDNN version 9.0 7.2.1 GPU model and memory Tesla M60 also tested on GeForce GTX TITAN Black Describe the current behavior I ve created a tensor set to zero except a single entry at 1 1 0 that was set to a large number 1e10. I ve then convolved the tensor with a random kernel. When the batch size is small e.g. 5 then the output at 0 0 is zero because the input array is zero there but when the batch size is large e.g. 100 then these entries contain numbers significantly larger than zero. This only happens on the GPU. Describe the expected behavior The expected behavior should be zero regardless if running on CPU or GPU because windows at these locations contain only zero entries. Because the only non zero entry is outside the window used to compute the value then this is most likely a buffer overflow or some other memory access issue . Code to reproduce the issue If the following does not immediately produces the bug then try increasing the batch size from 100 to something greater. ,Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 24196 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 24196 No a Closing out this issue because it hasn t been updated in the last year. Please reopen if this is still relevant. This is still an issue both on latest TF1 and TF2. See attached Colab notebooks demonstrating the issue TF1 https colab.research.google.com drive 1eomQKlLSBvZsyDLU3EM Bv74imxT RwU usp sharing TF2 https colab.research.google.com drive 1mlHwozUWUyMv4ShGlhIYMzkxqtLrGcny usp sharing 
24221, tf.estimator.Estimator methods .evaluate .train calls deplete CPU RAM, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow True OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device Null TensorFlow installed from source or binary pip install tensorflow gpu TensorFlow version use command below v1.10.0 0 g656e7a2b34 1.10.0 Python version 3.6.7 Bazel version if compiling from source Null GCC Compiler version if compiling from source Null CUDA cuDNN version cudNN 7.4.1 GPU model and memory NVIDIA GTX 1070 8GB Describe the current behavior While using a tf.estimator.Estimator class each separate call of .train .evaluate results on extra CPU RAM permanent usage. In my use case every time I call .evaluate I get another 200MB RAM allocated on my CPU even though I m training on the GPU RAM which then never gets free. This ultimately results in MemoryError since after a few training validation rounds the RAM gets full. Describe the expected behavior Memory used by these methods should be released when done. Code to reproduce the issue To figure out why I get MemoryError s I have used the Python interactive mode and called .evaluate ... and .train ... while observing RAM usage after each call another 200MB RAM got occupied on the CPU RAM eventually depleting all available RAM.,Apologies for the delay in response. Can you please build against latest version of TF and test again. Note that TF 1.13.0 rc0 comes pre built cuda 10 binaries thus you may have to upgrade your cuda version. Thanks Automatically closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks I ll try to run the code again and monitor its RAM when I ll have time bit busy 
25614,Segmentation fault Convolution in tflite, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution Linux Ubuntu 16.04 TensorFlow installed from github TensorFlow version tf 1.13 Python version 3.6 Bazel version if compiling from source 0.21.0 GCC Compiler version if compiling from source 5.4.0 20160609 CUDA 9.0 RAM 32GB Hi I encountered a segmentation fault in the Im2Col of Convolution while running a tflite model. I observed that when the Im2Col buffer size 2GB this crash occurs. For example input 1x828x1000x32 output 1x828x1000x3 kernel 9x9 does not crash but for input 1x832x100x32 output 1x832x1000x3 kernel 9x9 convolution crashes. I want to know if there is any memory limit for convolution in tflite to run I kindly request you to let me know this. sooner the better Awaiting you reply Vedavyas. , ved27 Could you share some reproducible code and also please mention what is your OS. Please provide as many details as possible to resolve this faster. Thanks Hi I m unable to share the exact code. However I am able to reproduce the issue setting the above input shapes and kernel sizes in the testing code of quantized convolution in tflite. Awaiting your reply Hi I kindly request you to look into this soon as it would be helpful for me to rule out the models which seem to take high memory while executing tflite interpreter. Awaiting your reply Hi I kindly request you to address this issue. It has almost been 9 days. Thankful if someone can give me a confirmation on the memory limitation of tflite Regards ved27 Did you encounter this issue while running some model If yes would you be able to share the model Just for my reference . Any way i have found the root cause for your issue. NOTE As of now TFLite does not have any memory limit for convolution. I have verified with 3GB memory allocation there is no issue. But in your case the root cause is classic Integer overflow . Currently GEMM supports limited by row col 2 31 . As in your case it has exceeded the limit it is leading to memory corruption. Currently i am working on a solution for this will keep you updated if anything come up. Would suggest to use models which has lower memory requirement lower than 2 31 for Convolution currently. If my post helped you would appreciate if you close this issue. Please feel free to post if anything from your end Thanks. Yeah. I debugged and found the root cause earlier. There has been inconsistency between the datatype usage in various parts of tflite . int size t unsigned int . And ultimately the GEMM s limitation causing the crash to occur. I wished a better implementation of convolution in tflite. Implementations other than Im2Col such as Kn2row can be deployed . Thank you much for the reply Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 25614 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 25614 No a 
26108,2.0 Compile is leaking memory., System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 2 different machines TensorFlow installed from source or binary Binary TensorFlow version use command below gpu 2.0.0 dev20190214 gpu 2.0.0 dev20190224 Python version 3.6.6 CUDA cuDNN version 10 GPU model and memory 7gb K5200 4gb GTX 970 Describe the current behavior Compile leaks memory. Describe the expected behavior Compile clears the memory when overwritten Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Memory slowly increases about 20mb at a time. I discovered this whilst working on a genetic algorithm which generates models which are then compiled and tested but it fills my memory and I have narrowed it down to this simple code. ,Changing the parameters of compile seems to makes no difference. And the size of the network also doesn t seem to make a difference. Me too... Hope this can get solved soon for now I have to use an old version Same issue... And K.clear session doesn t work. Worthy7 Can you try this gist https colab.sandbox.google.com gist jvishnuvardhan 6cf3a9ee853a805963e461df92e224bf oom tf26108.ipynb and let me know if the issue still persists. I don t see any issue if I add tf.keras.backend.clear session to the end of your code. That seems to work in that notebook on the nightly build. In my original 2.0a the clear session doesn t fix it. So I assume you fixed a bug somewhere in clear session which seems to work. I think that clear session shouldn t be necessary really... don t you I feed there should be no residue from a previous compile when running another compile on the same model. I m not able to replicate your 20 mb figure I see an increase of just under 0.5 mb compile https colab.sandbox.google.com gist robieta 9820655313efc01f15bd864110e98a45 sequential leak test.ipynb Given that compile does add ops to the graph gradient ops metric variables etc. I think this probably reasonable since compile is an infrequent call. And yes there was a bug with clear session in the 2.0 alpha. It should be fixed in the beta. See https github.com tensorflow tensorflow issues 28844 for an explanation of why there is some unnecessarily persisted state for the time being. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 26108 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 26108 No a robieta I said at a time I didn t mean per compile I was just watching the task manager window probably per second. Thanks for sorting it That seems to work in that notebook on the nightly build. In my original 2.0a the clear session doesn t fix it. So I assume you fixed a bug somewhere in clear session which seems to work. I think that clear session shouldn t be necessary really... don t you I feed there should be no residue from a previous compile when running another compile on the same model. I agree with you Worthy7 . A similar situation if I train the model in the main thread and load the model in another thread AT THE SAME TIME . All things is in a loop. However if I use clear session method in one thread the code in another thread won t work I test pytorch and mxnet and there is no any memory leak in a loop. why amazing tensorflow I think that clear session shouldn t be necessary. 
26218,tf gpu 1.13.1 35 less batch size before OOM vs tf gpu 1.11.0, System information Windows 7 TensorFlow installed from source or binary pip TensorFlow version use command below 1.11.0 1.13.1 Python version 3.6.5 CUDA cuDNN version 9 7.1.4 10 7.4.1 GPU model and memory GTX 1060 6GB Describe the current behavior I have standard AE network with pixel shuffler layer. on tf.1.11.0 cuda 9 maximum batch size for my GTX 1060 6GB is 132 but after upgrade to tf.1.13.1 cuda 10 tf cannot handle same batch size it produces OOM error and maximum now 90 for my card. Describe the expected behavior expected not to downgrade performance when upgrading tensorflow Code to reproduce the issue Other info logs ,still no solution I guess it is not fixed in tf 2.0 iperov Is this still an issue tatianashp are you kidding reproduced significantly performance downgrade no fix or official comment about it smit hinsu Can you look into what s happening here iperov Sorry it fell through the cracks. Compared to 1.12 I m finding that the exact same code uses about 10 extra GPU memory as per tf.profiler. Specifically I get about 6400MB usage total i.e. for TFProfRoot on 1.12.0 but about 7100MB for 1.13.1. With a smaller version of the same model the proportional difference is about the same about 3450MB for 1.13.1 and about 3100MB for 1.12.0. rightaditya thx you reproduced the bug too. What are next actions Sorry for the long delay for the update. This looks like a bug introduced between cuDNN v7.2 and v7.4. We will report this to NVIDIA and update this issue after that. Thanks for providing with a small example to reproduce the issue. The difference seems to be coming from a change in CUDNN CONVOLUTION FWD ALGO FFT TILING convolution algorithm implementation causing it to use more memory with cuDNN v7.4. I don t see any mention of this in the release notes but this might be an intended change. I was able to work around this issue by setting TF CUDNN WORKSPACE LIMIT IN MB environment variable to 1024 . If for some reason that does not work out for you then also give it a try setting TF CUDNN WORKSPACE LIMIT IN MB to 0 and then TF CUDNN USE AUTOTUNE to 0 . Let us know if either of the above helps resolve the issue. smit hinsu tried both solutions but oom still happens last cudnn 7.5.1 also fails I don t have the exact code that I tested earlier but it didn t use convolutions at all. It was all LSTMs the CUDNN ops if I m remembering correctly. However I should point out that some tests I ran yesterday found substantial variation in the memory usage reported by tfprof for the same code and data so what I saw before may have been within the bounds of that variation. I did try it a few times with each version to get an idea of the distributions though and they didn t seem to overlap. We are experiencing the same issue even with tf 2.0. The model we use consumes approx 1GB of more GPU memory than the pytorch implementation of the same model though tf model is way faster. Looking forward to the solution. last cudnn 7.5.1 also fails Did cudnn 7.6.0 solve this issue cudnn 7.6.0 same problem Hi any updates on this issue timshen91 Can you please take a look at this I think the next steps are 1. Verify that this is a problem still. 2. If it is still a problem file a bug with NVIDIA CC nluehr With tf2.0.0 and batch 132 I cannot reproduce the OOM with the garbage collector on. With GC off I can still see the OOM. I ll close the bug as GC seems to deallocate dead memory. Please re open it if the problem still remains. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 26218 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 26218 No a 
26684,Repeatedly allocating a graph and summary writer leaks memory, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow I have OS Platform and Distribution e.g. Linux Ubuntu 16.04 macOS 10.14.3 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary Source TensorFlow version use command below v1.12.0 10061 gf3954bf900 1.13.1 Python version 3.7.2 Bazel version if compiling from source 0.23.1 GCC Compiler version if compiling from source Apple LLVM version 10.0.0 clang 1000.11.45.5 CUDA cuDNN version N A GPU model and memory N A Describe the current behavior Repeatedly allocating a graph and making a summary writer leaks memory. Describe the expected behavior Memory should be freed when the graph leaves scope. Code to reproduce the issue Other info logs Here s what the output looks like ,Forgot that I was using a slightly messy TensorFlow tree. I ve reconfirmed that the bug persists at https github.com tensorflow tensorflow pull 26705 which is 5b24fba0857394dab67359963726b3bcce071575 plus a one line header include addition to make it build on my machine. No more TF bugs skye nfelt are you familiar with summary writers skye To clarify what I wrote wasn t intended to ask you to do anything was just expressing sympathy as a fellow ex tensorflow person who occasionally gets added to bugs. Haha no worries girving I just switched teams this week so still figuring out what to do with all my github issues Thanks for the report. I can reproduce this against last night s tf nightly on both macOS and Linux for anyone else reproducing maxrss is in bytes on macOS but kb on linux so the raw numbers are about 1000x smaller . I also could still reproduce this even with tf.init scope removed. cc alextp if you have any intuition about what might be causing this. rohan100jain I think the issue is that SUMMARY WRITER INIT OP keeps a strong reference to the graph instead of a weak reference so the graph can never be GC d. It looks like there are two problems one large and one small Large Every tf.Operation has a strong reference to the graph it lives inside https github.com tensorflow tensorflow blob master tensorflow python framework ops.py L1921 . This should probably be a weak reference. This is presumably the cause of the leak since we store a long lived reference to the op in SUMMARY WRITER INIT OP at https github.com tensorflow tensorflow blob master tensorflow python ops summary ops v2.py L221. Small SUMMARY WRITER INIT OP should be something like a https docs.python.org 3 library weakref.html weakref.WeakKeyDictionary with keys being the graphs not keys being strings that live forever. Even if we fix the large issue the small issue would remain and thus pretty much any use of graph key is a bug. Update I made a brief attempt at removing that one reference but it didn t work so I think there are others. Probably a more concerted push is required. alextp rohan100jain Did you get a chance to look at this I want to make sure it isn t dropped since it s blocking my upgrade of TensorFlow. nfelt can you look What s the likely ETA here I m still blocked from upgrading due to this so if no one is planning to fix I d like to know for my own planning purposes. In particular if it s going to be weeks more I will have to fix it myself but then you have to be happy with my weakref design decisions. I will try to take a closer look today but if SUMMARY WRITER INIT OP isn t the main issue then I don t have a good guess at what the leak might be so it may take some time to figure this out. Just so I understand which upgrade path exactly are you blocked on Is this upgrading from tf.summary to tf.contrib.summary Or did you notice that this leak occurs across a TF version update nfelt Thanks The leak appears going from TensorFlow 1.12 to 1.13 so it s blocking the 1.13 upgrade and therefore also my Python 3.7 upgrade . Note that it s quite possibly I simply failed to fix the SUMMARY WRITER INIT OP reference but as I mentioned it does seem like every use of graph key is a bug and therefore it feels likely that there are other strong references. Can you check whether using gc frees the graph I.e. is this a simple reference cycle problem or is a reference to the graph kept hidden someplace Ah... Of course as long as the summary init op reference exists that won t help. Nope gc.collect has no effect. Per discussion with alextp I think what s easiest here is to revert the part of https github.com tensorflow tensorflow commit aa8f428a9310b3fd8371bddf612e480b27618b2e that changed this from a graph collection to a python dict. That seems very likely to be the root cause and it was changed due to deprecation of global collections but this is a 1.x only usage anyway and that seems like the easiest way to fix the regression. That sounds good. Arguably global variables with references to graphs should be even more deprecated. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 26684 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 26684 No a I ve confirmed that this fixes both my minimized test case and my unminimized original code. Thank you nfelt Glad to hear that 
27202,Tflite JNI wraps seems failing to release int array.,Hi it seems that current impl of tflite jni overlooked ref release for array And the current tflite really could make JNI reference table overflow some phones with Android 4.4.4 API 19 . How Just invoke resizeInput every time you run interpreter even put the same int array to it. You can see the reference table is booming. version I ve tried 1.13.1 and 0.0.0 nightly it s the same. the relevant code is here https github.com tensorflow tensorflow blob c18034bc927f733e5e5a43d0c421775f969e0d04 tensorflow lite java src main native nativeinterpreterwrapper jni.cc L104 The strange thing is the code above althrough didn t deal with release in some situation should work fine with same int array. ,You re absolutely right will push a fix shortly. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27202 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27202 No a 
27288,GPU Memory Leak for eager mode tf.scatter nd update., Have I written custom code yes OS Platform and Distribution ubuntu 16.04 TensorFlow installed from binary TensorFlow version v1.12.0 Python version 3.6.8 CUDA cuDNN version 9.2 7.2.1 GPU model and memory Tesla P40 24GB when i use tf.scatter nd update if ref is tf.int32 Variable everything is fine. if ref is tf.float32 Variable then there is a memory leak. i hope the folloing code will reproduce the bug. when i run the code get the following results ,Can you remove allow growth and add a import gc gc.collect before printing the bytes in use to confirm it s not an issue just of the python gc taking a while to run alextp hi thanks for your reply. i add import gc gc.collect remove session config and get the same result. codes results jaingaurav can you look at this Also does this also reproduce on tf nightly 1.12 is a little old and it had some known memory leaks. I do know there are some known memory leaks for v1.12.0 19671 this issue mentioned that tf.set random seed 1 can fix the known issue. As far as i know tf nightly gpu need cuda 10 instead of cuda 9. It s a bit difficult for me to run the code on tf nightly right now. I may have a try in two days. tf nightly gpu 1.14.1 dev20190404 can reproduce the bug. jaingaurav could you please look at this irvingzhang0512 A little swamped at the moment but I plan on starting work on this bug next week. same issue here any updates irvingzhang0512 btw tf.set random seed x workaround could assign any value for x right XuesongYang yes i think tf.set random seed x clears eager mode caches by calling pywrap tensorflow.TFE ContextClearCaches self. context handle . so any value works. at what point do you add tf.set random seed 1 . i placed the line before model.compile but i m still having problems. I have to comment out tf.enable eager execution otherwise my RAM gets depleted and computer freezes. jaingaurav this is serious enough we should at least triage it before the 2.0 RC I believe I m able to reproduce this in a unit test now. Working on finding the source of the leak. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27288 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27288 No a 
27511, TF 2.0.0 alpha0 Memory leak with tf.keras.models.load model, System information Have I written custom code Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 macOS Mojave 10.14.3 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary Docker image tensorflow tensorflow 2.0.0a0 py3 jupyter TensorFlow version 2.0.0a0 py3 jupyter Python version 3.5 Using only CPU Describe the current behavior I have a memory leak when I load several keras model with load model function previously save with model.save . Here is what I obtain when i load 20 times a model the same model for the example . The output is obtain with the memory profiler library. https pypi.org project memory profiler see code below First Iteration After 20 iterations Describe the expected behavior I do not have this problem with tensorflow 1.13.1 Here is what I obtain with this version and the behavior I expect to have. First Iterations After 20 iteration Code to reproduce the issue Other info logs I also tried to put within the function model None del model gc.collect import gc with no effect, bguillouet Can you please provide the colab python link so that we can reproduce the bug as .h5 is not available currently here Hi You can reproduce the bug with any .h5 model. Here is an example with a Resnet50 model from keras. Here is the code on colab https colab.research.google.com drive 1Uafi1hSqaHaEXpYyi8LFVXdK2gTQQVEs I didn t get the memory profiler output on colab but here are the output on my laptop. tensorflow 1.13.1 First iteration 20 iterations tensorflow 2.0.0 aplha First iteration 20 iterations Thanks for reporting this. I believe the issue is with K.clear session which clears the default graph. In 1.x Keras uses the default tf graph but in 2.0 Keras manages its own graph. So the clear session call isn t actually clearing the keras graph and models are accumulating there and preventing memory from being freed. It should be a simple fix to make clear session clears the right graph in 2.0. https github.com tensorflow tensorflow commit 5956c7e9c44e23cd1a006df872ae468201fdb600 diff e329ed6b8d30dca9a441689005047035 should fix the issue. I tested on the latest tf nightly 2.0 preview and observed that memory is no longer increasing. This also means that our keras tests for 2.0 haven t been properly independent since https github.com tensorflow tensorflow blob 2fe0e5265cb74926df25552f7533b404ecaf2c3d tensorflow python keras keras parameterized.py L39 wasn t actually clearing the graph in 2.0. So we might also pick up some testing robustness for free as well. All in all this has been a tremendously useful bug report. Thanks so much Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27511 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27511 No a I have TensorFlow version 2.0.0 and Keras 2.3.1 but still having the same problem. 
28110, TF 2.0 TF 2.0 consumes twice as much memory as TF 1.x or CNTK., System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 x64 1809 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary pip install tensorflow gpu 2.0.0 alpha0 TensorFlow version use command below v1.12.0 9492 g2c319fb415 2.0.0 alpha0 Python version 3.6.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version CUDA 10.0 cudnn 10.0 windows10 x64 v7.5.0.56 GPU model and memory GeForce GTX 1070 8GB Describe the current behavior Evaluating TF 2.0 keras model allocates twice as much memory as TF 1.x or CNTK. Describe the expected behavior Memory usage of TF 2.0 should be same or similar to other libraries not double. Code to reproduce the issue For TF 2.0 or 1.x For TF 1.x or CNTK with keras With 8GB VRAM GPU TF 1.x and CNTK works successfully and TF 2.0 code are failed due to Resource exhausted exception.,TL DR It s actually a somewhat niche case a single weight is O gpu memory size but it s also a pretty easy fix. In your example the model is failing when it builds the kernel for the dense layer or more specifically when it calls the initializer. If you consider a much simpler example In graph mode when you evaluate c which is a symbolic tensor TensorFlow s memory allocator will reuse blocks of memory when it detects that they are no longer being used so by the time c is computed the buffer that backed a is long gone. By contrast in eager a handle to all three tensors co exist in the body so the runtime is not allowed to free them until they go out of scope on the python side. Since a user could presumably add other computations later unlike the graph case where all computations are known. Fortunately wrapping the initializer call in a tf.function fixes the issue. In general I suspect more and more of these utility functions are going to get function d for precisely this sort of reason. robieta Thanks for investigation. Can you provide sample code of tf.function usage What is the initializer Also my actual code is big CNN network with TF 2.0 keras API and has manual training loop by using tf.data.Dataset and GradientTape samely suffer from this memory and performance issue. If you provide sample code of the usage of tf.function for keras model and manual training loop it will be very helpful. For the case I described And my plan is to make builtin initializers do the tf.function wrap automatically. In general functions are documented in https www.tensorflow.org alpha tutorials eager tf function https www.tensorflow.org versions r2.0 api docs python tf function Thanks for sample code But I have one more question how about gamma initializer for BatchNormailization layer its initializer is a Class Ones or Zeros not method so I can t wrap it with tf.function. glorot uniform is also a class. When you trace through it s just the init ops.GlorotUniform class . That s why we call it on the second line. So tf.function is actually wrapping call of that particular class instance. So Ones or Zeros should work the same. Oh I mistaked that using ones instead of Ones . Using Ones works for gamma initializer. And my plan is to make builtin initializers do the tf.function wrap automatically. robieta You mean that this temporary fix will not be needed in official TF 2.0 release Also I found that TF 2.0 requires more memory than CNTK for training. In example completly same model same structure same of trainable variable with same PC cntk can train the model with x2 batch size. It may not be related this issue but disturb migrating from cntk to TF 2.0. Using GradientTape for manual training loop requires double GPU ram. I found that using Model.train on batch instead of GradientTape is more fast and comsume less GPU ram. Wrapping entire train loop by using tf.function makes GradientTape to be fast as train on batch but memory usage does not changed. Also train on batch has another issue that allocate CPU memory infinity without tf.keras.backend.set learning phase 1 . I ll open as new issue for this when I found minimul reproducible code. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28110 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28110 No a And my plan is to make builtin initializers do the tf.function wrap automatically. robieta You mean that this temporary fix will not be needed in official TF 2.0 release I m not sure if it will make it into the next release but yes that s the plan. I don t train. only loading the trained model takes more than twice memory. I have 2 environments. tf 1.14 installed for python 3.5 and tf 2 for python 3.7. I run the same code and load the same keras model in both environments. in py3.5 after model loaded it takes 560MB memory. while in py3.7 it takes 1.2GB. is there any way to reduce the memory for tf 2 Same problem here. Loading the model it s consuming a lot of memory. Some solution was found 
28646, TF 2.0 keras model.train on batch allocates extremely large CPU memory, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 x64 1809 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary pip TensorFlow version use command below 2.0.0 dev20190504 Python version 3.6.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 10 GPU model and memory Geforce GTX 1070 8GB Describe the current behavior When I try to train on batch TF 2.0 allcaotes memory endless and finally crashes. Using tf.keras.backend.set learning phase 1 can be temporary solution but this trick doesn work with lateset build dev20190511 . Describe the expected behavior The model can be trained regardless of tf.keras.backend.set learning phase 1 because I used model.train on batch . It should handle learning phase internally. Code to reproduce the issue If you uncomment tf.keras.backend.set learning phase 1 the model may be successully trained. But with TF 2.0 dev20190511 the traininig is always failed regardless of tf.keras.backend.set learning phase 1 . ,Related comment https github.com tensorflow tensorflow issues 28110 issuecomment 490082449 KichangKim Able to reproduce the issue using build dev20190511 allocates endless memory and finally crashes. I found that TensorFlow 2.0.0 beta1 still have this issue. Same problem here. Exact same code was running fine on tf.keras 1.14 causes OOM with tf.keras 2.0. Code snippet The above code won t throw OOM error when use a very small batch size e.g. 8 or with eager execution turned off. Same problem here. Exact same code was running fine on tf.keras 1.14 causes OOM with tf.keras 2.0. Will update code snippet. I think OOM issue is related with 31871 too. Also it is not fixed yet. I found that tf nightly gpu 2.1.0.dev20191112 does not have this issue anymore. Thanks. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 28646 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 28646 No a 
28798,tf.data.Dataset cache doesn t cleanup unused lock and tmp files , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below 1.13.1 1.14.dev 2.0.0 alpha0 Python version 3.7.3 CUDA cuDNN version GPU model and memory Describe the current behavior tf.data.Dataset cache doesn t cleanup unused lock and tmp files if the iterator doesn t complete a pass over the whole dataset or if the process exits before cache has been completed. This means subsequent runs of the same code with an incomplete cache will fail with Describe the expected behavior The iterator should cleanup incomplete cache files if finished or if the process exits. This would be really handy when running multiple runs on the machine and if processes might error or finish before the cache has been complete. Code to reproduce the issue , lgeiger After running the provided code got the below error. NotFoundError cache mnist 0.lockfile No such file or directory Op IteratorGetNextSync Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28798 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28798 No a 
28844,TF2.0 leaking memory when input shape is specified in keras layer, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow there is a custom MWE code snippet included in the issue OS Platform and Distribution Linux Elementary Loki Ubuntu 16.04 TensorFlow installed from source or binary binary TensorFlow version 2.0.0 alpha0 Python version 3.7.1 Describe the current behavior Generating several models with no input shape specified results in normal memory occupancy no shape https user images.githubusercontent.com 472900 57986674 e39a2780 7a6f 11e9 909f ac244abbcc4b.png This is the behavior expected when the input shape is specified. However when multiple models are generated within the same program life cycle the specification of an input shape seems to produce a memory leak. If the input shape is specified when the models are created they pile up in memory without being destroyed. Also the execution time seems to increase quite a lot. shape https user images.githubusercontent.com 472900 57986711 67541400 7a70 11e9 9743 5528ece4417b.png Describe the expected behavior Model generation should behave the same from a memory point of view regardless of whether the input shape is specified or not. Code to reproduce the issue It is enough to execute the code snippets included in the issue. In order to generate the memory occupancy the memory profiler package can be used. Assuming the code snippet is pasted in a file called test.py run Other info logs I generated a list of all the objects for the 2 code snippets using Pympler . For the models generated without input shape here is the result And here is the snapshot for the models generated with the input shape. Edit typo in keras.models Edit typo in keras.layers , b3by I ran the code but did not see any output does this need any modifications in the code if not please suggest me to reproduce the issue Unless you look at your system manager or htop the way you can see the memory leak is by plotting the memory consumption using memory profiler . Did you execute the script by running the following It should open up a matplotlib plot with the memory taken by the script. This is expected behavior as the model has to be rebuilt if no input shape is available. You can use clear session https www.tensorflow.org api docs python tf keras backend clear session to clear the current Keras session. I just tried to include into the test script both before and after the sleep but no luck with that the memory leak is still there. Now if this was really an expected behaviour that would mean the bug is actually in the clear session method. I understand your point but I just can t wrap my mind around the idea that old model instances are not destroyed just because the new ones have to be rebuilt. To follow up on what karmel said When you make a Sequential model without input shape specified there isn t actually enough information at compile time to create the weights. For instance in the dense layer above there s no way to know that the kernel needs to be shape 10 120 and it can t allocate a 120 Variable. So Sequential defers weight creation until it sees the first batch at which point it can create variables. So the lack of memory growth is simply reflecting the fact that the example doesn t reach the point to which allocation is deferred. On the other hand when input shape is provided then Sequential actually can allocate weights before seeing the first batch. The reason that those weights aren t freed when the model goes out of scope is that unless otherwise specified all tensors used by keras share the same global graph. It has the nice property of allowing mixing and matching of models this is why you can compose models from other models for instance but it also means that this graph holds references to weights and therefore blocks their garbage collection even beyond the life of the python Model object. tf.keras.backend.clear session destroys the underlying global graph and therefore allows variables to be freed. The reason you re seeing different behavior is that there was a bug where clear session didn t work in 2.0 and the fix https github.com tensorflow tensorflow commit 5956c7e9c44e23cd1a006df872ae468201fdb600 was added after the alpha cut. If you try tf nightly 2.0 preview it should do the right thing. I ve replicated your example which was stellar by the way. top notch repro. and confirmed that clear session works as intended in the latest nightly build https gist.github.com robieta 474aa37a76ba242d9f5ea2e5284c9fab We re also actively looking into the broader issue of making keras more gc friendly so stay tuned. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28844 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28844 No a robieta spot on. Brilliant explanation and excellent notebook thank you very much. The shared global graph explains things I was just unlucky to stump in a bug fixed in nightly that was concealed in a normal behaviour that I took for a bug. I already patched my code so that I spawn a new process every time I train a new fold but I might try with the nightly version and see if my whole project breaks. will this fix work in beta1 
29027,Cpu memory leak when using tf.function with gpu model., System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes. OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below v1.12.1 2319 g81f2165 2.0.0 dev20190520 Python version 3.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 10.1 7.1 GPU model and memory Describe the current behavior When using tf.function the cpu memory usage increase forever. Describe the expected behavior No cpu memory leak when training models on gpu device using tf.function . Code to reproduce the issue A part of my code since the whole source code is huge... And the dataset train in the following code is a tf.data.DataSet object. , muddham ry jmhodges eggie5 Can anyone help me please thank you jiarenyf Can you check this issue https github.com tensorflow tensorflow issues 26108 which is similar to yours and try it with TF.2.0.0 beta0 and let us know how it works. Thanks Possibly similar to https github.com tensorflow tensorflow issues 29075. jvishnuvardhan Thank you for the suggestion. However the problem still occurs after pip install tensorflow gpu 2.0.0 beta0 . I also tried tf.keras.xxx.clear session and it took no effect for me. I have uploaded the code in the codes https 1drv.ms u s Au2Wv1iKFA69hLpV3g9jjxsQa1AqtA e J24DMb could you please help me to debug thank you. You could run the code by I have refactor the code so it look different from the one I mentioned 20 days ago while the problem is the same When running without tf.function the cpu memory cost is as most 4.8GB and the gpu util is 70 100 while running with tf.function the cpu memory keep increasing till over 10GB and the gpu util is always 0 . Thank you. Since no one answers this question I will delete the codes uploaded in one drive and switch my codes to use pytorch instead. Bye bye. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29027 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29027 No a Check if batch data which in the train step function has changed its dimention or some data in batch data is accumulating every epoch. Because if that tf.function will create a new graph every epoch. Have been troubled by this problem for a long time and finally solved with help of a classmate Have a good day. jiarenyf Sir have you solved your problem I also met a problem like this but some reshape is used in my programme but i have no idea how to fixed the leak of CPU memory. Sir have you solved your problem I also met a problem like this but some reshape is used in my programme but i have no idea how to fixed the leak of CPU memory. Hi The follow tf.function instruction in Tensorflow may provide some clues Note that unlike other TensorFlow operations we don t convert python numerical inputs to tensors. Moreover a new graph is generated for each distinct python numerical value for example calling g 2 and g 3 will generate two new graphs while only one is generated if you call g tf.constant 2 and g tf.constant 3 . Therefore python numerical inputs should be restricted to arguments that will have few distinct values such as hyperparameters like the number of layers in a neural network. More information about tf.function https www.tensorflow.org api docs python tf function . So check if have obey this otherwise it will generate new graph each batch and that will significantly raise your CPU memory. Good luck. Thank you Ain Crad . tf function decorator is main culprit for increase of ram . In my case total loss loss 1 inp tar lambda loss 2 inp tar does lambda here cause a problem 
29213,GPU OOM error when using keras and distributions strategy, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 18.04 TensorFlow installed from source or binary binary TensorFlow version use command below tf nightly gpu 1.14.1.dev20190524 Python version Python 3.7.3 CUDA cuDNN version CUDA 10 CUDNN 7.4.2.24 GPU model and memory 4 x NVIDIA V100 Describe the current behavior Using tf.distribute.MirroredStrategy together with Keras to train models as described in https www.tensorflow.org alpha tutorials distribute keras results in GPU out of memory errors appearing after several epochs of training. We excluded our custom written code as the source of the memory leaks and made sure that the model actually fits into memory with enough headroom. It seams that either tf.data or tf.keras.metrics have a memory leak that starts showing up after several epochs of training and evaluation. Describe the expected behavior Tensorflow doesn t throw OOM errors. Code to reproduce the issue Unfortunately I cannot give a concrete code example to reproduce this issue since memory leaks appear anytime in between 10min to 12h of training. Though I am happy to provide more information and would be eager to get suggestions on how to properly debug this problem. Other info logs ,Do you have anything in your code such as a callback that keep adding new ops to the graph Do you have anything in your code such as a callback that keep adding new ops to the graph As far as we can tell no. We are using the TensorBoard and ModelCheckpoint callbacks to monitor training process and don t have any other callbacks. yuefengz Is there a way to check if TensorFlow or our code adds new ops to the graph when training the keras model We are now able to also reproduce this error on a single GPU without distributions strategy. One thing to note is that we are using tf.data.Dataset prefetch tf.data.experimental.AUTOTUNE as the last operation in the tf.data pipeline though if I recall correctly tf.data.Dataset prefetch shouldn t prefetch onto the GPU memory correct Once we are able to reliably reproduce this issue I am happy to share a repo gist with the code. Any help or pointer for debugging would be greatly appreciated in the meantime. To check whether there is any new op added you graph after training starts you can add loggings to the create op method https github.com tensorflow tensorflow blob master tensorflow python framework ops.py L3221 jsimsa for dataset questions. lgeiger tf.data without distribution strategy will not prefetch any data to GPU unless you are using prefetch to device which has been deprecated in lieu of multi device iterator used by the distribution strategy under the hoods To check whether there is any new op added you graph after training starts you can add loggings to the create op method Thanks. I double checked that we don t add any operations to the graph after training starts. Same problem. I also double checked my code that no new op after training starts. Suspect that there s some memory leak somewhere in tf. We where able to resolve this issue by not using .prefetch tf.data.experimental.AUTOTUNE as the last dataset transformation which caused problems with TensorFlow running inside a Kubernetes cluster without the correct resource requests and limits and data streaming from GCS. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29213 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29213 No a I encountered this issue today. For me it turned out that I accidentally batched my dataset multiple times instead of once. Initially I did train ds train ds.cache .prefetch buffer size 2 then i removed .prefetch still errors then i removed .cache and it worked. Note cache.prefetch works fine on 1 5 the number of images. I had this error while scaling up the data. Also there are other symptoms it will fail. I see my RAM go from 5gb to 15gb using up all of it as the training steps begin and always exactly at step 308 330 i get the error Resource Exhausted Failed to allocate memory for the batch of component 0 node IteratorGetNext . After removing cache and prefetch my RAM goes up to 9gb and maintains. Anyone has an explanation for how cache and prefetch affects RAM Also how can we be sure AUTOTUNE will not prefetch too many batches and cause errors in the middle of training Trezorro Could you share how you batch your dataset multiple times accidentally and why would that cause this ResourceExhaustedError Is it something like in a single method chain of data.cache .batch 32 .prefetch 1 .batch 32 Or 4 assignments separated some distance apart in code eg. data data.cache data data.batch 32 data data.prefetch 1 data data.batch 32 I used assigments so that I could put some steps within a conditional statement. I used prefetch only at the very end however. While fiddling with my pipeline I accidentally had two lines with examples examples.batch batchsize This of course got me in some memory issues even when every single example is just a 224x224 image. Op ma 29 jun. 2020 om 10 53 schreef gitgithan notifications github.com Trezorro https github.com Trezorro Could you share how you batch your dataset multiple times accidentally and why would that cause this ResourceExhaustedError Is it something like in a single method chain of data.cache .batch 32 .prefetch 1 .batch 32 Or 4 assignments separated some distance apart in code eg. data data.cache data data.batch 32 data data.prefetch 1 data data.batch 32 You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 29213 issuecomment 651026270 or unsubscribe https github.com notifications unsubscribe auth AESTRG2VUV2PNURLJ5OGD5DRZBI7DANCNFSM4HR2RBWA . Met vriendelijke groet Milan Tresoor 
29403,tf 2 memory leak of Flatten and BatchNormalization layer, System information OS Platform and Distribution e.g. Linux Ubuntu 16.04 win10 x64 TensorFlow installed from source or binary binary TensorFlow version use command below v1.12.0 9492 g2c319fb415 2.0.0 alpha0 Python version 3.6.6 Describe the current behavior memory leak when model has Flatten or BN layer. memory usage keep increasing and never down after gc collection. Describe the expected behavior models created will be collected by gc and memory usage go down to initial level Code to reproduce the issue ,Will it be possible to provide the full code snippet that can replicate the issue. It will be really helpful to proceed further. Thanks Will it be possible to provide the full code snippet that can replicate the issue. It will be really helpful to proceed further. Thanks I ve updated the code snippet. Please see https github.com tensorflow tensorflow issues 28844 for a detailed explanation of what s going on. Feel free to re open if upgrading to tf nightly 2.0 preview and adding tf.keras.backend.clear session doesn t resolve the issue. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29403 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29403 No a 
29996,TF2 apparent memory leak when running dataset ops eagerly, System information Have I written custom code yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 OSX TensorFlow installed from source or binary 2.0.0beta TensorFlow version use command below v1.12.1 3259 gf59745a381 2.0.0 beta0 Python version 3.6.8 Describe the current behavior When using the function tf.autograph.to graph I see a memory leak which I don t see if I use the annotation tf.function Describe the expected behavior There should not be a memory leak. Code to reproduce the issue ,I have tried on Colab with TF version 2.0beta and was able to reproduce the issue. I did a few tests. In short this seems to be related to Dataset.reduce which is what is generated in this case . The leak reproduces even without autograph.to code if we run the code that would be effectively generated The leak only seems to happen for datasets if we replace p data with reshaped version of data in effect causing the for loop to run as a tf.while loop the leak doesn t reproduce jsimsa any insight into this Jiri and I spoke offline. For technical reasons Dataset.reduce re traces its function whenever called during Eager execution. This in effect adds a new trace to the default graph which accounts for the increase in memory use. For this reason we recommend caution when iterating over Datasets repeatedly in Eager execution even with AutoGraph. We recommend using tf.function as in increases performance and avoids these memory build ups. tf.function also applies AutoGraph so you won t need to call it separately. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29996 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29996 No a mdanatg Is there any other way than wrapping the iteration of a dataset into a tf.function. My custom training loops need eager tensors to use the tf.summary API. Can I manually free the memory from the tf.dataset Furthermore this should be noted in the documentation https www.tensorflow.org beta tutorials eager custom training walkthrough training loop since it iterates several times of the dataset while being in eager mode. jaingaurav I agree that it s worth noting it in the docs somewhere. 2649 as far as I know the tf.summary API should work inside tf.function as well unless you re perhaps encountering a different bug Unfortunately I don t know of a way to manually free up the memory. jsimsa might know of a way. mdanatg No it is not really a bug. I have some numpy operations in my summary calls which do not work in tf.function. But I just changed them to tf operations. It is just a comfort problem. Ok the memory leak has a higher impact on my custom training loops than I expected. For everyone who needs a quick fix You can disable trace tf.summary.trace off before you iterate over a dataset and enable it afterwards tf.summary.trace on . This worked in my case. mdanatg Do you see any problems with this workaround It s hard to make a in the general case but I don t see a problem with disabling trace if it s not needed. Thanks 2649 for proposing this quick fix. I wonder if you could provide some intuitions explanations as to why this worked Checking as this doesn t seem to work for me. BTW I strongly think this problem should at least be mentioned if not fixed in all of the official docs whenever iterating a dataset happens. 
30165,TF 2.0 Put Tensor into some Numpy functions continuously increases memory usage, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04 TensorFlow installed from source or binary pip package tensorflow 2.0.0 beta1 TensorFlow version use command below v2.0.0 beta0 17 g8e423e3 2.0.0 beta1 Python version 3.7.3 CUDA cuDNN version 10.0.0 7.3.1 GPU model and memory Titan Xp 11Gb Describe the current behavior Memory leak when we put Tensor into some Numpy functions ex np.array np.zeros like . Following attached code continuously increases memory usage. Describe the expected behavior No memory usage explosion. Code to reproduce the issue ,I have tried on Colab with TF GPU version 2.0beta1 and was able to reproduce the issue. The code snippet caused crashed in Colab after using all available RAM. superbobry is this related to the recent array interface changes I couldn t reproduce the issue using the nightly when running on CPU. Will be able to try on GPU on Monday. I ve reproduced the leak on GPU fix is on the way. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30165 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30165 No a 
30324,Memory leak in eager mode when creating keras model in loop, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Arch Linux Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device not tested TensorFlow installed from source or binary binary TensorFlow version use command below v1.12.1 5259 ge703239 1.15.0 dev20190629 Python version 3.7.3 Bazel version if compiling from source not compiled from source GCC Compiler version if compiling from source not compiled from source CUDA cuDNN version using CPU GPU model and memory using CPU Describe the current behavior In eager execution when creating a tf.keras.Sequential model inside a loop and discarding it immediately the memory increases over time. The following code shows this by printing the used memory at each iteration. Output The same result happens when using the Functional API or Model subclassing API. Adding tf.keras.backend.clear session in the loop solves the leak in all cases like in graph mode. To see this effect better one should additionally use gc.collect in the loop. Describe the expected behavior While adding tf.keras.backend.clear session to the loop helps this should not be necessary because in eager execution there is no graph to clear which according to the documentation https www.tensorflow.org api docs python tf keras backend clear session seems to be the only thing this function does Destroys the current TF graph and creates a new one. Therefore it is also suprising that this function helps at all during eager execution. The expected behavior is that there is no memory leak even without tf.keras.backend.clear session . Code to reproduce the issue Code is in description above. Other info logs Nothing here.,Does del model Help Does del model Help It doesn t work bionicles A similar situation if I train the model in the main thread and load the model in another thread AT THE SAME TIME. All things are in a loop. However if I use clear session method in one thread the code in another thread won t work I test pytorch and mxnet and there is no any memory leak in a loop. why amazing tensorflow I think that clear session shouldn t be necessary. bionicles tjume if I use clear session method in one thread the code in another thread won t work That is purely logical. Threads share memory thus if you call clear session to swipe a model out of memory in a thread don t expect other threads to access the now deleted memory... Now I do agree after reproducing the issue which is all the more present in TF 2.0 with Eager execution enabled by default that the absence of implicit garbage collection inside the loop is a bit annoying. Note that if you use clear session without gc.collect the garbage collector will take out the past models from memory every few seconds which shows if you add some time.sleep instruction in the loop so I guess there are two things at stake in what you raise keras models created in a given scope are not swiped out when they go out of scope causing more or less of a memory leak this indeed probably requires either fixing or documenting so that users will either use clear session or not need it the python garbage collector can take some time before deallocating the memory taken by those models once their clearing has been ordered which I don t think tensorflow developers can do much about My humble opinion however is that it really is not an amazing effort from your end as a programmer to add a couple of memory freeing instructions now and then in your code... This is actually pretty common in my experience when you are creating stuff faster than the garbage collector will take care of. But that will be up to the developers maintainers to decide Maybe I didn t make it clear. I mean if I train model A in the main thread and load model B and predict in another thread AT THE SAME TIME. All things are in a loop. However if I use clear session method in one thread to clear model A e.g BUT the model B in another thread doesn t work DOESN T predict . As you can see there is no any relationship between model A and model B. It seems that clearing model A can influence model B. why That isn t logical. pandrey fr It seems that clearing model A can influence model B Oh I see Thanks for clarifying the issue. From my understanding but I might be wrong I am just a tensorflow user not an expert let alone a developer tf.keras.backend.clear session clears all graphs that have not been called yet . So if you declare models A and B then clear session will remove all those whose predict or evaluate or fit etc. method has not yet been called. On the other hand once you have called such a method the model cannot be deleted using clear session but can be so using del my instance . In either cases using gc.collect ensures the freed memory is effectively deallocated otherwise it will be done once the garbage collector s routine check on it . Examples That being cleared I now agree with you that it could be useful to dispose of a generic function to clear out a specific keras model without having to worry about its having been used or not. Let s wait for somebody from the mainteance development team to actually pick up this issue OK. Thanks in advance pandrey fr Why is this not fixed in all the github threads I ve come across Seems pretty useless to be unable to call model.predict in a loop without eventually maxing out your memory and dumping the whole thing Looks like I may have to switch to pytorch brandonbell11 Why is this not fixed in all the github threads I ve come across There has been some great improvement on those issues notably in 2.0 rc0 and we can expect some more with the upcoming actual 2.0 and 1.15 release s . The issue arises from Eager execution triggering the creation of sometimes usefully redundant back end graphs which sometimes end up not being properly discarded. It seems to no longer happen when using tf.data.Dataset objects which feels logical to me Datasets ensure the homogeneity of samples specification hence making it safe to re use the same back end graph but there still are some issues when feeding individual EagerTensors. I would hope it will be fixed at some point but you have to understand that Eager execution is a big turn compared to how TensorFlow s back end works which used to be the normal way of writing TF code until not so long ago. It is therefore bound to take a little time fixing everything and to be honest I am personally amazed by how fast it is going when I moved to using Eager a few months ago I felt like it was a terrible choice leading to huge performance drops and memory leaks issues while today the former have mostly vanished and the latter are progressively solved. So my point is we as users have to show a little patience. Seems pretty useless to be unable to call model.predict in a loop without eventually maxing out your memory and dumping the whole thing The problem is honestly not that great but yes in some cases such a problematic behaviour arises. Note however that you can work it around notably by using a Dataset object and I can hear that this is an effort you would rather not have to make. You could also stick with 1.14 and Eager disabled. Looks like I may have to switch to pytorch Honestly I do not believe anyone has to switch to PyTorch but nor to TensorFlow. You should pick up the framework that suits you best at a given point and be opened to change when relevant which is less and less hard as their high level APIs look more and more similar . If you feel like PyTorch works better for you switch to it but please do not look at it as a forced thing nor as being part of a choose your holy side and be verbose about it decision. Both frameworks devs are doing their best they disagree on some points and there is something of a competition for users between them but in my humble opinion we as users actually tend to benefit from it. Eager is clearly a response to PyTorch having a similar behaviour but TF devs have also shown their ability to make it great while preserving the back end specifics of TF and not just make it a fa ade filled with bugs which it kind of felt like in the beginning . So what I am saying is if you want to move to PyTorch do it but please do not make it sound like TF devs are not doing there job this is rather disrespectful and pointless since it is easy to see that they are actually working on solving issues and rather succeeding to do so . Seems pretty useless to be unable to call model.predict in a loop without eventually maxing out your memory and dumping the whole thing I don t follow. Creating models in a loop will increase memory this is a known issue with the way the keras backend manages state. However I don t see any leak when calling predict in a loop https colab.sandbox.google.com gist robieta cc5e2ccb179d97441e08fab3220ca5bf predict leak test.ipynb robieta When I run a similar test on tf2.0 rc0 I can actually see using psutils as in the code initially provided by the person who opened the issue a small RAM usage increase unless calling tf.keras.backend.clear graph and gc.collect explicitly in the loop. I do agree that this feels like a very minor problem though and I never run into GPU memory exhaustion which I do in the models in loop creation issue which as you mentioned is a well known one . robieta Apologies I should have mentioned this is specific to my use case and perhaps should make another thread. The models I use are only loaded in and build once. In short I am having this issue when implementing a monte carlo rollout policy implemented in seqGAN https arxiv.org pdf 1609.05473.pdf which requires me to complete a sentence N times i.e. N sentence length calls to model.predict to get the next word as well as model2.predict to get the score for that sentence. If I have a sentence of length 30 tokens and I want 20 example for each word I end up having to make 9280 total calls to model.predict to get the rewards I need. Using tf.keras.backend.clear session and gc.collect at the end of each loop doesn t prevent the GPU ram from gradually being filled up. I cannot even make it through one complete iteration without everything dumping and getting an OOM error. Is the only fix for this at the moment just rolling back to TF 1.15 I m using TensorFlow 2.0.0 and Keras 2.3.1 and still having the same issue. I m going to close this since the original issue growth from model creation has been addressed as a known issue and now the thread is kind of drifting. We have also recently made a fix to the internals of TF that eliminates a leak when invoking model.predict many many times. If you are still seeing issues with predict feel free to open a new issue with a repro.. Thanks for all of the feedback. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 30324 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 30324 No a 
30448,Keras has memory leak when passing in dataset object to predict ... function, Summary Performance degrades quickly and memory increases consistently when calling the Keras predict function in a loop with a dataset object. This does not happen when passing predict a numpy array or when passing in a tensor from a dataset iterator. System information Have I written custom code Minimally reproducible example below uses only stock 1.14.0 code. OS Platform and Distribution Ubuntu 18.04 Linux Mint 19.1 TensorFlow installed from source or binary pip install tensorflow gpu example not using GPU CUDA VISIBLE DEVICES 1 TensorFlow version use command below v1.14.0 rc1 22 gaf24dc9 1.14.0 Python version 3.7.3 Describe the current behavior Looping over model.predict x mydataset in a continuous loop degrades in performance after a few hundred iterations. The minimally reproducible example below starts at 0.04s per loop iteration and within about a minute of running is near 0.5s per loop iteration. Memory continues to climb. This does not happen when passing in a numpy array to model.predict x myndarray . The problem is also less severe when passing in tf.data.Iterator rather than a tf.data.Dataset . If you pass an iterator the performance will continue to degrade at a fifth to a tenth the rate. The cause of the difference between the dataset performance and the iterator performance is likely at training utils.py 1314 where Keras creates a new iterator for each predict loop. The issue is completely ameliorated when passing predict the tensor produced from tf.data.make one shot iterator mydataset .get next . In this case no additional dataset operations appear to be created by keras in the predict loop. Describe the expected behavior Multiple calls to predict should not degrade in performance over time when passing in a dataset. Code to reproduce the issue This code reproduces the issue and is copy paste runnable performance will degrade significantly within 30 seconds running this example. This example demonstrates passing a numpy array does not have the same issue. This issue started at SO at https stackoverflow.com questions 56910950 keras predict loop memory leak using tf data dataset but not with a numpy array I decided to post it here when I realized that predict is creating a new iterator each predict loop iteration and works when the get next tensor is passed in directly.,I could able to reproduce the issue by provided code snippet on Colab with Tensorflow 1.14.0. Thanks Indeed this is because in v1 making a dataset iterator adds ops to the graph. print Processing time .2f ops on graph .format time.time debug time len inp.graph.get operations In v2 tf.enable v2 behavior you ll see that there is no accumulation of ops and run time does not increase over subsequent model.predict calls. It s not obvious why the performance drops off so quickly since the dataset iterator doesn t add that many. In any event though as long as you use 2.0 you should be fine. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30448 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30448 No a I still have this problem with TF 2.0. 
30873,TF2 Memory leak during implicit conversion from EagerTensor to numpy, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04 TensorFlow installed from source or binary binary TensorFlow version use command below tensorflow gpu 2.0.0 beta1 Python version 3.7 CUDA cuDNN version 10.0 7.5 GPU model and memory GTX 1080Ti Running the following code on GPU causes a memory leak on my machine during implicit conversion to numpy. Leak does not manifest when doing explicit conversion r np.sum x.numpy . Code to reproduce the issue ,Was able to reproduce the issue with TF version gpu 2.0.0 beta1.Thanks Thanks for reporting that ivankreso I think the fix d5b287d6c93332ba73b99b375bd21f81266e3112 did not make it into the beta1 release. Could you try with the latest nightly and let us know if it works OK It works ok with the latest nightly version. Thanks. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30873 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30873 No a 
30912,TF GPU v1.14 CUDA runs out of memory ,Hello TF Team I am hoping you can please pull down my custom TF GPU CUDA X and Anaconda container solution with Jupyter. I am hoping you can assist because whatever the problem is the fix for the container is same as Ubuntu. Link to repo here https github.com joehoeller Anaconda CUDA Accelerated TensorFlowGPU Development Environment . Here is the error when running Tensorboard and benchmarks.py see instructions in README.md as to how to run it after spinning up container . This is the error that I get scroll right to read entire line ,I have addt l erros and info here https github.com joehoeller Anaconda CUDA Accelerated TensorFlowGPU Development Environment issues 1 https github.com joehoeller Anaconda CUDA Accelerated TensorFlowGPU Development Environment issues 1 Please fill the issue template https github.com tensorflow tensorflow issues new template 10 build installation issue.md . Could you update them if they are relevant in your case or leave them as N A Along with the template please provide as many details as possible to find the root cause of the issue. It would be great if you can provide a small code to reproduce the error. Thanks Closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30912 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30912 No a Is it resolved joehoeller Yes plz close Sent from my iPhone On Apr 8 2020 at 8 55 PM super.single430 notifications github.com wrote Is it resolved joehoeller You are receiving this because you were mentioned. Reply to this email directly view it on GitHub or unsubscribe. 
30952, tf.keras.layers.Embedding causes memory leak, System information Have I written custom code yes OS Platform and Distribution Linux Mint 19.1 TensorFlow installed from binary using pip TensorFlow version 2.0.0 beta1 v2.0.0 beta0 16 g1d91213fe7 Python version 3.6.8 CUDA cuDNN version 10.0 7.5 GPU model and memory Nvidia Quadro P1000 4 GB GDDR5 Describe the current behavior A GPU edit CPU as well see addendum below memory leak rapidly emerges from using high dimensional tf.keras.layers.Embedding layers. To be more precise I am working on Transformer networks and found out that when I try to fit one e.g. on the portuguese to english translation task presented in this official tutorial https www.tensorflow.org beta tutorials text transformer a GPU memory leak emerges after a few iterations. Based on this StackOverflow post https stackoverflow.com questions 42499592 resourceexhaustederror oom when allocating tensor with shape 42512916 I rapidly came to suspect that the issue comes from the learnable embedding layers at the base of both the encoder and decoder parts of the network. To further assess the issue and its source I implemented a pseudo Transformer network see code linked below that is stripped of most technical components the actual model embarks e.g. I removed positional encoding residual connections masking mechanisms etc. the rationale being to provide a more condense and faster run code to document this issue but also to confirm that the leak does not come from custom layers or any complex data processing mechanism. The provided code includes a data pre processing pipeline entirely based on the aforementioned tutorial https www.tensorflow.org beta tutorials text transformer a model construction function that makes use of the keras functional API and a main function to call the former and start the fitting process. On my computer everything runs fine and I can see the first few fitting iterations pass until an ugly stack of allocation error messages show up see full log linked below whose informative part seems to be W tensorflow core framework op kernel.cc 1546 OP REQUIRES failed at cwise ops common.cc 70 Resource exhausted OOM when allocating tensor Addendum I re ran the provided code disabling access to the GPU and it turns out there also is a high memory usage when running on CPU. During the first epoch and mostly during its first half memory usage goes up multiple GB in my case from 2 to 10 GB with an increase from 2 to 7 within the first 60 train steps out of 704 and keeps slowly increasing throughout the following epochs with minor decreases between increases thus displaying local plateaux which I would guess are related to the loading discarding of data batches . Although it is a bit less of a problem than with GPU since it is relatively common to have quite some RAM available plus some swap space on linux it still does not feel right that fitting the fake model on a dataset which can be fully loaded in memory creating a list of Eager Tensors from the tf.data.Dataset object containing the batched padded training set results in a marginal usage of around 100 MB of RAM would end up using 16GB or RAM. I would also like to note that calling gc.collect after training does not empty the used RAM which is only freed instantly when ending the python process. Describe the expected behavior The fitting process should go one fine and the memory should not get saturated I would expect some tensors to be de allocated as iterations pass . Code to reproduce the issue The script I wrote to illustrate the issue is publicly accessible as a gist here https gist.github.com pandrey fr c4ba8022c5dd956388e984f49c89ce61 . Other info logs The full error stack with GPU enabled is publicly accessible as a gist here https gist.github.com pandrey fr ff004b4cdd6d22b9cd84f82ef4e3a5ac , pandrey fr Hi Paul this is not related to your problem. However I am stuck at one step before you since you have helped me earlier so I thought maybe you can give me some suggestions. I am training my tf.keras model on multiple GPUs 2 exactly and following TF2.0 s example https www.tensorflow.org beta guide distribute strategy using tfdistributestrategy with keras for doing that. I am adding these two lines mirrored strategy tf.distribute.MirroredStrategy with mirrored strategy.scope before the model s definition and compilations as also shown in the link. Now I am receiving errors. My question from you is if there is anything else that has to be added in order to train it on multiple GPUs I have my own tf.data.Dataset separates into train and test sets after padding and batching. Thank you for your helpful reply. I already opened an issue https github.com tensorflow tensorflow issues 30843 event 2502562470 on TF but haven t gotten any response. After additional testing I found out the high memory usage is not exclusive to the GPU and update the initial post accordingly. rishabhsahrawat Hi I unfortunately have no experience with GPU distribution strategies and only have 1 GPU machines at my current disposal hence I would not know how to help you... Sorry I could reproduce the reported issue on Colab with Tensorflow version 2.0.0.beta1. Please take a look at gist of colab https colab.research.google.com drive 1ubO2h45rdehaPuoKiX4MzeKrYF HIeG5 . Thanks Additional information since this aforementioned post https stackoverflow.com questions 42499592 resourceexhaustederror oom when allocating tensor with shape 42512916 recommends taking the embedding lookup out of the training loop I ran a modified version of the code where the embedding layers are declared outside of the instantiated keras Model which now takes pre embedded Tensors as inputs and applied to the datasets at the time of their reshaping in the reformat inputs function . This does not resolve the issue and the GPU runs out of memory just as fast. However if I try to load the entire training set in non GPU memory e.g. data list iter trainset it works but the memory used which is greater than when loading the non embedded data which makes sense is not freed upon deletion. I.e. it appears that every time the data is loaded memory is allocated that cannot be de allocated and apparently more data loading occurs when fitting the model than I would expect since the increase with GPU disabled is far greater when running model.fit than when listing the contents of the trainset object . With some effort I found a way to export and reload the datasets after their creation which requires Eager execution so that I was able to run setup dataset dump the results restart python tf.compat.v1.disable eager execution reload the datasets setup model and fit it without Eager. Long story short it turns out the issue does not show up when Eager is disabled and the fitting goes slightly faster on CPU 250s epoch instead of 280s and obviously enabling GPU use makes for a great runtime gain with less than 80s epoch . So Eager execution messes things up badly... Why does that seem to be the endpoint of each and every issue I encounter these days Anyway I hope someone can find out where things go wrong with Eager enabled and how to fix this because disabling Eager is not exactly a fix just a workaround for the times being and an option I would personally like to keep in the future outside of the compat sub module but that is another question . Code to reproduce not including the functions defined in the aforeshared gist https gist.github.com pandrey fr c4ba8022c5dd956388e984f49c89ce61 First session Eager execution is enabled. Second session Oh and for the sake of it I tried fitting a model with Eager enabled after reloading the data from the .npy dumps and the memory issue is still there i.e. it is not caused by the use of dataset transformations in setup dataset . As I am still hoping that someone will pick up this issue I conducted yet additional testing namely replacing tf.keras.layers.Embedding with a subclass that overrides the call method in order to use one hot encoding and dot product to retrieve embeddings instead of tf.nn.lookup see code below . This does not fix the issue which therefore seems to be a general input tensors non discarding issue in Eager execution regardless of how they are created . The class I used to replace tf.keras.layers.Embedding inside the setup pseudo model function Again disabling Eager has everything run as smoothly as I want... I am happy to see some activity popping robieta you seem to be quite the expert on this kind of issue however I see the type bug tag was removed which in my humble opinion is incorrect this is not just a performance issue as is the case e.g. in 30561 this is a bug in the sense that training is not just slower it is made altogether impossible on some configurations namely when a GPU is visible or the amount of RAM is under 16 GB which it really should not given the size of the model and dataset . Hi Thanks for the report and the reproduction script. I am not able to reproduce the memory leak with the beta1 release on CPU with the script provided. I will try GPU next. The tutorial script itself does not seem to feature a memory leak either on CPU or GPU. Please try your reproduction script with your local configuration with the TF2.0 nightly build https pypi.org project tf nightly 2.0 preview So I was actually able to reproduce the problem on GPU but not CPU . Will investigate further. It seems that updating the TF version from beta1 to the latest nightly fixes the issue for me. Could you check if the update works for you as well Hey I just want to say that I experienced the same problems with memory leaks very high memory usage during first epoch and installing the nightly version instead of beta1 fixed it for me. Hi Thank you for the follow up on this. I currently am on the move and will only have access to the machine I ran those tests on next week but hopefully the nightly build comes with the rightful fixes indeed. I will post the results and hopefully close this issue next week. fchollet As suggested I installed a gpu enabled 2.0 nightly build from binary using pip version 2.0.0 dev20190731 git version v1.12.1 7529 g3e0ad8a004 and ran the test script again without the line disabling Eager execution both with and without GPU unfortunately I still encounter the same issue. When using the GPU I get the following error after the first 38 training steps When running solely on CPU the training runs but the RAM usage goes up as before i.e. very much during the first steps and more slowly but still up as further steps are run at the end of the first epoch I reached nearly 12 GB of RAM usage . The amount of RAM used remained stable during the second epoch with small up and down fluctuations seemingly related to the loading and discard of data batches which is normal . I also re ran the tests adding the tf.compat.v1.disable eager execution line which again avoids triggering the issue. On CPU RAM usage fluctuates between 2 and 2.4 GB and on GPU the available 4GB of dedicated memory are not exhausted and training goes way faster than the first steps run with both GPU and Eager enabled . pandrey fr This was resolved in 2.0.0rc0 and was tested internally. Can you check and let us know whether it was resolved for you when you use pip install tensorflow 2.0.0rc0 . Thanks Awesome I will test on my custom model tomorrow but as for the example case I initially shared it is indeed running just fine with 2.0.0rc0 . Many thanks and congratulations on the work already achieved towards the 2.0 release Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30952 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30952 No a As a conclusive note I ran my actual model with 2.0 rc0 comparing performances with and without disabling Eager execution. Most importantly I am happy to report that leaving Eager enabled no longer causes memory issues. Regarding fitting runtimes disabling Eager still yields a slight gain 122 seconds per epoch versus 145 for the first and 135 for the following ones when Eager is left enabled so around 10 runtime difference but this is a relatively small gap compared to what I encountered in 2.0b1. On the overall Eager now seems much more stable than a couple of months ago an impressive progress which must have taken a lot of hard work from all people involved so many thanks and congrats for that 
31175,TF2.0 memory leak caused by autograph retracing due to bound method argument, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Fedora 30 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below v2.0.0 beta0 16 g1d91213fe7 2.0.0 beta1 Python version 3.5.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Describe the current behavior Factoring out a train step function into our custom model lead to a memory leak due to continuously retracing the full autograph epoch dataset loop. The root cause was that bound instance methods are not identical model.step is not model.step . Describe the expected behavior According to Issue 36175 Identity of bound methods Python tracker https bugs.python.org issue36175 this is supposed to be expected behavior in Python. I merely want to clarify the autograph behaviour and document this for other tensorflow users. I guess there might be reasons to use identity is comparison on arguments when deciding whether an autograph needs retracing but if it were possible to use equality comparison for arguments that are methods this surprising behaviour could be avoided. Code to reproduce the issue Other info logs Possible workarounds store the bound method in a variable and thus keep using the same instance step fn model.step declare the method as staticmethod and pass the model instance explicitly as first parameter ,I have tried on colab with TF version 2.0 beta1 and was able to reproduce the issue.Please find the gist https colab.research.google.com drive 1KpbCu6lgchU1A zWcYc62m6Mxkpopz4l here.Thanks kkimdev alextp FYI This is a surprising and subtle behavior. I believe the proper fix would be to be resilient to this case by comparing functions using the equality operator instead of is In the mean time another workaround I would recommend is to use the model object as argument instead of methods I concur that the best workaround is to just pass the model. I don t think relying on equality comparison instead of identity comparison for functions is ideal since equality comparison is more expensive and this would create a harder to understand mental model. I concur that the best workaround is to just pass the model. Well our actual model has a train and eval step functions and this refactoring was the result of reusing the same code for metrics computation. Passing both the model and a staticmethod function is indeed the chosen workaround. The performance argument seems mood how would argument comparison play a measurable role w.r.t. executing an autograph function. But as mentioned I m not positive that working around this specific Python detail in TF would be worthwhile merely wanted to document it. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31175 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31175 No a 
31253,Using class weights in fit generator causes continuous increase in CPU memory usage until depletion. OOM, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04.2 LTS Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device Not Relevant. TensorFlow installed from source or binary TensorFlow version use command below tf nightly gpu 1.15.0.dev20190728 Python version Python 3.7.4 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version release 10.0 V10.0.130 NVIDIA SMI 418.43 Driver Version 418.43 CUDA Version 10.1 GPU model and memory GeForce RTX 2080Ti 11GB Describe the current behavior When using class weights in fit generator causes the training process to continuously consume more and more CPU RAM until depletion. There is a stepped increased in memory usage after each epoch. See below for the reproducible example. To keep the reproducible example small I decreased the size of the dataset and batch size which shows the trend of increasing memory. While training with my actual data it depletes the full 128GB RAM by 70 EPOCS. In the code below if you comment out the class weights the program trains without depleting memory. Describe the expected behavior Not leak memory. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. Memory Usage When class weights are used Mem with class weights https user images.githubusercontent.com 46456896 62335750 aea84900 b492 11e9 9b23 72b29c6f2499.png Memory Without class weights Mem wo class weights https user images.githubusercontent.com 46456896 62335751 aea84900 b492 11e9 9f2a ef9c296850ee.png ,Was able to reproduce the issue on Colab with Tf nightly gpu version. Please have a look at Colab link https colab.research.google.com drive 1udDjMNZ0AkbApQLy43fvQpXTvIih4Qlf . Thanks Any workarounds or updates this Thanks I m not able to reproduce this can you update to the latest tf nightly gpu package and try again Thanks talipini This was resolved in the pip install tf nightly gpu Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 20aeb317e65ecb8a0ec3e15812c41b10 tf31253 keras memory issue.ipynb . I am closing the issue here. Please feel free to open the issue if it persists again. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31253 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31253 No a Looks like new nightly build has fixed this issue. But training time is significantly longer about 30 in my case between 1.15.0.dev20190728 and 1.15.0.dev20190813. Same codebase takes 30 longer to train in the newer nightly build but it definitely seems to solve the memory issue though. I will try to create a reproducible example and open an new issue. Thanks for checking into this. talipini Sure. Thanks for your support. I m still having the same memory issue when using class weights with Keras model.fit generator. Running the stable version of TF 2.0. TKassis Please open a new issue with details about your issue platform details and a standalone code to reproduce the issue. Thanks TKassis Please open a new issue with details about your issue platform details and a standalone code to reproduce the issue. Thanks Thanks it seems multiple people have reported the issue already. TKassis do you have a link to the other issues I am not fining them. Thank you talipini Thanks for the issue Can you please try this again with the latest nightly pip install U tf nightly an report back I believe this issue should be fixed in the latest nightly 
31535,Colab TF 2.0 runs out of memory if eager execution is enabled, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Google Colab GPU Runtime TensorFlow installed from source or binary binary pip TensorFlow version use command below 2.0.0 beta1 Python version 3.6 Google Colab 3.6 Environment CUDA cuDNN version Google Colab GPU Runtime GPU model and memory Google Colab GPU Runtime Describe the current behavior If I use a large U net with an Inception ResNetv2 backbone and eager execution is enabled default with tf 2.0 the machine runs out of memory. If eager execution is disabled tf.compat.v1.disable eager execution it works fine. Maybe this is a bug in Google colab as the code works fine with eager execution enabled on my machine with 8 GB of RAM and a Geforce 1060. Describe the expected behavior I would expect it to work in both modes similar. Code to reproduce the issue https colab.research.google.com drive 1 xgkBBUqZw6rS2WhxUMBZuY6j1uxG e , kielnino I tried to reproduce your code in Google Colab but i am getting the below error.Please help me to reproduce the issue. image https user images.githubusercontent.com 51902062 62949114 3c752400 be03 11e9 91ab 614ced24a032.png ravikyram Thank you for having a look. I forgot the efficientnet module in the pip installs. The colab is updated now. kielnino I have reproduced the issue in Colab with TF 2.0 beta1 in eager mode i got the below error OSError Errno 12 Cannot allocate memory .However i tried by disabling eager execution it was stuck for more than 30 mins after 1st epoch and it is till executing. Can you please confirm Thanks Yes thats another deadlock issue with the tf.keras.utils.Sequence class and enabled multiprocessing which I m investigating at the moment. I ve disabled multiprocessing in the Colab now so that it should work with eager execution disabled. I am able to execute colab link provided with eager execution disabled.Please find the gist https colab.research.google.com drive 15MT 9pG2UBiXRrkUDXW5BGoQGzw0BpBS here.Thanks kielnino I don t see any issue when I replace pip install q tensorflow gpu 2.0.0 beta1 with pip install q tf nightly gpu 2.0 preview . Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 0ef0f98f59b3496bab443546511fac02 tf 31535.ipynb . Thanks Your are right. Thanks for the hint. So I will use the nightly build instead. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31535 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31535 No a 
31842,Tensorflow2.0 Training OOM when tf.function retraces excessively, em When I am training my model using tf.function CPU memory is leaking and after some steps of training it is getting killed. em System information OS Platform Linux Ubuntu 16.04 TensorFlow installed from source or binary binary TensorFlow version 2.0 beta Python version 3.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 10.01 GPU model and memory p100 16GB When is graph mode of training using tf.function CPU memory gets leaked and after some steps it gets killed but when I train my model using eager mode it works fine I am using above code for graph mode training as recommended by TensorFlow 2.0 community. ,Hi akanyaani is the shape of x varies between each batch Yes The shape of x varies between each batch. That s the reason why Because tf.function will re trace the graph due to the variable sequence lengths or variable batch sizes the number of cached graphs will increase over time if it do not see this shape before. Thus OOM occurred. To avoid re tracing you might want to specify the input signature . For detailed explanation please refer to https www.tensorflow.org beta tutorials eager tf function https www.tensorflow.org beta tutorials text transformer search tf.function Thanks for your prompt response. But for that I was calling this method for prevent tensorflow to create new graph tf.compat.v1.get default graph .finalize But let me try with the input signature It tried with tf.function input signature tf.TensorSpec shape None None dtype tf.int32 with variable sequence length but still it is happening but when I run with a fix sequence length it works fine but it will cost extra computation. akanyaani Can you please provide complete code snippet to reproduce the issue reported here.Thanks focus on Hi My model has transformer based encoder decoder as explained here. https www.tensorflow.org beta tutorials text transformer url Hi akanyaani Could you provide the complete code ideally minimized that we can run and reproduce Thanks. Also if you have been passing Python arguments you can try passing Tensors instead https www.tensorflow.org beta tutorials eager tf function python or tensor args Hi kkimdev You can produce the error using this repo by making dynamic batch padding. padded shapes 1 1 https github.com akanyaani gpt 2 tensorflow2.0 akanyaani it looks like the general suspicion is that subsequent calls to train step cause the function to be re traced a slow process that should only happen once or twice likely due to the parameter configuration. For example it could be because the shape of x varies but it could also depend on the value of step . I m not sure I can completely follow your instructions for reproducing it but here is a way to diagnose this issue add a print call to train step like so Since the pure Python functions like print are only called during tracing it s an effective way to tell whether the function is re traced too much. If it does it also helps explain why. We ve just updated the API docs as well with a bit more relevant information it s worth checking out here https github.com tensorflow tensorflow blob master tensorflow python eager def function.py L989 Hi mdanatg Sorry for my late response Actually I tried with one parameter also but the same thing was happening. So I found that the sequence length of x was the culprit. It was working fine with fix sequence length. And I have noticed that if you restart training with different batch size same problem happens. Thanks Just to double check you are still seeing the leak if you try And you see just one tracing message at the console I met the same problem. Did you solve the same problem Hi zxk19981227 Can you give me some idea about your model architecture then I will be able to give you the answer. Thanks mdanatg I have use print in train step it shows that each epoch has a print work. and my code killed because of OOM. are there some solutions for it thank you in advance. I m sorry to hear that. But I solved my problems by turning off auto trace. Maybe your problem is not the print but the auto trace. You can try this. I hope it helps anna notifications github.com 2020 3 8 6 30 mdanatg https github.com mdanatg I have use print in train step it shows that each epoch has a print work. and my code killed because of OOM. are there some solutions for it thank you in advance. You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 31842 email source notifications email token AJTMZW4BHJKNCR5OCKDNWXDRGNXVLA5CNFSM4IOHPLUKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEOESGFY issuecomment 596189975 or unsubscribe https github.com notifications unsubscribe auth AJTMZW3KTZJJQKQFWOLYSDTRGNXVLANCNFSM4IOHPLUA . akanyaani Could you please update on the above comment let us know if the issue still exist. akanyaani Could you please update on the above comment akanyaani Automatically closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 31842 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 31842 No a That s the reason why Because tf.function will re trace the graph due to the variable sequence lengths or variable batch sizes the number of cached graphs will increase over time if it do not see this shape before. Thus OOM occurred. To avoid re tracing you might want to specify the input signature . For detailed explanation please refer to https www.tensorflow.org beta tutorials eager tf function https www.tensorflow.org beta tutorials text transformer search tf.function Great it helps. Could you please add a warning in this function to alert users 
31871,TF 2.0 dev20190820 consumes more memory than dev20190504 when training keras model using train on batch , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 x64 1903 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary pip TensorFlow version use command below 2.0.0 dev20190820 Python version 3.6.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version CUDA 10 cuDNN 7.6.2.24 GPU model and memory Geforce RTX 2080 ti 11GB Describe the current behavior Program crashes due to insufficient memory issue. But with same code dev20190504 can train the model without any problem. Describe the expected behavior Can train large model using dev20190820 without memory issue like dev20190504. Code to reproduce the issue ,I have tried on Colab with TF version 2.0.0 dev20190820 and was able to reproduce the issue.I am able to train the model using 2.0.0 dev20190504 without any problem.Please find the gist https colab.research.google.com drive 1y4Z77hmagOig2fqVN1tnp2vOnXHdhuZg here.Thanks tensorflow gpu 2.0rc1 still has this issue. robieta jvishnuvardhan Is there any news for this issue I can t test latest RC release due to this problem. tensorflow gpu 2.0rc2 still has this issue. tensorflow gpu 2.0.0 still suffers this issue. robieta jvishnuvardhan TF 2.0.0 is released today and this issue sill is there. Any news or is it not a bug but TF2.0 s intended behaviour This should be fixed by https github.com tensorflow tensorflow commit 28d07261622a589b7f427756677c2178ccaca7fb. Can you check whether it resolves your issue robieta I can confirm that when I ran KichangKim code with tf nightly code runs without any issue. Here https colab.sandbox.google.com gist jvishnuvardhan 5e8af6cf47aee62ec1596328d8fd84ba untitled134.ipynb is the gist. Thanks I checked tf nightly gpu 2.1.0.dev20191112 and it worked correctly. Thanks. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 31871 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 31871 No a 
32420,TF2.0 Multiple calls to Keras .fit and .evaluate makes RAM explode and is 25x slower, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOS Mojave Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary pip TensorFlow version use command below 2.0.0 dev20190909 Python version 3.6.5 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Describe the current behavior Everything is done on the CPU Consecutive calls to either .fit or .evaluate increase the RAM used even if calling with the same data. The calls takes approximately 10 times longer than with TF1.x Describe the expected behavior I expect the RAM usage to remain constant just like in TF1.x Code to reproduce the issue Using TF2.0. 229MB RAM used and evaluate completed in 99ms Using TF2.0 1508MB RAM used after calling .evaluate 3312 times. Using TF1.x the RAM used is not increasing over consecutive calls of .evaluate . RAM stays at 176MB indefinitely iteration 5100 below . Also note that it is 25 times faster I just discovered that wrapping x y into a tf.data.Dataset does not have this issue Modified code Using tf.data.Dataset there is no exploding RAM. 217 MB RAM used after calling .evaluate 8154 times. It is also only 2.5 times slower than TF1.x , grananqvist I tried reproducing the issue. I am attaching the gist https colab.sandbox.google.com gist ravikyram 211ae1084a4b922d38cf36bd122ffcad untitled180.ipynb for your reference. Is this the expected behavior .Thanks From what I see in the notebook using profile to measure the RAM consumption does not work in jupyter grananqvist Sorry for the delay in response. Can you please try tf nightly and compare that with TF1.15.0rc3 . I see similar runtime 0.004 with tf nightly and TF1.15.0rc3 . Please let us know what you think. Thanks grananqvist Did you had time to review my comments Thanks Sorry I have not had the time to test this yet. Automatically closing this out since I understand it to be resolved but please let me know if I m mistaken.Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32420 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32420 No a Would using tf.function help in this situation 
32500,Memory continues to grow after repeated calls to model.predict tf.one hot states dtype float32 depth 3 , em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOS Mojave 10.14.6 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary tensorflow 2.0.0 rc1 TensorFlow version use command below v2.0.0 rc0 101 gd2d2566 2.0.0 rc1 Python version 3.7.4 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Intel Iris Plus Graphics 640 1536 MB You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior Running the below code in Docker version 19.03.2 causes the memory to grow without limit. This is visible in docker stats eventually crashing docker. Describe the expected behavior The memory should not grow indefinitely Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. The aforementioned code runs in an image generated by the following Dockerfile ,I replicated the issue on colab with Tf 2.0.0.rc1. Please take a look at colab gist https colab.sandbox.google.com gist gadagashwini cc35a1c57a228661157480b1ba307116 untitled150.ipynb . Thanks I can reproduce this issue with the same setting. If I add keras.backend.clear session after each call to model.predict it is getting super slow. still a bug after upgrading to tensorflow 2.0.0 LuisSaybe Thank you for the repro case I added some memory tracking to it and found a pretty clear linear increase in memory utilization of the model object you ve defined. It does seem to be fixed in TF 2.1.0 so I guess I ll be waiting until that s released to move off of 1.14. memoryleak https user images.githubusercontent.com 16674595 68467202 65773c00 01db 11ea 907d d15d133654b7.png memoryleak21 https user images.githubusercontent.com 16674595 68467208 6a3bf000 01db 11ea 97e5 c9cc00d5eb61.png This issue has been fixed recently. LuisSaybe Could you try tf nightly to verify it Thanks LuisSaybe Is this still an issue. I ran it in colab for 25000 iterations without any issue. Thanks will check today sorry was on vacation yhliang2018 yes it is fixed now in tf nightly thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32500 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32500 No a 
32709,out of memory flood on the simplest op, System information OS Platform and Distribution e.g. Linux Ubuntu 16.04 win7 x64 TensorFlow installed from source or binary pip TensorFlow version use command below 2.0.0rc1 Python version 3.7.4 CUDA cuDNN version 10.0 3.7.5 GPU model and memory GTX 1060 6GB Describe the current behavior results infinite flood Describe the expected behavior expected to work properly Code to reproduce the issue Other logs , iperov Tried running the code in colab for tf 2.0rc1 did not face any error please take a look at the gist https colab.sandbox.google.com gist oanush f92701b1542fca43d2d1f66b0f262077 32709.ipynb of the colab. kindly share us the gist if the issue is replicating.Thanks oanush did you try it with cudnn 7.5.0 I installed 7.4.1 and all is fine. also problem in windows not in linux colab. Read System information. Are you using the GPU device only for this compute task Seems like an issue in your GPU not in TensorFlow mihaimaruseac are you troll issue in my GPU srsly I thought developers of tensorflow are a bit smarter. this is simple task to reproduce the bug. This bug is reproduced only with cudnn 7.5.0 on windows but 7.4.1 works fine. No not a troll. Please always assume good intent as I was basing my question on https stackoverflow.com questions 34514324 error using tensorflow with gpu 34514932 34514932 That being said can you also post output of nvidia smi on both 7.5.0 and 7.4.1 before and after running your script nvidia smi on both 7.5.0 and 7.4.1 nvidia smi does not depend on cudnn. cudnn is only the one dll that loaded by python environment. I can pass to it any version I want. So 7.5.0 has bug but 7.4.1 has not. It doesn t depend on cudnn but it also shows the state of the GPU so we can detect if there is a memory leak. I don t know why but bug is no more reproducable Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 32709 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 32709 No a So it turns out it was the state of your system. or the hidden bug of tf Doubtful to be a hidden bug of TF as it seems to always happen on your issues but if it reproduces again please post full details and debug how much memory your GPU still has available. 
33009,Memory leak , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes see below OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 also demonstrated in Windows 7 TensorFlow installed from source or binary Binary TensorFlow version use command below v2.0.0 rc2 26 g64c3d38 2.0.0 problem disappears on v1.14.0 rc1 22 gaf24dc91b5 1.14.0 Python version 3.6.8 CUDA cuDNN version Cuda v10.0 CuDNN v7.6.2.24 GPU model and memory Nvidia GeForce 840M but problem persists in non GPU version Describe the current behavior When creating a trivially simple model and then entering a loop that calls predict with dummy input memory consumption increases indefinitely over time. On my system a model with a single hidden layer of only 32 nodes will consume all available system RAM 10gb after only 10 minutes. The problem happens on v2.0 GPU or CPU of tensorflow but disappears when running identical code on v1.14. Describe the expected behavior Expect memory consumption to quickly stabilize but it never does. Code to reproduce the issue ,I have also experienced this critical issue. I have managed to get around this error by using model.predict on batch instead of model.predict . This returns an object of type class tensorflow.python.framework.ops.EagerTensor not a numpy array as claimed in the docs https www.tensorflow.org api docs python tf keras Model predict on batch but it can be cast by calling np.array model.predict on batch input data to get the output I want. Side note I also noticed a similar memory leak problem with calling model.fit in a loop albeit with a slower memory accumulation but this can be fixed in a similar way using model.train on batch . I think same error here. This code https github.com ipsec dqn tf2 blob master nchain train.py not finishing. MProx Can you confirm if the issue is resolved with the workaround Thanks Hi. Yes it did. I tested it overnight with no leak. I m not sure why the original functions do not work as expected but the workaround has fixed my problem. This issue is NOT resolved. I use predict on batch and still get an OOM error after some time of processing data. Interesting. I think that OOM is telling you your GPU memory is being depleted and that sounds like a different problem because mine was filling up system RAM not GPU. Using predict on batch and train on batch I ran 800k iterations of my model over 10 hours last night and it seemed to work fine. Another thing is that I had to set the environment variable TF FORCE GPU ALLOW GROWTH true to avoid tensorflow errors on startup. Maybe try that Yeah I don t have any critical errors when running on CPU. It s definitely GPU memory that s leaky. The GPU memory doesn t deplete immediately but only after some number of calls to predict on batch . I am able to train using fit with no problems which runs over that same data set over many epochs. I can confirm actually that when run on CPU the leak still persists. In that case I m going to re open this issue so that maybe the TF team can help you. Issue replicating for TF 2.0 kindly find the gist https colab.sandbox.google.com gist oanush 07da0eef3fbfbd8ff74e6c840ad5b6d5 33009.ipynb of colab.Thanks This is an issue with how certain object lifetimes are being managed in the tf function cache. Behind the scenes Model.predict is creating some functions which aren t being spun down properly which is why there is a small but consistent leak each iteration. We are currently working on a fix. robieta would the particular problem that you mentioned affect GPU memory or just system memory Is there a Github Issue associated with it I was also running a predict loop and getting OOM errors after many iterations. Using predict on batch instead eliminated this problem. It also sped up the prediction rate by a factor of 4. Thanks for the workaround ialdencoots could you confirm whether or not you get an OOM error using the specific example code that MProx provided in the issue description except using predict on batch novog Hmm using the code that MProx provided with predict on batch I do not get the OOM error. In my considerably less minimal code where I am feeding inputs via a TFRecordDataset and definitely using predict on batch I still encounter the memory leak. Ok so I messed around a bit w my model and I ve narrowed down the problem a little bit more. When I load my trained model from an .h5 file and run predict on batch repeatedly I get the OOM error eventually. However if I create a new model with the loaded model s inputs and outputs I can predict on batch for my whole dataset without problem. If I then run compile on the model with an optimizer and loss I get the OOM error again. So it seems to be a problem only for compiled models. ialdencoots It sounds like you might be having a different OOM issue than the author of this issue and I were having. Incidentally in my case I am also loading my model from an .h5 file and predict on batch fixed it. I would suggest trying to create a simpler version of your code that still has the problem to narrow down the cause and post it as a separate issue. MProx I would recommend leaving this issue open. Although we have a workaround there is a defect in the TensorFlow version of Keras s predict function that should be fixed. In case it helps this issue does not appear to be present in release 2.0.0.alpha0. You can confirm this by running oanush s Colab with that version. Please switch to 2.0.0 final release . You might need to upgrade pip and setuptools to get it but 2.0 has been available for a while no need to still be on alpha I know. But this problem along with several others is not present on alpha. Oh you meant that the breakage happens in between alpha and rc2. If it s not too much to ask would you be willing to compare rc1 The issue does appear to be present in rc1. I went ahead and tried beta0 as well and it does not seem to be present there. EDIT It is also present in rc0 but not in beta1. Thank you. so the failure is in between beta1 and rc0. Unfortunately we fast forwarded the branch in between these two points so probably we got the failure from master. One more test please. Is the issue also manifesting on latest master Oh I see robieta already pinned the issue https github.com tensorflow tensorflow issues 33009 issuecomment 539226841 . Apologies for the extra tests we are expecting this to be fixed on master soon if not already No worries about the tests. I just hope a new release with the fix comes out soon. It s worth noting that the solution that I was referring to will only fix CPU OOM it wouldn t explain a GPU leak. I was experiencing the same issue on the 2.0.0 docker latest gpu py3 image I changed to the nightly image tensorflow tensorflow nightly gpu py3 to be exact and the problem seems to be fixed no more memory leak So anyone else here looking for a workaround until the changes are reflected in the stable release I d try this out Avoiding model.predict and using model.predict on batch solves the Memory Error for me. Here s an example to create batched predictions on your test set. Note that if only pre allocating the results array already results in a MemoryError then simply the array does not fit in your available memory regardless of the memory leak issue. model.predict on batch solved this problem for me too Same issue with TF 2.0 stable Solved with tf.compat.v1.disable eager execution 
33030,Memory leak on TF 2.0 with model.predict or and model.fit with keras, System information OS Platform System Version macOS 10.14.6 18G103 Kernel Version Darwin 18.7.0 TensorFlow installed from binary using pip install tensorflow Python version GPU model and memory No GPU TensorFlow version Describe the current behavior While running using tensorflow 1.14 or theano backends this code works fine. After upgraded to tensorflow 2.0.0 it stops working and memory usage increasing without finish the program. Describe the expected behavior Using theano I get 28 seconds by iteration. Using tensorflow 2.0.0 I expect same behavior or better . Code to reproduce the issue ,I am able to reproduce the issue in colab using TF 2.0.0 rc2. Please find the gist here https colab.sandbox.google.com gist ravikyram ca9f659a22917431c694527e76decee0 untitled244.ipynb .Thanks I have that same problem Same problem here as well. No issues with 1.14 but suddenly appeared when I installed 2.0. Got OOM with 2.0 no issues with 1.14 Adding robieta who is an expert I reported the same issue here with an even simpler test case on 2.0.0 https github.com tensorflow tensorflow issues 32500 It didn t happen in tensorflow 2.0 alpha but in 2.0. pip install tensorflow gpu 2.0.0 got memory leak pip install tensorflow gpu 2.0.0 alpha everything s fine I am using tensorflow gpu 2.0.0 I spent 1 full day in checking my code for memory leaks and eventually I spotted the leak with tf.model.predict ... By the way I used memory profiler https pypi.org project memory profiler for profiling my code. Thank God I am not alone with this issue. Welcome Also experiencing this issue. It seems some tensors are being kept in GPU memory. Following. Hi guys Any progress Maybe a temporary fix Hi there were a couple of recent fixes related to this https github.com tensorflow tensorflow commit 082415b2ff49bfb8890f7d5361585bac04749add https github.com tensorflow tensorflow commit c2fc448fe253bc59d3f0417d7d08e16d53f2a856 kkimdev Great I think my problem is probably due to a leak in Dataset.map because it does not leak when I use Keras Sequence generators. can i get the tensorflow whici is fixed the memory leak from conda Will the fixes be added to the current release any time soon Thanks for checking in We re still verifying the fix solves the issue and should have updates soon. Same issue here traced back to model.predict If I replace that inference with a static value the leak goes away so it must be in there. I tried to just blast away the model and copy the weights over but the leak persists even with the model deleted. rchao Thanks for working on this. Any updates are duly appreciated My workaround for the memory leak in dataset.map is not to use tf.data API stick with Keras Sequence data generators. And a workaround for the leak in model.predict call model.predict on batch which does not have a leak. I am also facing this problem where I am using a TF model and running in a loop for cross validation. In each iteration of the loop the memory used grows although it is the same model and it shouldn t. As a result after few iterations the code returns segmentation fault. Looking for a fix on this or a workaround. I am using model.fit . This did not happen in TF1.14. tf gpu Attached the memory profile for same code using different TF versions. The drop and rise occurs at each epoch tf1 14 memory profile https user images.githubusercontent.com 6624314 69166778 c33a4b00 0b2e 11ea 8491 1e0e400bcd93.png tf2 memory profile https user images.githubusercontent.com 6624314 69166792 c7666880 0b2e 11ea 8da7 c9ce665a72c0.png Thanks for the updates I verified locally that the memory issue has been mitigated with TF nightly. Can you try pip install tf nightly and see if it resolves your issue I am using tf nightly gpu and I still face the same issue Name tf nightly gpu Version 2.1.0.dev20191119 The memory profile still keeps increasing image below with each training fold which was not observed in TF1.X tfnightly2 x memleak https user images.githubusercontent.com 6624314 69220250 7cd90080 0baf 11ea 9184 80c4b5aa9b25.png I haven t tested with tf nightly non gpu version yet UPDATE Also the same outcome with tf nightly. Thanks SivamPillai for the update. We ll continue to investigate into this. rchao nightly fixed it for me Thanks birdmw Can you share with us the version of TF you re using I forget. It s whatever the latest nightly was on Saturday the 16th this last weekend. On Thu Nov 21 2019 12 49 PM Rick Chao notifications github.com wrote Thanks birdmw https github.com birdmw Can you share with us the version of TF you re using You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 33030 email source notifications email token AAWKJTBCQLD4XH5XY2JWOELQU3X47A5CNFSM4I5HVD6KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEE3TIHA issuecomment 557265948 or unsubscribe https github.com notifications unsubscribe auth AAWKJTGINVQREMQ5WTSML7LQU3X47ANCNFSM4I5HVD6A . Yes if you can tell me the commit version or something I can experiment and share the results here. Thanks this should be fixed in 2.1.0 rc0 Closing the issue as it s confirmed fixed. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33030 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33030 No a Unfortunately it is not fully fixed for me in 2.1.0 rc0 . The memory usage for the code posted by ipsec has greatly improved but it works almost 10x slower with 2.1.0 rc0 than with 1.14.0 . rchao should I post more details here or open a new issue 
33174,TF2.0 OOM error training imagenet with vgg fine when eager execution off, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04.3 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary source TensorFlow version use command below 2.0 Python version 3.7 Bazel version if compiling from source GCC Compiler version if compiling from source 7.4.0 CUDA cuDNN version V10.0.130 GPU model and memory GeForce RTX 2080ti 10G You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior Getting OOM error when training vgg16 on imagenet with batch size 256. Was able to get away with OOM error with batch size 8 or turning off eager execution. Describe the expected behavior The exact same code was running fine with tf1.14 with no memory problem Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,I could reproduce the issue with tensorflow 2.0.0. Please see the colab gist https colab.sandbox.google.com gist gadagashwini e1417d6841204fb99b7056c8398b551a untitled189.ipynb . Thanks don tpanic Please check the response https github.com tensorflow tensorflow issues 32707 issuecomment 539246480 from robieta for similar OOM issue with 2.0 and no OOM when same code ran in graph mode. The response has a nice presentation on improving the code for better performance. Thanks To add to jvishnuvardhan s answer on why eager matters see https github.com tensorflow tensorflow issues 33024 which diagnosed that the Model. generator methods are running eagerly in 2.0. I am currently working on aliasing them to their normal counterparts Model.fit generator Model.fit etc which will restore graph based optimizations. Including those around memory use. In the mean time you can switch to Model.fit and Model.evaluate directly in your code. Sorry for the inconvenience. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33174 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33174 No a 
33178,Potential memory leak when using LSTM TimeDistributed, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution Windows 10 TensorFlow installed from binary TensorFlow version v2.0.0 rc2 26 g64c3d382ca 2.0.0 Python version 3.6.9 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version 10.0 CUDA 7.5 cuDNN GPU model and memory TITAN RTX 24GB Exact command to reproduce N A Describe the problem Potential memory leak when using LSTM TimeDistributed I have a standard time series model that consists 3 layers of convolutional layers feeding into 2 LSTM layers. Up until now I have had no problems mapping a Dense layer to the last output of the top LSTM and making a prediction etc. However I want to implement a model where I use a TimeDistributed Dense .. layer on top of the top LSTM and feed back the error signal at each time point. I have implemented this but after only training a few epochs I get a resource exhausted error. It doesn t seem to be affected by how small I make the model after training for a few epochs. The error I get is ResourceExhaustedError OOM when allocating tensor with shape 25600 9 11 128 . This comes after a call to tape.gradients full error reported in section below . In my non TimeDistributed I monitor the number of objects via len gc.get objects and during training the object count remains the same as expected but when I only change the model to handle this TimeDistributed change i.e. making sure the labels are correctly repeated and making return sequences 1 for the top level LSTM then all of a sudden at each training epoch thousands of new variables are being added during each epoch of training. gc objects 249861 TRAIN End epoch 0 loss 0.693269372 train accuracy 0.5 TEST End epoch 0 loss 0.691318274 test accuracy 0.500683606 gc objects 251746 1885 objects TRAIN End epoch 1 loss 0.691800237 train accuracy 0.500202894 TEST End epoch 1 loss 0.690349817 test accuracy 0.502343774 gc objects 254144 2398 objects TRAIN End epoch 2 loss 0.690762699 train accuracy 0.500456572 TEST End epoch 2 loss 0.689480364 test accuracy 0.504296899 gc objects 254996 852 objects TRAIN End epoch 3 loss 0.692312837 train accuracy 0.501090705 TEST End epoch 3 loss 0.689140499 test accuracy 0.505468726 gc objects 269643 14647 objects TRAIN End epoch 4 loss 0.688487 train accuracy 0.501116097 TEST End epoch 4 loss 0.686942577 test accuracy 0.508886695 gc objects 270444 801 objects So in 4 epochs of training while no other process is running 20 583 new objects were created and I presume resulted in this resource exhausted error. I ve tried to force the garbage collector to collect any unused variables but the object count increases whether this is included or not. I ran a snapshot comparison from the tracemalloc library which I will include below as it might be helpful it wasn t to me . Something is creating variables during every epoch vastly using up all the memory and not releasing them leading to this resource exhausted error. This doesn t occur if I don t use TimeDistributed so I don t think anything about this layer requires the creation of additional memory hungry variables. It looks more like a leak. Do you have any idea of what I could do to alleviate this problem It seems like a bug fix at a technical level. Maybe there is a technical solution. Please let me know if any further info from my end would be useful in looking at this issue. Source code logs tracemalloc top 10 differences between snapshot calls at adjacent epochs C Users AXM1390 AppData Local Continuum anaconda3 envs tf2 lib site packages tensorflow core python eager execute.py 61 size 111 KiB 69.9 KiB count 677 426 average 168 B string 14 size 7464 B 46.9 KiB count 107 749 average 70 B C Users AXM1390 AppData Local Continuum anaconda3 envs tf2 lib tokenize.py 609 size 2944 B 43.6 KiB count 46 698 average 64 B C Users AXM1390 AppData Local Continuum anaconda3 envs tf2 lib site packages tensorflow core python framework tensor shape.py 193 size 59.9 KiB 33.8 KiB count 1305 732 average 47 B C Users AXM1390 AppData Local Continuum anaconda3 envs tf2 lib site packages tensorflow core python training tracking data structures.py 768 size 54.0 KiB 31.3 KiB count 386 219 average 143 B C Users AXM1390 AppData Local Continuum anaconda3 envs tf2 lib site packages tensorflow core python framework tensor shape.py 718 size 55.7 KiB 30.8 KiB count 1018 564 average 56 B C Users AXM1390 AppData Local Continuum anaconda3 envs tf2 lib site packages tensorflow core python framework tensor shape.py 776 size 51.0 KiB 28.7 KiB count 1235 690 average 42 B C Users AXM1390 AppData Local Continuum anaconda3 envs tf2 lib site packages tensorflow core python keras utils generic utils.py 564 size 40.9 KiB 25.8 KiB count 675 426 average 62 B C Users AXM1390 AppData Local Continuum anaconda3 envs tf2 lib site packages tensorflow core python framework ops.py 1035 size 39.3 KiB 23.3 KiB count 950 566 average 42 B C Users AXM1390 AppData Local Continuum anaconda3 envs tf2 lib site packages tensorflow core python training tracking data structures.py 809 size 27.1 KiB 15.9 KiB count 3 0 average 9248 B full error ResourceExhaustedError Traceback most recent call last ipython input 14 422df5497d33 in module 1 best val best epoch tmp history run model once 0 25 epochs 50 ipython input 12 61614e3bb222 in run model once start end epochs 36 printed cm False 37 38 train loss val loss acc metric val acc metric train RCNN model optimizer train ds test ds cm 39 tf.print f TRAIN End epoch i loss train loss train accuracy acc metric.result 40 tf.print f TEST End epoch i loss val loss test accuracy val acc metric.result ipython input 11 00ed72fa26fb in train model optimizer train ds test ds cm 60 for x true y true in train ds 61 if TIME DISTRIBUTED 62 train loss train one step timedistributed model optimizer x true y true training True 63 else 64 train loss train one step model optimizer x true y true training True ipython input 11 00ed72fa26fb in train one step timedistributed model optimizer x true y true training 22 print f model trainable variables len model.trainable variables 23 24 gradients tape.gradient loss model.trainable variables 25 optimizer.apply gradients zip gradients model.trainable variables 26 AppData Local Continuum anaconda3 envs tf2 lib site packages tensorflow core python eager backprop.py in gradient self target sources output gradients unconnected gradients 1012 output gradients output gradients 1013 sources raw flat sources raw 1014 unconnected gradients unconnected gradients 1015 1016 if not self. persistent AppData Local Continuum anaconda3 envs tf2 lib site packages tensorflow core python eager imperative grad.py in imperative grad tape target sources output gradients sources raw unconnected gradients 74 output gradients 75 sources raw 76 compat.as str unconnected gradients.value AppData Local Continuum anaconda3 envs tf2 lib site packages tensorflow core python eager backprop.py in gradient function op name attr tuple num inputs inputs outputs out grads skip input indices 136 return None num inputs 137 138 return grad fn mock op out grads 139 140 AppData Local Continuum anaconda3 envs tf2 lib site packages tensorflow core python ops math grad.py in TanhGrad op grad 712 with ops.control dependencies grad 713 y math ops.conj y 714 return gen math ops.tanh grad y grad 715 716 AppData Local Continuum anaconda3 envs tf2 lib site packages tensorflow core python ops gen math ops.py in tanh grad y dy name 11410 else 11411 message e.message 11412 six.raise from core. status to exception e.code message None 11413 Add nodes to the TensorFlow graph. 11414 op op def lib. apply op helper AppData Local Continuum anaconda3 envs tf2 lib site packages six.py in raise from value from value ResourceExhaustedError OOM when allocating tensor with shape 25600 9 11 128 and type float on job localhost replica 0 task 0 device GPU 0 by allocator GPU 0 bfc Op TanhGrad , algrmur Can you share a simple and standalone code to reproduce the issue reported here Thanks Hi algrmur There are a couple of bugs in TF2.0 one possible reason could be variable sequence length. It would be great if you can share sample code which can produce the same error. Hello oanush and akanyaani Thanks for your replies. I ve stripped down all of the non essential code in my program and set it up to use randomly generated data to recreate the problem. Please see below for 1 code and 2 output given for me. Imports Data Generation Model definition Loss Optimiser tf.Dataset Model instantiation Training functions Running the model The output of this shows that many objects are being created during the runs of the epochs. Output akanyaani Thanks for your comment about it potentially being related to variable length input but in my case and also in the case of the dummy code above there is no variable length input as there are always 100 time points in the data I am using. If you take out the 3 lines using TimeDistributed in the bottom of the model s call function alter the call to the loss function to use y true and not y true expanded set the last LSTM s return sequences parameter to 0 then the same code runs while keeping the amount of objects relatively stable as seen below. This is what made me sure it was related to TimeDistributed. Output without using TimeDistributed I hope you can also recreate the issue and thereby potentially see where the problem might lie. Any advice solutions would be greatly appreciated Kind regards Alex Hi algrmur Screenshot 2019 10 10 at 5 16 14 PM 2 https user images.githubusercontent.com 11317416 66566131 cdf1df80 eb81 11e9 959a 864fb9106a46.png It works fine on my system could you please try with smaller batch size. Hi akanyaani Interesting I tried via notebooks command line etc. and it always gave the same error. I will try it on my Linux laptop to see if it also breaks there. Do you mind if I ask about your specs so I can see what else I might be able to try mainly just CUDA cuDNN version and what version of Python you used Then I can try on Windows with the same things running as you as that might fix my problem. Once I am back at my main workstation tomorrow I will try with a lower batch size to answer your question as to whether that might be the problem. I didn t think it would be because TimeDistributed Dense .. uses the same weights for each time step so I thought the computation would be equivalent in terms of the gradient call to the non TimeDistributed case that does work . I could be wrong though. Furthermore I don t know why it would be fine with the first 5 6 epochs and then fail afterwards. If it can handle the first few nothing new should be done during the training so I still have no explanation as to how the OOM could occur. More information tomorrow Thanks again I am also seeing a memory leak. No LSTM though just TimeDistributed. This model fails after printing 11 on a 2080Ti. Batch Size is always 1 so that s not the problem. EDIT Shrunk minimal example by quite a bit. Is now 1 Conv1D layer and 1 TimeDistributed Dense layer. from tensorflow.keras.models import Model from tensorflow.keras.layers import import tensorflow.keras.backend as K import numpy as np import tensorflow def BuildGenerator i Input shape None 2 n input 12 21 to n Input shape n input s n Dense 12 21 activation softmax to n s n Reshape 12 21 s n n base Model inputs to n outputs s n b Conv1D n input 11 dilation rate 1 padding same activation relu data format channels last i n TimeDistributed n base b return Model inputs i outputs n def InputGenerator for iter in range 1000 print iter i np.zeros 1 10 60 1000 2 n np.zeros 1 10 60 1000 12 21 yield i n with tensorflow.device device gpu 0 m2t BuildGenerator m2t.compile optimizer adam loss mse for epoch in range 1 for inout in InputGenerator m2t.train on batch inout 0 inout 1 So does this line do what I think it does https github.com tensorflow tensorflow blob ed04c8639d53c284874240d58abba725865f454e tensorflow python keras layers wrappers.py L250 https github.com tensorflow tensorflow blob ed04c8639d53c284874240d58abba725865f454e tensorflow python keras layers wrappers.py L250 Because that looks like it s permanently storing the inputs in a map that never gets cleared using a global UUID as the key. Hi akanyaani Interesting I tried via notebooks command line etc. and it always gave the same error. I will try it on my Linux laptop to see if it also breaks there. Do you mind if I ask about your specs so I can see what else I might be able to try mainly just CUDA cuDNN version and what version of Python you used Then I can try on Windows with the same things running as you as that might fix my problem. Once I am back at my main workstation tomorrow I will try with a lower batch size to answer your question as to whether that might be the problem. I didn t think it would be because TimeDistributed Dense .. uses the same weights for each time step so I thought the computation would be equivalent in terms of the gradient call to the non TimeDistributed case that does work . I could be wrong though. Furthermore I don t know why it would be fine with the first 5 6 epochs and then fail afterwards. If it can handle the first few nothing new should be done during the training so I still have no explanation as to how the OOM could occur. More information tomorrow Thanks again algrmur Any update on the issue Thanks Hi akanyaani Interesting I tried via notebooks command line etc. and it always gave the same error. I will try it on my Linux laptop to see if it also breaks there. Do you mind if I ask about your specs so I can see what else I might be able to try mainly just CUDA cuDNN version and what version of Python you used Then I can try on Windows with the same things running as you as that might fix my problem. Once I am back at my main workstation tomorrow I will try with a lower batch size to answer your question as to whether that might be the problem. I didn t think it would be because TimeDistributed Dense .. uses the same weights for each time step so I thought the computation would be equivalent in terms of the gradient call to the non TimeDistributed case that does work . I could be wrong though. Furthermore I don t know why it would be fine with the first 5 6 epochs and then fail afterwards. If it can handle the first few nothing new should be done during the training so I still have no explanation as to how the OOM could occur. More information tomorrow Thanks again algrmur Any update on the issue Thanks Hi oanush I tried running a reduced model on a very small batch size 16 and it ran longer than it did last time but there was still a considerable increase of objects in memory on each training loop a few hundred at each iteration . It just made a bit more space for more epochs to run but then ran into OOM errors at a later point. I think Tetragramm above is having the same issue and further confirms my belief it s with the TimeDistributed layer as without it my model runs fine . I wanted to run the same model using the same setup as akanyaani but so far I ve not seen what the exact details were in his case whether he has much more memory available. I had this issue in version 2.0.0. Beta1 version is working and running faster per epoch. arthurflor23 Are you sure The Beta1 code is nearly identical to the release 2.0.0 code. The only change is to regularization which has nothing that would do a memory leak. Upon further review actually reading through the code I m pretty sure the input map variable is both the cause and useless. I think lines 56 246 247 249 308 and 309 can be removed and line 310 replaced with output mask self.layer.compute mask inputs inner mask Unfortunately I m having trouble building tensorflow from source to test. Hi I was training a model today via google colab using tensorflow gpu 2.0.0 .. I ve added two TimeDistributed layer and I realized that the time of the epochs was increasing.. started with 200s in first epoch and stopped with 350s in the last.. then the issue mentioned happened. I don t know if it s related to this or another module version but Beta1 doesn t happen and make 140s per epoch.. I will do more tests with the two models that I m studying cause I already have another problem in the recurrent layers and ThenRnnBackward Just to complement this behavior that I mentioned appears since rc0. I installed version by version to check via google colab two weeks have passed since success Not quite. Some complications. Awaiting someone with better understanding of the system than me. I also have a memory leak since 1.14 up to 2.0. On 1.13 the leak disappears. I m used TF 1.14 and not have a memory leak Hi I m using TF GPU 2.0.0 and having the same issue when using the TimeDistributed wrapper... Adding a self. input map.clear before this https github.com tensorflow tensorflow blob ed04c8639d53c284874240d58abba725865f454e tensorflow python keras layers wrappers.py L250 does not result in an increasing gpu memory allocation... But I don t know if it is now still correct I only saw self. input map be called at the preparation of the training and only referring to the last input uid. So I thought that clearing before adding the latest element would not destroy the logic behind it but still fix the memory leakage. I can verify that arnemoos s workaround prevents the OOM for me. I m running tf nightly gpu today and I have no more error can anyone confirm I can confirm that using TimeDistributed also runs my model into resource allocation errors for tf 2.0.0. Using the fit generator training function with a model that has 3x 2DConv layers each wrapped in TimeDistributed on a batch of 39 32 MB total memory footprint batch size 32 . arthurflor23 Will try tf nightly gpu now and confirm not confirm arthurflor23 I can confirm that the issue has been gone for me as well arthurflor23 yes it s try Again confirm that TimeDistributed is the culprit. In my case tf nightly breaks my model. Solved the problem by writing a custom for loop in subclassed model rather than using TimeDistributed. But this bug has to be fixed for those using non subclassed model. I was able to work around this issue by reshaping my tensor to combine the first two dimensions applying the convolution dense layer and reshaping back to the expected output shape. The fix has been merged. https github.com tensorflow tensorflow pull 33441 event 2967461126 Thanks Tetragramm . algrmur please let us know if your issue has been fixed and we can close this issue. The PR has been rollback due to a test failure. I will try to update the internal code test to fix the memory leak. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33178 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33178 No a qlzh727 So is the problem solved Can I put the second version yes the issue is resolved by d064c6f 
33255,tcmalloc large alloc on Colab and Tensorflow killed on local machine due to over consumption of RAM, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 LTS Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary Conda TensorFlow version use command below tensorflow gpu version 1.9.0 Python version 3.6 Bazel version if compiling from source N A GCC Compiler version if compiling from source 7.4.0 CUDA cuDNN version V10.1.243 GPU model and memory Quadro RTX 5000 and 16 GB RAM Describe the current behavior The tensorflow API always tries to consume the maximum RAM even when I have a GPU and the kernel gets killed while training my deep learning algorithm. I referred online on multiple sources 1 https stackoverflow.com questions 45077571 tensorflow training killed by system 2 https stackoverflow.com questions 42205205 tensorflow python script getting killed 3 https stackoverflow.com questions 49442670 my process being killed the moment it start training tensorflow object detectio 4 https stackoverflow.com questions 45150773 tensorflow object detection training killed resource starvation 5 https github.com tensorflow models issues 3497 6 https github.com tensorflow tensorflow issues 29365 and tried the following things 1. Reduce the batch size 2. Change the optimizer from adam to momentum However none of these suggestions helped to solve the problem. Describe the expected behavior Be able to train without over consumption of memory and not cause the tensorflow kernel to get killed Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. I ran the following code in an ipython notebook in both my local machine local GPU and Google Colab Other info logs The error log is very long and hence I am attaching it in a separate text file here ERROR LOG.txt https github.com tensorflow tensorflow files 3718556 ERROR LOG.txt , arunumd You would like to try with latest Tenosrflow version 2.0.0. Thanks gadagashwini I tried to install tensorflow 2 using the following command pip install upgrade tensorflow 2.0.0 rc0 and then ran the ipython notebook. But the symptoms still don t seem to change. I get the same error still. As you can see below the tcmalloc error still persists and there is also a new error related to lack of compatibility between tensorflow2 and tensorflow1 arunumd Tensorflow supports manual device placement and limiting gpu memory growth. Please refer this link https www.tensorflow.org guide gpu manual device placement . Please let us know if that helps. Thanks gadagashwini After running through a bunch of problems with tensorflow during initial development stage we felt it was better to look for another reliable API. Hence we made a decision to move to PyTorch. I will be glad to look into this problem again later when I will have time. For now the issue is not resolved. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33255 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33255 No a arunumd In Tensorflow 2.0 there is no placeholder. If you are using TF 1.x code You need to update your TF1.x code to TF2.0 code and then run it. Please take a look at this question. https stackoverflow.com questions 56226284 why do i get attributeerror module tensorflow has no attribute placeholder . gowthamkpr That suggestion sounds irrelevant to the problem. As per the original description the stated Tensorflow version is 1.x. So technically there is no necessity to upgrade code to TF 2.x when the installed Tensorflow version is 1.x The code is supposed to run properly as a Tensorflow 1.x code. But it clearly doesn t neither in 1.x nor in 2.x I tried to install tensorflow 2 using the following command pip install upgrade tensorflow 2.0.0 rc0 and then ran the ipython notebook. But the symptoms still don t seem to change. I get the same error still. As you can see below the tcmalloc error still persists and there is also a new error related to lack of compatibility between tensorflow2 and tensorflow1 I was referring this comment arunumd as there is no concept of palce holders in TF2.0 I tried to install tensorflow 2 using the following command pip install upgrade tensorflow 2.0.0 rc0 and then ran the ipython notebook. But the symptoms still don t seem to change. I get the same error still. As you can see below the tcmalloc error still persists and there is also a new error related to lack of compatibility between tensorflow2 and tensorflow1 I was referring this comment arunumd as there is no concept of palce holders in TF2.0 gadagashwini suggested I try running the same code with TF2.0. So I tried. It didn t work neither. I am not sure if gadagashwini also expected me to refactor the whole code I do not want to refactor anything.. I am interested in knowing whether or not TF1.x is capable of running this code without the tcmalloc error Why is this error being caused I think we have to focus on the root problem.. We cannot exactly trace what the error is but please take a look at this issue here https stackoverflow.com questions 9077457 how to trace tcmalloc large alloc and try to replicate it. Closing this issue due to the lack of recent activity. Please add additional comments and we can open this issue again. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33255 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33255 No a We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks gowthamkpr i faced the same issue . tensorflow gpu 1.15.0 keras 2.2.4 in colab same here. torch 1.4.0 in colab I am interested in seeing a long term solution for this issue which seems to be quite common for many people who are using Tensorflow 1.x. For what it is worth based on the face value of it it seems like an API issue which cannot be fixed by users. This issue is not resolved until it is proven that there is a permanent fix to this problem. I think it s because of loading a large file. I have the same problem but when I truncate my file error disappears What about tensorflow cpu There is no options to lower memory usage like tensorflow for gpu. I ve 32GB RAM with a 5900x and it just kill when memory is full like a bad c program I am working on sentiBert https github.com WadeYin9712 SentiBERT I am facing this issue again and again writing two files each one having 6GB and 3GB NumPy binary files. using torch 1.1.0 torchvision 0.3.0 Can anyone help me with that Thanks Screenshot 2022 11 25 152329 https user images.githubusercontent.com 77248954 204062847 2764f526 e5cf 4044 96d6 d5425f1405c2.png 
33376,importing tensorflow inside a function object causes a memory leak, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 OSX 10.15 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device No TensorFlow installed from source or binary pip install tensorflow 1.14 TensorFlow version use command below 1.14.0 Python version 3.6.8 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Describe the current behavior When importing tensorflow from a function or object the import statement somehow keeps a reference to the function and increasing it s reference count. The full import stacktrace is never freed making it impossible for the object and anything referenced from that object or function to be freed from memory. Describe the expected behavior It should be possible to free the function calling import tensorflow . This is not an issue with any other imports like import logger . Code to reproduce the issue this outputs So importer2 is only freed after the python application finishes. Neither gc.collect nor deleting the object causes it to be released in python. This is not an issue in this toy example but importer2 could have a reference to a large number of other objects that take considerable space in memory in reality. Also this only happens for the first import . importer3 can be freed without issues. Other info logs tf env.txt https github.com tensorflow tensorflow files 3729501 tf env.txt ,Issue replicating for Tf 1.14 kindly for the gist https colab.sandbox.google.com gist oanush e224d0fa208b982d84e5cb18c1317ce9 33376.ipynb .ThThanks annarev is this related to lazy loaders or estimator keras integration It appears to be due to saving error when importing portpicker here https github.com tensorflow tensorflow blob master tensorflow python framework test util.py L44 We can probably save the error text instead or import portpicker inside the function that creates a cluster. Yeah it seems weird that we d survive the import only to error out later. Let s either import it inside the cluster creation or print the error directly on import but still continue and later error with a note to look for the earlier error. Honestly I think the local import is preferable in this case. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33376 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33376 No a 
33516,Dataset.map with tf.data.experimental.AUTOTUNE runs out of memory when using batch size 1, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow YES OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 19.04 Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device Not tried on Mobile TensorFlow installed from source or binary BINARY TensorFlow version use command below 2.0.0 Python version 3.7 Bazel version if compiling from source NO GCC Compiler version if compiling from source NO CUDA cuDNN version CUDA 10.0 cuDNN 7.6 GPU model and memory RTX2070 8GB Describe the current behavior I use Dataset.map to normalize images. When using tf.data.experimental.AUTOTUNE and BATCH SIZE of 1 memory consumption grows up till the program is killed. The most intriguing part is that when setting the BATCH SIZE to greater than 1 the program works correctly This issue happens both with tensorflow 2.0.0 and tensorflow gpu 2.0.0 Describe the expected behavior Code should work for batch size of 1 Code to reproduce the issue Other info logs Might be related to Issue 32052 https github.com tensorflow tensorflow issues 32052 , System Information Linux Ubuntu 18.04 TensorFlow installed from pip install tensorflow gpu TensorFlow version 2.0.0 beta1 Python version 3.6.8 Bazel version if compiling from source NO GCC Compiler version if compiling from source NO CUDA cuDNN version CUDA 10.0 cuDNN 7.6 GPU model and memory GTX 1080 Ti Describe the current behavior I found the same problem as EduardoGRocha that for batch size 1 and number of parallel calls set to AUTOTUNE for map the memory consumption of the program rises until the program is killed. This problem seems to occur due to an infinite number of calls to the map function. Below you can find a minimal example. Code Could reproduce this issue with TF Version 2.0 in Google Colab with CPU and GPU as Runtime. Here is the Gist https colab.sandbox.google.com gist rmothukuru f4c7bf2b0b0e9908a4343be6f0935843 33516 oom error.ipynb . Found the same issue in my project Update In case wrapping tf.data.Dataset with tf.distribute.Strategy.experimental distribute dataset and batch size 1 num replicas batch size 1 for each replica the problem still exists. Sorry I am not allowed to share the code for reproducing this problem my work flow is standard like this For ppl come across the same issue it s not recommended to use tf.data.experimental.AUTOTUNE at this time tf 2.0 manage buffer size num parallel calls yourself. Hello I faced the same issue. I can confirm that the problem occurs on Windows 10 32 GB RAM tf version v2.0.0 rc2 26 g64c3d382ca 2.0.0 Windows 7 8 GB RAM tf version v2.0.0 beta1 5101 gc75bb66a99 2.0.0 rc0 Python version is 3.7.3 It consumes all available RAM causing out of memory error on both devices. Update again this is really a serous bug With the pipeline in my previous comment and batch size 1024 the RAM was eat slowly from 20 at the start of training to out of memory after one night training total memory is 48G . So 1. with batch size 1 for each gpus the bug is triggered and runs out the memory after several training step. 2. with batch size 1 for each gpus the memory increases slowly. 3. without any AUTOTUNE at any batch size testing. rachellim could you please take a look Thanks for the repro. Looking into it. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33516 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33516 No a This was indeed intriguing. I ve submitted a fix which will be in TF 2.1 
33960,Docker Kubernetes memory limits not respected OOMKilled when deployed to GCP , Describe the current behavior I m using Kubeflow to deploy a simple Keras TF model training job two LSTM layers 28000x150x27 input from CSV . The Dockerfile it produces is ...and the output at the top of this issue is from within the resulting container. The container has a CPU limit of 7 cores and a memory limit of 26Gi the host node has 8 cores and 30Gi . By the 3rd of 20 epochs after about 50 minutes the container is killed by Kubernetes with OOMKilled. Looking at a graph of memory use it increases linearly over the 50 minutes and evidently ignores the limits. I have previously trained this model on my laptop 8 cores 16GB RAM 20 epochs without issue so this looks to be related to the Docker or Kubernetes environment. Describe the expected behavior Memory limits are respected. Code to reproduce the issue The above Dockerfile plus ,Additionally running the same image locally on my MBP default Docker setup 2GiB RAM allocated the process exited with a Killed message. Via dmesg after that A quick note when I use tensorflow tensorflow latest py3 I do not experience the bug. Let me know if there s somewhere else I should raise this it s gcr.io deeplearning platform release tf cpu.1 14 GCP s purpose built for GCP image which doesn t work . kierenj What file are you using i.e. raw data in your code It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue I m very sorry my time for this whole project was very limited and I ve worked around using tensorflow tensorflow latest py3 . I haven t managed to find even an hour to share more detail though I know that is not helpful. I am gonna close this issue as it has been resolved. Please add additional comments and we can open this issue again. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33960 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33960 No a 
34379,Memory leak in custom tensors,Hello everyone cc alextp There is a memory leak for custom tensors from https github.com tensorflow tensorflow issues 24865. You can find an MWE here https colab.research.google.com drive 1v PEv3 qubszfK0gFS2RLhcP6qbDUvIK. Long story short. In GPflow we define parameters of the model as custom tensors variables https github.com GPflow GPflow blob develop gpflow base.py L36. The parameter is a tf.Module container with single tf.Variable and TFP bijector inside. The custom tensor behaves as a standard TensorFlow tensor plus it applies transformation on internal tf.Variable and returns forward transformed Tensor of the variable every time when someone decides to read it user tensorflow function . When I use a model with Parameter class and compute gradients w.r.t. the loss I observe constant memory growth. And when I use the model with same variables but do transformation of the variables manually the memory stays the same. If I do forward mode only there are no issues. It happens only when GradientTape is involved. PS memory profiler doesn t work in colab you will have to run it on your computer. macOS 10.14.6 Python 3.7.0 TensorFlow version Middle of the training In the end , kkimdev Do you know if this is related to the leak you ve been investigating It seems like it s using tf.data There were some recent tf.data memory leak fixes https github.com tensorflow tensorflow commit 082415b2ff49bfb8890f7d5361585bac04749add https github.com tensorflow tensorflow commit c2fc448fe253bc59d3f0417d7d08e16d53f2a856 so I encourage to try on tf nightly or tf nightly gpu . alextp kkimdev I replaced data with tensors and installed tf nightly still the issue is present. Looks like it s leaking through tensorflow probability s FillTriangular class and bijector. I m not sure if I ll have time to dig this deeper soon. I attached the script I used to profile and it s outputs below. https colab.research.google.com gist kkimdev f69960cb84d05a8d865eaadf44f47967 memory leak in custom tensor.ipynb image https user images.githubusercontent.com 503414 69096361 1dd98700 0a09 11ea 9602 d254be7f0d41.png jvdillon have you seen this leak before We definitely used to leak memory but switched to weakref dict. Given the intrinsic complexity here my guess is we have a bug. Ill see if someone on the team is willing to dig in. alextp jvdillon kkimdev Thanks everyone for quick responses. Welcome csuter This is a TFP issue which we re presently resolving. Closing this please follow https github.com tensorflow probability issues 647 for TFP updates Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34379 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34379 No a 
34390, TF2.0 tf.reduce mean crashes Python Floating point exception if the count becomes zero due to overflow, System information Have I written custom code yes OS Platform and Distribution Ubuntu 18.04 TensorFlow installed from binary source tested as well TensorFlow version 2.0 Python version 3.7 Describe the current behavior Running the script Crashes the Python interpreter e.g. Floating point exception core dumped . Likely as 256 overflows in uint8 to 0 leading to an uncaught division by zero. Describe the expected behavior A result possibly incorrect due to too small dtype or some other way to deal with the issue e.g. assertion errors or other exceptions however no crashing of Python. Ideally tf.reduce mean could yield correct results for non floating point dtypes as well. Code to reproduce the issue See above. ,Issue replicating for TF 2.0 and tf nightly kindly find the gist https colab.sandbox.google.com gist oanush dcb73f2c1dea1d095f2acaf69df575f0 34390.ipynb of colab.Thanks from the call stack the core dump happens in the eigen lib csachs division by zero do happens which cause the core dump csachs Looks like this was resolved. I couldn t reproduce the issue. Here https colab.sandbox.google.com gist jvishnuvardhan 1f50643112231016fa21d70942febf93 untitled838.ipynb is the gist. I am closing this issue as it was resolved. Please feel free to open if I am mistaken. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34390 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34390 No a Looks good in nightly. 
34579,Suspected memory leak model.predict, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device NA TensorFlow installed from source or binary binary wheel via PyPI TensorFlow version use command below v2.0.0 rc2 26 g64c3d382ca 2.0.0 Python version 3.6.8 Bazel version if compiling from source NA GCC Compiler version if compiling from source NA CUDA cuDNN version NA GPU model and memory NA You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior I m suspecting a memory leak on keras model.predict running on cpu only . Performing model.predict in an infinite loop demonstrates memory leak trend 400MB in 30min please see image below . This trend happens even though I call gc.collect on every iteration. In addition using gc.get objects I can see that every iteration leaks exactly 1298 new objects. Using objgraph the leaked objects are Describe the expected behavior Memory shouldn t increase over time when calling gc.collect nor should there be objects leaks per prediction. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. image https user images.githubusercontent.com 17004015 69528569 27b63980 0f77 11ea 8db3 ada1688c4d3b.png ,This seems to be related to this Keras issue https github.com keras team keras issues 13118 . It s hard to say though whether the bug is rooted in Keras or TensorFlow. I have tried on colab with TF version 2.0 2.1.0 dev20191125 and was able to reproduce the issue.Please find the gist here https colab.sandbox.google.com gist ravikyram 255d880fc37a666d928184da1579fb88 untitled410.ipynb . Thanks Hi ravikyram I run your colab notebook and it seems that at least in term of objects leak version 2.1.0dev is stable I can t see addition of python objects per iteration . Can you please approve If you believe it solved in 2.1.0 do you think there will be a fix in 2.0 too or should I wait for 2.1.0 Thanks awaizman1 you are right the issues seems to be resolved now. Thank you Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34579 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34579 No a model.predict on batch fix for me. 