ID,title,summary,comments
358,SWIG signature mismatch at RecordWriter,I tried to run the example tensorflow g3doc how tos reading data convert to records.py but there is an error saying function signature mismatch on the bool WriteRecord tensorflow StringPiece record call. My swig version is SWIG Version 1.3.40 A simple fix is to change bool WriteRecord tensorflow StringPiece record to bool WriteRecord const string record for these two files , StringPiece is faster than string in some cases so we don t want to change the signature in that way. Can you add the full error message Also what happens if you change tensorflow StringPiece to just tensorflow StringPiece i.e. remove the Removing the solves the problem. Thanks As a reference following is the TypeError with . Thanks I ll fix this in the code. 
378,matmul gradients incorrect with complex64 tensors,The conjugation step was skipped. See https tensorflow review.googlesource.com c 1154 for unit tests and a fix ,Thanks for the fix We ll try to get that reviewed and integrated soon. vrv Are you on this or should I grab it Sorry was supposed to assign this to zffchen78 ZF can you review https tensorflow review.googlesource.com c 1154 vrv zffchen78 What happened to this It was submitted to gerrit but it broke the build so we had to roll it back. So I guess this is back to contributions welcome For whoever looks at this next I fixed tf.test.compute gradient error a while ago to handle complex64 input so it should be easy to test this change. girving Did you get a chance to test the fix Can you please post your findings This will help to resolve update the status of this issue. ymodak The fact that I fixed tf.test.compute gradient error means that others can easily test the fix. I m not going to do it myself. mborgerding Can you please test the fix and post your findings This will help to resolve update the status of this issue. Sigh ... almost three years after a issue was described unit tests were provided and a fix was given. Sorry I don t have the time right now to go back and set my computer back up to verify this. I seem to recall that it has been fixed but I am only 95 sure of this since it has been years. Closing this issue since the fix has been made. 
391,Fatal error in TileOps invalid combination,I have an RNN I adapted from the PTB example that works fine on CPU. But when I try to run it on a GPU it dies with F tensorflow core kernels tile ops.cc 131 TileOp Invalid combination of Device DT and NDIM N5Eigen9GpuDeviceE float 0 I m guessing this is complaining about a 0 dimensional tensor What s going on ,https github.com tensorflow tensorflow blob 9c3043ff3bf31a6a81810b4ce9e87ef936f1f529 tensorflow core kernels tile ops.cc L383 Looks like it girving is there a fix on the way or is this something more fundamental Tile is a nop for scalars so we just need to copy the input to the output. Eigen can stay broken. Fix in review. Fixed. bndnn Thank you for reporting Thanks for the quick attention When can I expect the fix to hit github Or I m happy to pull from googlesource.com if that ll happen faster. We push to github googlesource at the same time I ll try to upstream our commits in a few hours. Awesome Thanks 
444,Python3 bug filter has no len,I have build the latest source a5d8217c4ed and I tried running the mnist tutorial. This fails with File l psmit tensorflow env py3 lib python3.4 site packages tensorflow python ops gradients.py line 447 in gradients if gate gradients and len filter None in grads 1 TypeError object of type filter has no len A filter in py3 is not a list so it has not length. This could be solved by changing https github.com tensorflow tensorflow blob master tensorflow python ops gradients.py L447 to e.g. sum 1 for in filter ,Good catch. We should fix this and add a unit test. Fix in review. 
449,TF code crashes python kernel,The following piece of code crashes the Python kernel. I realize it s not right for one thing I m not feeding it a feed dict but it should give an error and not crash the kernel. ,Nice catch Yes that is a bad bug. I have this fixed at the kernel level so it won t crash anymore. However I could also fix it at the shape inference level so it throws a Python exception at graph construction time. To do this I d need to make tf.Dimension insist on nonnegative input. vrv Do you think that s wise I d be a bit worried that negative dimensions are used somewhere but it s currently a bit inconsistent that C TensorShape requires nonnegative but Python TensorShape does not. C part in review. I m not sure yet if its wise the C TensorShape class thinks about shapes more concretely it doesn t support unspecified dimensions 1 shape inference etc. I think when we push shape inference down into the C code we ll likely have a C class that more closely represents what the python TensorShape class does. Cool I m happy to leave it the way it is now that the C is fixed. try using numpy befor importing tensorflow or keras 
466,tf.unsorted segment sum exits without error message if segment ids contains negative number.,When running tf.unsorted segment sum data segment ids num segments name None and passing a tensor containing 1 as segment ids tensorflow just exits without error message. I m on the current master. Desired behavior in my case would be to these values are rejected i. e. are not summed anywhere at all. Would it make sense to extend this method this way And if not can you give me an idea how to implement this behavior ,Using 1 to reject is interesting but it turns coding errors into silent incorrect behavior so I m leery of doing it. For now you could replace 1s with a positive value represent an ignored class and then drop it. It would be reasonable to add a boolean drop negatives attr to ignore negative values defaulted to false to preserve the existing error detection behavior. Ah but the fact that it exits without the error message is a bad bug. I ll fix the bug part now. Thanks for the fix I ll open a new issue with my feature request. 
481,Error Running cifar10 train.py in Ver 0.6.0 ,The cifar 10 demo ran fine under 0.5.0 but gives errors under 0.6.0 I rolled back to 0.5.0 and confirmed this . Also cifar10 multi gpu train.py works under 0.6.0 as well as 0.5.0. The output I m getting when running cifar10 train.py in 0.6.0 is attached. I m running under Ubuntu 14.04. error.txt https github.com tensorflow tensorflow files 59525 error.txt ,Apologies we ve noticed this internally and fixing it. Fixed today. Will be available in next release. 
505,Momentum and Adagrad don t work with reshape and embeddings,Momentum and Adagrad optimizers do not work when I use tf.reshape like this The code above works with SGD and AdamOptimizer. With Momentum or Adagrad it produces this error This might be connected to 464 the error is similar and also appears with Momentum and Adagrad. When I replace the tf.reshape with following code the error disappears ,I ll try to track down this issue. Tomas would you be able to write a self contained example that fails with this error including a definition of the input any size constants and the momentum optimizer itself Indeed the graph where I had the bug 464 was also using this mix of embedding layer and reshape. So I might have misinterpreted and there is actually two bugs one with tf.reshape and Momentum and Adagrad and the other with tf.nce loss and RMSProp... I will try to check further... Here is a small self contained example that demonstrates the issue This is the smallest example I was able to write. Embeddings seem to be an important part of the problem. On further investigation this looks like a bug in tf.gather s gradient when the indices are 1 dimensional. I m working on a fix and it should be available shortly. Nice. Thank you. 
521,resize image with crop or pad doesn t work with input pipelines,Because tf.image.resize image with crop or pad requires its input to have a fully defined shape it s not useful as part of an input pipeline since the size of images loaded will not be known in advance. Ideally the above would work... ,Yes that s a bug. I needed to use the method resize image with crop or pad as part of an input pipeline and ran into a similar issue. I m not yet proficient enough with TensorFlow to tell if this is useful. I made a small workaround for personal use. If this workaround would be a route to use in fixing this issue I d be happy to update the tests docs and change out the logic in the existing implementation. If anyone can advise on its usefulness I d appreciate the feedback. Example Implementation of resize image with crop or pad https gist.github.com eerwitt 51aba4bffd9ddd5c581c file resize image with crop or pad pipeline py L6 eerwitt It works flawlessly thanks. Send a PR On Wed Feb 10 2016 at 11 25 AM Pouya Samangouei notifications github.com wrote eerwitt https github.com eerwitt It works flawlessly thanks. Reply to this email directly or view it on GitHub https github.com tensorflow tensorflow issues 521 issuecomment 182539003 . So any news to solve the problem Other than the workaround posted above no. Here is a monkey patching module based on PR https github.com tensorflow tensorflow pull 2737 resize image patch.py https gist.github.com gaohuazuo 4093eee2aee4d5b8746bff698506ee57 file resize image patch py Note Remember to pass argument dynamic shape True Input image must have static rank of 3. This can be done by tf.Tensor.set shape None None None Looks like the issue has been fixed by PR 2737. Maybe this issue could be closed Thanks 
528,tf.image.resize images and resize method should work with single image as well as batch,Or at least not transparently do the wrong thing with the resulting shape. At present it s necessary to image tf.expand dims image 0 and then squeeze it before passing to resize. ,I ve submitted a fix that corrects the broken case of this. There was a missing squeeze if the input image was the same size as the desired output. aselle I am encountering this again where image is a single image prints out Tensor DecodeJpeg 0 shape 3 dtype uint8 
539,Bug report easy 1 line fix.,filename tensorflow python framework ops.py def set shapes for outputs op Uses the registered shape functions to set the shapes for op s outputs. try shape func shape registry.lookup op.type except LookupError try shape func default shape function registry.lookup op.type except LookupError raise RuntimeError No shape function registered for standard op s op.type shapes shape func op if len op.outputs len shapes raise RuntimeError Shape function for op s returned g shapes but expecting g op len op.outputs len shapes for output s in zip op.outputs shapes output.set shape s The call of Shape function for op s returned g shapes but expecting g is reversed should be op len op.shapes len outputs i swapped the shapes and outputs ... took me awhile to debug an unrelated bug because i was getting the wrong debug message ,Submit a pull request 
540,Extract glimpse has no shape function,I m receiving this error when calling tf.image.extract glimpse No shape function registered for standard op ExtractGlimpse Could someone confirm that this is a problem with the API itself If so how can I go about implementing a solution ,I found this was a problem too and hacked a solution together by registering a shape for it Thanks a lot can I just run this once in my own code or do I have to paste it into some of the tensorflow files I ve found an almost identical piece of code in usr local lib python2.7 dist packages tensorflow python ops attentions ops.py It must be that the code isn t being called correctly how did you fix this Or in which file should the code be pasted Thanks for reporting this there is a bug in the imports for this code. I m working on a fix and it should be available soon. 
649,translate.py empty buckets sets will crash,If the dev set does not contain all the string related to all the buckets the script will crash. Eg. If the dev set does not contain a string between 20 and 25 words the script will crash. translate.py around line 178 I think that before that line you should had add an if to avoid that ,marcotrombetti please do send a PR with a fix. encounter same issue as marcotrombetti I just send a PR with fix keveman 1496 looks like the PR fixed this 
686,tf.Fill has no gradient. Was ValueError No inputs provided when creating optimizer seems like a bug ,I am in the process of implementing a version of BinaryConnect in tensorflow and ran into this weird crash. I pasted a simple example to demonstrate the crash. Seems that line w tf.select draws mx fill mx fill is causing the problem when the optimization graph is being constructed. ,Sorry I don t know what is going wrong here. I ll see if I can get someone to look at it. a the code is broken it uses tf.types.float32 which should just be tf.float32. b The problem is because when you re doing the tf.select draws mx fill weights you re potentially sending the gradients to the result of tf.fill . Fill has no gradient registered. Fill should have a gradient. Hang on. Fixed. Should be pushed here within a day. Thanks for catching this Thanks for the quick turnaround guys look forward to the update just to confirm it got pushed in the above commit so it s available at HEAD now 
889,Large Strides for 1x1 Convolutions,When a conv2d operator has a stride greater than the kernel size the following error is thrown ValueError stride must be less than or equal to filter size stride 2x2 filter Dimension 1 xDimension 1 This makes implementing the 1x1 convolutions used to reduce spatial resolution in several papers MSRA 2015 among others awkward. Also a convolution with stride greater than kernel size may be unusual but is it still well defined. Fixing this may be as simple as removing this assertion. Thanks to everyone who developed TensorFlow it s a fascinating tool. ,I think this has been called atrous convolution at least in the Eigen code. Does anybody know whether cudnn supports this zheng xq I don t think Cudnn supports convolutions for stride larger than 1. Not sure the FFT algorithm they were using would still be correct. On Tue Jan 26 2016 at 10 56 AM Vijay Vasudevan notifications github.com wrote I think this has been called atrous convolution at least in the Eigen code. Does anybody know whether cudnn supports this zheng xq https github.com zheng xq Reply to this email directly or view it on GitHub https github.com tensorflow tensorflow issues 889 issuecomment 175176178 . I stand corrected. The stride larger than 1 should be supported by Cudnn. I was looking at upscalex instead. it s not just stride 1 it s stride the input patch. E.g. skipping regions of the input. vincentvanhoucke just corrected me atrous is introducing holes in the input patch so the input has holes . So it s different. I don t know if our eigen CPU implementation supports stride input patch. If it does and cudnn does too we can remove this assertion. FYI last week I did try to look at this and cudnn appeared to give different results than I expected eigen was correct for both CPU and GPU . Not yet sure whether this is a bug in cudnn or how we were calling it. The same error is thrown when calling tf.nn.max pool or tf.nn.avg pool with stride greater than filter size. I hit this bug while trying implement ResNet. I m working around it by using a 2x2 kernel with all but the top left zeroed out. https github.com ry tensorflow resnet blob 2775805c4c4af3a3a082100c36a2b2f77791df4d convert.py L119 L122 ry I used resize nearest neighbour to circumvent that problem https github.com cesarsalgado tensorflowed blob master models residual nets.py L46 Or you could also use average pooling for sub sampling although it will not be exactly what the paper proposes. I m marking this a bug since it really should work. Also contributions welcome. See also the recent duplicate 1815. martinwicke I d like to take a look at fixing it because I m hitting this still. But I ve never hacked on TF before if you could give a rough outline of what s involved it d help ry I wish I could still find my code. I remember Eigen doing something consistent across GPU and CPU and expected and cudnn doing something different and seemingly wrong but I might have introduced a bug. The best thing to start with is via a test add a simple test to conv ops test.py that exercises say a 1x1 convolution with input depth 1 and output depth 1 for a 1x3x3x1 input with stride 2 so you get four values the corners multiplied by a filter . If you try to run that you ll first run into a problem doing conv2d shape inference in python because it checks for stride filter remove that and make sure that the shape calculation is correct. Then you ll probably have to make the same fix in the C code https github.com tensorflow tensorflow blob master tensorflow core kernels ops util.cc L41 to remove that check. Other than that I don t think there was anything else I had to do to get the calls to run but that s when I started seeing different output and didn t have time to debug. It s possible it has something to do with padding or the shape inference difference between us and cudnn. Happy to help how I can. 
897,gradient of tf.floor, It looks like the gradient of tf.floor x w.r.t x is 1. But I m expecting it to be 0 as in theano. Right now I could use a double casting as a work around but why is it different ,Yes the gradient of floor is always zero. Well almost always. Should it be as simple as changing in python ops math grad.py Yes changing it to return None would be the fix. Thanks. Could you explain why None is preferred over 0 I observed that the gradient of tf.cast tf.cast vif tf.int32 tf.float32 is also None that s why I added vif in the example code. None will be treated by the gradient code as no connection which is mathematically equivalent to zero but faster since it will never construct large zero matrices. There s some related discussion here 783. 
1102,AttributeError module object has no attribute Copy on fully connected.py. Built from source on 15th February.,I built from source on 15th February. I can run cifar10 train.py with the warning during training as stated here. https github.com tensorflow tensorflow issues 1076 I can run the convolutional mnist with no problems. However when I try fully connected.py this error shows up. Traceback most recent call last File fully connected feed.py line 228 in module tf.app.run File usr local lib python2.7 dist packages tensorflow python platform default app.py line 30 in run sys.exit main sys.argv File fully connected feed.py line 224 in main run training File fully connected feed.py line 130 in run training data sets input data.read data sets FLAGS.train dir FLAGS.fake data File usr local lib python2.7 dist packages tensorflow examples tutorials mnist input data.py line 198 in read data sets local file maybe download TRAIN IMAGES train dir File usr local lib python2.7 dist packages tensorflow examples tutorials mnist input data.py line 42 in maybe download tf.gfile.Copy temp file name filepath AttributeError module object has no attribute Copy I have also tried uninstalling and reinstalling TensorFlow. I don t think it s a problem with the installation since some examples run properly. Anyone else facing this too ,This looks like a bug a missing function that we need to implement. Thanks for reporting. Thanks for the fix. I have verified that Copy is back. But it s still missing Open. File usr local lib python2.7 dist packages tensorflow examples tutorials mnist input data.py line 57 in extract images with tf.gfile.Open filename as f gzip.GzipFile fileobj f as bytestream AttributeError module object has no attribute Open Yep I have another fix for that coming. 
1105,cifar10 not running,Execution tracebacks of cyfar10.py and cifar10 train.py python cifar10.py python cifar10.py Using tensorflow v 0.6 Titan GPU configured for GPU usage as per instruction in setup. ,Something similar with MNIST TF built from source from scratch 10 minutes ago master branch Traceback most recent call last File home expr.py line 133 in module mnist input data.read data sets MNIST data File usr local lib python2.7 dist packages tensorflow examples tutorials mnist input data.py line 199 in read data sets train images extract images local file File usr local lib python2.7 dist packages tensorflow examples tutorials mnist input data.py line 57 in extract images with tf.gfile.Open filename as f gzip.GzipFile fileobj f as bytestream AttributeError module object has no attribute Open Extracting MNIST data train images idx3 ubyte.gz Process finished with exit code 1 I fixed the latter one I m not sure why the former is happening. You shouldn t be running cifar10.py it s a library called by the other executables. Note that you have to build from sources to get the fix. Our first post 0.7 binary release will likely get this fix. I m confused by the post 0.7 part as far as I can see the 0.7 branch is still behind in some aspects to master you have already built the 0.7 release binaries Yeah the binaries are being built right now we can maybe make a patch release with these fixes I expect we re going to want a protobuf fix in the patch release too . In the instructions says https www.tensorflow.org versions v0.6.0 tutorials deep cnn index.html That the following needs to be executed to train the network python cifar10 train.py Is this fixed now If you build from sources at HEAD yes. updating tensorflow install with sudo pip install upgrade https storage.googleapis.com tensorflow linux cpu tensorflow 0.7.1 cp27 none linux x86 64.whl fixed the cifar10 issue. PS for my case I am using the CPU version. 
1121,AttributeError module object has no attribute gfile in 0.7.0 Fixed in source will be in next binary release ,I m on an Ubuntu machine installed tensorflow and ran the image processing example as in the website but it failed complaining about gFile not found Here s the exception stack ,Same here got this error when running cifar10 eval.py My guess is that you are running source code from HEAD with a pip install from 0.6.0. Either check out the code at the same branch tag as the pip install or compile from sources at HEAD to use code at head. Let me check that thanks for looking into it. I had the same problem executing the file classify image.py. Changing the branch to 0.6.0 solved this problem. Thanks akhld Kecksdose Yes using r0.7.0 and run cd tensorflow models image mnist python convolutional.py will have this problem. When changing to 0.6.0 no this problem. So the temporary fix for this is to use 0.6.0 for TensorFlow Try upgrading to 0.7.1 pip uninstall tensorflow pip uninstall protobuf pip install path to 0.7.1.wheel It works fine on 0.8 Was with this error to run ptb word lm.py in the old version. Really after upgrading worked properly thanks 
1123,Error encountered when serializing moving average variables,I compiled Tensorflow from source. Running cifar10 train.py outputs the following warning message WARNING tensorflow Error encountered when serializing moving average variables. Type is unsupported or the types of the items don t match field type in CollectionDef. unbound method to proto must be called with Variable instance as first argument got Tensor instance instead ,Also shlens in case he knows. Having the same issue. And the training command hangs. In fact even the mnist example on setup page hangs too. Thanks for pointing out this warning It s benign for the training process but unfortunate to have this in an example model. I have a patch in the pipeline that will fix the root cause for the warning. Issue Cleanup 
1135,Saver errors in 0.7.0,In 0.7 there are different errors about Saver first Warnings in serialization such as moving averages or dictionaries and then it throws an OS Error But this is working well in 0.6.0 so Saver might have some issues in 0.7.0 ,Thanks for reporting this bug You can safely ignore the warnings which are related to the new MetaGraphDef support. If you want to be able to serialize your collections and avoid this warning you can call the tf.register proto function function https github.com tensorflow tensorflow blob 03bff43060229357cbe2cc1659e7d129c2799b06 tensorflow python framework ops.py L3447 for your collection. The OSError is a bigger problem and is caused by an incompatibility between our internal and external tf.gfile.MakeDirs functions. As a temporary workaround prepend . to the save path argument when calling Saver.save . I have a change in the pipeline that will fix this error. If someone can validate that the above commit fixes the problem with Saver that would be appreciated I was able to run cifar10 train.py with no problems after the above commit . It is working fine now thanks 
1198,reverse sequence s inability to accept int32 can break bidirectional rnn,In the latest releases bidirectional rnn has been changed to accept int32 tensors for the sequence length argument but tf.reverse sequence only accepts int64 tensors and this is currently causing an error when an int32 tensor is passed to bidirectional rnn . , ludimagister Is this problematic to fix Sorry for the delay this is fixed at head now. Mike ludimagister For future use if you include Fixes issue in the commit description the bug will be automatically closed on push. 
1343,sigmoid cross entropy doesn t verify shapes,Hello I have found a misleading behaviour in sigmoid cross entropy with logits function. If I pass a tensor of shape n 1 as logits and a tensor of shape n as targets it outputs a tensor of n n which is obviously different from tensor n if both tensors have the same shape n . Although api docs say that logits and targets must have the same shape this example doesn t fail properly nor broadcast correctly. OS Fedora 21 TF v0.7.1 compiled from sources tagged commit . ,That is indeed a bug. We can add the proper shape validation code in that function to prevent the unexpected broadcasting. 
1399,Error using tf.image.random. numpy.ndarray object has no attribute get shape , Intro I am using a modified version of the Tensorflow tutorial Deep MNIST for experts with the Python API for a medical images classification project using convolutionnal networks. I want to artificially increase the size of my training set by applying random modifications on the images of my training set. Problem When I run the line flipped images tf.image.random flip left right images I get de following error AttributeError numpy.ndarray object has no attribute get shape My Tensor images is an ndarray shape batch im size im size channels of batch ndarrays shape im size im size channels . Just to check if my input data was packed in the right shape and type I have tried to apply this simple function in the not modified tutorial Tensorflow Mechanics 101 and I get the same error. Finally I still get the same error trying to use the following functions tf.image.random flip up down tf.image.random brightness tf.image.random contrast Questions As input data is usually carried in Tensorflow as ndarrays I would like to know 1. Is it a bug of Tensorflow Python API or is it my fault because of the type shape of my input data 2. How could I get it to work and be able to apply tf.image.random flip left right to my training set ,The function requires its input to already be a tensor. You can call convert to tensor to make a tensor from a numpy array or you can put the numpy data into a tf.constant first. Or you could fix flip left to right to call convert to tensor which would be the right thing to do. It looks like we could use a cleaning pass through some of the image ops. A lot of them unnecessarily require known shape tensors as well. On the random ones now which only work on a single image anyway. I think. I ll work on this fix flip left to right to call convert to tensor I believe this can be closed now that PR 1428 has been merged I m surprised this issue wasn t auto closed on merge. PR 1428 didn t include something like Fixes 1399 in the commit message so it didn t autoclose. 
1409,Image ops do not all accept NumPy arrays as arguments,From Stack Overflow http stackoverflow.com q 35824798 3574081 When I run the line flipped images tf.image.random flip left right images I get the following error AttributeError numpy.ndarray object has no attribute get shape My Tensor images is an ndarray shape batch im size im size channels of batch ndarrays shape im size im size channels . On looking at the code this happens because our shape checks expect arguments to already have been converted to a tf.Tensor . We should call ops.convert to tensor on them before performing the check. ,Same for tf.keras.activations.softmax 
1432,seq2seq tutorial example not working for python 3.4, Environment info Operating System Ubuntu 14.04 1. Which pip package you installed. sudo pip3 install upgrade tmp pip tensorflow .whl 2. The output from python c import tensorflow print tensorflow. version . 0.7.1 Built from commit 263d00d271077 with TF UNOFFICIAL SETTING 1 . configure Steps to reproduce 1. Apply https github.com tensorflow tensorflow commit cdd0e2b7c542a59322c054aa1a52b2753c1cf69e 2. Run python3 .. tensorflow tensorflow models rnn translate translate.py data dir . 3. Fail with trying to decode training data as ascii see log at the end What have you tried Modified https github.com tensorflow tensorflow blob master tensorflow python platform default gfile.py L63 to self. fp open name mode encoding latin 1 This seems to fix this issue for python 3.4. Logs or other output that would be helpful Creating vocabulary . vocab40000.fr from data . giga fren.release2.fr Traceback most recent call last File .. .. .. tensorflow tensorflow models rnn translate translate.py line 276 in module tf.app.run File usr local lib python3.4 dist packages tensorflow python platform default app.py line 30 in run sys.exit main sys.argv File .. .. .. tensorflow tensorflow models rnn translate translate.py line 273 in main train File .. .. .. tensorflow tensorflow models rnn translate translate.py line 137 in train FLAGS.data dir FLAGS.en vocab size FLAGS.fr vocab size File usr local lib python3.4 dist packages tensorflow models rnn translate data utils.py line 268 in prepare wmt data create vocabulary fr vocab path train path .fr fr vocabulary size File usr local lib python3.4 dist packages tensorflow models rnn translate data utils.py line 136 in create vocabulary for line in f File usr local lib python3.4 dist packages tensorflow python platform default gfile.py line 176 in next return next self. fp File usr lib python3.4 encodings ascii.py line 26 in decode return codecs.ascii decode input self.errors 0 ,Can you confirm that you re using python 3 You say pip3 and python above is just python Python 3 If so what version Sorry for being unclear. The command was left over from the github issue template. python3 version Python 3.4.3 python3 c import tensorflow print tensorflow. version 0.7.1 python version Python 2.7.6 python c import tensorflow print tensorflow. version ImportError No module named tensorflow Next question what exception are you seeing It may just be my eyes glazing over but it looks like the initial comment has a traceback but not the actual exception. The important line was missing yes. python3 .. .. .. tensorflow tensorflow models rnn translate translate.py data dir . I tensorflow stream executor dso loader.cc 105 successfully opened CUDA library libcublas.so locally I tensorflow stream executor dso loader.cc 105 successfully opened CUDA library libcudnn.so locally I tensorflow stream executor dso loader.cc 105 successfully opened CUDA library libcufft.so locally I tensorflow stream executor dso loader.cc 105 successfully opened CUDA library libcuda.so.1 locally I tensorflow stream executor dso loader.cc 105 successfully opened CUDA library libcurand.so locally Preparing WMT data in . Extracting tar file . training giga fren.tar Unpacking . giga fren.release2.fr.gz to . giga fren.release2.fr Unpacking . giga fren.release2.en.gz to . giga fren.release2.en Downloading http www.statmt.org wmt15 dev v2.tgz to . dev v2.tgz Succesfully downloaded dev v2.tgz 21393583 bytes Extracting tgz file . dev v2.tgz Creating vocabulary . vocab40000.fr from data . giga fren.release2.fr Traceback most recent call last File .. .. .. tensorflow tensorflow models rnn translate translate.py line 276 in module tf.app.run File usr local lib python3.4 dist packages tensorflow python platform default app.py line 30 in run sys.exit main sys.argv File .. .. .. tensorflow tensorflow models rnn translate translate.py line 273 in main train File .. .. .. tensorflow tensorflow models rnn translate translate.py line 137 in train FLAGS.data dir FLAGS.en vocab size FLAGS.fr vocab size File usr local lib python3.4 dist packages tensorflow models rnn translate data utils.py line 268 in prepare wmt data create vocabulary fr vocab path train path .fr fr vocabulary size File usr local lib python3.4 dist packages tensorflow models rnn translate data utils.py line 136 in create vocabulary for line in f File usr local lib python3.4 dist packages tensorflow python platform default gfile.py line 177 in next return next self. fp File usr lib python3.4 encodings ascii.py line 26 in decode return codecs.ascii decode input self.errors 0 UnicodeDecodeError ascii codec can t decode byte 0xc3 in position 14 ordinal not in range 128 Does changing with gfile.GFile data path mode r as f in models rnn translate data utils.py to have mode rb fix the problem If so that s a better fix than teaching gfile.py about latin. Line 134 to with gfile.GFile data path mode rb as f together with Line 140 to tokens tokenizer line.decode latin1 if tokenizer else basic tokenizer line.decode latin1 also seems to solve the issue for python 3. What error do you get if you only make the line 134 change The reason I ask is that it should work the same way in Python 2 and Python 3 and Python 2 doesn t automatically use unicode. python3 .. .. .. tensorflow tensorflow models rnn translate translate.py data dir . I tensorflow stream executor dso loader.cc 105 successfully opened CUDA library libcublas.so locally I tensorflow stream executor dso loader.cc 105 successfully opened CUDA library libcudnn.so locally I tensorflow stream executor dso loader.cc 105 successfully opened CUDA library libcufft.so locally I tensorflow stream executor dso loader.cc 105 successfully opened CUDA library libcuda.so.1 locally I tensorflow stream executor dso loader.cc 105 successfully opened CUDA library libcurand.so locally Preparing WMT data in . Creating vocabulary . vocab40000.fr from data . giga fren.release2.fr Traceback most recent call last File .. .. .. tensorflow tensorflow models rnn translate translate.py line 276 in module tf.app.run File usr local lib python3.4 dist packages tensorflow python platform default app.py line 30 in run sys.exit main sys.argv File .. .. .. tensorflow tensorflow models rnn translate translate.py line 273 in main train File .. .. .. tensorflow tensorflow models rnn translate translate.py line 137 in train FLAGS.data dir FLAGS.en vocab size FLAGS.fr vocab size File usr local lib python3.4 dist packages tensorflow models rnn translate data utils.py line 268 in prepare wmt data create vocabulary fr vocab path train path .fr fr vocabulary size File usr local lib python3.4 dist packages tensorflow models rnn translate data utils.py line 140 in create vocabulary tokens tokenizer line if tokenizer else basic tokenizer line File usr local lib python3.4 dist packages tensorflow models rnn translate data utils.py line 109 in basic tokenizer words.extend re.split WORD SPLIT space separated fragment File usr lib python3.4 re.py line 200 in split return compile pattern flags .split string maxsplit TypeError can t use a string pattern on a bytes like object So the right fix for that is to put b in front of the regular expression definitions to get bytes compatible regular expressions. A few more fixes along those lines may be required. If you want to keep trying to fix it I m happy to keep helping but I could also try to find someone to reproduce on our end. I ve posted an initial attempt to https github.com tensorflow tensorflow pull 1436 I ve not tested this on 2.7 and would also need pointers. I believe this was fixed by 1436. 
1595,using tensor as bool in cifar10 example,Hello I have just recompiled TF to commit f952246 and got an error on cifar10 example. Line if grad should be updated to if grad is not None due to recent changes. Just minor stuff. ,
1748,Optimizers incompatible with sampling missing docs ,Hi perhaps Tensorflow docs should mention that 5 out of 7 available optimizers will not work with sampling losses For now they fail with mysterious messages. The following script will fail for Momentum AdaGrad AdaDelta RMSProp and FTRL. Also where should I look if I d like to implement my own optimizers for GPU The error message is ,Does pinning the variables to CPU make this work i.e. mrry I assume this is fixed now in that we don t do improper placement but I m not sure all sparse optimizers are supported on GPU which is perhaps another bug which I believe we already have an issue for. 
1763,image processing functions should not convert dtypes unless necessary e.g. resize crop transpose rotate ... , Environment info Operating System OSX If installed from binary pip package provide 1. package tensorflow 2. version 0.7.1 Steps to reproduce Perform a resize action on a decoded jpeg before passing it to convert image dtype with a target of float32. The values will still be in the range 0 255 rather than 0 1. This line works as expected tf.image.convert image dtype tf.image.decode jpeg value channels 3 tf.float32 This line doesn t values in the tensor are still 0 255 rather than 0 1 tf.image.convert image dtype tf.resize images tf.image.decode jpeg value channels 3 x y tf.float32 What have you tried 1. Changing the order as described above works. ,Yes this is a bug. Either a Python wrapper or the op kernel for the different resize methods should perform conversion if necessary. I might take a shot at this then should I be fixing this on the python side of things or should I be patching the c code instead jhspaybar I believe that you should fix this on the python side of things. IMO the bug here is not actually that it doesn t scale the range. The bug is that range scaling depends on the input type. I will go out on a limb and say that a lot of people get bit by this and there are a lot of subpar models out there as a result. Some models are able to train around the lack of rescaling some suffer an accuracy loss especially if some kind of range dependent mean centering and stddev normalization is introduced later in the chain. Here s a scenario we encountered a preliminary version of our data augmentation code used nearest neighbor interpolation which does not convert dtype when resampling. We would resample crop do a few other things and then feed everything into convert image dtype . At that time we actually inspected the output of convert image dtype and verified it was doing the scaling. Then one of our researchers changed to bilinear resizing earlier in the chain which does convert dtype to float32 but does not rescale to 0 1.0 . This led to convert image dtype being essentially a no op and much head scratching regarding why our model wasn t performing as well as it should. Long rage opaque magic like this is bad design. Given that backward compat is an issue at this point I think the most robust solution seems to be to permanently deprecate convert image dtype and recommend that people use cast and saturate cast instead and handle range rescaling separately. In the presence of HDR images scaling unconditionally to 0 1 for float inputs is not a good idea. What would you scale by If a previous transformation incidentally left you with float data not in the range 0 1 say because you increased brightness by 10 would you divide by 255 By the max I disagree that this is a bug. Dealing with types in image processing is always annoying especially in the presence of int8 uint8 anywhere in the pipeline. convert image dtype is a conversion function not a scaling function. It converts between images encoded as fixed point images stored as integers and images encoded as floating point. It says as much in the docstring. There is however a bug in rescale image which should return the same dtype it ingested preferred or convert the dtype appropriately. This is different from the behavior of other image processing ops who take care to return the same dtype they got. Sadly here we do have the backwards compatibility problem but I would say that this function should be deprecated in favor of a safe alternative. We could start by emitting a warning if it gets non float input. But by the same argument to make the behavior predictable and non magical you now have to guarantee transparency into types all the way along the chain. I don t see how that d be feasible from API design standpoint short of requiring dtype specifications for all ops so that they assert if they see the type they didn t expect. That seems onerous. As things are today any op that converts image dtype without range rescaling will silently mess up the assumption that convert image dtype will do your rescaling for you. And they all can t really do range rescaling automatically upon dtype conversion because they don t know they are processing image data per se. Besides how many people deal with HDR vs just plain JPEG I bet not very many. And it s incredibly easy right now for the majority of TF users to get screwed by this. I bet most people don t even know about this unintended but likely behavior hence my extended comment above. I guess another easier recommendation for folks who care about correctness of their computations would be to convert to float32 right after decode op. But that will further slow down data augmentation which is already quite a bit slower than e.g. Torch PyTorch. We have made a choice to consider image data encoded as integer types to be fixed point data. As you say you could go the other way. Both have issues Mostly our image pipelines work in float which is slower. On the other hand if we computed things in uint8 or int16 or int32 we d be dealing with overflows and underflows in the middle of processing pipelines and you d probably see a lot more poor results because of that. The fundamental problem is that some ops take the image values at face value resize image when converting dtypes while most don t HSV2RGB etc. . We cannot change much about the behavior because of compatibility constraints but we can add an argument to resize image to make it not convert to float and work in its input dtype only there s no reason for it to convert to float . That sounds much better than the present situation especially if the new parameter is prominently mentioned in the documentation. Arguably changing the dtype during rescaling as well as in the composite op tf.image.crop and resize is the most aggravating problem with the status quo because short of casting back to uint8 t again there s no way to avoid float for the parts of the pipeline that strictly speaking do not need float. For performance reasons resize is typically done right after decode. At a high level I don t want to use any float32 until I have to be in float32 i.e. in our case until after we do resize crop and flip and when implemented rotate and shear as well these need not convert to float either . It certainly would be great if I could maintain compact uint8 t representation for as long as I can. IMO this problem is more important than it might appear. I can tell you that for us inability to do data augmentation quickly enough nearly killed TF as a viable option. Our more demanding training pipelines currently use PyTorch for data augmentation a solution which as an engineer makes me cringe but hey it runs at nearly twice the speed doing the same thing. There are a number of open issues in the issue tracker to that end and being more frugal with cycles could help speed things up. I have changed the title of this to reflect that. I think this is an excellent feature request. The constraints are that we cannot change existing behavior but adding a maintain dtype False arg and prominent documentation would be good. martinwicke Have your thoughts evolved No still a good idea and still a lot of work and in some cases hard to get right. This is really confusing it appears that convert image dtype converts individual values from 0 255 to 0 1 range ONLY if the original type is int8 which unfortunately if the tensor is resized prior does not works because now the values are 0.0 255.0 Is this interpretation correct I think convert image dtype documentation needs to be more explicit about this I encountered this bug issue when using a VGG model created by someone else which expected values in range 0 1 . I believe convert image dtype scales the values iff it converts between an integer type to a float type. Integer types are assumed to be fixed point numbers so int8 255 float 1.0 . It also expects float values to contain normalized images which contain numbers in the range 0 1 . So yes if your original contains float values in the 0.0 255.0 range you have to rescale. If there s a deficiency in the documentation it would be good to make this more explicit. I ve suffered the consequences of this unexpected value scaling during image type conversion also. I believe a better function name is urged. Maybe we could consider name it as convert image dtype with scaling . The docstring says Images that are represented using floating point values are expected to have values in the range 0 1 . Image data stored in integer data types are expected to have values in the range 0 MAX where MAX is the largest positive representable number for the data type. This op converts between data types scaling the values appropriately before casting. If you have suggestions on how it could be improved I d welcome a PR. Note that we cannot rename the function because that would break compatibility. We could make an alias but in my opinion that would just add more confusion. I understand that the current docs has already tell the truth of it. I would suggest adding a optional parameter named normalize False . Then by default convert image dtype with scaling won t do the magic that is not in its name. And people can still expect to use this feature if they set normalize True . I understand your point that there are image processing related hypothesis here about the float type. But I don t think it s a good idea to assume everyone agrees on that. We could introduce a new parameter though not with a behavior changing default but wouldn t you simply use a regular cast if that s the behavior you want The purpose of this function is to manage conversion between the common representations of image values. And those are fixed point encoded values stored in integers and floating point values as described in the docstring. Every library dealing with images has to agree internally on a standard and this is the one in TensorFlow and btw. other libraries such as OpenGL have as well including the conversion rules . I understand that there may be images in other representations out there in particular floating point images with a 0..255 range these need to be scaled before they will work with TensorFlow functions. So assuming I decode a jpeg image from the raw file stream the first function I can think of about image type conversion is convert image dtype . I don t think people will think of cast in the first place since it s too general. I still think it s the name that hints people this is only about dtype which I think should be amended by parameters. About the common standard in TensorFlow I agree with your point. As you have mentioned there are different standards with different scale. My point here is that Float32 conversion and unit8 conversion at least should have the same behaviour. Both scaled or both not scaled. Your image from decode is likely uint8. If you need float you should use convert image dtype. If you use cast you ll get a float image scaled 0..255 which isn t what you want it won t work with a lot of TensorFlow s image processing functions. Note that you can of course also leave the image as uint8 especially for operations such as cropping and transpose there s no harm in doing that in uint8. Yes just like you said the ambiguity lies in that my image from decode may or may not be uint8 which will cause a problem when considering the scaling issues. But at least decode jpeg will always return uint8. You should.always know the dtype of.your input as far as I know. How do you end up with an image for which this is unknown During optimization of our computing graph we accidentally put an tf.float32 placeholder before the conversion. I wouldn t say it s unknown. It definitely could be avoided in our side. Same issue. martinwicke Do you have a plan to fix the issue I think it s better to add a range argument for convert image dtype . Would you mind me make a PR for that What would be the behavior of that argument The target range The input range Do you need both I agree that It is not ideal that range and dtype are coupled. I am however wary of introducing additional complexity here. How would an additional argument have helped here Nagging Awaiting Response It has been 14 days with no activityand the awaiting response label was assigned. Is this still an issue I ll close this issue. Please reopen if you have a concrete suggestion for improving the API. I just ran into this issue which is a very long standing bug that presents even these days TF 1.13.x and 2.0 . I strongly suggest it should be fixed with some breaking changes in behavior at TF 2.0 and the issue remain open. I will come up with a suggestion for improving API soon. 
1793,Missing gradient for tf.nn.max pool with argmax,I swapped out the first max pool operation in convolutional.py https github.com tensorflow tensorflow blob r0.7 tensorflow models image mnist convolutional.py tutorial for a max pool with argmax operation and there doesn t seem to exist a gradient for that op virtenv python convolutional.py Extracting data train images idx3 ubyte.gz Extracting data train labels idx1 ubyte.gz Extracting data t10k images idx3 ubyte.gz Extracting data t10k labels idx1 ubyte.gz Traceback most recent call last File convolutional.py line 316 in module tf.app.run File home name virtenv local lib python2.7 site packages tensorflow python platform default app.py line 30 in run sys.exit main sys.argv File convolutional.py line 244 in main global step batch File home name virtenv local lib python2.7 site packages tensorflow python training optimizer.py line 190 in minimize colocate gradients with ops colocate gradients with ops File home name virtenv local lib python2.7 site packages tensorflow python training optimizer.py line 241 in compute gradients colocate gradients with ops colocate gradients with ops File home name virtenv local lib python2.7 site packages tensorflow python ops gradients.py line 453 in gradients op.name op.type LookupError No gradient defined for operation MaxPoolWithArgmax op type MaxPoolWithArgmax Environment info Operating System Ubuntu 14.04.1 LTS If installed from binary pip package provide virtenv pip install upgrade https storage.googleapis.com tensorflow linux cpu tensorflow 0.7.1 cp27 none linux x86 64.whl virtenv python c import tensorflow print tensorflow. version 0.7.1 Steps to reproduce 1. Change line 192 5 of convolutional.py https github.com tensorflow tensorflow blob r0.7 tensorflow models image mnist convolutional.py from pool tf.nn.max pool relu ksize 1 2 2 1 strides 1 2 2 1 padding SAME to pool pos tf.nn.max pool with argmax relu ksize 1 2 2 1 strides 1 2 2 1 padding SAME 2. Run virtenv python convolutional.py What have you tried 1. Logs or other output that would be helpful If logs are large please upload as attachment . ,I had the same issue when I was trying to get the max indices. this is a bit of a hack but copying the gradient for maxpool and registering it for max pool with argmax works. The function signature has an additional arg presumably this has to do with the argmax . Also the op doesn t have a data format so that is left default NHWC . mwalton some other arg should be called unused argmax grad but otherwise that s basically it. Would you be interesting it submitting your change as a PR We d have to add tests if so mirroring those for the gradient of the normal max pool. girving Sure Should I go ahead and put in the PR or implement the tests first mwalton The tests should go in the same PR. mwalton Not sure if you ve made any progress on this but I believe the correct change would be the following Seems to be working on a model I m training. Haven t had time to write tests for it yet nor do I know how what tests are needed for registering an existing op that appears to have some test coverage see https github.com tensorflow tensorflow blob master tensorflow python kernel tests pooling ops test.py L521 . girving If you can provide some direction on what tests I d need to add for this change I can try to pull together a PR. bcaine Awesome Tests for gradients are pretty easy just use tf.test.compute gradient error and verify that the error is small 1e 4 or 1e 3 typically due to numerical differences . You can look at other compute gradient error examples and mimic. Are there any updates on this issue It seems that adding this gradient was attempted in 4014 but not taken place due to commit conflicts. Should I cherry pick commit dc3d4782cbdf2d98b into a PR Enet4 Definitely looks stagnated so feel free to take over. Hi. I trained a network with bcaine register ops solution in python. Now I exported my graph using freeze graph.py to use my network in C . But there this gradient is not registered. Can anyone help how this can be for C source Hey is anyone doing anything about the lack of gradient 
1816, tf.reverse sequence doesn t work with arguments of totally unknown shape.,From Stack Overflow http stackoverflow.com questions 36480456 dynamic rnn and array ops reverse sequence problems I am trying to reverse my inputs with array ops.reverse sequence before sending it to dynamic rnn the inference graph can be build with no problem but when building the training graph I got the following error The problem appears to arise when input shape.ndims is None which is a valid possibility . ,For some reasons I am not allowed to post my code here but the bug occurs when I try to stack multiple layers of BLSTM using dynamic rnn see this related request https github.com tensorflow tensorflow issues 1779 . For the backward pass I first reversed the input using array ops.reverse sequence then reversed the output using the same function. I also used feed forward layers in between the BLSTMs. However with one BLSTM layer everything is fine. Thanks for all the help I m not certain that fixing this issue will solve 1779 but I have a simple fix to this issue in review so hopefully you ll be able to check soon Great Works like magic solved all my problems Thanks a lot 
1853,shuffle batch gives ZeroDivisionError when computing capacity stat, this fails with Unlike TF tensors Python doesn t handle float division by zero. One solution is to wrap capacity and min after dequeue into TF tensors so that you get inf as a result instead of RuntimeError ,
2127,tf.nn.dynamic rnn fails when batch size is 0, Environment info Operating System Ubuntu 15.10 Installed version of CUDA and cuDNN Commit hash 8d310bfcffcd46418d68dd535fb0fbcfee74b8a0 Steps to reproduce Run the following test case When TensorFlow is compiled with c dbg this fails with the following message Using gdb we get the following backtrace ,Thanks for the detailed report. tensorflow core kernels tensor array ops.cc 753 is the key line from the stack trace. ebrevdo is the author but is on vacation at the moment. Other calls to functor Split can be found at tensorflow core kernels split op.cc 172 and tensorflow core kernels unpack op.cc 86. Looks like girving might have some familiarity. The Split kernel assumes its inputs are nonempty and TensorArrayUnpackOp doesn t check this. It should. ebrevdo Would you have time to look at this now you re back Or did it get fixed in the mean time Looks like it was fixed in https github.com tensorflow tensorflow commit b7f7fe26fd2e15978d84a1f02e983faf7324a131. 
2163,Floating point exception when computing gradients,Hi tensorflow developer team I just wanted to first let you know that I really appreciate all the work you are putting into this amazing open source system. It has been a terrific system for me so far. However I have come into a roadblock when using the tf.gather function in gradients. I am using Environment info Operating System CentOS Installed version of CUDA and cuDNN please attach the output of ls l path to cuda lib libcud CUDA version 7.5 cuDNN version 7.0 64 bit tensorflow 0.8.0 gpu version Steps to reproduce I obtain a floating point exception when I run the following code A ph tf.placeholder tf.float32 shape None 2 ind ph tf.placeholder tf.int32 shape None 2 gather tf.gather A ph ind ph redsum tf.reduce sum gather 1 l2 tf.reduce sum redsum A np.array 0 0 0 0 0 0 ind np.zeros 0 2 grad op tf.gradients l2 A ph out sess.run grad op feed dict A ph A ind ph ind What have you tried If you replace ind with a non empty array such as 0 1 it works fine. I suspect that the output of redsum which makes l2 disconnected from A ph for the given ind. Fed forward to computing l2 I get the correct value of 0.0 since there is nothing to be summed . However for backpropagation of gradients I think the output of redsum chokes the algorithm and causes it to evaluate some illegal operation using an empty tensor instead of just backpropagating 0.0. Are there any solutions to this problem currently In my use of this computation I won t know ahead of time whether or not the intermediate computations produce empty tensor but I also want to be able to get the correct gradients when this happens. Thank you so much for your consideration ,Thanks for the bug report. girving tf.gradiensts is generating an unguarded div by 0. Internal bug opened. Thanks for reporting We re going to fix TensorFlow to produce nice exceptions for integer zero divisions. Ah there are two bugs here one bug that we crash the process on integer division by zero and another that we generate a division by zero. Yes I think there should never be any division by zero in the above operations at least from what I can tell from the backpropagation formulas. I think I should definitely be able to get a None gradient to indicate that that l2 and A ph are disconnected rather than an exception. Is the bug a complex one to fix altaetran It s conceptually straightforward one has to go through and sanitize all the occurrences of in math grad.py . Unfortunately there are a bunch of them for a variety of different ops and the fixes look different in the different cases. However there s no way to get a None gradient since the variables are connected just through an empty tensor bottleneck. Since we don t necessarily know that the shapes are empty until runtime we can t produce None . altaetran To make my initial comment make sense we want to produce nice error messages even for malformed ops so we want to fix the underlying divisions by zero regardless of whether we fix the gradient code. Oh I see so those would be two separate tasks. Is there a reason for why an empty bottle neck tensor in the gradients produces division by zero though Or is that something to be changed as well As I said the fixes are straightforward but there are a number of them. Here is an example of a bad line https github.com tensorflow tensorflow blob 5e22e3a3187e6729488f4a6da59b4cfbedc40946 tensorflow python ops math grad.py L36 Oh I see ok. So then this is likely something to occur in a future patch I m looking at it now so not too long except for the upcoming weekend. I ll probably fix the gradients first since making integer division safe is slightly more awkward to do in a reasonably performant manner . CL submitted so it should be on Github within 24 hours. To clarify I fixed the gradients not to divide by zero which should solve your issue. Integer divide by zero on its own will still crash the process. Great Been running into a similar problem myself. As a question is there a local version of the patch I could use I m running Tensorflow 0.8.0 on a cluster and the sysadmins won t rebuild the cluster version before TF 0.9.0 or the next stable version rbharath Do you want source or binaries The source version should have the fix tomorrow. Not sure when the nightly appears tomorrow or the next day but the links for those are here https github.com tensorflow tensorflow blob master README.md girving Sorry should have been clearer. Is the fix python only If so would I be able to just use the corrected python module instead of the standard module without rebuilding the rest of the source The issue is that our cluster runs with an old CentOs with an old glibc so the sysadmins have to do a ton of work to get tensorflow to build so it s infeasible for us to use binaries or rebuild except at stable releases . rbharath Ah yes the fix I just made is Python only. However we don t have any setup for using different Python alongside C and in any case you d have to carefully cherry pick just my change on top of 0.8.0 to have a hope of succeeding. 
2295,pow gradients create inf at 0.0,Hi I had a pipeline where I was first clamping to 0..1 and then doing x 2.4 converting to from gamma RGB to linear RGB However this instantly created inf values and NaNs in the next step seemingly due to the use of log to compute the gradients. The gradient of pow in 0.0 should be pretty well defined as long as the exponent is nonzero at the very least it is not infinity. As a workaround clamping to 1e 5 instead of 0.0 caused the NaNs to disappear. ,Just out of curiosity what is the gradient value at y 1e 5 then I have no idea I never printed them out. But the graph trained just fine. The fast way to solve this is an add an extra op that does x log y and does the right thing for x y 0 . The slow way is to do that in Python via tf.select tf.equal x 0 0 x tf.log y . Unfortunate that simplistic code doesn t work out of the box because tf.select doesn t broadcast. The simple way to solve it is to use x tf.log y y.dtype.epsilon in the gradient. Probably we should just do that. sesse I m happy to review if you want to make the change but won t be able to work on this soon. 
2447,TensorFlow segfaults on attempting to save a large variable,The checkpoint format writes the value of a variable into a Protocol Buffer which has a 2GB limit. The saving mechanism does not validate the size of a variable mdash which can be much larger than 2GB mdash before attempting to write it into the Protocol Buffer. The following code can cause a segfault ,
2619,tf.image.decode png returns wrong values for uint16 images, Environment info Operating System Ubuntu 14.04 Installed version of CUDA and cuDNN CUDA 7.5 cuDNN R4 If installed from sources provide the commit hash d8eb4bb6470d4cb3d0f67f2111a39fa50f1c28e5 Steps to reproduce Please see the below code. After saving numpy array as uint16 png image I loaded it using tf.image.decode png. It returns different values from the original array. Results are I checked the function works properly in case of uint8 png image. ,What architecture are you running on little endian or big endian It is little endian x86 processors . lscpu Architecture x86 64 CPU op mode s 32 bit 64 bit Byte Order Little Endian CPU s 12 On line CPU s list 0 11 Thread s per core 2 Core s per socket 6 Socket s 1 NUMA node s 1 Vendor ID GenuineIntel CPU family 6 Model 63 Stepping 2 CPU MHz 2599.968 BogoMIPS 4797.20 Virtualization VT x L1d cache 32K L1i cache 32K L2 cache 256K L3 cache 15360K NUMA node0 CPU s 0 11 As a workaround the values can be fixed by swapping the bytes via integer operations This is obviously a dirty hack and would break correct values on systems where the decode function works correctly dave andersen Could you take a look since you touched the png reading most recently Urgh. I m concerned that we re not properly setting the IS LITTLE ENDIAN flag in the OSS build https github.com tensorflow tensorflow search utf8 E2 9C 93 q IS LITTLE ENDIAN I ll take this on. girving do you know off the top of your head if we have an existing flag for little endian I can repurpose here I ll dig just trying to be lazy. dave andersen I don t remember nor do I envy you this task. I m um just going to use the existing flag and assume it s someone else s responsibility to make the flag work. https github.com tensorflow tensorflow blob master tensorflow core platform host info.h L25 static const bool kLittleEndian true Actually I m not sure that flag is the issue. beopst Are you on a little endian machine girving the issue is that we don t use that flag in the PNG decoder we use the IS LITTLE ENDIAN flag which is only defined by some transitively included google internal header. The second part of the issue is that we don t have any tests for the png decode op that would catch this on jenkins. Fixing. Fix submitted should hit github at next sync. Will let the fix close it. beopst huge thanks for providing an easy way to reproduce with your bug report 
2641,Error in gradient of reduce prod, yields 2. 1. which is correct. But yields nan 0. which is incorrect. The correct gradient is 2. 0. ,Note that this is not an issue with regular TensorFlow multiplication. The following works correctly It prints 2.0 and 0.0 which is correct. I have reproduced this. Thanks aselle. Idea This could be caused if the gradients of reduce prod . are computed like the gradients of tf.exp tf.reduce sum tf.log . The gradients are computed by computing the full prod and then dividing which is broken as you point out. For example for a length two produce x y the gradient w.r.t. x is x y y . Indeed there s an ancient TODO in the code that it is broken. ibab benoitsteiner The easiest way to fix this would be to do two scan products to get the sequence of partial products from both directions then multiply them together to get all products with one element removed. Unfortunately we don t yet have scan. Adding contributions welcome but note that it ll have to wait until after 2711. I ve had a shot at solving this today using cumprod and it s working great for full reductions. But if reduce prod is performed with reduction indices things get more difficult. In that case the cumprod needs to be performed over the reduced dimensions and not over the remaining ones. Any ideas on how this could be solved Maybe if there was a tf.reshape selected op that allows you to group all selected dimensions into a single dimension and all remaining ones into a second one like this Then the cumprod could be performed over the first dimension. This is essentially a transpose followed by a reshape. I ve tried to build this using existing ops but it seems that tf.cond is required to act based on the contents of the reduction indices tensor. ibab It does seem like it has to be transpose reshape stuff reshape transpose. I don t think a custom tf.reshape selected makes sense separate transpose and reshape is cleaner especially since you have to invert it. The tf.cond is indeed necessary in general. This is getting ugly but I don t know a cleaner way. I think tf.listdiff can be used to separate the axis indices into reduced and non reduced parts If I concatenate both of these I can use them as the permutation in tf.transpose . I can get the sizes of the two dimensions using tf.segment prod or tf.gather and tf.reduce prod . So maybe this can be solved without resorting to tf.cond but it s certainly not beautiful. ibab Have you considered extending the cumsum operation to take a list of indices instead of a single index over which to sum That should make the gradient computation for reduce prod a lot simpler. benoitsteiner The awkward thing about that is that cumprod on multiple axes at a time is a very strange operation. It would implicitly flatten and expand which would mean you have to do the same implicit flattening at the Eigen level. benoitsteiner That s a good idea I just realized that I could make use of tensor.shuffle from Eigen to do the same operations as described above permute the axes so that the scan axes are in the front and then reshape . This would also avoid copying the tensor as shuffle is implemented through .coeff . girving My first impression was also that it would be awkward but now I m thinking that it could make sense The list of indices identifies the sequence in which cumsum should traverse each sub tensor. It also plays well with the existing exclusive and reverse options. I m open to using either solution making the gradient more complicated or making cumsum prod more complicated For reference here s the gradient implementation I came up with This makes the existing tests pass and also works if there are zeros in the input array. This has been fixed by 3351 so this issue can be closed. The gradient of reduce prod does not support negative axis unlike reduce prod itself. It is caused by gather not supporting negative axes. This code illustrates the problem. pvanhaes Thanks for the bug report Can you file it as a separate issue since it s unrelated to the current thread It helps us to keep Github issues organized. 
2645,Persistent Tensor deletion fails when tensor was moved,TLDR handle movers register Python SessionHandle instead of string session handle which causes InternalError Unable to get element from the feed. when garbage collection is triggered on moved persistent tensors. The solution is to replace self. register dead handle handle with self. register dead handle handle.handle in session.py Discussion There are two types of handle objects Python TensorHandle and native string session handle. The conversion from Python TensorHandle object to native handle string object is done in session.py and session ops.py in one of those 3 ways. self. handle compat.as str any handle str handle handle.handle The str conversion is not very Python since str is supposed to be used for display debugging purposes. Some suggestions to make it easier to avoid bugs like above 1. Annotate which kind of handle object is being dealt with. Perhaps py handle variable should refer to Python TensorHandle objects and handle refer to native string handle objects 2. Only use one kind of conversion method perhaps as py handle.handle 3. str method of TensorHandle could be return TensorHandle s self.handle for easier debugging Test case The failure is yuanbyu keveman ,Actually for fix above to work auto gc enabled has to be set to False for all TensorHandles. The issue is that when update with movers moves CPU Tensor to GPU it ll register CPU tensor handle for deletion. But if auto gc enabled is True it is True by default at the point when Python runtime garbage collects that TensorHandle object it ll register same handle for deletion again so the program will fail on next gc pass with something like InvalidArgumentError Failed to delete a tensor with handle GetSessionHandle 4 9 job localhost replica 0 task 0 cpu 0 in the session store. 
2727,tf learn bugs when running DNNClassifier examples,from sklearn import datasets metrics iris datasets.load iris classifier learn.DNNClassifier hidden units 10 20 10 n classes 3 classifier.fit iris.data iris.target score metrics.accuracy score iris.target classifier.predict iris.data print Accuracy f score ,seems solved in the master branch now Looks like this problem is resolved. Closing the issue. 
2740,ExponentialMovingAverage.average duplicates the current scope name,Using tensorflow nightly. ,The problem seems to come from https github.com tensorflow tensorflow blob master tensorflow python training slot creator.py L83 where the name of an op is used to create a variable. But the name of the op already contain the current scope therefore the variable to create will contain the scope twice. It s been a while. I suppose this is a bug right .. Yes that looks like a bug. I m asking the author of the code to do the quick fix. Unfortunately after checking internally fixing this bug would result in losing backward compatibility which we would like to avoid both for internal and external users. So right now I am closing the issue. girving mrry Isn t it true that ExponentialMovingAverage is used currently almost always outside any scopes It shouldn t break many existing use cases. If I remember correctly the class simply doesn t work well if the scope name is duplicated and users have to do manual hacking to fix the variable names. The workaround that I m using currently is to wrap the class with It s not just ExponentialMovingAverage but also the accumulators used by optimizers that have this behavior. Anyone who s using either of these inside a scope of any kind would suffer backwards incompatible checkpoints due to the naming difference. Especially given there is an easy workaround i.e. do not create use ExponentialMovingAverage or Optimizers inside a name scope the pain that fixing this would cause outweighs the benefits. We don t need to share the create slot function between EMA and optimizers. It s essentially just a variable creation. Two notes 1. create slots is more than just a variable creation there s some subtle logic in there to deal with properly supporting slots for partitioned variables. So I d think reusing the logic makes sense. 2. Even if we were to only fix this for ExponentialMovingAverage it would still be a backwards incompatible graph change for anyone who was using EMA inside a name scope. Is it possible to keep the compatibility for now but print a warning about a future change that will break compatibility similar to numpy ppwwyyxx I m not sure I follow. How would we be able to safely break backwards compatibility in the future An advance warning that models will break is not a sufficient fix. I mean it will provide a time window for people to make necessary changes to prepare for a future break. But right it won t safely break compatibility. ppwwyyxx I don t think it is worth making this change. The name fix would be nice and I agree that the current code is a mistake but there is a high bar for breaking existing models. On latest tensorflow the with tf.name scope None hack still introduces variables with duplicated scope name will print The recently introduced new variables in EMA also brings error when using with reuse True. The example below seems like a common pattern in batch normalization. It works before but now There are a number of TF paradigms that previously worked because ExponentialMovingAverage didn t respect variable scopes see https github.com tensorflow tensorflow issues 5652 for example . In the case you have here I m not sure that I understand the semantics of what you are trying to do regardless of debiasing it is an error to call apply on the same variable multiple times ValueError If the moving average of one of the variables is already being computed. from ExponentialMovingAverage description of apply . Are you arguing that the wrong error message is thrown in this case joel shor Oh. You can change W to W2 and move it outside of the scope it still throws error. I m only trying to point out that EMA cannot work inside reuse True. By the way I don t think I m calling EMA on the same tensors because calling add twice creates two tf.Operation . 1. The issue should be fixed currently zero debias is no longer the default in ExponentialMovingAverage. 2. There is a strong possibility that is will be the default in the future. 3. ExponentialMovingAverage create variables so for future safety you might consider using it as if it respected variable scopes ie don t put it in resuse True variable scopes that it doesn t need to be in joel shor Thanks. Since there are dedicated issues for the bug we can use them to track. This issue is more about the naming and I see there are still very long variable names with duplicated scopes for the newly introduced biased local step variables. Should there be a fix for this We are not going to fix this behavior. The reason is that there are two relevant scopes the scope that the variable was created in and the scope that the EMA was created in. There will be potential ambiguity if one is removed. Since removing the duplication isn t clearly better we re going to keep this behavior. Hi joel shor I am facing the same problem while trying to reuse batch normalization in a network which is used twice. I get error convnet branch conv1 1 bn moments Squeeze ExponentialMovingAverage does not exist or was not created with tf.get variable . Did you mean to set reuse tf.AUTO REUSE in VarScope Can you suggest how to deal with this problem I read all the above comments but couldn t figure out how it could be solved. Thanks a lot Hi nainadhingra2012. On tensorflow 1.12.0 I had the same problem and fixed it by adding the line with tf.variable scope tf.get variable scope reuse tf.AUTO REUSE before ema.apply 
3165,AttributeError Tensor object has no attribute shape , Environment info Operating System BashOn Windows Installed version of CUDA and cuDNN doesnt matter CPU version only If installed from binary pip package provide 0.9.0 If installed from sources provide the commit hash Steps to reproduce label is a string point list is a list of an array of x y values ex What have you tried googling why a tensor would not have a shape attribute Logs or other output that would be helpful ,Okay figured it out. Fit can not take a tensor as an input. it would be helpful if that was the error message instead Actually there is a bug in data feeder.py it should use get shape instead .shape. I m also having this issue. I m using LinearClassifier. I tried passing the X argument as a tensor Pandas dataframe and NumPy matrix. None work and all attempts result in the same AttributeError. Is there any progress with this Thanks Have you tried at head We know there s a bug in 0.9 but it should work at head. Closing for now since martinwicke says this is fixed. 
3172,float division by zero,It seems that if you only run 1 training step at a time then you can come to a point where it is too fast This happens because the time between the start of the step and the end is exactly the same because it is a single step. Here is the code snippet Maybe add an if statement to just say zero if elapsed time is zero ,You can measure the elapsed time in microseconds datetime.timedelta and than scale it to get steps done per seconds. I added an if statement that says if elapsed time is zero just set it to added steps. That fixed it too On Jul 3 2016 4 33 PM ahmetemir notifications github.com wrote You can measure the elapsed time in microseconds datetime.timedelta and than scale it to get steps done per seconds. You are receiving this because you authored the thread. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 3172 issuecomment 230179376 or mute the thread https github.com notifications unsubscribe AB2CDKGUOklf0AvwMg82Da6RemPqdkhGks5qSDjCgaJpZM4JD5RE . It can mislead you.Think that elapsed time is zero and you get 10 steps.This doesn t mean 10 steps per seconds.If the elpased time is actually 100 microseconds than you should know that it s 100 steps per seconds. I think the right answer is probably inf steps second since as seconds goes to zero the relative performance should become arbitrarily large. The question is whether the Summary system is able to handle that.. And statistically you can t trust the results of such a low sample measurement.To get more precise results same test have to run for a long period of time over and over and you have to analyze the results using statistics. This references code that moved quite substantially in the last few months. Feel free to open a new github issue if the problem still persists in recent versions. 
3233,partial run can t do incremental feeds in InteractiveSession,Here is a section of code copied from the docstring of partial run with required import statements added. However note that this constructs an InteractiveSession instead of a Session . The code fails with a NotFoundError . When the InteractiveSession is replaced by a normal Session the code runs correctly. The difference between the two is that the interactive session sets the place pruned graph option to True starting a plain Session with place pruned graph True reproduces the issue. I do all of my work in an interactive terminal and I would find it useful to have partial run work correctly in this setting. System info Ubuntu 14.04 tensorflow 11834fb02bfa9296f4aa48ee1eaa2a002fecbf1f python3 cudnn 4.0.4 cuda 7.0.28 ,I am able to reproduce this. yuanbyu could you take a look as you have looked at other partial run issues Any updates on this issue Thank you for reporting this. Since the code has changed substantially since the issue was open it s not clear whether it s still valid. Please open a new issue if the problem persists with new versions of tensorflow. drpngx Please don t close this. Nothing has changed substantially that I am aware of it took me less than a minute to re run the above sample code in tensorflow 0.12 I just had to fix some trivial renames . The same error persists. Code to reproduce is yuanbyu this looks like a bug in partial run . Here s the stack trace do run calls do call with this both working and non working paths print the same OK I need to call it a day for now but we re failing to find it in name to node here https www.github.com tensorflow tensorflow blob master tensorflow core common runtime direct session.cc L822 and when using InteractiveSession we re not inserting it in the values https www.github.com tensorflow tensorflow blob master tensorflow core common runtime direct session.cc L932 only add and mul get inserted not any of the placeholders . drpngx looks like this is still a problem in the latest version. Any updates on the problem nikitakit did you try a regular Session instead of an InteractiveSession It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. Nagging Assigneee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. 
3277,Broadcast 0 rank tensors when computing gradients for tf.nn.relu, Environment info Operating System OSX macOS.... CPU only version 0.9.0 Perhaps this is desired behavior but I would have much appreciated a more descriptive warning at least which would have saved much debugging. I haven t found a small reproducible case for this but in the code I originally found this bug no error is raised as the other variables are trained leaving me scratching my head as to why the linear rectified variable was not being trained. The same issue also occurs for tf.nn.softplus and perhaps other methods as well. Steps to reproduce What have you tried The problem is resolved by expanding the dimensions of x I wonder if it would be possible to do this automatically ,So I checked this out does seem to be a bug. The good news is it looks like it might be really easy to fix. https github.com tensorflow tensorflow blob master tensorflow core framework numeric op.h L90 Just add NDIM CASE 0 on line 90 and the template magic should take care of the rest. I ll work on getting this submitted. Looks like the fix got pushed. Could you check whether this solves the issue with your more complicated example Thank you for the awesome bug report Yup All is well. Thanks for quickly dispatching this one 
3356,can t construct gradient of gradient,In general I ve had no trouble constructing the gradient of a gradient in Tensorflow but I hit on a weird edge case involving complex numbers where I couldn t. Below I show a case where the double gradient works and one that is minimally different where it doesn t. Here s where double gradient works As expected evaluating h produces Here s where double gradient doesn t work Running the last line gives I m running version 0.9.0 CPU only in OS X. , girving Can you take a look In the complex case h tf.gradients tf.reduce sum g x is trying to take the gradient of 2 reals w.r.t. 10 reals which TensorFlow doesn t support. We only support gradients of single real scalars. I believe h tf.gradients tf.real tf.reduce sum g x should work fine. girving xx is a tensor of reals so g is a gradient of a real w.r.t. reals. Supporting this is the fact that the expression tf.real tf.reduce sum g produces the following error Whoops you re right. Apologies for misreading. Assigning to mrry to take a look although he s currently out. I think this was fixed by 946f515 which doesn t appear to be included in r0.9. It doesn t reproduce for me at HEAD but it does with the 0.9 binary. Can you try upgrading to a nightly or to the imminent r0.10rc0 and see if this is still a problem I m going to assume this is fixed now and close the issue. Feel free to reopen if the problem persists 
3359,Gradient computation fails concatenation in while loop body, Description When I was trying to implement RNN with while loop I tried to concatenate output to a matrix. This worked in forward passes but not in applying gradients. Also I saw that there are discussions 2237 about supporting Recursive NN. There s a workaround by transforming tree structures to a matrix. For example if we have a binary tree with its node values and structure be like Then we could transform it to a value vector V and a strictly bottom up structure matrix M Then we could build our graph with while loop by iteratively index into previous output. Note that this is without any specific inputs only knowing they ll be a vector and a matrix instead. For more details see https github.com jacobvsdanniel tf rnn inspired by https github.com ofirnachum tree rnn I also ran into problems of computing gradients for nested gather inside while loop but managed to worked around before fixes come up to 418 and 206 . Tensorflow version 0.9.0 Reproduction steps Workaround Error Logs ,Assigning to ebrevdo can you take a look Thanks. Have you tried using a TensorArray See tf.nn.dynamic rnn for example usage. Line 498 of gradients.py is too strong for while loop in grad.set shape t in.get shape It looks like a bug introduced recently and could potentially cause other problems. We probably want to add a test case to guard this when we fix it. Can t quite understand neither TensorArray nor any rnn implementations based on RNNCell at this moment You re experiencing a bug because of the way while loop gradients handle shapes of inputs and outputs. If your input and output shapes are different gradients can get confused. For this reason yuanbyu will soon add a parameter called shape invariants to while loop in which you can declare what shape information is static and what can change from iteration to iteration. gradients will respect these shapes. the default behavior will now require that shapes do not change from iteration to iteration so your original code without passing shape invariants would fail early with a useful error message Expect a fix to show up in a week. However the fix requires the user to provide a shape invariant for a loop variable if its shape is changed by the loop body. This was fixed with https github.com tensorflow tensorflow commit c46abae176717ef6c6649ba0ca1099c35b90194e. Please reopen if you are still experiencing this bug with the nightlies master branch. 
3726,nested scan functions break in gradient calculation bug ,I tried to produce some code to read out the values at multiple locations from a vector of results. To do this I used two nested scan functions to go through the batches and the vector of the multiple locations. This function correctly calculates the values when used alone but when I want to calculate gradients the code breaks at initialisation of the variables in the session with a error message I do not feel responsible for. Environment info Operating System iOS 10.11.5 Installed version of CUDA and cuDNN please attach the output of ls l path to cuda lib libcud rwxr xr x 1 root wheel 8280 Aug 9 18 11 usr local cuda lib libcuda.1.dylib rwxr xr x 1 root wheel 8280 Apr 13 08 02 usr local cuda lib libcuda.dylib lrwxr xr x 1 root wheel 45 Apr 13 08 03 usr local cuda lib libcudadevrt.a Developer NVIDIA CUDA 7.5 lib libcudadevrt.a lrwxr xr x 1 root wheel 50 Apr 13 08 03 usr local cuda lib libcudart.7.5.dylib Developer NVIDIA CUDA 7.5 lib libcudart.7.5.dylib lrwxr xr x 1 root wheel 46 Apr 13 08 03 usr local cuda lib libcudart.dylib Developer NVIDIA CUDA 7.5 lib libcudart.dylib lrwxr xr x 1 root wheel 49 Apr 13 08 03 usr local cuda lib libcudart static.a Developer NVIDIA CUDA 7.5 lib libcudart static.a lrwxr xr x 1 root wheel 47 Aug 9 18 38 usr local cuda lib libcudnn.5.dylib Developer NVIDIA CUDA 7.5 lib libcudnn.5.dylib lrwxr xr x 1 root wheel 45 Aug 9 18 38 usr local cuda lib libcudnn.dylib Developer NVIDIA CUDA 7.5 lib libcudnn.dylib lrwxr xr x 1 root wheel 48 Aug 9 18 38 usr local cuda lib libcudnn static.a Developer NVIDIA CUDA 7.5 lib libcudnn static.a If installed from source provide 1. The output from python c import tensorflow print tensorflow. version . 0.10.0rc0 2. The commit hash git rev parse HEAD 056db850614e0a06ce6f65b30f877c5789ab74f5 3. The output of bazel version Build label 0.3.1 homebrew Build target bazel out local fastbuild bin src main java com google devtools build lib bazel BazelServer deploy.jar Build time Thu Aug 4 09 58 27 2016 1470304707 Build timestamp 1470304707 Build timestamp as int 1470304707 Steps to reproduce 1. run code What have you tried 1. all orders of initialisation and running only gradient calculation Logs or other output that would be helpful Errortrace InvalidArgumentError Traceback most recent call last Users heiko anaconda lib python3.5 site packages tensorflow python client session.py in do call self fn args 964 try 965 return fn args 966 except errors.OpError as e Users heiko anaconda lib python3.5 site packages tensorflow python client session.py in run fn session feed dict fetch list target list options run metadata 946 feed dict fetch list target list 947 status run metadata 948 Users heiko anaconda lib python3.5 contextlib.py in exit self type value traceback 65 try 66 next self.gen 67 except StopIteration Users heiko anaconda lib python3.5 site packages tensorflow python framework errors.py in raise exception on not ok status 449 compat.as text pywrap tensorflow.TF Message status 450 pywrap tensorflow.TF GetCode status 451 finally InvalidArgumentError Input 0 of node gradients scan 1 while scan TensorArrayPack grad TensorArrayGrad TensorArrayGrad was passed string from gradients scan 1 while scan TensorArrayPack grad TensorArrayGrad TensorArrayGrad StackPop 0 incompatible with expected string ref. During handling of the above exception another exception occurred InvalidArgumentError Traceback most recent call last ipython input 1 d5cce368f52a in module 29 30 with tf.Session as sess 31 sess.run tf.initialize all variables 32 print sess.run grad Users heiko anaconda lib python3.5 site packages tensorflow python client session.py in run self fetches feed dict options run metadata 708 try 709 result self. run None fetches feed dict options ptr 710 run metadata ptr 711 if run metadata 712 proto data tf session.TF GetBuffer run metadata ptr Users heiko anaconda lib python3.5 site packages tensorflow python client session.py in run self handle fetches feed dict options run metadata 906 if final fetches or final targets 907 results self. do run handle final targets final fetches 908 feed dict string options run metadata 909 else 910 results Users heiko anaconda lib python3.5 site packages tensorflow python client session.py in do run self handle target list fetch list feed dict options run metadata 956 if handle is None 957 return self. do call run fn self. session feed dict fetch list 958 target list options run metadata 959 else 960 return self. do call prun fn self. session handle feed dict Users heiko anaconda lib python3.5 site packages tensorflow python client session.py in do call self fn args 976 except KeyError 977 pass 978 raise type e node def op message 979 980 def extend graph self InvalidArgumentError Input 0 of node gradients scan 1 while scan TensorArrayPack grad TensorArrayGrad TensorArrayGrad was passed string from gradients scan 1 while scan TensorArrayPack grad TensorArrayGrad TensorArrayGrad StackPop 0 incompatible with expected string ref. , ebrevdo Should this be fixed after https github.com tensorflow tensorflow issues 593 Though it s not a related error yuan is pushing the fix for this approximately on Monday. This has already been fixed so wait for the next push. The problem is that we need to use non ref for the handle input of TensorArrayGrad op. Well the ref handle fix doesn t fix the problem completely. I think we are back to the problem with nested scan 593 . A possible fix should be available in the next push. I still have this issue in r0.11. I have a nested scan and line 197 in control flow grad.py https github.com tensorflow tensorflow blob master tensorflow python ops control flow grad.py L197 grad ctxt graph. get control flow context results in a None value for grad ctxt. Does the problem still presist with latest versions Closing due to lack of response. Please reopen if necessary. 
3862,tf.cumprod s gradient produces nans given zeros,Example ibab Do you know the right way to fix this ,I ve placed a comment on this in the gradient code here tensorflow python ops math grad.py L889 https github.com tensorflow tensorflow blob 51fd9c024c4544ba1ef60862ec3f55b6e3ae79b1 tensorflow python ops math grad.py L889 I ve spent some time trying to come up with a version that doesn t use a division by x but didn t come up with anything elegant. Hmm that is pretty awkward So in general you need all prefix products with each possible entry excluded. Unfortunately if you write those as products of contiguous ranges you need quadratically many ranges. Ug. A fast O n solution. Assume the input vector x and the upstream gradient g has the form Elements are irrelevant in this computation because the leftmost 0 cause cumprod become 0 after that. The downstream gradient is To gain full speedup the forward computation should record the index of first occurrence of 0 and automatically memset 0 for the results after that. This optimization is unique to cumprod and can t be done to cumsum . The backward computation can just use that index to quickly do the job. Maybe it s a good idea to modify cumprod interface to def cumprod input use unsafe grad False so that user can force unsafe method if it can ensure no zero elements in input. So far this is useful in Deepmind s DNC model because it has a cumprod which often encounters zero. Sorry not familiar with C so no PR yet. The following workaround helps to avoid the NaN gradient issue it lacks the exclusive or reverse flags but they re easy enough to implement Can anyone comment on efficiency of this approach Automatically closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks It looks like there is a workaround posted. I submitted a fix to this issue in the PR 32714 
3932,l2 normalize broken in master, tf.nn.l2 normalize tf.constant np.ones 2 2 2 2 0 1 2 crashes with the following error ValueError Shape 1 3 must have rank at most 1 The problem is that l2 normalize function converts the dimension list into 0 1 2 double brackets and it stopped being supported recently. It works fine in 0.10 branch as far as I can tell. Shorter repro to get the same error tf.reduce sum tf.constant np.ones 2 2 2 2 0 1 2 ,Thanks the bug is in l2 normalize. tf.reduce sum is working correctly the dims argument must be a scalar or a vector. I ll work on a fix test The change that broke this was that reduce sum was not checking its shape contract and now it is. 
3978,Tensorflow hangs when enqueueing with session timeout specified,I am creating a simple FIFOQueue and enqueueing elements to it. Everything works fine when the session timeout is not specified code as below with timeout commented out Everything finishes correctly. But when I run the same code with timeout specified to 60 seconds The number of enqueue s is not deterministic but the code never finishes. Note that with timeout specified the code didn t manage to finish in a minute in comparison to less than 2 seconds with no timeout . Environment info Operating System lubuntu 14.04 kernel 3.13.0 32 generic Installed version of CUDA and cuDNN no CUDA using just CPU If installed from binary pip package provide 1. Which pip package you installed https storage.googleapis.com tensorflow linux cpu tensorflow 0.10.0rc0 cp27 none linux x86 64.whl 2. The output from python c import tensorflow print tensorflow. version 0.10.0rc0 ,PS I tried .10rc0 build with cuda on mac and ubuntu and that code snippet always finishes in a couple of seconds Thanks for reporting I was able to reproduce with the nightly version and should have a fix soon. sygi This should be fixed now in the nightly builds. thanks mrry Please can you reopen if the problem persists. 
4084,Process hanging when using TF SessionRun with multiple times the same input,It seems that if the same input appears multiple times in the inputs argument of TF SessionRun from c api.h then the TF SessionRun call never returns. This issue can be reproduced by modifying c api test.cc and replacing the line csession.SetInputs feed Int32Tensor 3 With csession.SetInputs feed Int32Tensor 3 feed Int32Tensor 3 According to gdb the process is waiting for a mutex in the RunState destructor from DirectSession. What related GitHub issues or StackOverflow threads have you found by searching the web for your problem none Environment info Operating System Linux 4.4 Installed version of CUDA and cuDNN none If installed from source provide 1. The commit hash git rev parse HEAD 008bcaea38815f46804fc3f56492f4dd93837a56 2. The output of bazel version Build label 0.3.1 Build target bazel out local fastbuild bin src main java com google devtools build lib bazel BazelServer deploy.jar Build time Fri Jul 29 09 09 52 2016 1469783392 Build timestamp 1469783392 Build timestamp as int 1469783392 If possible provide a minimal reproducible example We usually don t have time to read hundreds of lines of your code See above. What other attempted solutions have you tried Logs or other output that would be helpful If logs are large please upload as attachment or provide link . ,Thanks for the report. In general the C API is unforgiving when it comes to usage but this should be an easy and cheap piece of validation code to add. Running tests now... it should appear after the next push. 
4293,The train and test data of wide and deep example is broken,Wide and deep example code will download train and test data from https archive.ics.uci.edu ml machine learning databases adult adult.data and https archive.ics.uci.edu ml machine learning databases adult adult.test . But those files are broken now and you can wget to read the raw data. If I modify the file and run with this command it works. Maybe we should fix the raw dataset or change to other valid files. What related GitHub issues or StackOverflow threads have you found by searching the web for your problem Someone has asked the similar question in stackoverflow http stackoverflow.com questions 38558976 tensorflow wide deep example not working . Environment info Operating System Ubuntu 14.04 TensorFlow 0.9.0 If possible provide a minimal reproducible example We usually don t have time to read hundreds of lines of your code Clone the tensorflow project and run python wide n deep tutorial.py directly. Logs or other output that would be helpful , tobegit3hub Thanks for filing this issue I see that the 1x3 Cross validator line in adult.test is unexpected. Did you find any other issues in the two files We welcome pull requests to fix these kinds of problems Thanks tatatodd . These two files have unexpected empty line at the end of the files. After removing the first line of 1x3 Cross validator and the empty lines at the end it will work. I m not sure who maintains these datasets. How can we create the pull request and contribute for this You can fork the repo work on your own branch to make the changes and then submit a pull request. Thanks terrytangyuan and I knew how to create pull request. I mean how to fix this issue. The dataset is from https archive.ics.uci.edu which may be beyond the scope of TensorFlow. Would you like to maintain the dataset by yourselves Or contact someone who can fix this. Automatically closing due to lack of recent activity. Since this issue is old at this point please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you. 
4308,tf.image.rot90 returns None if the argument is not a Python integer, What related GitHub issues or StackOverflow threads have you found by searching the web for your problem Pointed out in this Stack Overflow http stackoverflow.com q 39418948 3574081 question. Environment info 1. The commit hash git rev parse HEAD bf5b2f0185d4b61329a0bf7a56827661ef6526a1 If possible provide a minimal reproducible example We usually don t have time to read hundreds of lines of your code ,
4742,Atrous convolution does not preserve tensor shape,For an input with an undefined batch size atrous conv2d emits tensors where all except the final dimension are undefined For concrete batch sizes everything works as expected. Tested on 0.10.0rc0 ,Indeed I have reproduced the problem. I believe gpapan implemented atrous conv2d and might have thoughts on how easy this would be to fix. 1 1 Can I work on this I think I can solve this. That would be great AnishShah can you describe what the problem is vrv The problem is in this line https github.com tensorflow tensorflow blob master tensorflow python ops nn ops.py L974 . It is using ShapeOp to estimate paddings for ShapeToBatchOp . That is why it is not able to predict the output shape. I tried few things but I was unsuccessful. What do you suggest AnishShah do you have any updates I tried but not able to solve it. Sorry. On Feb 15 2017 2 35 PM Andrew Selle notifications github.com wrote AnishShah https github.com AnishShah do you have any updates You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 4742 issuecomment 279891930 or mute the thread https github.com notifications unsubscribe auth ADB1P8wK8L7VKrPwcRuxAvXN3hPCgxaoks5rclZtgaJpZM4KNQDp . I think this may also shrink the dimensions as well I believe a source of confusion here may be due to varying definitions of atrous convolution depending on the paper being read. Basically some papers defined atrous convolutions incorrectly when they really meant dilated convolutions. This is explained in Multi Scale Context Aggregation by Dilated Convolutions https arxiv.org abs 1511.07122 with the authors implementation in https github.com fyu dilation. Also see the related issue https github.com tensorflow tensorflow issues 3492. I think what people are hoping for is a new function or perhaps simply an additional parameter to the atrous function is the ability to specify a constant scale of the output data so this can behave the same as these papers where the output dimensions are the same as the input since this is particularly useful for semantic segmentation. I believe this is implemented in tensorflow models slim ... resnet utils.py in the function conv2d same https github.com tensorflow models blob master slim nets resnet utils.py L77 . It may even be simple enough to migrate that option directly upstream. warmspringwinds is also very familiar with this and may be able to verify that everything I ve said here is correct or perhaps contribute some additional information. vrv or tatatodd regarding the TensorFlow API design if this version of dilated convolution with constant input output dimensions is supported directly in tf.nn should it be applied via 1. atrous conv2d with the SAME padding flag 2. atrous conv2d with a separate parameter 3. a totally separate function I m going to delegate to gpapan on this one who knows more about semantic segmentation and atrous conv . I would suggest we d need a totally separate function because of potential confusion between atrous and dilated. That being said perhaps someone just posts a good implementation of it here for now instead of having to add it to the API Usually a good sniff test for adding something to the API is whether it s used fundamental in a state of the art model for an important problem. Otherwise everything under the sun gets added to the core API and our team can t support it all . ahundt In tensorflow atrous convolution and dilated convolution are used as synonyms to mean dilated convolutions as in the Multi Scale Context Aggregation by Dilated Convolutions https arxiv.org abs 1511.07122 paper you cited. AnishShah tf.nn.convolution now provides a more generic interface for atrous convolution for any number of dimensions and I believe it has slightly more complete shape inference but there are still cases where it does not infer some of the output shape dimensions even when it could. If you are going to add better shape inference code I suggest adding it to tf.nn.convolution as there is separate work underway see 7545 to make atrous conv2d simply forward to tf.nn.convolution. To fix it you will need to use set shape function on the output tensors to set the additional shape information. I think it would be possible to do this inside of with space to batch specifically on the input converted tensor and then again on the result converted tensor. You will unfortunately have to duplicate some of the work done in calculating the shapes for space to batch nd and batch to space nd. The reason is that a tensor can either be constant or non constant but not partially constant. jbms Thanks for your comment. Does the current code in with space to batch https github.com tensorflow tensorflow blob 57688fab6ad584034b1e6949e3101adc2c08ca10 tensorflow python ops nn ops.py L143 or 7545 have a mode where the output tensor has the same dimensions as the input tensor This is the case for conv2d same https github.com tensorflow models blob master slim nets resnet utils.py L77 in tensorflow models I think it would be very productive to add a note in with space to batch https github.com tensorflow tensorflow blob 57688fab6ad584034b1e6949e3101adc2c08ca10 tensorflow python ops nn ops.py L143 explaining what the output dimensions would be relative to given input dimensions in as they vary by configuration. Regarding your comment on atrous vs dilated convolutions I quoted the following from a footnote in the Multi Scale Context Aggregation by Dilated Convolutions https arxiv.org abs 1511.07122 Some recent work mistakenly referred to the dilated convolution operator itself as the algorithme a trous . This is incorrect. The algorithme a trous applies a filter at multiple scales to produce a signal decomposition. The algorithm uses dilated convolutions but is not equivalent to the dilated convolution operator itself. Perhaps this is a bit pedantic but if the paper is stating this correctly wouldn t it mean TensorFlow is mistaken in its use of atrous and dilation as synonyms This seems to imply that what is described as the atrous algorithm only dilated filter size while the dilated version can be configured so the output is the same size as the input. Okay I answered my own question. Yes both tf.nn.atrous conv2d and tf.nn.convolution produce the same output dimensions with the SAME flag. I was mixing up the effect of filter size on output dimension sorry about that. I made this test and ran it on tf 1.0 which does confirm the original issue with None values Which produces this output with the additional None values on the last set of printouts ahundt in TF atrous and dilated convolution mean the same thing. One of the parameters that they accept is which specifies the dilation rate. Definition of rate is consistent in the Deep Lab paper and the paper that you have cited. I think the confusion with the naming is similar to the case with which a lot of people use to mean that they perform while at the same time refers to a completely different operation in Signal Processing field. There are different ways to implement dilated convolution. TF has it implemented by sampling the input feature map which is described in the Deep Lab paper. The piece of code that you refer to actually uses this implementation under the hood. At the same time dilated convolution is itself an ordinary convolution meaning that if you apply it with the padding it should produce the output with the same spatial dimensions. In case of Image Segmentation dilated convolution is used to make it possible to use weights from Image Classification networks after reducing their in network downsampling by means of removing layers responsible for downsampling or setting their stride to . All of this allows to acquire the prediction map that is downsampled by a smaller factor most Image Classification models have a downsampling factor of for example it can be reduced to by following the approach described in these papers . After that you can use bilinear upsampling or learn the upsampling kernel yourself during training to get the prediction map of the same size as the input image. You can find an example of adopting Resnet 101 for Image Segmentation by employing the aforementioned approach here https github.com warmspringwinds tf image segmentation blob master tf image segmentation models resnet v1 101 8s.py L108 . warmspringwinds Thanks Got it now. All sorry I ended up hijacking the issue due to the mismatch between my mental model and the design. At least a test script came from it and I learned something thanks for the clarifications. ahundt The precise definition of with space to batch is given in the docstring. However the actual output dimensions depend entirely on the behavior of the underlying op that is passed as an argument. I don t think it is possible to specify the output dimensions in a particularly concise way. It isn t intended to be used directly normally but rather is intended to be used to define new dilated operations. 8411 doesn t actually fix this issue it just adds some documentation but does not actually improve the static tensor shape information which is what this issue is about. Sorry about that. Sorry that was actually my fault I meant to write that it resolves a point of confusion discussed in this issue. No it s my fault I did edit your description to make the PR close this issue. That was a little optimistic. Aha didn t realize that well at least things are set correctly now. This seems to be fixed in at least tensorflow version 1.1.0 rc2 It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. Closing since this seems obsolete but please reopen if it needs attention. 
4913,Saver returning paths that cause GetMatchingPaths to go digging around in parent dirs,Note This works without problems in 0.10.0rc0 default pip install ed according to website . Environment info Operating System Ubuntu 15.10 Installed version of CUDA and cuDNN please attach the output of ls l path to cuda lib libcud If installed from binary pip package provide 1. A link to the pip package you installed. Tried with two packages https ci.tensorflow.org view Nightly job nightly matrix linux gpu TF BUILD IS OPT OPT TF BUILD IS PIP PIP TF BUILD PYTHON VERSION PYTHON2 label gpu linux lastSuccessfulBuild artifact pip test whl tensorflow 0.11.0rc0 cp27 none linux x86 64.whl https storage.googleapis.com tensorflow linux gpu tensorflow 0.11.0rc0 cp27 none linux x86 64.whl 2. The output from python c import tensorflow print tensorflow. version . If possible provide a minimal reproducible example We usually don t have time to read hundreds of lines of your code What other attempted solutions have you tried Logs or other output that would be helpful Relevant part of error the file tmp 6d7079c109e5 0 saver 853 is supposed to be loaded by self.saver.restore tf.Saver instance it exists . After restarting the computer and re running the above the PermessionDeniedError now points at tensorflow.python.framework.errors.PermissionDeniedError tmp systemd private bc171111db824721b4f4cec0e0a363ec systemd timesyncd.service MARv8I ,Is there any chance you might be running into the same issue as 4852 jart thanks for pointing to the updated 4852. However I m not sure if it is related to numpy in my case I see in the log of 4852 that something with numpy happens. This is not the case in my code. The model I want to save was stored by TF with early stop name self.saver.save self.sess save path path . No numpy involved inbetween this saving and restoring a few lines later. Also it seems that according to the error message above it wants to access some systemd temp file which it should not try to access. A quick experiment confirmed If the above save path is something like tmp i.e. in my local home the problem does not exist. So the global tmp causes problems. tensorboard logs into tmp without problems as additional information. Reading https github.com tensorflow tensorflow blob master tensorflow python lib io file io.py L252 it says errors.OpError If there are filesystem directory listing errors. So the above error is supposed to happen if I interpret this correctly. However it should not as the listing error is concerned with a directory tensorflow has no business in dealing with. jart rohan100jain Going a bit through the history this seems to be related to a larger change introduced some time ago here https github.com tensorflow tensorflow commit 02adcaeec5f541870750e46ef5b1663bd9b61246 diff 1cc1b8538a5abd0cad0209d2aa34b116R134 Your most recent comment leads me to suspect this might be related to 4921. Could you please take a look at that I took a closer look. What is the save path you re passing to self.saver.restore If it s a value like tmp foo then FileSystem GetMatchingPaths is going to go digging around in your tmp directory. It needs to be a value like tmp foo bar of nnnnn which is returned by Saver.save . So the save path parameter really should be called save pattern . It is tmp 6d7079c109e5 0 saver 853 returned by Saver.save though I set this name there too . So you are saying that it needs to be in a separate directory in tmp e.g. like tmp 6d7079c109e5 away from computer can t check rohan100jain This one sounds like it could be a file i o related bug. I m not sure if it s necessarily related to 4921. Although there does appear to be overlap. osdf I m stating the painfully obvious here but sudo might be a workaround for the time being. I think for a short term fix can you make it sub dir I need to change GetMatchingFiles to ignore some of these permission failures and not throw an error. The way get matching files works is that it finds the top level directory that doesn t have a wildcard which is tmp and then recursively looks into it to find matches. So making a subdir will not only fix this but make this go faster too. But I agree that we need to refine the saver GetMatchingFiles behavior to not do so badly on such cases. Thanks rohan100jain and jart for taking a look I made https github.com tensorflow tensorflow commit c15bb7b6f64fbc4bfd19aeccfd8b8df99012b74c This should help a bit I think. Hopefully this would mean that the Permission denied error directories aren t being looked at. I ll fix that too now though. Thanks rohan100jain for your effort I resolved the issue now by having a subdirectory with in tmp which then is used to write saver objects. Actually this is much cleaner and easier to handle just move the whole subdirectory out of tmp . Should I close this issue Actually I would like to fix this entirely i.e. even without the sub dir . Could you try that out and let me know if there are still issues I think this should be fixed now. Closing issue now. Please reopen if you still face problems. 
5321,Filesystem FileExists interface is broken,The FileSystem define the following interface which can t handle temporary runtime errors properly but just swallow them like GCS HDFS and any customized DFS It should be changed to I can provide a patch if no one is working on this., jhseu I m marking this contributions welcome assuming you agree with that change. Yeah I agree that s an issue. I can make the change though because we also have to change the implementation for our internal filesystem. This is more friendly less error handling code for the caller to handle unexpected errors And IsDirectory should also be changed to both for API semantics consistency and making the caller easier to handle the runtime error most of the current caller code just ignored them May be this can be addresses by another change. I m creating a patch for FileExists first. Fixed internally. It ll show up during the next sync. As mentioned in your pull request we decided on for consistency with how we ve been using these error codes elsewhere. 
5407,tf.contrib.metrics.streaming precision doesn t accept predictions and labels of dtype tf.bool,According to documentation and comments in code tf.contrib.metrics.streaming precision should accept predictions and labels of boolean type but it doesn t seem to be true. To reproduce modify testAllCorrect procedure in this file https github.com tensorflow tensorflow tree master tensorflow contrib metrics python ops metric ops test.py by adding dtype tf.bool to tf.constant as below The test will fail This happens in streaming true positives function in this file https github.com tensorflow tensorflow tree master tensorflow contrib metrics python ops metric ops.py when executing Can be repeated simply as Environment info Operating System Ubuntu 14.04.1 ,The links to files are both broken for me. Updated the links please check again To fix this we ll need to replace the 1 with True if the input is a bool. Contributions welcome Automatically closing due to lack of recent activity. Since this issue is old at this point please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you. 
5477,Deal with control flow context when copying op,In the current implementation of copying ops both tf.contrib.copy graph and tf.contrib.graph editor The code of copying an op looks like this But the op. control flow context is not copied at all. This causes problems when trying to compute gradients on a copied subgraph with control flow op like tf.cond . The error looks like This is because the function ZerosLikeOutsideLoop uses control flow context when the op is tf.switch I tried setting new op. control flow context as op. control flow context Now the error step passed. But I m not sure whether this is right for dealing with control flow context copy. Do you have some advice ,You can use copy scoped meta graph in https github.com tensorflow tensorflow blob master tensorflow python framework meta graph.py. You can find examples of how to import export and copy scoped meta graph in https github.com tensorflow tensorflow blob master tensorflow python framework meta graph test.py and https github.com tensorflow tensorflow blob master tensorflow python training saver test.py. Please let me know if that works for you. Sherry sherrym Thanks. This is very similar to what I want. But I feel there are some difficulties when using it to copy ops in a subgraph. First these operations are not always in the same scope so I have to copy them one by one. What I do is to traverse the subgraph and get those ops. However some operations share the same name with their uproot scopes e.g. tf.random normal creates a name scope named random normal while the tf.add operation in it shares the same name. This causes problems when I m trying to copy this tf.add using its name because copy scoped meta graph will copy the outer name scope. Do you have some workaround for this Maybe a copy function accepting operation objects other than scope names could be more suitable for my case. sherrym After spending more time figuring this out I now think it should be of not that much effort to implement a copy op meta graph . I think I can work on it and will submit a PR. Do you think it s okay to serve as an api in tensorflow main library Because I m writing some high level library based on this and hope to keep the base utilities as stable as an api in tensorflow main library. thjashin You are always welcome to submit a PR for contrib. Leaving this open despite being old because I don t think it s fixed. In addition I don t think importing a metagraph will work either I haven t actually tested but have been looking at this code recently . thjashin do you know if this issue is still happening relevant annarev Sorry I missed this comment. Yes it s still happening. annarev Hi any updates on this Sorry for delay I missed it somehow. I am actually not the right owner since I am unfamiliar with copy graph and graph editor code. So I am reassigning to frankchn . Frank I saw that you might own moving tf.contrib.copy graph out of contrib. But feel free to reassign if there is a better owner. Hmm I am probably not the right person for this the internal CL was from a sync pull skye do you know who might know about this We re currently reimplementing control flow in TF such that it no longer uses control flow contexts. It s still under construction but you can set the environment variables TF ENABLE COND V2 1 TF ENABLE WHILE V2 1 to try it out. Please feel free to report any errors you see using the new control flow. thjashin Is this issue resolved Please close If it was resolved already. Thanks jvishnuvardhan I m not sure because I don t use copy anymore. Feel free to close the issue. thjashin Thanks I am closing the issue. Please feel free to open a new ticket if the bug persists in latest version. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 5477 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 5477 No a 
5543,Constant folding doesn t remove control edges,I believe that when constant folding takes place and a section of a graph is replaced by a constant that only the data output edge of the replaced node is removed. I believe that I can see that a graph of nodes ending up in a Div part of the gradient generation bit is replaced by a Const. The output of the Div goes to a Mul and this is changed to the new const correctly. However there is a control output from the Div going to a Const not sure why but it is . This is not changed to the Div replacement. Consequently the dead node pruning doesn t remove the original Div. Here is some trace During the constant folding Graph Before nodes 67 edges 109 Graph Constant graph nodes 32 edges 42 Constant foldable 32 67 Replacing name gradients Mean grad truediv id 28 op device job localhost replica 0 task 0 device ipu 0 def gradients Mean grad truediv Div T DT FLOAT device job localhost replica 0 task 0 device ipu 0 gradients Mean grad Tile gradients Mean grad Cast 0 with a constant Replacing edge to gradients Square grad mul 1 0 During the post constant folding pruning PruneForReverseReachability gradients Square grad mul 1 gradients Mean grad truediv 0 cf 1 PruneForReverseReachability gradients Square grad mul x gradients Mean grad truediv After the pruning Graph ConstFolding nodes 68 edges 110 When the graph is passed to the device it contains Node gradients Square grad mul x Const dtype DT FLOAT value Tensor type float shape values 2 device job localhost replica 0 task 0 device ipu 0 gradients Mean grad truediv ,Example During constant folding this leads to You can see that there is a constant replacing the Add but that the Add remains because it is referred to by the control edge. I think that the output should be One more example. This is the getting started.py that is in the TF documentation. I have modified it so that it uses placeholders rather than allowing the X and Y data to be constants. Node n29 which has a control dependency on n28 is the problem. n28 was the Div which was replaced. keveman would you please comment Thanks. I should point out that if I bind the nodes to the CPU then the results are the same. these examples are for my own device back end It seems that constant folding.cc ReplaceTensorWithConstant assumes that the node that will be replaced will have multiple outputs and only replaces those edges that are fed by the one replaced output. BroadcastGradientArgs is a good example. It really needs to be replaced by 2 constants and the code as it stands works well for that. Perhaps it is worth considering what it means to have a node that only has a control output assigned to it Is this valid If not then the control output should be eliminated then then the dead code pruning algorithm will kill off that node and it s upstream nodes. I ve not played with the control flow loops conditionals so I don t know if nodes with only control outputs are useful. I would expect that loops and conditonals are done with Boolean edges not control edges. If this is the case then I think that adding the control edge removal thing is the correct thing to do. I am also wondering what is the rationale behind the memory constraints in the ReplaceTensorWithConstant Why would you want to keep nodes that perform arithmetic on a device unnecessarily I can imagine that you might consider that it is important to do floating point arithmetic on the target platform because of differences in rounding although on a neural network I would hope that the result would work around that . In the case of integers I can t decide why constant folding would always take place. Could you describe why it would be limited to HOST MEMORY only Here is a diff for trying out remove control outputs from nodes that have only a control output after constant folding change diff.txt https github.com tensorflow tensorflow files 589083 diff.txt Sorry for the late response. Constant folding is conservative perhaps overly in what it replaces. In your case since the original node had an outgoing control dependence it didn t get replaced. Note that the analysis to decide whether that is safe to do is somewhat non trivial. We have to determine whether that node is control dependent on a stateful node. We chose to let it be conservative. If you have a real example where the performance is poor because of this conservativeness I ll be happy to take a look. I see the problem. My case isn t really a performance issue per se. I am trying to implement a device on TF for our new hardware. The extra nodes of integer math are getting in the way of the initial bring up. If there are stateful nodes in the graph then why does the constant tree finder consider them acceptable to be within the constant tree Isn t the constant that is generated there is one generated is not valid after the 1st run through the graph In which case the constant should not replace that section of the graph at all I realize i failed to write sensible language in that last message. What I intended to say is that replacing a stateful node with a constant doesn t seem right. If the stateful node changes state then the constant is invalid. Either the stateful node should not be included in the constant sub graph or the constant sub graph should be re evaluated whenever the stateful node changes in which case there doesn t seem to be a lot of point in having it. Hi In fact I m pretty certain that the following code taken from constant folding.cc will prevent the constant sub graph from containing any stateful nodes. In which case it should be ok to apply the control edge removal. Started looking at this. Will have a fix soon. Now that my own device is an XLA device I am no longer concerned about this. However this is still a problem as far as I can see. unless the dead code removal is removing nodes that only have a control output. The code below is my best effort to avoid constant folding at all costs . It https www.cheekywinx.com.au doesn t work. I end up with constant 100011111 in the instruction stream. llc O0 code.ll print after all reveals that the folding happens at Expand ISel Pseudo instructions pass. ModuleID 0 target triple x86 64 unknown linux gnu define i64 0 BlockEntry0 cell alloca i64 align 8 store volatile i64 0 i64 cell align 8 volatile zero3 load volatile i64 i64 cell align 8 base add i64 volatile zero3 100000000 volatile zero4 load volatile i64 i64 cell align 8 opaque offset add i64 volatile zero4 11111 casted base inttoptr i64 base to i8 gep getelementptr i8 i8 casted base i64 opaque offset as ptr bitcast i8 gep to i64 loaded load i64 i64 as ptr align 4 as function inttoptr i64 loaded to i64 i64 ret val tail call i64 as function i64 0 ret i64 ret val attributes 0 nounwind I realize that my problem can be solved by adding some intrinsic which at codegen level would unfold to simple movabs reg imm64. But I d like to have a temporary solution for the time being. 
5772,could not set cudnn filter descriptor CUDNN STATUS BAD PARAM,The version of cuda and cudnn meets the requirement but still cannot use cudnn properly. What related GitHub issues or StackOverflow threads have you found by searching the web for your problem Environment info Operating System Linux version 3.16.0 30 generic buildd kissel gcc version 4.8.2 Ubuntu 4.8.2 19ubuntu1 40 14.04.1 Ubuntu Installed version of CUDA and cuDNN please attach the output of ls l path to cuda lib libcud rw r r 1 root root 558720 Sep 15 07 02 usr local cuda lib64 libcudadevrt.a lrwxrwxrwx 1 root root 16 Sep 15 07 05 usr local cuda lib64 libcudart.so libcudart.so.8.0 lrwxrwxrwx 1 root root 19 Sep 15 07 05 usr local cuda lib64 libcudart.so.8.0 libcudart.so.8.0.44 rw r r 1 root root 415432 Sep 15 07 02 usr local cuda lib64 libcudart.so.8.0.44 rw r r 1 root root 775162 Sep 15 07 02 usr local cuda lib64 libcudart static.a lrwxrwxrwx 1 root root 13 Nov 22 10 55 usr local cuda lib64 libcudnn.so libcudnn.so.5 lrwxrwxrwx 1 root root 17 Nov 22 10 55 usr local cuda lib64 libcudnn.so.5 libcudnn.so.5.1.5 rw r r 1 root root 78065952 Nov 22 10 09 usr local cuda lib64 libcudnn.so.5.0.5 rw r r 1 root root 79337624 Nov 22 10 17 usr local cuda lib64 libcudnn.so.5.1.5 rw r r 1 root root 69756172 Nov 22 10 17 usr local cuda lib64 libcudnn static.a If installed from binary pip package provide 1. A link to the pip package you installed export TF BINARY URL https storage.googleapis.com tensorflow linux gpu tensorflow 0.11.0 cp27 none linux x86 64.whl 2. The output from python c import tensorflow print tensorflow. version . I tensorflow stream executor dso loader.cc 111 successfully opened CUDA library libcublas.so locally I tensorflow stream executor dso loader.cc 111 successfully opened CUDA library libcudnn.so locally I tensorflow stream executor dso loader.cc 111 successfully opened CUDA library libcufft.so locally I tensorflow stream executor dso loader.cc 111 successfully opened CUDA library libcuda.so.1 locally I tensorflow stream executor dso loader.cc 111 successfully opened CUDA library libcurand.so locally 0.11.0 If possible provide a minimal reproducible example We usually don t have time to read hundreds of lines of your code when trying to call a function that is only supported by cudnn for example conv2d , yetionyo Could you please supply a minimal repro example zheng xq Can you think of any reason why this might happen It is proved to be irrelevant with conv2d itself maybe it s related with the way I used conv2d because I can run this demo without this problem. import tensorflow as tf my data tf.random normal 20 20 20 3 my filter tf.random normal 3 3 3 10 conv result tf.nn.conv2d my data my filter strides 1 1 1 1 padding VALID sess tf.Session result sess.run conv result print result But it s a little strange that what kind of operation would lead to this problem it s more like a failure of calling cudnn Similar problem to 5476 4909 and 4111 All these seem to be mention passing an empty numpy array into TF.... zheng xq Is there perhaps some input validation missing on cuDNN ops Yeah these problems are similar to mine. Maybe empty numpy array is not main reason in this problem but some improper ops indeed exist. Thanks I d like to leave this open until we understand why an empty array causes a CUDA error rather than a TensorFlow runtime InvalidArgument error status. Looks like this is still an issue on current master . It would be nice to get this fixed The CUDA error is quite mysterious when you run into it. This issue seems to affect TensorFlow Fold which uses dynamic network structures and can often generate empty tensor if a path is not used in a dynamic batch It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. There is a pull request that should handle the issue. Please check after the pull request has been approved https github.com tensorflow tensorflow pull 15264 Nagging Assigneee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. 15264 has been merged so I believe the issue should have been fixed by that. Please reopen if it still exists. Just found this page. I m seeing this error with a fresh nightly tensorflow gpu on Ubuntu. So despite the merge this doesn t look resolved. Same here I get this error as well on ubuntu tf 1.4.1 not the nightly build. drscotthawley you need to provide more details logs small repro code etc for people to tell whether it s the same problem empty tensors into cudnn or not. The fix above only adds support of empty tensor on certain ops and very likely there are ops not covered. ppwwyyxx Thanks for the comment drscotthawley and kirk86 could you provide more info so that I can take a closer look ppwwyyxx yzhwang I had just downloaded a fresh CUDA from NVIDIA which defaults to version 9.1 not realizing that TF didn t support that yet. I resolved this problem by downgrading to CUDA 9.0. You can close this issue again. kirk86 try using CUDA 9.0 instead. Also I m using CUDNN 7.0.5 and it s working. Might be worth noting I ve built TF from source before but couldn t manage to do so using CUDA 9.1. I don t recall the errors just that downgrading to 9.0 finally enabled me to get back to work. drscotthawley Thanks for you answer but in my case I can t do that. It s a shared system and I m not an admin. 
6034,ctc beam search decoder s log probabilities holds invalid values, Environment info Operating System OS X 10.11.6 TF Version 0.10.0rc0 No GPU Example Ran log probabilities op created from The decoded result is as expected. However the log probabilities contains positive values which can not be log probabilities. For example with batch size 4 and top paths 10 the log probabilities printout is as follows Other attempted solutions None Logs or other output that would be helpful Link to the entire code in context 1 https github.com mozilla DeepSpeech blob issue8 DeepSpeech.ipynb ,I may not get to this before January. Is the returned log probabilities supposed to represented the logs of a probabilities as documented 1 https www.tensorflow.org versions r0.12 api docs python nn.html ctc beam search decoder or does it represent un normalized logs of a probabilities Should be normalized I ll have to trace this exact example to see what s going on I have the same problem with beam search I get positive scores but with greedy I get negative ones ebrevdo Has there been any progress on this Please provide a small example input that causes the issue so I can debug I am not really familiar with the code but is it possible that it is caused by line 183 in tensorflow tensorflow core util ctc ctc beam search.h scores b i beam log probabilities i ebrevdo Until I get time to write a smaller example one can look at ctc decoder ops test.py https github.com tensorflow tensorflow blob 584d4921014db921a8f4722749adff09737826a8 tensorflow python kernel tests ctc decoder ops test.py Documentation of ctc beam search decoder https www.tensorflow.org api docs python nn connectionist temporal classification ctc ctc beam search decoder states However for ctc decoder ops test.py https github.com tensorflow tensorflow blob 584d4921014db921a8f4722749adff09737826a8 tensorflow python kernel tests ctc decoder ops test.py the expected results the log prob truth values are These are not log probabilities as they are positive. ebrevdo Is there any progress on this issue I have this problem with TF 1.0 ebrevdo same issue here same issue This issue actually prevented a project here at Dropbox from migrating our training pipeline from Torch to TensorFlow unfortunately causing us to stay with Torch. kdavis mozilla in your initial report what were the seq len values passed in ebrevdo Sorry but I don t even know anymore it was over a year ago. We ve since created our own custom ctc beam search decoder operator extending the TF operator with KenLM. We use that now. As far a I remember the problem is still there. The relative ordering of results via their log probabilities is fine but the absolute log probability values are not. The problem is still there. It s related to the subtraction of the maxCoeff from the input and then the subsequent negation of the log probs. I can submit a PR with the fix but don t have time to workthrough the broken test cases. The subtraction is there to avoid overflow underflow issues iirc. Is your suggestion to add the values back in at the very end On Fri Jan 5 2018 at 9 38 AM Ryan Leary notifications github.com wrote The problem is still there. It s related to the subtraction of the maxCoeff from the input and then the subsequent negation of the log probs. I can submit a PR with the fix but don t have time to workthrough the broken test cases. You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 6034 issuecomment 355616640 or mute the thread https github.com notifications unsubscribe auth ABtimyYvcnLmtAmySIP3TuRdLm1wa5nPks5tHl4ogaJpZM4LCPHW . ryanleary Can we fix this issue in Python side with touching C top paths 3 beam width 400 I get these log probs ebrevdo yes I think something like that needs to occur to the extent that it can be done . What underflow overflow issues have you experienced Is that still really an issue even though we re in the log prob space Is there any progress on this issue Up. Sorry unfortunately i do not currently have cycles to look at this. It s an important issue though so I won t close it for now. On Tue Jul 10 2018 at 2 13 PM Igor Macedo Quintanilha notifications github.com wrote Up. You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 6034 issuecomment 403967625 or mute the thread https github.com notifications unsubscribe auth ABtim7citQ I7xw8IKTqZgkdkUjc6d Dks5uFRj5gaJpZM4LCPHW . What do you mean This issue has more than one year now and as you said it is an important issue. Isn t there anyone here that can have a look into this It s being triaged On Wed Jul 11 2018 5 20 AM Igor Macedo Quintanilha notifications github.com wrote What do you mean This issue has more than one year now and as you said is an important issue. Isn t there anyone here that can have a look into this You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 6034 issuecomment 404148969 or mute the thread https github.com notifications unsubscribe auth ABtim FIjcaaxPkbFdspsz3Ug ezjH3lks5uFe2PgaJpZM4LCPHW . Nagging Assignee ebrevdo It has been 34 days with no activity and this issue has an assignee. Please update the label and or status accordingly. I think this is fixed now ebrevdo What commit fixed this problem https github.com tensorflow tensorflow pull 21187 ebrevdo Thanks for the info 
6088,Strided slice op CHECK failure,Two reports of a CHECK failure in the strided slice op have surfaced on Stack Overflow Tensorflow print value of a tensor http stackoverflow.com q 40979001 3574081 Tensorflow evaluate Aborted core dumped http stackoverflow.com q 40963017 3574081 In both cases the error message is ...which appears to match the CHECK here https github.com tensorflow tensorflow blob d9c173fc7adf9443d58310dd08a84b9eb89d4c8f tensorflow core kernels strided slice op.cc L90 . It looks like both questions might be using the same model code so I ll ask the questioner to post additional details if possible.,I believe this bug has already been fixed at least a month and a half ago . Did they specify what verison of TensorFlow they are using and if they tried it with the latest RC Hi I have encountered this issue on TF 0.10 release on my macbook pro all of a sudden I wonder what has changed it has been fine all along. . It s MacOS Sierra 10.12.1. and TF release 0.10 is encountering this issue and changing it to 0.11 has already fixed the issue. Sounds like this has been fixed. Please reopen if this is still an issue in the latest release. 
6119,Different prediction result for tf.learn QuickStart ,Hello Today I upgrade tensorflow package and read the tutorials from the beginning https github.com tensorflow tensorflow blob master tensorflow g3doc tutorials tflearn index.md .And find the result of classifier prediction is different it is 1 1 . I remember that the old version tf run out 1 2 . So what s the problem The OS is OSX EI Capitan and tensoflow is 0.12.0 rc0 If I turn on the INFO log level the log lists below ,same issue with Linux server CentOS Linux release 7.2.1511 Core tensorflow version 0.12.0 I got the correct predictions 1 2 of my first run but my following runs were all 1 1 ... same problem .. still no solution Make sure you delete any pre existing models. Assuming this is the iris tutorial take a look at classifier tf.contrib.learn.DNNClassifier feature columns feature columns hidden units 10 20 10 n classes 3 model dir tmp iris model If you don t delete the files in model dir tmp iris model you will be modifying the old model rather than creating a new one. Update to tensorflow 0.12.1. It seems fixed. Also does anyone have a way to get rid of the error messages The API is deprecated and tf.contrib.learn keeps changing so it s a bit hard to keep track of API updates. I made a fix version of this here. Using the custom pipe lines. https github.com XRayCheng tensorflow iris fix Looks like jacktang said this was fixed in 0.12.1 and XRayCheng has a fix. I m closing this out. Feel free to file issues for new problems you may run in to With v1.1.0 this issue still occurs. The first output is 1 2 and all subsequent runs are 1 1 until I destroy the tmp model. XRayCheng s fix is not actually a fix README still forewarns that all non fresh runs will produce 1 1 . Some kind of clarification would be nice. Picking up TF for the first time and it s pretty rough when the main documentation examples aren t replicated locally. lukeed You ve already found the solution. After you run the model once it will be trained and stored in the tmp folder. If you do not delete the tmp folder you are retraining an existing model rather than doing one from scratch. It makes sense that it would have different results agriasgg Thanks But if that s truly the case then that should be included as a note imo. I was under the assumption that the model was only looking at the same CSV data every time which means it should have arrived to 1 2 every time. Nowhere in the example documentation did it say that every run was going to affect the subsequent outputs. 
6171,tfprof Python3 incompatibility, This line in tfprof logger.py https github.com tensorflow tensorflow blob 20c3d37ecc9bef0e106002b9d01914efd548e66b tensorflow contrib tfprof python tools tfprof tfprof logger.py L127 uses dict.iteritems which breaks my Python 3 code. ,Seems like that should be six.iteritems dict On Wed Dec 7 2016 at 12 46 PM Denny Britz notifications github.com wrote This line in tfprof logger.py https github.com tensorflow tensorflow blob 20c3d37ecc9bef0e106002b9d01914efd548e66b tensorflow contrib tfprof python tools tfprof tfprof logger.py L127 uses dict.iteritems which breaks my Python 3 code. You are receiving this because you are subscribed to this thread. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 6171 or mute the thread https github.com notifications unsubscribe auth AABaHIRRWmSWEUEGsT3CtYxPjRpN ZlWks5rFxsRgaJpZM4LHFgr . Can I work on this or anyone is working on this internally already A change has been submitted. Should be available at next release. Fixed 6171 panyx0718 Is there more documentation for this tool. The tool s output is hard to analysis for me. 1. TFProfRoot 0us 312.56ms what is the meaning of 0us and what is the meaning of 312.56ms. I will add more docs. x y. x is the current ops stats. y is aggregated stats of itself and all its descendants according to name scopes In your case the number is the sum of execution times of all ops. On Thu Dec 8 2016 at 7 30 PM Lin JM notifications github.com wrote panyx0718 https github.com panyx0718 Is there more documentation for this tool. The tool s output is hard to analysis for me. 1. TFProfRoot 0us 312.56ms what is the meaning of 0us and what is the meaning of 312.56ms. You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 6171 issuecomment 265924634 or mute the thread https github.com notifications unsubscribe auth ACwQexEaMq6kOdrEw7jNRSmt74oc6 bpks5rGMtogaJpZM4LHFgr . Thanks Xin 
6342,TypeError Cannot create initializer for non floating point type,Operating System Mac OS 10.11.6 Installed version of CUDA and cuDNN CUDA 8.0 cudnn 8.0 osx x64 v5.1 please attach the output of ls l path to cuda lib libcud If installed from binary pip package provide 1. A link to the pip package you installed tensorflow gpu 0.12.0rc1 2. The output from python c import tensorflow print tensorflow. version . 0.12.0 rc1 Running Text Classification Using Convolutional Neural Networks on Characters https github.com tensorflow tensorflow blob master tensorflow examples learn text classification character cnn.py python test.py I tensorflow stream executor dso loader.cc 128 successfully opened CUDA library libcublas.dylib locally I tensorflow stream executor dso loader.cc 128 successfully opened CUDA library libcudnn.dylib locally I tensorflow stream executor dso loader.cc 128 successfully opened CUDA library libcufft.dylib locally I tensorflow stream executor dso loader.cc 128 successfully opened CUDA library libcuda.1.dylib locally I tensorflow stream executor dso loader.cc 128 successfully opened CUDA library libcurand.dylib locally WARNING tensorflow Using temporary folder as model directory var folders gy 035w5b717yn01k9qlwvtcp1h0000gn T tmpU1WjvQ WARNING tensorflow From test.py 105 in main. calling fit from tensorflow.contrib.learn.python.learn.estimators.estimator with x is deprecated and will be removed after 2016 12 01. Instructions for updating Estimator is decoupled from Scikit Learn interface by moving into separate class SKCompat. Arguments x y and batch size are only available in the SKCompat class Estimator will only accept input fn. Example conversion est Estimator ... est SKCompat Estimator ... WARNING tensorflow From test.py 105 in main. calling fit from tensorflow.contrib.learn.python.learn.estimators.estimator with y is deprecated and will be removed after 2016 12 01. Instructions for updating Estimator is decoupled from Scikit Learn interface by moving into separate class SKCompat. Arguments x y and batch size are only available in the SKCompat class Estimator will only accept input fn. Example conversion est Estimator ... est SKCompat Estimator ... Traceback most recent call last File test.py line 121 in module tf.app.run main main argv sys.argv 0 unparsed File usr local lib python2.7 site packages tensorflow python platform app.py line 43 in run sys.exit main sys.argv 1 flags passthrough File test.py line 105 in main classifier.fit x train y train steps 100 File usr local lib python2.7 site packages tensorflow python util deprecation.py line 191 in new func return func args kwargs File usr local lib python2.7 site packages tensorflow contrib learn python learn estimators estimator.py line 355 in fit max steps max steps File usr local lib python2.7 site packages tensorflow contrib learn python learn estimators estimator.py line 699 in train model train ops self. get train ops features labels File usr local lib python2.7 site packages tensorflow contrib learn python learn estimators estimator.py line 1052 in get train ops return self. call model fn features labels model fn lib.ModeKeys.TRAIN File usr local lib python2.7 site packages tensorflow contrib learn python learn estimators estimator.py line 1023 in call model fn model fn results self. model fn features labels File test.py line 59 in char cnn model byte list N FILTERS FILTER SHAPE1 padding VALID File usr local lib python2.7 site packages tensorflow contrib framework python ops arg scope.py line 177 in func with args return func args current args File usr local lib python2.7 site packages tensorflow contrib layers python layers layers.py line 840 in convolution trainable trainable File usr local lib python2.7 site packages tensorflow contrib framework python ops arg scope.py line 177 in func with args return func args current args File usr local lib python2.7 site packages tensorflow contrib framework python ops variables.py line 244 in model variable caching device caching device device device File usr local lib python2.7 site packages tensorflow contrib framework python ops arg scope.py line 177 in func with args return func args current args File usr local lib python2.7 site packages tensorflow contrib framework python ops variables.py line 208 in variable caching device caching device File usr local lib python2.7 site packages tensorflow python ops variable scope.py line 1024 in get variable custom getter custom getter File usr local lib python2.7 site packages tensorflow python ops variable scope.py line 850 in get variable custom getter custom getter File usr local lib python2.7 site packages tensorflow python ops variable scope.py line 346 in get variable validate shape validate shape File usr local lib python2.7 site packages tensorflow python ops variable scope.py line 331 in true getter caching device caching device validate shape validate shape File usr local lib python2.7 site packages tensorflow python ops variable scope.py line 677 in get single variable expected shape shape File usr local lib python2.7 site packages tensorflow python ops variables.py line 224 in init expected shape expected shape File usr local lib python2.7 site packages tensorflow python ops variables.py line 327 in init from args initial value name initial value dtype dtype File usr local lib python2.7 site packages tensorflow python ops variable scope.py line 665 in lambda shape.as list dtype dtype partition info partition info File usr local lib python2.7 site packages tensorflow contrib layers python layers initializers.py line 120 in initializer raise TypeError Cannot create initializer for non floating point type. TypeError Cannot create initializer for non floating point type.,I m able to reproduce this martinwicke. Any thoughts I see this from TF SLIM OSX net slim.conv2d inputs 32 7 7 stride 2 scope end point File usr local lib python2.7 site packages tensorflow contrib framework python ops arg scope.py line 177 in func with args return func args current args File usr local lib python2.7 site packages tensorflow contrib layers python layers layers.py line 900 in convolution outputs layer.apply inputs File usr local lib python2.7 site packages tensorflow python layers base.py line 293 in apply return self. call inputs kwargs File usr local lib python2.7 site packages tensorflow python layers base.py line 259 in call self.build input shapes 0 File usr local lib python2.7 site packages tensorflow python layers convolutional.py line 138 in build dtype self.dtype File usr local lib python2.7 site packages tensorflow python ops variable scope.py line 1063 in get variable custom getter custom getter File usr local lib python2.7 site packages tensorflow python ops variable scope.py line 889 in get variable custom getter custom getter File usr local lib python2.7 site packages tensorflow python ops variable scope.py line 340 in get variable validate shape validate shape File usr local lib python2.7 site packages tensorflow python layers base.py line 249 in variable getter variable getter functools.partial getter kwargs File usr local lib python2.7 site packages tensorflow python layers base.py line 200 in add variable trainable trainable and self.trainable File usr local lib python2.7 site packages tensorflow contrib layers python layers layers.py line 1303 in layer variable getter return model variable getter getter args kwargs File usr local lib python2.7 site packages tensorflow contrib layers python layers layers.py line 1292 in model variable getter custom getter getter File usr local lib python2.7 site packages tensorflow contrib framework python ops arg scope.py line 177 in func with args return func args current args File usr local lib python2.7 site packages tensorflow contrib framework python ops variables.py line 268 in model variable partitioner partitioner custom getter custom getter File usr local lib python2.7 site packages tensorflow contrib framework python ops arg scope.py line 177 in func with args return func args current args File usr local lib python2.7 site packages tensorflow contrib framework python ops variables.py line 225 in variable partitioner partitioner File usr local lib python2.7 site packages tensorflow python ops variable scope.py line 332 in true getter caching device caching device validate shape validate shape File usr local lib python2.7 site packages tensorflow python ops variable scope.py line 683 in get single variable validate shape validate shape File usr local lib python2.7 site packages tensorflow python ops variables.py line 225 in init expected shape expected shape File usr local lib python2.7 site packages tensorflow python ops variables.py line 322 in init from args initial value name initial value dtype dtype File usr local lib python2.7 site packages tensorflow python ops variable scope.py line 672 in lambda shape.as list dtype dtype partition info partition info File usr local lib python2.7 site packages tensorflow contrib layers python layers initializers.py line 120 in initializer raise TypeError Cannot create initializer for non floating point type. TypeError Cannot create initializer for non floating point type. image tf.to float image fixes it for now. Could someone check 7318 to see whether the fix works 
6384,Error in einsum with unspecified dimensions,Code import tensorflow as tf x tf.placeholder dtype tf.int32 A tf.random normal 13 x 512 B tf.random normal 512 3 C tf.einsum ijk kl ijl A B with tf.Session as sess print sess.run C x 7 Output Traceback most recent call last File home noam code test test einsum.py line 7 in module C tf.einsum ijk kl ijl A B File home noam miniconda3 envs ml lib python3.5 site packages tensorflow python ops special math ops.py line 212 in einsum axes to sum File home noam miniconda3 envs ml lib python3.5 site packages tensorflow python ops special math ops.py line 341 in einsum reduction product reshape if necessary product uncompacted shape File home noam miniconda3 envs ml lib python3.5 site packages tensorflow python ops special math ops.py line 366 in reshape if necessary return array ops.reshape tensor new shape File home noam miniconda3 envs ml lib python3.5 site packages tensorflow python ops gen array ops.py line 2448 in reshape name name File home noam miniconda3 envs ml lib python3.5 site packages tensorflow python framework op def library.py line 493 in apply op raise err File home noam miniconda3 envs ml lib python3.5 site packages tensorflow python framework op def library.py line 490 in apply op preferred dtype default dtype File home noam miniconda3 envs ml lib python3.5 site packages tensorflow python framework ops.py line 669 in convert to tensor ret conversion func value dtype dtype name name as ref as ref File home noam miniconda3 envs ml lib python3.5 site packages tensorflow python framework constant op.py line 176 in constant tensor conversion function return constant v dtype dtype name name File home noam miniconda3 envs ml lib python3.5 site packages tensorflow python framework constant op.py line 165 in constant tensor util.make tensor proto value dtype dtype shape shape verify shape verify shape File home noam miniconda3 envs ml lib python3.5 site packages tensorflow python framework tensor util.py line 441 in make tensor proto tensor proto.string val.extend compat.as bytes x for x in proto values File home noam miniconda3 envs ml lib python3.5 site packages tensorflow python framework tensor util.py line 441 in listcomp tensor proto.string val.extend compat.as bytes x for x in proto values File home noam miniconda3 envs ml lib python3.5 site packages tensorflow python util compat.py line 65 in as bytes bytes or text TypeError Expected binary or unicode string got 13 Using Tensorflow version 0.12.0 rc0 also tested and fails the same way on 0.12.0 rc1.,Having the same issue. Does anyone know of a workaround until this gets fixed I m having the same issuse. asrivat1 I guess you ve doing something like multiplication of each element of the batch of shape B I J with a matrix of shape J K . In that case you could just reshape the batch to B I J and do the ordinary tf.matmul to obtain the result of shape B I K and then reshape this to B I K . To do the reshaping you can determine the tensor shapes with tf.shape. This appears to work for me on python2.7 just fine. It appears you are using python3. Perhaps try forcing the string to be bytes and or unicode. Are you all using Python 3 I m in 2.7. aselle are you on 0.12.0 rc1 I found two workarounds. The first is to pad out the shape of the tensors to some fixed size using 0s via tf.pad and this fixes the tf.einsum bug. The other solution is what j88k suggested and it s about 80 faster for my program. Also worth noting that for joining on multiple dimensions tf.batch matmul works wonders. The only disadvantage is that according to Timeline I m now spending as much time transposing and reshaping my tensors as I am actually multiplying them. Also worth noting I m using Cython and Anaconda. It s possible that s part of the problem. 1 I m having the same error using einsum where one of the dims is unknown. I m using python 3.5 Anaconda . It will be great to have this fixed. aselle Tested on 2.7 fails the same way Traceback most recent call last File tmp test.py line 7 in module C tf.einsum ijk kl ijl A B File home noam miniconda3 envs 2.7test lib python2.7 site packages tensorflow python ops special math ops.py line 212 in einsum axes to sum File home noam miniconda3 envs 2.7test lib python2.7 site packages tensorflow python ops special math ops.py line 341 in einsum reduction product reshape if necessary product uncompacted shape File home noam miniconda3 envs 2.7test lib python2.7 site packages tensorflow python ops special math ops.py line 366 in reshape if necessary return array ops.reshape tensor new shape File home noam miniconda3 envs 2.7test lib python2.7 site packages tensorflow python ops gen array ops.py line 2448 in reshape name name File home noam miniconda3 envs 2.7test lib python2.7 site packages tensorflow python framework op def library.py line 493 in apply op raise err TypeError Expected binary or unicode string got 13 Tensorflow 0.12.1 installed from pip . I created a fresh empty anaconda environment conda create n 2.7test python 2.7 issued pip install tensorflow and ran the code in the original bug report. Could you try building master from source A fix https github.com tensorflow tensorflow commit 30d2a2235a66e158757b13b6dd0bab7bf5e60492 went in that isn t in r0.12 that sounds as if it might address the issue at least from the title. michaelisard I didn t build from source but I did add the line new shape tuple 1 if x is None else x for x in new shape to tensorflow python ops special math ops.py in 0.12.1 manually and it does seem to fix the issue. I probably won t have time to build from source in the upcoming week. OK I will close for now but please reopen if the problem doesn t go away with the new code. Just built from source and can confirm that this is now working. Thanks 
6438,Cannot show stderr when using Jupyter,Hello Could you please have a look about this. I am using TF and Jupyter. But what makes me confuse is that the log text cannot be shown in Jupyter output cell but it output correctly in ipython . I think it is because of the stderr. This issue have been discussed before in 3047. You add several lines to determine whether or not current context is in an interactive environment. However even if I use Jupyter the return value of sys.flags.interactive is still zero. and the logger lever can never be setted to info and use stdout instead of stderr . Thanks a lot ,This would be good to fix meanwhile I ve been using this work around with FDRedirector from here https github.com bitemyapp ipython blob master IPython kernel core fd redirector.py The downside is that this hangs inside tensorflow printf if pipe buffer is exceeded 65k zheng xq do you think we need a fix like the one you did for interactive Or just a flag Same issue happens exists Jupyter QtConsole no logs. ipython shows the logs. Also this does not help tf.logging.set verbosity tf.logging.INFO It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. This bug still seems valid. This behavior has not changed. Is there a way to tell whether you re in that particular Jupyter environment We are already checking for some interactive environments and redirecting logs appropriately from https github.com tensorflow tensorflow blob master tensorflow python platform tf logging.py L65 If anyone knows of a better way to determine interactive send a PR or tell me and I will . martinwicke How about adding a check to see if IPython has injected the appropriate attribute I have no current environment to test this at the moment else i d do the code change myself. Excellent I will try this. Seems to work sent PR. Thanks Loki Poifect... my pleasure. Nagging Assigneee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. It looks like I made a mistake verifying this earlier. I was using tf.Print which is still not jupyter compatible . tf.logging is currently working well with jupyter already. I hope I didn t waste too much of anyone s time. tf.Print still works in Jupyter with the FDRedirector trick from above Is there a clean way to fix the issue or some manual override to make tf.Print works with Jupyter FDRedirector is a barely workable solution as it is not available in latest IPython anymore. 
6466,CudnnLSTM dropout takes no effect,My environment is Tensorflow 0.11.0rc2 in unbuntu 16.04 cuda8.0 cudnn5.1 GPU is GTX1080. I am using the CudnnLSTM from tensorflow.contrib.cudnn rnn package. I found that the dropout setting in CudnnLSTM seems take no effect and I checked that there is no test for dropout in op unit test. So I write a simple code to test it the code is below import tensorflow as tf from tensorflow.contrib.cudnn rnn import CudnnLSTM class Cudnn model def init self dropout self.model CudnnLSTM num layers 1 num units 8 input size 8 input mode skip input direction unidirectional dropout dropout params size t self.model.params size self.params tf.Variable tf.ones params size t validate shape False def run step self rnn inputs outputs output h output c self.model input data rnn inputs input h tf.zeros 1 1 8 input c tf.zeros 1 1 8 params self.params is training True self.outputs outputs return outputs def main inputs tf.pack 0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 m1 Cudnn model dropout 0.0 output1 m1.run step inputs m2 Cudnn model dropout 0.5 output2 m2.run step inputs output3 tf.nn.dropout output1 0.5 output4 m1.run step tf.nn.dropout inputs 0.5 config tf.ConfigProto allow soft placement True config.gpu options.allow growth True sess tf.Session config config sess.run tf.initialize all variables for i in range 5 out1 out2 out3 out4 sess.run output1 output2 output3 output4 print Try time d i print cndnn dropout 0 out1 print cudnn dropout 0.5 out2 print tf out dropout 0.5 out3 print tf in dropout 0.5 out4 return And the result is Try time 0 cndnn dropout 0 0.6082834 0.70377535 0.74009657 0.75365263 0.75866807 0.76051712 0.76119781 0.76144838 cudnn dropout 0.5 0.6082834 0.70377535 0.74009657 0.75365263 0.75866807 0.76051712 0.76119781 0.76144838 tf out dropout 0.5 1.2165668 0. 0. 0. 0. 1.52103424 1.52239561 1.52289677 tf in dropout 0.5 0.6082834 0.6082834 0.6082834 0.76119781 0.6082834 0.76158684 0.6082834 0.6082834 Try time 1 cndnn dropout 0 0.6082834 0.70377535 0.74009657 0.75365263 0.75866807 0.76051712 0.76119781 0.76144838 cudnn dropout 0.5 0.6082834 0.70377535 0.74009657 0.75365263 0.75866807 0.76051712 0.76119781 0.76144838 tf out dropout 0.5 0. 0. 0. 0. 0. 0. 0. 0. tf in dropout 0.5 0.6082834 0.74009657 0.6082834 0.76119781 0.6082834 0.76158684 0.6082834 0.6082834 Try time 2 cndnn dropout 0 0.6082834 0.70377535 0.74009657 0.75365263 0.75866807 0.76051712 0.76119781 0.76144838 cudnn dropout 0.5 0.6082834 0.70377535 0.74009657 0.75365263 0.75866807 0.76051712 0.76119781 0.76144838 tf out dropout 0.5 1.2165668 0. 0. 1.50730526 1.51733613 1.52103424 1.52239561 0. tf in dropout 0.5 0.6082834 0.74009657 0.6082834 0.76119781 0.6082834 0.76158684 0.6082834 0.761594 Try time 3 cndnn dropout 0 0.6082834 0.70377535 0.74009657 0.75365263 0.75866807 0.76051712 0.76119781 0.76144838 cudnn dropout 0.5 0.6082834 0.70377535 0.74009657 0.75365263 0.75866807 0.76051712 0.76119781 0.76144838 tf out dropout 0.5 0. 0. 1.48019314 0. 0. 0. 1.52239561 1.52289677 tf in dropout 0.5 0.6082834 0.6082834 0.6082834 0.76119781 0.76154053 0.6082834 0.76159316 0.761594 Try time 4 cndnn dropout 0 0.6082834 0.70377535 0.74009657 0.75365263 0.75866807 0.76051712 0.76119781 0.76144838 cudnn dropout 0.5 0.6082834 0.70377535 0.74009657 0.75365263 0.75866807 0.76051712 0.76119781 0.76144838 tf out dropout 0.5 0. 1.40755069 0. 1.50730526 1.51733613 0. 1.52239561 1.52289677 tf in dropout 0.5 0.6082834 0.74009657 0.75866807 0.76119781 0.6082834 0.76158684 0.76159316 0.6082834 From the result I see that the cudnn dropout 0.5 takes no effect the result is always same with cudnn dropout 0.0. , robotnc Sorry for my late response I was on vacation. Yes dropout is not supported yet see here https github.com tensorflow tensorflow blob master tensorflow contrib cudnn rnn kernels cudnn rnn ops.cc L701 . Adding zheng xq If we do get dropout support can we get recurrent dropout as that s the form that seems to actually help. Similar to what s in LayerNormBasicLSTMCell https github.com tensorflow tensorflow blob cb17d1b0e2b581b5da1d9597b7929ba764749d38 tensorflow contrib rnn python ops rnn cell.py . Any updates on this Not having any dropout support for cuDNN based RNNs seems really limiting especially considering how much further ahead PyTorch support is for this. This isn t exactly a new or rarely used feature. What s actually involved in adding input dropout support Is it just wiring the places in this file https github.com tensorflow tensorflow blob master tensorflow contrib cudnn rnn kernels cudnn rnn ops.cc L701 which are marked with dropout Hi all I tried the FusedBlockLSTM in tf.contrib.rnn in Tensorflow V1.0 the speed is almost same with CudnnLSTM. And the FusedBlockLSTM can support Dropout and is easier to use. 2017 4 22 5 36 Mohammed AlQuraishi notifications github.com What s actually involved in adding input dropout support Is it just wiring the places in this file https github.com tensorflow tensorflow blob master tensorflow contrib cudnn rnn kernels cudnn rnn ops.cc L701 which are marked with dropout You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 6466 issuecomment 296310827 or mute the thread https github.com notifications unsubscribe auth ARpmyx4oomxYmgcQB PXvAat4tW0Uhvbks5rySFTgaJpZM4LUiQt . My experience is very different. I still see a 3x gap between FusedBlockLSTM and CudnnLSTM so I would still think this is very useful to have. And the issue is that the feature is present as an option but it doesn t actually do anything. So it s very misleading as it is. What s your tensorflow version I found the new V1.0 is faster than V0.1x 2017 4 23 8 52 Mohammed AlQuraishi notifications github.com My experience is very different. I still see a 3x gap between FusedBlockLSTM and CudnnLSTM so I would still think this is very useful to have. And the issue is that the feature is present as an option but it doesn t actually do anything. You are receiving this because you modified the open close state. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 6466 issuecomment 296441256 or mute the thread https github.com notifications unsubscribe auth ARpmyyzaGa3VGqyFWY561JLgDVMC6n1Iks5ry0mkgaJpZM4LUiQt . TF 1.1rc2. And like I mentioned there s not just a performance difference but a bona fide bug because the CudnnLSTM API exposes a dropout option that does not do anything. If you don t mind reopen the ticket. Otherwise I ll start a new ticket. FYI this is on a Pascal Titan X with a bidirectional LSTM of 800 units each way and 700 timesteps. The dropout is still needed reopen again. CudnnRNN dropout change is submitted please keep an eye on the nightly builds. protoget has this been resolved protoget I see your commit last May but I just tried again with TF 1.4.0rc0 and dropout still doesn t seem to do anything. skye alquraishi The dropout is supported. Dropout is applied between layers if you only have one layer it doesn t show up even if dropout ratio isn t zero. 
6472,Timeout does not work with session created with server, What related GitHub issues or StackOverflow threads have you found by searching the web for your problem None Environment info Operating System Ubuntu Installed version of CUDA and cuDNN please attach the output of ls l path to cuda lib libcud CPU only If installed from source provide 1. The commit hash git rev parse HEAD c32663d119a4cbd9d4926d4eed2a0705f03a57f9 2. The output of bazel version Extracting Bazel installation... Build label 0.4.3 Build target bazel out local fastbuild bin src main java com google devtools build lib bazel BazelServer deploy.jar Build time Thu Dec 22 12 31 25 2016 1482409885 Build timestamp 1482409885 Build timestamp as int 1482409885 If possible provide a minimal reproducible example We usually don t have time to read hundreds of lines of your code Above I create a cluster and get a session from the server. I then try to dequeue an empty FIFOQueue with a timeout of 1 second. For some reason on the latest commits of the tensorflow source this does not successfully time out. This bug does not happen for the released versions of tensorflow .11 .12 . To run python script.py 1 worker tmp output 2 1 python script.py 0 ps tmp output2 2 1 python script.py 0 worker where script.py is the above example.,This looks like a bug in the recently added in process shim for contacting the master without performing a full RPC. I m testing a fix right now. This should be fixed at HEAD now that 6530 is merged. See e1362157e33a6fd6a66caefd083bd4afc27dc030 for the actual fix. 
6537,file io.get matching files fails if any unrelated non matching sub directory is non readable,Let filename tmp mymodel.005 . file io.get matching files filename will raise an exception like PermissionDeniedError tmp .xrdp xrdp sesman jjz1ox for me. I think this is a bug as this is an unrelated non matching directory so it does not matter and it will anyway not match the filename thus it should not raise an exception. I get this via saver.restore sess ... save path filename where saver tf.train.Saver ... . , albertz The underlying GetMachingFiles performs a pattern matching on all files in the directory. Yes I see that it does that and even for all sub directories but it shouldn t break on sub directories which do not match anyway. It add all sub folders before doing the match and the error is thrown when checking if it is a directory. I know. And I say that this should not fail if it does not match the pattern anyway. yup you re right... the directory check shouldn t need to be done on children that don t match the pattern. That check is done for adding stuff to the queues but not for the IsDirectory check. I ll fix this. I believe commit https github.com tensorflow tensorflow pull 6719 commits 493188ebe0f68ace157bd97453c41e2cb0d642c8 should have fixed this issue. Can you verify and make sure things work fine now 
6591,WideNet in examples learn throws error on UBUNTU 16.04 LTS,Am running TensorFlow 0.11.0rc0 On Ubuntu 16.04 LTS installed via pip I was going through the guides at https www.tensorflow.org tutorials wide and deep I thought that my code had an error but then running the example script at https github.com tensorflow tensorflow blob master tensorflow examples learn wide n deep tutorial.py Also throws the same error is the problem my environment the script or Ubuntu 16.04 I haven t tried it in 14.04. Training data is downloaded to tmp tmp1SYKCv Test data is downloaded to tmp tmpWhX5FI model directory tmp tmp9TzI1B WARNING tensorflow The default value of combiner will change from sum to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from sum to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from sum to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from sum to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from sum to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from sum to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from sum to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from sum to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from sum to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from mean to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from mean to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from mean to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from mean to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from mean to sqrtn after 2016 11 01. WARNING tensorflow The default value of combiner will change from mean to sqrtn after 2016 11 01. WARNING tensorflow Change warning default value of enable centered bias will change after 2016 10 09. It will be disabled by default.Instructions for keeping existing behaviour Explicitly set enable centered bias to True if you want to keep existing behaviour. WARNING tensorflow Using default config. Traceback most recent call last File widenet.py line 208 in module tf.app.run File usr local lib python2.7 dist packages tensorflow python platform app.py line 30 in run sys.exit main sys.argv 1 flags passthrough File widenet.py line 204 in main train and eval File widenet.py line 197 in train and eval m.fit input fn lambda input fn df train steps FLAGS.train steps File usr local lib python2.7 dist packages tensorflow contrib learn python learn estimators estimator.py line 333 in fit max steps max steps File usr local lib python2.7 dist packages tensorflow contrib learn python learn estimators estimator.py line 660 in train model features targets input fn File widenet.py line 197 in lambda m.fit input fn lambda input fn df train steps FLAGS.train steps File widenet.py line 159 in input fn for k in CATEGORICAL COLUMNS File widenet.py line 159 in dictcomp for k in CATEGORICAL COLUMNS TypeError init got an unexpected keyword argument dense shape ,Could you see if the same problem exists in 0.12.1 Yeah it works.. thanx.. Sorry issue persists in 0.12.1 Hi as commented at https github.com tensorflow tensorflow issues 6648 issuecomment 270806057 please try example file on v0.12 branch. works.. thanx nagachika 
6616,fake quant with min max args has odd behavior, Description On a simple linear regression example fake quant with min max args is not working. If I change to fake quant with min max vars with trainable quantization min max ranges it works just fine. The min max values are the same in both approaches. Reproducer included below. What related GitHub issues or StackOverflow threads have you found by searching the web for your problem None Environment info Operating System Ubuntu 14.04.5 Python 2.7 Installed version of CUDA and cuDNN Cuda 8 CuDNN 5.1 If installed from binary pip package provide 1. A link to the pip package you installed https storage.googleapis.com tensorflow linux gpu tensorflow gpu 0.12.1 cp27 none linux x86 64.whl 2. The output from python c import tensorflow print tensorflow. version . 0.12.1 If possible provide a minimal reproducible example We usually don t have time to read hundreds of lines of your code Below is a small reproducer adapted from the example at https www.tensorflow.org get started https www.tensorflow.org get started Output is a random rounded values for W and b depending on run but not the expected W 123 b 79 and the loss is large. What other attempted solutions have you tried If I instead use fake quant with min max vars as illustrated below it works fine by printing we have verified that the quantization ranges are 0 255 for each training iteration . The loss decreases and the values for W and b are as expected. The example is adapted from https www.tensorflow.org get started https www.tensorflow.org get started Logs or other output that would be helpful If logs are large please upload as attachment or provide link .,It looks like the gradient function for fake quant with min max args forgets to pass the min max range. https github.com tensorflow tensorflow blob master tensorflow python ops array ops.py L1877 I ll send a fix. Hi yoslber suharshs in the prcess of quantification how to decide the value of min and max in tf.fake quant with min max vars input min max 
6669,Python3 pickle treat tf.gfile.GFile wrong,NOTE Only file GitHub issues for bugs and feature requests. All other topics will be closed. For general support from the community see StackOverflow https stackoverflow.com questions tagged tensorflow . To make bugs and feature requests more easy to find and organize we close issues that are deemed out of scope for GitHub Issues and point people to StackOverflow. For bugs or installation issues please provide the following information. The more information you provide the more easily we will be able to offer help and advice. What related GitHub issues or StackOverflow threads have you found by searching the web for your problem None Environment info Operating System macOS Sierra 10.12.2 Installed version of CUDA and cuDNN None please attach the output of ls l path to cuda lib libcud None If installed from binary pip package provide 1. A link to the pip package you installed 2. The output from python c import tensorflow print tensorflow. version . If installed from source provide 1. The commit hash git rev parse HEAD 4d924e796368163eff11a8151e8505715345f58d 2. The output of bazel version 0.4.3 homebrew If possible provide a minimal reproducible example We usually don t have time to read hundreds of lines of your code What other attempted solutions have you tried I tried to manually read all data into a variable s f.read and unpickle it pickle.loads s encoding latin . It works well. Logs or other output that would be helpful If logs are large please upload as attachment or provide link . TypeError a bytes like object is required not str I don t know why this happened. I checked the file io code and it returns byte object when calling read . So maybe this is something about pickle any ideas ,This kind of usage question is best asked on Stackoverflow http stackoverflow.com questions tagged tensorflow . Github issues are for bug reports and installation issues. michaelisard I believe it s a bug about how we implement file io s readline . It would return str . But according to the python3 pickle document The argument file must have two methods a read method that takes an integer argument and a readline method that requires no arguments. Both methods should return bytes. Thus file can be an on disk file opened for binary reading an io.BytesIO object or any other custom object that meets this interface. Any thoughts sherrym what do you think I can confirm this bug in Ubuntu 16.04 python 3.5. I fixed this bug replacing tf.gfile.GFile something with open something saxenasaurabh rohan100jain sherrym Any updates orlov alexander dd I think that s a workaround since you use python open file API instead of the c implementation. This should be fixed by now... As long as the file is opened in binary mode it should work. rohan100jain Will try thanks 
6717,Incorrect gradient for categorical distribution entropy,The Categorical distribution class provides an awesome entropy operator but apparently the gradient calculation w.r.t. the input operators doesn t work . In the outputs we see that the entropy calculation works but the gradients are somehow lost. This obviously also doesn t work when I try to maximize this entropy using any optimizer. ,I suspect this a numerical stability issue. Looking https gist.github.com yaroslavvb 97504b8221a8529e7a51a50915206d68 at your working graph it uses LogSoftmaxGrad . Whereas in the categorical distribution it computes SoftmaxGrad and combines it with LogGrad ebrevdo img width 389 alt screenshot 2017 01 08 18 31 59 src https cloud.githubusercontent.com assets 23068 21756041 dd734ea4 d5d0 11e6 82d0 001d9fab5bea.png Taking a look. For numerical stability we use tf.nn.softmax cross entropy with logits to calculate the entropy. I just found out that this operation does not perform backprop with respect to the labels argument the exponentiated probabilities term of the entropy . Thus the zeros. Probably the best way to fix this is for us to implement the numerically stable equivalent of your reduce sum. Fix going into master hopefully today tomorrow. thank you for the fix i guess we can close this then 
6735,slim parallel read should pass seed to string input producer,Currently the seed passed to parallel read is only passed to the RandomShuffleQueue but not to the string input producer. This should be fixed. Furthermore it should also be documented that the output will never be deterministic if num readers is greater than 1. https github.com tensorflow tensorflow blob e121667dc609de978a223c56ee906368d2c4ceef tensorflow contrib slim python slim data parallel reader.py L212, sguada I m not sure if this is a bug but jonasrauber seems to be requesting a relatively trivial change. Totally it makes sense. 
6749,sparse placeholder no longer accepts python ints in shape argument,Hi After recently updating tensorflow sparse placeholder stopped working correctly. It appears that the shape argument must now be int64 in order for tensorflow to convert the shape to a tensor so the following fails with error message This is inconsistent with the behavior of tf.placeholder for which succeeds. Thanks Shawn ,Maybe mrry can comment on what might be an issue with the tf.sparse placeholder shape API. Looks like the problem is in array ops. normalize sparse shape . It converts its argument to a tensor which for a list of Python int values will result in a tf.int32 tensor. I suspect the correct answer is to add a dtype dtypes.int64 to the ops.convert to tensor call but since there doesn t appear to be any unit test for this I ll defer to ilblackdragon who originally added that function. simple workaround shape 1 3 shape np.array shape dtype np.int64 tf.sparse placeholder tf.int32 shape shape Any update on this issue ilblackdragon In my experience the workaround does not work because then it complains about ValueError Tensor Tensor Input 2 shape 1 0 shape 2 dtype int64 may not be fed. In other words it doesn t want to do Tensor conversion itself but it also doesn t accept to be fed with the int64 shape Tensor. I also seem to be having the same problem. Has there been any progress on this issue The workaround is all well and good .... until you want a variable dimension Marking this as contributions welcome. ilblackdragon please let me know if you plan to fix this. Added a PR 11153 with test cases. 
6774, import meta graph appends 1 to node in GraphDef but doesn t add 1 to Variable name in Collection,In an example below second import meta graph will create variable nodes a a 1 but corresponding global variables collection has variables a a . So now report uninitialized variables is empty even though there s an uninitialized variable a 1 in the graph. Example below crashes with uninitialized error. , sherrym maybe simplest solution is to just throw an error when import meta graph tries to add an op that exists in the graph already This issue still exists in r1.2. It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. Is this still a problem I dunno....metagraph is seems to be getting replaced with SavedModel anyway I ll close it unless someone complains I ve confirmed that this issue still exists in version 1.5.0 dev20171219 . To run the example you need to add . to this line saver.save sess . dummy Then it fails with Nagging Awaiting Response It has been 14 days with no activityand the awaiting response label was assigned. Is this still an issue The original poster has replied to this issue after the stat awaiting response label was applied. yaroslavvb are you in a position to send a fix I think yaroslavvb was probably right to close it earlier. While I confirmed that is is reproducible that s doesn t say anything about it s priority. 
6799,Go Unable to create two ops of the same type using generated op wrapper functions,As reported in https github.com tensorflow tensorflow issues 10 issuecomment 272045853 the following test at head d4b5c606fc9fbd1a20b5b113b4bc831f31d889a3 fails with Thanks for pointing this out sdeoras .,Thanks folks for quick resolution. It is working great now Woohoo 
6808,Wrong argument order for input producer,The signature of input producer is and this function adds scalar summary of the fraction of the queue that is full not actually queue size with name summary name . But range input producer is calling input producer like this It looks like a bug to me name and summary name might need to swap.,Nice catch. BTW easy fixes like this may be easier to deal with as Pull Requests. I was able to make this PR in about 60 seconds using github s Edit Feature Then in bugfix branch on yaroslavvb tensorflow find file click Edit button edit file then click commit then click Pull Requests and select tensorflow master on left and yaroslavvb tensorflow bugfix on right 
6854,XLA random numbers are the same across session.run calls,Not sure if that s intended but that changes behavior of training pipelines Output , leary for comment if that s normal That s definitely not working as intended. We don t seem to be setting the RNG seed correctly which is why you get the same result. I ll check in a change disabling JIT compilation of the RNG ops until they are fixed. Actually it turns out that Tensorflow is constant folding away the XlaLaunch ops containing the random number generation code. Fix coming shortly mark XlaLaunch as stateful . Good to know it s an easy fix. BTW if you setup the internal github google email it ll add you to TensorFlow org automatically and I ll be able to assign future XLA issues to you martin knows the place for the mapping Hry folks Must have been some typo when you launched this thread. I am not related to the tensor flow project. You aren t communicating with who you think you are. On Jan 15 2017 9 53 AM Yaroslav Bulatov notifications github.com wrote Good to know it s an easy fix. BTW if you setup the internal github google email it ll add you to TensorFlow org automatically and I ll be able to assign future XLA issues to you martin knows the place for the mapping You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 6854 issuecomment 272703655 or mute the thread https github.com notifications unsubscribe auth ACV3mIOcM ZI890MJSfNxuvalDtmuLkbks5rSkESgaJpZM4LjvWp . Ah I see I can t be assigned. I emailed Martin to add me to the Github org. hawkinsp invited Also it is leary g I think hawkinsp btw if you include string like below in CL public description it ll automatically close the issue when commit is merged Fixes issue 6854 Fixes Github issue 6854 does not trigger it 
6893,get matching files issue,In saver.py https github.com tensorflow tensorflow blob master tensorflow python training saver.py if the argument of get matching files is a relative path with no . in the beginning get matching files will return an empty list. e.g. get matching files filename returns while get matching files . filename returns filename Thus when using freeze graph https github.com tensorflow tensorflow blob master tensorflow python tools freeze graph.py with input checkpoint in the same directory I found the issue below ,Treatment of . in file io.py matches semantics of similar functionality in io ops.cc IE this works This finds nothing Got it. I think the PR was in the right direction but depending on the io ops maybe in the wrong place. If it s a general bug in io ops then it should go there otherwise it should go in the wrapper code. The problem that I had was really about . which is not always portable. Do you want to resubmit the PR again Seems related to this one https github.com tensorflow tensorflow issues 4921. I m not sure if it was fixed already. Thanks ppwwyyxx for the cross reference Yes it s a peculiarity in the lookup. It does look odd. Automatically closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks 
7014,clip by value clips NaN to clip value max,Running sess.run tf.clip by value float nan 0.0 100.0 returns 100.0 I m not sure if this is expected behavior or convenient for clipping gradients but I believe it should return nan as np.clip does or be documented. Environment info Windows 7 Only CPU Python 3.5.2 from Anaconda Tensorflow 0.12.0 rc1 installed from binary pip package, aselle for numpy compat. In general I think it s probably a good idea to follow np.clip . We might be using isfinite instead of isnan . If you submit a PR we could discuss the issues there. rmlarsen FYI This is generally the case... Also tf.clip by value should be consistent with the implementation of tf.minimum and tf.maximum s.t. clip by value v a b is equivalent to tf.minimum tf.maximum a v b tf.minimum and maximum have the correct nan behavior. I cannot reproduce this on 0.9 0.10 0.11 0.12 or the last nightly build from master in ubuntu 14 and docker ubuntu 14. I get nan. What version and platform are you using What version of CUDA The issue template we ask people to complete asks what version and platform you are using so we can help you on these types of issues better. Please use it in the future. Thank you. I have filled in the information. I will do so in the future too. Sorry. I will confirm the precise tensorflow version and test with a newer version as soon as I am able. Same results on tensorflow 0.12.1. And I also get the float instead of the nan if I call tf.minimum float nan 0.0 or tf.maximum float nan 0.0 Just to be clear garibarba you re running Precisely python c import tensorflow as tf print tf.Session .run tf.minimum float nan 0.0 python c import tensorflow as tf print tf.Session .run tf.maximum float nan 0.0 python c import tensorflow as tf print tf.Session .run tf.clip by value float nan 0.0 100.0 OK it gives nan for me on Linux and master . mrry gunan do you happen to have a windows build available where you could repro Trying now. Reproduced the issue on windows both on master and 0.12.1 Thank you gunan We should definitely add a test for that once fixed. Assigning to aselle . I might take a look later It looks like the problem is caused by maximum and minimum. And the implementations of those seem to just use eigen if I am not mistaken. Hopefully it s not a problem with NaN on SIMD. I fixed the underlying NaN problem for SIMD max min Eigen in https bitbucket.org eigen eigen commits 738d7c6becf04a2a8f400d5edd2c13ea44f28428 plus a few followup . Please notice that it is a bit subtle since max and min are defined to match the elementwise behavior of std max std min. See http eigen.tuxfamily.org bz show bug.cgi id 1373 for a discussion. Unfortunately we have not been able to update TensorFlow to use a recent version of Eigen containing this fix due to build breakage in Eigen. I ll try to give this another try. Nice rmlarsen I m unable to clone the Eigen repo because bitbucket s https clone doesn t work. Has this been fixed yet Fixed a while ago apparently. 
7038,multiple dequeue ops are optimized away in latest TF,Multiple ops Dequeue ops from the same queue started getting optimized away in latest TensorFlow The following executes dequeue once in Jan17 head but 3 times in 12.1 sess.run q.dequeue q.dequeue q.dequeue One gets expected behavior 3 dequeues when graph optimization is turned off Self contained repro https github.com yaroslavvb stuff blob master parallel dequeue test.py Came up in http stackoverflow.com questions 41830206 how to share a queue containing variable length sequences batches between multip,cc mrry the queue expert Can you print the GraphDef for that program In particular I d like to know if the FIFOQueueV2 or some other ...QueueV2 op is being used. Yes it s using V2 ops for queue. The 12.1 version is using regular http pastebin.com gsu7xrU7 alextp It looks like the switch to resource typed queues has caused a subtle bug in some of the queue ops. As far as I can tell multiple instances of queue.dequeue in the same subgraph are now being treated as common subexpressions whereas before the fact that they had an incoming ref typed edge meant they were never candidates for CSE https github.com tensorflow tensorflow blob 3570b5de07c8079d1005c7877e235bd8aa0e0ccc tensorflow core graph optimizer cse.cc L162 . I can see at least two solutions Modify the CSE optimizer to reject anything with a resource typed input as a candidate for elimination. Modify all of the Queue V2 accessor ops to be stateful which also inhibits the optimization https github.com tensorflow tensorflow blob 3570b5de07c8079d1005c7877e235bd8aa0e0ccc tensorflow core graph optimizer cse.cc L158 . https github.com tensorflow tensorflow commit 06e3aa655506773e49ce9a855f81ba3eae2a6b88 diff 333845d139890198962551b3a1c2a33b should have fixed this issue it adds a test even I just tried in tf1.0 and this behavior seems to still be here back. To reproduce try this Result is If you uncomment config line to remove rewriting optimization then result is as expected Version info It seems like the commit which fixed this bug didn t make the cut into the 1.0 release. master is fixed and so will 1.1. If you need the fix by itself cherry pick https github.com tensorflow tensorflow commit 06e3aa655506773e49ce9a855f81ba3eae2a6b88 diff 333845d139890198962551b3a1c2a33b Got it I forgot that releases are non linear it worked in version I built before TF 1.0 was released 
7146,Nested tf.while loop s hang when they are placed on CPU,When I place all ops within nested tf.while loop on CPU evaluating the output of nested tf.while loop makes the program hang. I also posted a question on StackOverflow http stackoverflow.com questions 41929472 why does nested tf while loop freeze in test session of tf test testcase . Environment info Operating System Ubuntu 16.04 Installed version of CUDA and cuDNN If installed from source provide 1. The commit hash git rev parse HEAD 28f5099d532ce59787ee58012b8ef04c498947ae 2. The output of bazel version If possible provide a minimal reproducible example We usually don t have time to read hundreds of lines of your code The following code is run on python 3.5 What other attempted solutions have you tried If I replace num inner iters tf.constant 5 with num inner iters 5 the program will exit normally. Logs or other output that would be helpful The program hangs without any error message. ,I tried your code with the latest head current master configured with XLA and it works normally there Can you try it with latest head and see if problem persists The program still froze with latest master branch. 4cc0d1e7905454de7bd3cb6c20c3f9fb459ed335 I got the following output before the program froze I used gdb to attach the program and the output of bt was yuanbyu any idea Why would replacing tf.constant 5 with 5 make a difference for while loop OK I can reproduce the hang I was mistakenly running with CUDA VISIBLE DEVICES which makes it pass . This suggests that with tf.device cpu 0 tries to place some parts of the computation on GPU which causes the hang I think that this has been fixed in the HEAD. Re open if it is not the case. 
7311,tfrecords giving parsing error when saved examples are too big, When I try to save a example to a tfrecord that is too large a parsing error occurs. I need to save a large sequence of float valued images states of a fluid flow simulation and when the states and sequences get too large a parsing error occurs. What related GitHub issues or StackOverflow threads have you found by searching the web for your problem I think that this might be the same error and problem that other people are getting when saving long sequences to tfrecords like seen here https github.com tensorflow tensorflow issues 5234 and https github.com OliviaMG xiaomeng issues 1. While both of these threads found solutions the underlining problem does not seem to be found. Environment info Operating System Ubuntu 16.04 CUDA 8.0 CUDNN 5.1 TensorFlow 0.12.1 If possible provide a minimal reproducible example Running this minimal script will cause the error. Lowering the value will cause the script to run with out error. What other attempted solutions have you tried I have tried saving the example in a variety of different ways including converting it to a string and breaking up the vector into multiple features. Logs or other output that would be helpful This is the output when the tfrecord is too big W tensorflow core framework op kernel.cc 975 Invalid argument Could not parse example input value image ERROR tensorflow Exception in QueueRunner utf8 codec can t decode byte 0x9b in position 40 invalid start byte Exception in thread Thread 3 Traceback most recent call last File usr lib python2.7 threading.py line 801 in bootstrap inner self.run File usr lib python2.7 threading.py line 754 in run self. target self. args self. kwargs File usr local lib python2.7 dist packages tensorflow python training queue runner impl.py line 234 in run sess.run enqueue op File usr local lib python2.7 dist packages tensorflow python client session.py line 766 in run run metadata ptr File usr local lib python2.7 dist packages tensorflow python client session.py line 964 in run feed dict string options run metadata File usr local lib python2.7 dist packages tensorflow python client session.py line 1014 in do run target list options run metadata File usr local lib python2.7 dist packages tensorflow python client session.py line 1021 in do call return fn args File usr local lib python2.7 dist packages tensorflow python client session.py line 1003 in run fn status run metadata File usr lib python2.7 contextlib.py line 24 in exit self.gen.next File usr local lib python2.7 dist packages tensorflow python framework errors impl.py line 468 in raise exception on not ok status compat.as text pywrap tensorflow.TF Message status File usr local lib python2.7 dist packages tensorflow python util compat.py line 84 in as text return bytes or text.decode encoding File usr lib python2.7 encodings utf 8.py line 16 in decode return codecs.utf 8 decode input errors True UnicodeDecodeError utf8 codec can t decode byte 0x9b in position 40 invalid start byte Thank , ebrevdo I m suspicious that this is a TFRecord problem since the sizes don t approach INT MAX. The next likely candidate is the example parsing code. Thoughts I have the same issue when writing large float lists to the tfrecords file. 64 MB seems to be the limit. jonasrauber Ah right. Protobufs are limited to 64 MB outside of Google for some unfathomable reason. martinwicke Was there a hack to increase this Using the other protobuf library as mentioned here doesn t help https github.com tensorflow tensorflow issues 5676 I am having a similar problem when trying to read my Example protocol bufffer in batches. I have also tried the other protobuf library but that did not help. Same issue for me. Increasing size of protobufs would be great Facing the same issue working with high resolution images. Anyone find a hack around this Looks like installing Tensorflow 1.1.0 rc0 solved the issue for me. I can also confirm that using Tensorflow 1.1.0 solved the problem for us. Same here D On Apr 24 2017 4 33 PM Elmer Garduno notifications github.com wrote I can also confirm that using Tensorflow 1.1.0 solved the problem for us. You are receiving this because you authored the thread. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 7311 issuecomment 296828348 or mute the thread https github.com notifications unsubscribe auth AEv26DKUPuzl kTfBrjuYtZKn7gvPlCwks5rzRU1gaJpZM4L4 up . Thank you for checking back. I will close this issue as resolved in 1.1. 
7356,Potential UnboundLocalError in error handling context manager,I m working on unrelated things while this came up and it s possibly unlikely to happen in real development but just to track it in case someone cares https github.com tensorflow tensorflow blob 5e5dc97dd3523509ce5f536d7be5122d016fc6b5 tensorflow python framework errors impl.py L471 that is an UnboundLocalError if it was line 463 that raised there i.e. if TF NewStatus failed entirely . Without any insider knowledge possibly that line belongs outside the try block.,Nice catch thank you I ll fix unless you feel like submitting a PR. how to make tensorflow pull request https gist.github.com yaroslavvb bdb0a92f2a516ebbe2d386c48f5e2c45 
7376,Distributed mnist training in synchronous mode raises a ValueError in worker node,I am trying to run distributed mnist training using the file given here https github.com tensorflow tensorflow blob master tensorflow tools dist test python mnist replica.py https github.com tensorflow tensorflow blob master tensorflow tools dist test python mnist replica.py The async mode doesn t have any issues but I cannot get the sync mode to work. As soon as I start a worker in the sync mode I get a ValueError exception. The command used to start the worker is For python2 the Traceback looks something like this For python3 it looks something like this I have tried running the example on Python2 and Python3 on Ubuntu 14.04 and Ubuntu 16.04 in cpu only mode. In all the 4 cases the version of tensorflow used was 0.12.0. This issue is very similar to the one here https github.com tensorflow tensorflow issues 6687 https github.com tensorflow tensorflow issues 6687 except that I m using v0.12.0 and running in CPU only mode. That issue mentions that the example was running in 0.12.0 but that is not the case for me. ,Can you insert a print statement to print self. replica id before using it in training sync replicas optimizer.py 751 I suspect that value is None in which case jmchen g may know why that would happen This is a known issue that mnist example with sync rep is not working as expected. We need to troubleshoot this... caisq Can you insert a print statement to print self. replica id before using it in training sync replicas optimizer.py 751 I suspect that value is None in which case jmchen g may know why that would happen yaroslavvb You are right it is None. The replica id should no longer be in the file though... It is a variable used in the old optimizer... This issue no longer exists in version 1 of tensorflow. I guess you shall close this issue if it doesn t exist in version 1.0 
7406,Should check whether n class is zero before calling sample n in mixture.py, Problem Description Mixture model first use categorical to sample how much samples it need for each mixture components this is variable n class at line 308 https github.com tensorflow tensorflow blob master tensorflow contrib distributions python ops mixture.py L308 but it actually means number of samples per component and then it pass n class to sample n . The problem is n class could be 0 and you can t pass shape 0 to tf.random gamma shape alpha ... which is used in Beta distribution. see line 310 https github.com tensorflow tensorflow blob master tensorflow contrib distributions python ops mixture.py L310 in mixture.py If possible provide a minimal reproducible example We usually don t have time to read hundreds of lines of your code It s easy to reproduce just create a mixture of Beta Uniform with 50 50 probability. Half of the time it ll sample from uniform and half of the time it ll sample from Beta. What other attempted solutions have you tried Two possible solutions 1. Add a conditional branch in mixture.py like this tested with the above script Just create a zero tensor with shape 0 when n class is 0 and let the reshape operator at line 330 https github.com tensorflow tensorflow blob master tensorflow contrib distributions python ops mixture.py L330 to worry about the shape. 2. Support shape 0 in random gamma shape alpha ... . Personally I think it s a bad idea. It already caused InvalidArgumentError exception which means the one who implemented this might already considered this problem before. Logs or other output that would be helpful If logs are large please upload as attachment or provide link . P.S. The variable name n class confused me for a while.,The bug here is that RandomGamma doesn t allow zero samples. https github.com tensorflow tensorflow blob master tensorflow core kernels random op.cc L306 should be fixed to exit early in that case rather than crashing. brianwa84 Can you confirm this interpretation botonchou Pending Brian s ack would you be interested in submitting a pull request Returning a zero shape tensor is OK for the zero sample use case. Feel free to submit the pull request. On Fri Feb 10 2017 at 11 49 AM Geoffrey Irving notifications github.com wrote The bug here is that RandomGamma doesn t allow zero samples. https github.com tensorflow tensorflow blob master tensorflow core kernels random op.cc L306 should be fixed to exit early in that case rather than crashing. brianwa84 https github.com brianwa84 Can you confirm this interpretation botonchou https github.com botonchou Pending Brian s ack would you be interested in submitting a pull request You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 7406 issuecomment 278996301 or mute the thread https github.com notifications unsubscribe auth AVJZI8fwfqaPDIeg0TDDN fBUXmQdIpAks5rbJURgaJpZM4L8 pY . brianwa84 Not to pry but did you really intend to post your phone number in a Github comment heh thx. email signature fail 
7414,extract image patches gradient only works with float32, tf.extract image patches works well with tf.float64 however its gradient requires that tf.float32 s are passed. Otherwise tf.gradients raises TypeError Input b of SparseTensorDenseMatMul Op has type float64 that does not match type float32 of argument a values . . Here is an example of the offending code. If I take the gradient with respect to X without casting I get the error above. ,The problem appears to be this line https github.com tensorflow tensorflow blob master tensorflow python ops array grad.py L641. It explicitly sets a dtype to be float32 . Can you change tf.float32 to grad flat.dtype and see if it solves your problem Automatically closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks I faced the same issue with calculating gradients for a graph containing extract image patches using TF 1.10.0. After upgrading TF to 1.13.0 the error stack is slightly different but fundamentally the problem remains the same This issue happens while running official NVidia docker container on a Tesla V100. Code runs fine in single precision only crashes with half precision. I faced the same issue with calculating gradients for a graph containing extract image patches using TF 1.10.0. After upgrading TF to 1.13.0 the error stack is slightly different but fundamentally the problem remains the same This issue happens while running official NVidia docker container on a Tesla V100. Code runs fine in single precision only crashes with half precision. I come across the same issue as yours when I use tf.image.extract patches . Do you have ways to solve it 
7417,Crashes in tf.sparse tensor dense matmul, What related GitHub issues or StackOverflow threads have you found by searching the web for your problem Possibly related to 4282 Also SO seems to suggest https stackoverflow.com questions 29401116 abort trap 6 in c program that abort trap 6 is related to accessing memory that TF doesn t own. Both problems boil down to bounds errors crashing the Python session instead of throwing an error. Environment info Operating System macOS Sierra 10.12.3 TF version 0.12.1. Pretty sure I m using a CPU only build and my graphics card is not CUDA compatible. If possible provide a minimal reproducible example We usually don t have time to read hundreds of lines of your code I ve managed to crash Python with sparse tensor dense matmul in two different ways. I ve included a reproducible example for each. Let me know if you d prefer them split into two separate issues. Crash 1 An off by one error crashes my Python session. This kills my Python session and I get the following at my bash prompt Crash 2 It looks like I m exceeding the number of rows this time and getting a crash instead of an error. What other attempted solutions have you tried N A, updated with a second reproducible example and more information Yep this is a bug. ebrevdo Can you take a look Pure Python shouldn t be able to get a CHECK crash like this. Thanks for reporting this. I ll take a look likely after end of next week ebrevdo Which week were you referring to Indeed. This op has been rewritten since the original report. Let s close it and op can reopen if it still happens. On Jun 16 2017 11 24 AM Geoffrey Irving notifications github.com wrote ebrevdo https github.com ebrevdo Which week were you referring to You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 7417 issuecomment 309099384 or mute the thread https github.com notifications unsubscribe auth ABtimx RmqumLzMk8aVyxc930I4pmaNLks5sEshagaJpZM4L9iNG . 
7444,tfprof model analyzer ignores scalar parameters, What related GitHub issues or StackOverflow threads have you found by searching the web for your problem Searched issues for tfprof Environment info gcr.io tensorflow tensorflow 1.0.0 rc2 devel gpu If possible provide a minimal reproducible example We usually don t have time to read hundreds of lines of your code This will show total parameters of 0 despite there being a parameter. What other attempted solutions have you tried N A Logs or other output that would be helpful I believe this is coming from the logic here https github.com tensorflow tensorflow blob v1.0.0 rc2 tensorflow tools tfprof internal tfprof show.cc L37 which skips nodes with empty shapes which includes scalars., petewarden does tfprof maintainer have github username tensorflow tensorflow tools tfprof I believe it s panyx0718 but I m not certain. Let me look into it A fix has been submitted. Will be available in next release Fixed 7444 Cool should we close this issue then or does that typically wait until the change lands in the public GH repo I would suggest you wait until the release. It would be great if you can further verify the fix. Thanks panyx0718 possibly related issue http stackoverflow.com questions 42309202 profiling tensorflow using tfprof sorry I d cc you on email but couldn t find it Confirmed this was fixed by https github.com tensorflow tensorflow commit 1849946c09ad93bc2e4f49c658049223e1efe908 from https github.com tensorflow tensorflow pull 7539. 
7457,UnboundLocalError local variable status referenced before assignment,The output from python c import tensorflow print tensorflow. version python c import tensorflow print tensorflow. version I tensorflow stream executor dso loader.cc 128 successfully opened CUDA library libcublas.so locally I tensorflow stream executor dso loader.cc 128 successfully opened CUDA library libcudnn.so locally I tensorflow stream executor dso loader.cc 128 successfully opened CUDA library libcufft.so locally I tensorflow stream executor dso loader.cc 128 successfully opened CUDA library libcuda.so.1 locally I tensorflow stream executor dso loader.cc 128 successfully opened CUDA library libcurand.so locally 0.12.1 Here s the error Epoch 2 2 19125 19125 78s loss 0.4568 acc 0.8681 val loss 2.1682 val acc 0.4104 Exception ignored in bound method BaseSession. del of tensorflow.python.client.session.Session object at 0x7f747161e160 Traceback most recent call last File home p3 anaconda3 lib python3.5 site packages tensorflow python client session.py line 581 in del UnboundLocalError local variable status referenced before assignment ,This should ve been fixed in https github.com tensorflow tensorflow pull 7386 can you see if error persists in latest nightly version Automatically closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks I ve got this in tensorflow 1.0.1 . Still persists. Happens intermittently with tensorflow tensorflow 1.0.1 gpu py3 docker image and keras. Also happened for me upon saving a model in Ubuntu 16.0.4 Python3 and Keras Happend to me too today on our GPU Cluster On a GeForce GTX 980 Ti. zheng xq could you take a look The exception comes from session.py. Passing to Derek fro now. I m pretty sure this has been fixed since the 1.1 release. I m closing this issue for now but feel free to reopen it if you can reproduce with a newer version of TensorFlow. I got this error today while training my network on keras. Epoch 10 10 19308 19286 62s loss 0.0220 acc 0.1795 val loss 0.0231 val acc 0.1846 dict keys val acc val loss acc loss Acc 0.1782905629139073 0.1796146675076869 0.18056705298013245 0.17775015535748767 0.18656870860927152 0.17137973897756442 0.17694536423841059 0.18142738762061394 0.17860099337748345 0.17951108349797049 Loss 0.032948918208874613 0.029692213859633675 0.028213497242020654 0.027055541726004047 0.025659046015572666 0.025640698771453779 0.024098622677416005 0.023113698205617934 0.02143701246557202 0.02202132222925083 Validation Loss 0.027665206672329651 0.026977688277141799 0.026762272280297782 0.025648139429476319 0.024419905385002494 0.024124456143497338 0.024165717970677895 0.022718481179619013 0.021766701916234244 0.023056170290434046 Exception ignored in bound method BaseSession. del of tensorflow.python.client.session.Session object at 0x7fc9535b4a90 Traceback most recent call last File home carnd anaconda3 envs carnd term1 lib python3.5 site packages tensorflow python client session.py line 581 in del UnboundLocalError local variable status referenced before assignment I also get this error version 1.5 2018 03 19 02 51 05.913 EDT The replica master 0 exited with a non zero status of 1. Termination reason Error. Traceback most recent call last File usr lib python2.7 runpy.py line 174 in run module as main main fname loader pkg name File usr lib python2.7 runpy.py line 72 in run code exec code in run globals File root .local lib python2.7 site packages object detection train.py line 167 in module tf.app.run File usr local lib python2.7 dist packages tensorflow python platform app.py line 124 in run sys.exit main argv File root .local lib python2.7 site packages object detection train.py line 163 in main worker job name is chief FLAGS.train dir File root .local lib python2.7 site packages object detection trainer.py line 366 in train saver saver File usr local lib python2.7 dist packages tensorflow contrib slim python slim learning.py line 790 in train return total loss UnboundLocalError local variable total loss referenced before assignment To find out more about why your job exited please check the logs https console.cloud.google.com logs viewer project 301256752837 resource ml job 2Fjob id 2Fshreya object detection 1521441960 advancedFilter resource.type 3D 22ml job 22 0Aresource.labels.job id 3D 22shreya object detection 1521441960 22 i also get this error version 1.6 Traceback most recent call last File object detection train.py line 167 in module tf.app.run File usr local lib python2.7 dist packages tensorflow python platform app.py line 126 in run sys.exit main argv File object detection train.py line 163 in main worker job name is chief FLAGS.train dir File usr local lib python2.7 dist packages tensorflow models research object detection trainer.py line 360 in train saver saver File usr local lib python2.7 dist packages tensorflow contrib slim python slim learning.py line 791 in train return total loss UnboundLocalError local variable total loss referenced before assignment I also get a similar error when attempting to train using the object detection API identical to the comment above Traceback most recent call last File object detection train.py line 167 in module tf.app.run File usr local lib python2.7 dist packages tensorflow python platform app.py line 126 in run sys.exit main argv File object detection train.py line 163 in main worker job name is chief FLAGS.train dir File home dave tensorflow master models research object detection trainer.py line 370 in train saver saver File usr local lib python2.7 dist packages tensorflow contrib slim python slim learning.py line 791 in train return total loss UnboundLocalError local variable total loss referenced before assignment 
7500,TensorFlow version 1.0.0 rc2 on Windows OpKernel op BestSplits device type CPU for unknown op BestSplits with test code,I installed TensorFlow version 1.0.0 rc2 on Windows 7 SP1 x64 Ultimate Python 3.5.2 Anaconda custom 64 bit using pip install upgrade https storage.googleapis.com tensorflow windows cpu tensorflow 1.0.0rc2 cp35 cp35m win amd64.whl When I try running the test script from https web.archive.org web 20170214034751 https www.tensorflow.org get started os setup test the tensorflow installation in Eclipse 4.5 or in the console import tensorflow as tf print TensorFlow version 0 .format tf. version hello tf.constant Hello TensorFlow sess tf.Session print sess.run hello I obtain some error message TensorFlow version 1.0.0 rc2 Hello TensorFlow E c tf jenkins home workspace release win device cpu os windows tensorflob w core framework op kernel.cc 943 OpKernel op BestSplits device type CPU for unknown op BestSplits E c tf jenkins home workspace release win device cpu os windows tensorflow core framework op kernel.cc 943 OpKernel op CountExtremelyRandomStats device type CPU for unknown op CountExtremelyRandomStats E c tf jenkins home workspace release win device cpu os windows tensorflow core framework op kernel.cc 943 OpKernel op FinishedNodes device type CPU for unknown op FinishedNodes E c tf jenkins home workspace release win device cpu os windows tensorflow core framework op kernel.cc 943 OpKernel op GrowTree device type CPU for unknown op GrowTree E c tf jenkins home workspace release win device cpu os windows tensorflow core framework op kernel.cc 943 OpKernel op ReinterpretStringToFloat device type CPU for unknown op ReinterpretStringToFloat E c tf jenkins home workspace release win device cpu os windows tensorflow core framework op kernel.cc 943 OpKernel op SampleInputs device type CPU for unknown op SampleInputs E c tf jenkins home workspace release win device cpu os windows tensorflow core framework op kernel.cc 943 OpKernel op ScatterAddNdim device type CPU for unknown op ScatterAddNdim E c tf jenkins home workspace release win device cpu os windows tensorflow core framework op kernel.cc 943 OpKernel op TopNInsert device type CPU for unknown op TopNInsert E c tf jenkins home workspace release win device cpu os windows tensorflow core framework op kernel.cc 943 OpKernel op TopNRemove device type CPU for unknown op TopNRemove E c tf jenkins home workspace release win device cpu os windows tensorflow core framework op kernel.cc 943 OpKernel op TreePredictions device type CPU for unknown op TreePredictions E c tf jenkins home workspace release win device cpu os windows tensorflow core framework op kernel.cc 943 OpKernel op UpdateFertileSlots device type CPU for unknown op UpdateFertileSlots Why I didn t have such issues with TensorFlow 0.12.1 installed with pip install tensorflow 0.12.1 TensorFlow version 0.12.1 b Hello TensorFlow Stack Exchange thread TensorFlow version 1.0.0 rc2 on Windows OpKernel op BestSplits device type CPU for unknown op BestSplits with test code http stackoverflow.com q 42217532 395857 drpngx , mrry might have a clue. As far as I can tell this is fixed at HEAD but didn t make it into the release candidate. Fortunately you can ignore this message unless you want to use tf.contrib.tensor forest. but upgrading to the latest nightly should fix it. Could you verify that this works Franck Dernoncourt and close it if so I just ran into this error on the 1.0 release linked to on the new install guide. Too soon for fix to get there I tested and can confirm as mrry pointed out that today s nightly build http ci.tensorflow.org view Nightly job nightly win 85 worked. I just received those SSE warnings which are unrelated. Hi I am also facing similar type of issue. I am running tensorflow 1.0 on windows 10. When I run the following program import numpy as np import tensorflow as tf one real values column features tf.contrib.layers.real valued column dimension 1 estimator tf.contrib.learn.LinearRegressor feature columns features dataSet tf.contrib.learn.datasets.base.Dataset data np.array 1 2 3 4 target np.array 0 1 2 3 estimator.fit x dataSet.data y dataSet.target steps 1000 estimator.evaluate x dataSet.data y dataSet.target I get the following error messages. WARNING tensorflow Using temporary folder as model directory C Users CRCV AppData Local Temp tmp2rbt5gl8 C Users CRCV AppData Local Programs Python Python35 lib site packages tensorflow python util deprecation.py 247 FutureWarning comparison to None will result in an elementwise object comparison in the future. equality a b WARNING tensorflow From C Users CRCV documents visual studio 2015 Projects HelloTF HelloTF tf contrib basic.py 14 calling BaseEstimator.fit from tensorflow.contrib.learn.python.learn.estimators.estimator with y is deprecated and will be removed after 2016 12 01. Instructions for updating Estimator is decoupled from Scikit Learn interface by moving into separate class SKCompat. Arguments x y and batch size are only available in the SKCompat class Estimator will only accept input fn. Example conversion est Estimator ... est SKCompat Estimator ... WARNING tensorflow From C Users CRCV documents visual studio 2015 Projects HelloTF HelloTF tf contrib basic.py 14 calling BaseEstimator.fit from tensorflow.contrib.learn.python.learn.estimators.estimator with x is deprecated and will be removed after 2016 12 01. Instructions for updating Estimator is decoupled from Scikit Learn interface by moving into separate class SKCompat. Arguments x y and batch size are only available in the SKCompat class Estimator will only accept input fn. Example conversion est Estimator ... est SKCompat Estimator ... E c tf jenkins home workspace release win device cpu os windows tensorflow core framework op kernel.cc 943 OpKernel op BestSplits device type CPU for unknown op BestSplits E c tf jenkins home workspace release win device cpu os windows tensorflow core framework op kernel.cc 943 OpKernel op CountExtremelyRandomStats device type CPU for unknown op CountExtremelyRandomStats E c tf jenkins home workspace release win device cpu os windows tensorflow core framework op kernel.cc 943 OpKernel op FinishedNodes device type CPU for unknown op FinishedNodes E c tf jenkins home workspace release win device cpu os windows tensorflow core framework op kernel.cc 943 OpKernel op GrowTree device type CPU for unknown op GrowTree E c tf jenkins home workspace release win device cpu os windows tensorflow core framework op kernel.cc 943 OpKernel op ReinterpretStringToFloat device type CPU for unknown op ReinterpretStringToFloat E c tf jenkins home workspace release win device cpu os windows tensorflow core framework op kernel.cc 943 OpKernel op SampleInputs device type CPU for unknown op SampleInputs E c tf jenkins home workspace release win device cpu os windows tensorflow core framework op kernel.cc 943 OpKernel op ScatterAddNdim device type CPU for unknown op ScatterAddNdim E c tf jenkins home workspace release win device cpu os windows tensorflow core framework op kernel.cc 943 OpKernel op TopNInsert device type CPU for unknown op TopNInsert E c tf jenkins home workspace release win device cpu os windows tensorflow core framework op kernel.cc 943 OpKernel op TopNRemove device type CPU for unknown op TopNRemove E c tf jenkins home workspace release win device cpu os windows tensorflow core framework op kernel.cc 943 OpKernel op TreePredictions device type CPU for unknown op TreePredictions E c tf jenkins home workspace release win device cpu os windows tensorflow core framework op kernel.cc 943 OpKernel op UpdateFertileSlots device type CPU for unknown op UpdateFertileSlots WARNING tensorflow From C Users CRCV AppData Local Programs Python Python35 lib site packages tensorflow contrib learn python learn estimators head.py 1362 scalar summary from tensorflow.python.ops.logging ops is deprecated and will be removed after 2016 11 30. Instructions for updating Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de duplicate summary names based on the scope they are created in. Also passing a tensor or list of tags to a scalar summary op is no longer supported. WARNING tensorflow From C Users CRCV documents visual studio 2015 Projects HelloTF HelloTF tf contrib basic.py 16 calling BaseEstimator.evaluate from tensorflow.contrib.learn.python.learn.estimators.estimator with y is deprecated and will be removed after 2016 12 01. Instructions for updating Estimator is decoupled from Scikit Learn interface by moving into separate class SKCompat. Arguments x y and batch size are only available in the SKCompat class Estimator will only accept input fn. Example conversion est Estimator ... est SKCompat Estimator ... WARNING tensorflow From C Users CRCV documents visual studio 2015 Projects HelloTF HelloTF tf contrib basic.py 16 calling BaseEstimator.evaluate from tensorflow.contrib.learn.python.learn.estimators.estimator with x is deprecated and will be removed after 2016 12 01. Instructions for updating Estimator is decoupled from Scikit Learn interface by moving into separate class SKCompat. Arguments x y and batch size are only available in the SKCompat class Estimator will only accept input fn. Example conversion est Estimator ... est SKCompat Estimator ... WARNING tensorflow From C Users CRCV AppData Local Programs Python Python35 lib site packages tensorflow contrib learn python learn estimators head.py 1362 scalar summary from tensorflow.python.ops.logging ops is deprecated and will be removed after 2016 11 30. Instructions for updating Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de duplicate summary names based on the scope they are created in. Also passing a tensor or list of tags to a scalar summary op is no longer supported. WARNING tensorflow Skipping summary for global step must be a float or np.float32. Press any key to continue . . . any solution Running the same code as sajjo79 and seeing the same issue using tf 1.0.0 on Windows 10 Python 3.5.2 CUDA 8.0 cuDNN 5.1 Going to try installing the nightly EDIT Nightly build has the same issue sajjo79 It looks like most of the issues in your post are due to 2 things. First is the display issues that opkernal.cc is outputting. The second is deprecated features being used. For the deprecated features the the warning message shows the code changes you will need to make. And as mrry mentions upgrading to nightly should fixed the opkernel issues until the update gets pushed to the main. Also https www.tensorflow.org install migration contains a list of breaking changes in 1.0. I had to update several of my notebooks because things have moved around got renamed or args were changed. JerryKurata I agree with you on the possible fixes. This is tutorial code though. It s strange it doesn t just work Seeing the exact same issue as Franck Dernoncourt with the Windows installation test code using the 1.0 release on Windows 10 Anniversary Edition. maxamante Any time your write a tutorial it is the best your can do at the time. But over time things change and there is only so much time to keep up the documentation. FWIW does anyone know if the TensorFlow team is looking for help with the documentation tutorials etc on tensorflow.org For everybody replying with this same issue the fix was already provided with downloading and installing the most recent nightly build 0 if you want to get it going right now. Otherwise the alternative is to wait for the nightly build to be released as an official build and upgrade. If you are having a different problem please open a new issue. maxamante Could you please give more details on what you did because the nightly build 85 with GPU support worked for me https github.com tensorflow tensorflow issues 7500 issuecomment 280157912 . JerryKurata Not a TensorFlow member but I think PRs usually are welcome. If there is something you want to address open an issue and feel free to submit a PR. 0 http ci.tensorflow.org view Nightly job nightly win 85 Carmezim I was commenting on sajjo79 s issue with the tutorial code not executing to completion. Sorry for the derail. Nightly build does solve the OpKernel for me maxamante This got a little confusing here. I think this issue could be closed when Franck Dernoncourt gives a follow up of his side and folks having other problems could open new issues. Thanks installing today s nightly build http ci.tensorflow.org view Nightly job nightly win 85 CPU version pip install upgrade http ci.tensorflow.org view Nightly job nightly win 85 DEVICE cpu OS windows artifact cmake build tf python dist tensorflow 1.0.0rc2 cp35 cp35m win amd64.whl fixed the issue no more OpKernel op BestSplits device type CPU for unknown op BestSplits etc. . But as Carmezim remarked there are now some SSE warnings aselle Should I close this issue and reopen one about the SSE warnings gunan fyi On Feb 15 2017 5 03 PM Franck Dernoncourt notifications github.com wrote Installing today s nightly build http ci.tensorflow.org view Nightly job nightly win 85 CPU version fixed the issue no more OpKernel op BestSplits device type CPU for unknown op BestSplits etc. . But as Carmezim https github.com Carmezim remarked there are now some SSE warnings TensorFlow version 1.0.0 rc2 b Hello TensorFlow 2017 02 15 19 56 22.688266 W c tf jenkins home workspace nightly win device cpu os windows tensorflow core platform cpu feature guard.cc 45 The TensorFlow library wasn t compiled to use SSE instructions but these are available on your machine and could speed up CPU computations. 2017 02 15 19 56 22.688266 W c tf jenkins home workspace nightly win device cpu os windows tensorflow core platform cpu feature guard.cc 45 The TensorFlow library wasn t compiled to use SSE2 instructions but these are available on your machine and could speed up CPU computations. 2017 02 15 19 56 22.689266 W c tf jenkins home workspace nightly win device cpu os windows tensorflow core platform cpu feature guard.cc 45 The TensorFlow library wasn t compiled to use SSE3 instructions but these are available on your machine and could speed up CPU computations. 2017 02 15 19 56 22.689266 W c tf jenkins home workspace nightly win device cpu os windows tensorflow core platform cpu feature guard.cc 45 The TensorFlow library wasn t compiled to use SSE4.1 instructions but these are available on your machine and could speed up CPU computations. 2017 02 15 19 56 22.689266 W c tf jenkins home workspace nightly win device cpu os windows tensorflow core platform cpu feature guard.cc 45 The TensorFlow library wasn t compiled to use SSE4.2 instructions but these are available on your machine and could speed up CPU computations. 2017 02 15 19 56 22.689266 W c tf jenkins home workspace nightly win device cpu os windows tensorflow core platform cpu feature guard.cc 45 The TensorFlow library wasn t compiled to use AVX instructions but these are available on your machine and could speed up CPU computations. You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 7500 issuecomment 280194967 or mute the thread https github.com notifications unsubscribe auth AT Sbd3iNXDX7RJrYJ2Cx4ukIo PkI3yks5rc6BYgaJpZM4MAjbp . I see the same issue using the raw win32 interpreter cmd . However it s working for me in python IDLE shell I too faced the same issue. After installing nightly build error gone. Now getting below warnings sess tf.Session 2017 02 17 13 01 59.790943 W c tf jenkins home workspace nightly win device cpu os windows tensorflow core platform cpu feature guard.cc 45 The TensorFlow library wasn t compiled to use SSE instructions but these are available on your machine and could speed up CPU computations. Validate tf installation Windows CMD C Windows system32 python Python 3.5.3 v3.5.3 1880cb95a742 Jan 16 2017 16 02 32 MSC v.1900 64 bit AM D64 on win32 Type help copyright credits or license for more information. import tensorflow as tf hello tf.constant Hello TensorFlow sess tf.Session 2017 02 17 20 00 30.151037 W c tf jenkins home workspace nightly win device cp u os windows tensorflow core platform cpu feature guard.cc 45 The TensorFlow li brary wasn t compiled to use SSE instructions but these are available on your m achine and could speed up CPU computations. 2017 02 17 20 00 30.151421 W c tf jenkins home workspace nightly win device cp u os windows tensorflow core platform cpu feature guard.cc 45 The TensorFlow li brary wasn t compiled to use SSE2 instructions but these are available on your machine and could speed up CPU computations. 2017 02 17 20 00 30.151958 W c tf jenkins home workspace nightly win device cp u os windows tensorflow core platform cpu feature guard.cc 45 The TensorFlow li brary wasn t compiled to use SSE3 instructions but these are available on your machine and could speed up CPU computations. 2017 02 17 20 00 30.152531 W c tf jenkins home workspace nightly win device cp u os windows tensorflow core platform cpu feature guard.cc 45 The TensorFlow li brary wasn t compiled to use SSE4.1 instructions but these are available on you r machine and could speed up CPU computations. 2017 02 17 20 00 30.153561 W c tf jenkins home workspace nightly win device cp u os windows tensorflow core platform cpu feature guard.cc 45 The TensorFlow li brary wasn t compiled to use SSE4.2 instructions but these are available on you r machine and could speed up CPU computations. 2017 02 17 20 00 30.154072 W c tf jenkins home workspace nightly win device cp u os windows tensorflow core platform cpu feature guard.cc 45 The TensorFlow li brary wasn t compiled to use AVX instructions but these are available on your m achine and could speed up CPU computations. print sess.run hello b Hello TensorFlow Python 3.5.3 Shell import tensorflow as tf hello tf.constant Hello TensorFlow sess tf.Session print sess.run hello b Hello TensorFlow TrisDing jameslem As the issue itself is solved please feel free to open a new one to address theses warnings. Thanks Carmezim and Franck Dernoncourt for confirming that this is fixed in the nightlies. I m going to close this issue because the original problem has already been solved and lock it for new discussion. Please open a new issue if you still have problems 
7508,import graph def s input map doesn t remap control inputs,If a graph g has a node y with a control input like x I would have thought that tf.import graph def g input map x z would result in a graph having a node y with a control input z . Instead I get an error ValueError Attempted to map inputs that were not found in graph def x 0 . This is on master. Complete example ,Nice reproducible example It works if you do x instead of x tf.import graph def g1.as graph def input map x z Looking at the relevant logic in importer.py it seems it takes name of node directly from graphdef so x would have two names x and x depending on the role and you d need to specify two remapping rules for it. Not ideal not sure if there s an easy way to fix it cc mrry because he left a comment there for i input name in enumerate CanonicalInputName x for x in node.input Thanks Unfortunately if you use the C API via TF GraphImportGraphDefWithReturnOutputs in analogy to the above example it still doesn t work to use x instead of x as the remapping key. AFAICT mapping control inputs is impossible in the C API. Reassigning this to skye who s currently reconciling the Python and C APIs for importing graphs. If the trick yaroslavvb suggested works that s only by accident since we didn t originally intend to provide a way to remap control inputs using tf.import graph def . It doesn t seem too problematic to allow it though... As yaroslavvb noted you should use x in the input map. Using x in the input map is equivalent to x 0 i.e. the non control output of x . However when I try running your code snippet above with x it looks like that functionality is broken. I ll work on patching it. malmaud the C API should work with control inputs. You d use something like the following TF ImportGraphDefOptionsAddInputMapping opts x 1 z 1 Note the use of 1 to indicate a control edge. This should be documented in the header comment sorry for the confusion. For those following along this functionality is only checked in on master not 1.0 . Thanks I did eventually figure out the 1 magic constant and wrote a PR https github.com tensorflow tensorflow pull 7724 to document it. It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. A member of the TensorFlow organization has replied after the stat awaiting tensorflower label was applied. Nagging Assignee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. skye any updates on the patch Oops forgot to update this issue. This has been resolved. To remap a control input use the F ImportGraphDefOptionsRemapControlDependency method https github.com tensorflow tensorflow blob master tensorflow c c api.h L922 The 1 trick should still work too although I think is technically undefined behavior. 
7554,TFLearn Estimator Summary Writer Fails., Issue TFLearn estimator summary writer fails because write dict to summary in tensorflow contrib learn python learn estimators estimator.py https github.com tensorflow tensorflow blob master tensorflow contrib learn python learn estimators estimator.py L324 dose not allow int values which naturally stops the np.int64 valued global step parameter from being written thus causing the summary writer to fail. How to reproduce Try and run wide n deep tutorial.py https github.com tensorflow tensorflow blob master tensorflow examples learn wide n deep tutorial.py and you will get INFO tensorflow Saving dict for global step 202 accuracy 0.818254 accuracy baseline label mean 0.236226 accuracy threshold 0.500000 mean 0.818254 auc 0.742517 global step 202 labels actual label mean 0.236226 labels prediction mean 0.182356 loss 0.680463 precision positive threshold 0.500000 mean 0.764145 recall positive threshold 0.500000 mean 0.333593 WARNING tensorflow Skipping summary for global step must be a float or np.float32. Pull Request 7555, ilblackdragon martinwicke could you please take a look at this issue Thanks Just looking over this again why does this fail It emits a warning and does not write the global step which is probably not a bad thing. Is there an actual failure Hello I m facing the same warning as thesuperzapper that prevents me from writing to the summary and visualizing on tensorboard. same problem using early stopping ... no output to see the values accuracy .. and it seams the early stopping does not activate and stops as it should. I have this monitor to test every step just for debug validation monitor tf.contrib.learn.monitors.ValidationMonitor x test inputs y test output eval steps 1 every n steps 1 and this output ... Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de duplicate summary names based on the scope they are created in. Also passing a tensor or list of tags to a scalar summary op is no longer supported. INFO tensorflow Starting evaluation at 2017 04 27 16 05 14 INFO tensorflow Restoring parameters from C Users birinhos AppData Local Temp tmpn8b3fjgp model.ckpt 1 INFO tensorflow Evaluation 1 1 INFO tensorflow Finished evaluation at 2017 04 27 16 05 15 INFO tensorflow Saving dict for global step 1 accuracy 0.221193 global step 1 loss 1.6383 WARNING tensorflow Skipping summary for global step must be a float or np.float32. INFO tensorflow Validation step 1 loss 1.6383 global step 1 accuracy 0.221193 INFO tensorflow global step sec 5.17624 INFO tensorflow loss 1.59089 step 101 19.303 sec INFO tensorflow global step sec 6.90738 INFO tensorflow loss 1.58918 step 201 14.477 sec ... only after 4201 steps the monitor seams to appear to print something INFO tensorflow loss 1.52565 step 4001 13.839 sec INFO tensorflow global step sec 6.71895 INFO tensorflow loss 1.52449 step 4101 14.881 sec INFO tensorflow global step sec 7.22961 INFO tensorflow loss 1.52349 step 4201 13.835 sec INFO tensorflow Saving checkpoints for 4265 into C Users birinhos AppData Local Temp tmpn8b3fjgp model.ckpt. WARNING tensorflow From C Users birinhos AppData Local Programs Python Python35 lib site packages tensorflow contrib learn python learn monitors.py 662 calling BaseEstimator.evaluate from tensorflow.contrib.learn.python.learn.estimators.estimator with y is deprecated and will be removed after 2016 12 01. Instructions for updating Estimator is decoupled from Scikit Learn interface by moving into separate class SKCompat. Arguments x y and batch size are only available in the SKCompat class Estimator will only accept input fn. Example conversion est Estimator ... est SKCompat Estimator ... WARNING tensorflow From C Users birinhos AppData Local Programs Python Python35 lib site packages tensorflow contrib learn python learn monitors.py 662 calling BaseEstimator.evaluate from tensorflow.contrib.learn.python.learn.estimators.estimator with x is deprecated and will be removed after 2016 12 01. Instructions for updating Estimator is decoupled from Scikit Learn interface by moving into separate class SKCompat. Arguments x y and batch size are only available in the SKCompat class Estimator will only accept input fn. Example conversion est Estimator ... est SKCompat Estimator ... WARNING tensorflow From C Users birinhos AppData Local Programs Python Python35 lib site packages tensorflow contrib learn python learn estimators head.py 615 scalar summary from tensorflow.python.ops.logging ops is deprecated and will be removed after 2016 11 30. Instructions for updating Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de duplicate summary names based on the scope they are created in. Also passing a tensor or list of tags to a scalar summary op is no longer supported. INFO tensorflow Starting evaluation at 2017 04 27 16 15 14 INFO tensorflow Restoring parameters from C Users birinhos AppData Local Temp tmpn8b3fjgp model.ckpt 4265 INFO tensorflow Evaluation 1 1 INFO tensorflow Finished evaluation at 2017 04 27 16 15 16 INFO tensorflow Saving dict for global step 4265 accuracy 0.290535 global step 4265 loss 1.55965 WARNING tensorflow Skipping summary for global step must be a float or np.float32. INFO tensorflow Validation step 4265 loss 1.55965 global step 4265 accuracy 0.290535 INFO tensorflow global step sec 5.2094 INFO tensorflow loss 1.52247 step 4301 19.196 sec INFO tensorflow global step sec 7.4686 INFO tensorflow loss 1.52147 step 4401 13.443 sec ... I m also getting this warning using global step tf.contrib.framework.get global step . Tensorflow v1.1.0 
7585,Invalid argument error from tf.gather nd after upgrade to r1.0,I just upgraded to r1.0 and ran into an issue which I find hard to dissect further. The issue did not occur before when I was using version r0.12. The error message is the following and the stack trace tells me it stems from the last of the following lines This function is called to calculate a cost function which is used during an optimization procedure. See below for a minimum working example. The error as far as I can tell from the stack trace does not happen when the graph is built but when the computation is executed which is why I m not sure how I can further narrow down the problem. Environment info Operating System Ubuntu 14.04 x64 Installed version of CUDA and cuDNN 8.0 and cuDNN 5.1 Installed from source from revision 16485a3fb5ffcbaa244e55c388e43279d2770982 using bazel 0.4.4 Minimum working example ,I just tried your minimal test case in 0.12 and I got an error as well. Are you sure things were working in 0.12. Thanks aselle Thanks for your feedback. I tested it again in 0.12 too and also got an error but this error is due to a bug in the minimum working example I ll edit this in just second. I then ran the fixed version on both 1.0 and 0.12 on 1.0 it does not pass while on 0.12 it passes. EDIT minimum working example is now fixed was an issue with indexing kaufManu The error message in your original post comes from a ScatterNd op which is the gradient code for the gather nd op in your example. The shape validation code for scatter nd was changed recently. ebrevdo could you take a look at this please Is there any update on this Just encountered it as well. Feels indeed that the shape validation part of scatter nd is buggy. It checks for updates.shape indices.shape IXDIM params shape IXDIM but as far as I understand something like updates.shape indices.shape 1 params shape indices.shape 1 is what is expected. EDIT by the way this test scatter nd ops test.py https github.com tensorflow tensorflow blob 63a21e054007d86269ed1ad0145ebce04ee57a81 tensorflow python kernel tests scatter nd ops test.py L480 fails for me when I reproduce it EDIT2 apaprently was corrected here already https github.com tensorflow tensorflow commit 3b7b39ac5dd2dceebe4b80b5e0b12316720a924b Are you using tensorflow nightlies or master ebrevdo I am not using nightlies See if the bug has been fixed in the nightlies on master. We worked on this code after TF 1.0 was released. On Fri Mar 17 2017 at 2 53 AM kaufManu notifications github.com wrote ebrevdo https github.com ebrevdo I am not using nightlies You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 7585 issuecomment 287311974 or mute the thread https github.com notifications unsubscribe auth ABtimzo8qqBI3iIjSYkJ59GHfn76RKu7ks5rmlgEgaJpZM4MDRDB . Shouldn t the automatically gradient scatter nd code have the correct shape if the forward mode gather nd is correct I have code that works fine in forward mode but produces this error when I try to run training. I just built the master branch on my machine commit c44601f5411408c324c81dd38bc8686cbd2568aa but it doesn t make any difference. I just went back and checked again and saw that my install of the master branch didn t take because the version hasn t changed. I forced the install and indeed this fixes the problem for me so this should be in 1.0.2 whenever that comes out. Looks like a bug. On Mar 24 2017 5 22 PM Christopher Barber notifications github.com wrote Shouldn t the automatically gradient scatter nd code have the correct shape if the forward mode gather nd is correct I have code that works fine in forward mode but produces this error when I try to run training. I just built the master branch on my machine commit c44601f https github.com tensorflow tensorflow commit c44601f5411408c324c81dd38bc8686cbd2568aa but it doesn t make any difference. You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 7585 issuecomment 289172834 or mute the thread https github.com notifications unsubscribe auth ABtim8IZr5rcEWKtbnkc5ABwZxtMk0d ks5rpF4qgaJpZM4MDRDB . What are the shapes of your Tensors On Mar 24 2017 6 32 PM Eugene Brevdo ebrevdo gmail.com wrote Looks like a bug. On Mar 24 2017 5 22 PM Christopher Barber notifications github.com wrote Shouldn t the automatically gradient scatter nd code have the correct shape if the forward mode gather nd is correct I have code that works fine in forward mode but produces this error when I try to run training. I just built the master branch on my machine commit c44601f https github.com tensorflow tensorflow commit c44601f5411408c324c81dd38bc8686cbd2568aa but it doesn t make any difference. You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 7585 issuecomment 289172834 or mute the thread https github.com notifications unsubscribe auth ABtim8IZr5rcEWKtbnkc5ABwZxtMk0d ks5rpF4qgaJpZM4MDRDB . Same problem here on r1.0 with different shapes for example tf.gather nd params indces works for params.shape 24 1296 1 indices.shape 24 20 2 whears does not for params.shape 24 1296 21 indices.shape 24 20 2 Getting same incorrect message. Environment info Operating System Archlinux CUDA cuDNN 8.0 and cuDNN 5.1 I don t think you saw that I edited my comment. The bug is NOT present on the current master branch so it must have been fixed. Piotr does the bug exist on master the nightlies On Mar 25 2017 3 12 PM Christopher Barber notifications github.com wrote I don t think you saw that I edited my comment. The bug is NOT present on the current master branch so it must have been fixed. You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 7585 issuecomment 289243050 or mute the thread https github.com notifications unsubscribe auth ABtim2tYjyzXT2QfaDBcYQP85SOiRhqQks5rpZFEgaJpZM4MDRDB . FWIW the shape of my tensors for output tf.gather nd params indices are params.shape 16 8 8 8 indices.shape x 16 2 output.shape x 16 8 8 where x is a batch size depending on data. Any updates on this Notika do you still get this error in the nightlies On Mar 27 2017 11 01 AM Nitika Verma notifications github.com wrote Any updates on this You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 7585 issuecomment 289512842 or mute the thread https github.com notifications unsubscribe auth ABtim0Jk1Z 9DHIKUEAEJqiGFORX nsMks5rp g3gaJpZM4MDRDB . For me similar error was gone after upgrading to v1.1 Also fixed for me in 1.1. Thanks for confirming the fix everyone. 
7657,is jpeg function only detects JFIF and not EXIF jpeg images in decode image ,I am using tf.image.decode image function to dynamically decode jpeg png or gif on the fly. However inside decode image it checks if passed tensor if jpeg image using this condition is jpeg math ops.equal substr b xff xd8 xff xe0 name is jpeg EXIF files have a marker of 0xff e1 JFIF files have a marker of 0xff e0 . So all code that relies on 0xffe0 to detect a JPEG file will miss all EXIF files. When I patched it to only match first 3 bytes its working fine.,Good catch. Could you submit a PR that replaces the check as that s a little more conservative than your 3 byte check but it is in the same spirit. Thanks so much Ok sure. Have never contributed to open source project till now. Any guides out there prats226 Thanks for catching this bug and offering to fix it There info on contributing to TensorFlow here https github.com tensorflow tensorflow blob master CONTRIBUTING.md That links to the following documentation about creating pull requests PRs https help.github.com articles about pull requests Closing due to lack of activity. Please reopen if necessary. 
7689,tf.nn.softmax errors out when dim dim size 1 instead of 1, Environment info Operating System MacOS 10.12.1 Installed version of CUDA and cuDNN lrwxr xr x 1 root wheel 50 Sep 26 15 00 usr local cuda lib libcudart.8.0.dylib Developer NVIDIA CUDA 8.0 lib libcudart.8.0.dylib lrwxr xr x 1 root wheel 47 Oct 24 21 11 usr local cuda lib libcudnn.5.dylib Developer NVIDIA CUDA 8.0 lib libcudnn.5.dylib Tensorflow version 0.12.1 If possible provide a minimal reproducible example We usually don t have time to read hundreds of lines of your code Error message InvalidArgumentError see above for traceback Requires start limit when delta 0 3 2 Node range 1 Range Tidx DT INT32 device job localhost replica 0 task 0 cpu 0 range 1 start Sub range 1 delta , annarev could you take a quick look Looks like this was fixed by this commit https github.com tensorflow tensorflow commit fb53109ad20b2e568246b05796296a047c271856 It doesn t seem to be in the r1.0 branch unfortunately but should be in the nightly build. Closing the issue as the fix seems to be in nightly build and should be in the next release. i have the same problem with you have you worked out SherryyHou i haven t tested it since. try upgrading to the latest version. 
7751,tf.contrib.crf doesn t support sequences of length 1, tf.contrib.crf https www.tensorflow.org versions master api docs python contrib.crf doesn t support sequences of length 1. For example if I run the example on https github.com tensorflow tensorflow tree master tensorflow contrib crf and replace num words 20 by num words 1 I get the error Tested with TensorFlow 1.0.0 on Windows 7 SP1 x64 Ultimate and TensorFlow GPU 1.0.0 on Ubuntu 14.04.4 LTS x64.,What happens with a size like 2 or 5 I think your problem might not be a general contrib.crf issue but the specific example in particular this line sequence lengths np.full num examples num words 1 dtype np.int32 num words 2 or num words 5 work. Good catch I don t know why num words 1 is used in the CRF example instead of num words in the line of code you quote. https github.com tensorflow tensorflow blob e121667dc609de978a223c56ee906368d2c4ceef tensorflow contrib crf python ops crf.py L121 is already decrementing the sequence length by 1 However changing sequence lengths np.full num examples num words 1 dtype np.int32 to sequence lengths np.full num examples num words dtype np.int32 does not solve the issue when num words 1 . This appears to be an unmaintained contrib utility. Would you like to propose a fix Automatically closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks Sorry I can t commit to proposing a fix in the near future. aselle Do you need more information from me as far as I know the issue is still present so I think it should be reopened Corresponding Stack Exchange question How can I pass sequences of length 1 to tf.contrib.crf in TensorFlow http stackoverflow.com q 42798518 395857 aselle if contrib crf is not maintained should it perhaps be deleted Is there any plan for supporting CRF or CRF like code if the one in contrib is currently unsupported I was planning to use this for Named Entity Recognition. I think contrib crf will help a lot other people who are trying to do the same. If given a little guidance I can contribute as well. anikdas I used the CRF layer to perform named entity recognition in https github.com Franck Dernoncourt NeuroNER if interested. Since in named entity recognition it s often preferable to add start and end tokens around each sequence it avoids having sequences of length 1. why is tf.contrib.crf discontinued what is the alternative then this is such a useful lib 
7754,tf.split num or size splits x ... fails for x Dimension 128 ,Seen while upgrading our code to TF 1.0 somehow this used to work in 12.1 but in TF 1.0 the following fails with IndexError list index out of range inside of array ops.py tf.split num or size splits X.get shape 1 ... Looking at array ops.py the relevant logic So Dimension 128 is treated as Tensor and code fails with IndexError list index out of range inside array ops.py . I think it would make more sense if the Tensor path checked if num or size splits was Tensor or convertible to non scalar Tensor and then have a catch all else for all other cases. Or perhaps documentation could be updated to say that this path is taken if num split is not a scalar which is what s happening now despite the documentation implying that this is only for Tensor arguments. I would fix this myself but I already have an outstanding PR and the current system makes it too painful to switch branches https github.com tensorflow tensorflow issues 6911 ,Thanks for the detailed bug report. Internal bug opened. Use python type conversion to solve this issue tf.split num or size splits int X.get shape 1 ... Close for now since I assume this has already been fixed. If not please reopen. 
7767,ParameterServer restart crashes distributed training process,I ve tried running distributed TensorFlow with Supervisor and new MonitoredTrainingSession and I observe following behavior if Parameter Server is restarted. Code is available at http pastebin.com HBUicRp2. I m using TensorFlow freshly built from r1.0. At first this error happens which is OK However after Parameter Server is restarted I see this error in logs and training worker crashes I expect it to recover from latest checkpoint instead., jhseu we discussed this on TF Dev Summit and you recommended raising issue. Thanks Adding to my todo list but may not get to it for a little bit . Sent out a fix internally. Will you backport to Supervisor s managed session as well It s still listed on TensorFlow documentation website as one of the recommended ways to do simplified session management https www.tensorflow.org programmers guide supervisor My understanding is that managed session never did recovery for failed workers. Also Supervisor is eventually going away. I never use Supervisor so I m probably wrong. AFAIK it is has similar level of recoverability as MonitoredTrainingSession before your fix . Here s my observations for Supervisor s managed session Parameter Server failure. Training throws UnavailableError on all workers. All workers crash. After restart of failed PS and all workers training continues from the last checkpoint. Chief worker failure. Training freezes. After chief worker is restarted training continues from the last checkpoint. Non Chief worker failure. Training freezes. After failed worker is restarted training continues from the last iteration. Backup worker failure. Training continues with potential slowdown. After failed backup worker is restarted training continues with original pace. Since it s still widely advertised a lot of people may jump to use it. alsrgv jhseu Hi sorry to comment on this closed issue. But I m wondering will training freezes if non chief workers failure in the latest release v1.5.0 
7774,after type run in tensorflow debugger the terminal reappears and stucks, My program can run and show some traing results but for a classification question the accuracies of training set and validation set both keep at around 0.5 for 40 epoches. So I want to use tensorflow debugger to watch what the variables are. When I type run and push enter it jump out of the debugger and the terminal reshows then the terminal screen stuck at below. name GeForce GTX 1080 major 6 minor 1 memoryClockRate GHz 1.835 pciBusID 0000 01 00.0 Total memory 7.92GiB Free memory 7.28GiB I tensorflow core common runtime gpu gpu device.cc 906 DMA 0 I tensorflow core common runtime gpu gpu device.cc 916 0 Y I tensorflow core common runtime gpu gpu device.cc 975 Creating TensorFlow device gpu 0 device 0 name GeForce GTX 1080 pci bus id 0000 01 00.0 the nvidia smi results is below NVIDIA SMI 375.39 Driver Version 375.39 GPU Name Persistence M Bus Id Disp.A Volatile Uncorr. ECC Fan Temp Perf Pwr Usage Cap Memory Usage GPU Util Compute M. 0 GeForce GTX 1080 Off 0000 01 00.0 On N A 0 43C P2 53W 200W 1857MiB 8105MiB 0 Default Processes GPU Memory GPU PID Type Process name Usage 0 1011 G usr lib xorg Xorg 300MiB 0 1802 G compiz 133MiB 0 3362 G ... Enabled MarkNonSecureAs show non secure 99MiB 0 14422 C python 1321MiB any suggestion for solve the problem , jetW Does your graph contain any tf.while loops If so it may get stuck because of a bug in tfdbg in version 1.0.0 that causes the debugger to freeze on certain while loops. This bug has been fixed in master HEAD. Maybe you can try our nightly builds https github.com tensorflow tensorflow installation or wait for the next release for the fix. The graph does not have tf.while. I may wait for the next release. Thank you Has the tf.while loop freeze in tfdbg been fixed in any new builds caisq Has this problem been resolved I m having a similar problem when using tfdbg. My network is returning nan after 1 optimization step so I m trying to resolve what s going on. My problem is same as above when I type run and push enter it jump out of the debugger and the terminal reshows then the terminal screen stuck at below. There are no errors or warnings of any kind. Also if I use the invoke stepper it freezes during some while steps indefinitely or at least several hours until I cut it off . In particular it freezes on steps that look like bilstm1 bilstm1 bidirectional rnn 1 fw fw while layer norm basic lstm cell state 1 batchnorm sub Enter I definitely have while loops from tf.nn.bidirectional dynamic rnn and tf.map fn but I set parallel iterations 1 for all of those calls. I read as well that there are some bugs with while loops on gpu s so I have tried running all ops with tf.device cpu 0 as well with the same results. Any ideas I can try to get tfdbg to work in this situation I am using the nightly build of tensorflow gpu 1.1.0rc2 cp35 cp35m win amd64.whl Build 149 Apr 19 2017 2 25 00 AM which is the last stable build There have been a few bug fixes for tfdbg related to while loops in the past several weeks. fredtony can you please let me know what version of TensorFlow you are using caisq I a using 1.1.0 rc2. I have the nightly build Build 149 Apr 19 2017 2 25 00 AM which is the last stable build . Revision ed6bb758dabc612234e9afd38bdaeb1b84e09955 fredtony Thanks for the info. I was under the impression that this PR https github.com tensorflow tensorflow commit 99559a3455d7c3b3663968a55bf56801743201d0 should have resolve a previous problem related to tf.while loop. This commit is in both the master and r1.1 branches. But there might be other corner cases not covered here. Can you provide some sample code and some fake data to reproduce the issue 
7917,Unreachable statement,tensorflow compiler xla service algebraic simplifier.cc line 675 additional return Status OK ,Could you fix this tatatodd. Thanks I m on it... thanks for filing the issue maddin200 OK this has been fixed it should get synced over to the github repo in the next couple of days. Closing this out. 
8011,TypeError Fetch argument None has invalid type class NoneType ,Feature request for a better error description OR for better summary handling The following code works fine if some summaries where defined before However if there were no summaries we get TypeError Fetch argument None has invalid type class NoneType Which is really saying One of the session.run ops where empty which is forbidden. Alternatively let merge all return a NoOp if there are no summaries., pannous Thanks for the concise and clear issue dandelionmane the suggestion sounds reasonable can you comment It should just work and return a tf.no op . On it. bump this one just cost me 5 minutes of my life P It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. Nagging Assigneee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Automatically closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks 
8025,quantize graph round and quantize modes are broken,Looking at code in master. CC petewarden A KeyError is always produced. Consider the round mode already visited which will fails for all calls https github.com tensorflow tensorflow blob fa4ba830f437fdb9dc1085b4d68a3bab41a16e20 tensorflow tools quantization quantize graph.py L402 L404 with a key error since the already visited dict will be empty. ,Thanks for filing this issue cancan101 I think you re completely right. I tried to track down where this bug was introduced. Assigning this to andrewharp since it looks like he touched this part of the code. Andrew feel free to re assign as appropriate. Thanks andrewharp Is this fixable It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. Nagging Assigneee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Automatically closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks andrewharp Is there a fix for this bug 
8047,Error in tensorflow debugger tfdbg while executing session run call in child thread,I ran tensorflow debugger using the command python m PythonModule debug but got the following error i e Signal only works in main thread screenshot3png https cloud.githubusercontent.com assets 21690396 23540767 24df69c6 ff98 11e6 96bf 5ee98a439f77.PNG The error is thrown when a session run call is executed in a child thread spawned from main thread . Is tensorflow debugger only supported for single threaded applications What related GitHub issues or StackOverflow threads have you found by searching the web for your problem None Environment info Operating System CentOS 7.2.1511 Installed version of CUDA and cuDNN please attach the output of ls l path to cuda lib libcud screenshot https cloud.githubusercontent.com assets 21690396 23540624 38732a46 ff97 11e6 9749 a43b8af6c5aa.PNG If installed from binary pip package provide 1. A link to the pip package you installed https storage.googleapis.com tensorflow linux gpu tensorflow gpu 1.0.0 cp35 cp35m linux x86 64.whl 2. The output from python c import tensorflow print tensorflow. version . screenshot2 https cloud.githubusercontent.com assets 21690396 23540694 b42648f8 ff97 11e6 93ff 4cabc2d2d199.PNG If installed from source provide 1. The commit hash git rev parse HEAD 2. The output of bazel version If possible provide a minimal reproducible example We usually don t have time to read hundreds of lines of your code What other attempted solutions have you tried Logs or other output that would be helpful If logs are large please upload as attachment or provide link . , agupta74 tfdbg s CLI class attempts to register signal handlers even when not running on the main threads which is the reason for this error. We will send a bug fix to the master branch. As a workaround until the bug is fixed you can pretend that the child process is a remote process and use the approach outlined here https www.tensorflow.org programmers guide debugger offline debugging of remotely running sessions Later in an environment that you have terminal access to you can load and inspect the data in the dump directory on the shared storage by using the offline analyzer of tfdbg. For example Thank you for reporting this bug agupta74 caisq Any updates regarding this issue I ve tried using offline analyzer per your suggestion but keep running into ValueError Duplicate node name as in 7051. Unlike 7051 I m running locally on CPU. The problem seems to stem from repeated calls to sess.run where sess is an instance of DumpingDebugWrapperSession but I m not sure how to get around this issue when using tf.get variable given that a call to an initializer seems necessary. Included below is a code snippet. j wilson With regard to the duplicate node name error you are getting are you using the same file debug URL i.e. the same directory for different session.run calls If that s the case can you try using a unique different URL i.e. directory for each session.run call caisq The script runs if modified s.t. a new wrapper directory is used for each call to sess.run however none of the tensors show up inside the debugger CLI i.e. 0 dumped tensor s . As a separate comment is there a reason why separate wrappers directories are needed for each call to sess.run The wrapper creates subdirectories run run id suggesting that reuse of a single wrapper might be possible which would help with usability. It should work with a single wrapper. As for the reason why you see 0 dumped tensor s it might be that 1 the node name regex whitelist value returned from your watch fn hits no nodes in the graph What if you use watch fn None so the default behavior i.e. watch all nodes can be used 2 ops are constant folded during graph optimization less likely 
8112,tf upgrade doesn t update RNN cells,Migration script tf upgrade.py does not update RNN cell locations from tf.nn.rnn cell to tf.contrib.rnn leading to error AttributeError module tensorflow.python.ops.nn has no attribute rnn cell Upgrading becomes when it should be , aselle just a gentle reminder. Unless we can close this jaakkopasanen Preparing a cl internally that will resolve this. aselle Hi any update on this Closing this out since I understand it to be resolved but please let me know if I m mistaken. Thanks 
8284,TF ImportGraphDef crashes for the following graph, What related GitHub issues or StackOverflow threads have you found by searching the web for your problem None Environment info Operating System Ubuntu 16.04 Installed version of CUDA and cuDNN please attach the output of ls l path to cuda lib libcud CUDA rw r r 1 root root 558720 Nov 30 13 53 libcudadevrt.a lrwxrwxrwx 1 root root 16 Nov 30 13 53 libcudart.so libcudart.so.8.0 lrwxrwxrwx 1 root root 19 Nov 30 13 53 libcudart.so.8.0 libcudart.so.8.0.44 rwxr xr x 1 root root 415432 Nov 30 13 53 libcudart.so.8.0.44 rw r r 1 root root 775162 Nov 30 13 53 libcudart static.a CUDNN lrwxrwxrwx 1 ajayaram ajayaram 13 Nov 30 14 02 libcudnn.so libcudnn.so.5 lrwxrwxrwx 1 ajayaram ajayaram 17 Nov 30 14 02 libcudnn.so.5 libcudnn.so.5.1.5 rwxrwxr x 1 ajayaram ajayaram 79337624 Jul 27 2016 libcudnn.so.5.1.5 rw rw r 1 ajayaram ajayaram 69756172 Jul 27 2016 libcudnn static.a If installed from binary pip package provide 1. A link to the pip package you installed 2. The output from python c import tensorflow print tensorflow. version . If installed from source provide 1. The commit hash git rev parse HEAD 29a6b4661258ef99842904d7c54993c963a8c2c0 2. The output of bazel version ............................. Build label 0.4.4 Build target bazel out local fastbuild bin src main java com google devtools build lib bazel BazelServer deploy.jar Build time Wed Feb 1 18 54 21 2017 1485975261 Build timestamp 1485975261 Build timestamp as int 1485975261 If possible provide a minimal reproducible example We usually don t have time to read hundreds of lines of your code lstm issue.zip https github.com tensorflow tensorflow files 834801 lstm issue.zip What other attempted solutions have you tried None. Stuck here. Works in python but I need to load this graph in C. Logs or other output that would be helpful If logs are large please upload as attachment or provide link . , ajayaraman Thanks for the very detailed report and the instructions to reproduce it. This is very helpful. There seems to be some issue triggered in the shape inference code I m looking into it. 
8311,Different output using CudnnGRU vs GRUCell,Operating System Arch Linux Installed version of CUDA and cuDNN libcudart.so.8.0.44 libcudnn.so.5.1.5 1. The commit hash git rev parse HEAD 57e40363eb40a692f7c5dfea3f53031a52024321 2. The output of bazel version 0.4.4 I set x 1 previous h 0 all weights and biases 0 output should be 0. Traditional tensorflow GRU returns 0 but CudnnGRU returns 0.20482421 NOTE need to use GPU for CudnnGRU to work properly , zheng xq This seems like something is wrong Try putting linear input instead of skip input linear input does correctly output 0. I think there s still a bug with skip input though because in my example input size num units so it shouldn t require a linear projection before the GRU layer. When I try auto select it returns 0.20482421. Here s the argument definition https github.com tensorflow tensorflow blob master tensorflow contrib cudnn rnn python ops cudnn rnn ops.py L377 . input mode indicate whether there is a linear projection between the input and The actual computation before the first layer. It could be skip input linear input or auto select . skip input is only allowed when input size num units auto select implies skip input when input size num units otherwise it implies linear input . skip input as far as I can tell skips the Wx linear transformation of the input and performs roughly half the computation as the other approach. This is probably in case you want to do batch norm through depth and so need flexibility to modify the outputs of Wx . It does NOT appear to apply an extra linear transform outside of the GRU. As you re expecting to apply Wx you should enable linear input . As a side note there still appears to be bug in skip input since for GRUs if you skip the initial linear transformation you need to pass in an input matrix that has dimensionality 3x that of the hidden dimension each gate in the GRU takes a separate linear transformation . CuDNN still checks for equality in dimensionality however and appears to just copy the input to all three gates. This is likely incorrect and I can t make sense of it. Using linear input will make the GRU behave as expected. eddiepierce is your issue resolved I m gonna close this but please let me know if it should be reopened. Thanks. 
8337,tfdbg error Causality violated in timing relations of debug dumps, What related GitHub issues or StackOverflow threads have you found by searching the web for your problem Nothing found Environment info Operating System Ubuntu 14.04 Installed version of CUDA and cuDNN CUDA 8.0 cuDNN 5.1 please attach the output of ls l path to cuda lib libcud If installed from source provide 1. The commit hash git rev parse HEAD 12a98726e769e988f6368a029ec2f5b0ac3ccbd4 2. The output of bazel version If possible provide a minimal reproducible example We usually don t have time to read hundreds of lines of your code I have a model that basically consists of several bidirectional rnns the longest of which has hundreds of timesteps. What other attempted solutions have you tried Logs or other output that would be helpful When I try to launch the first Session.run call in tfdbg using command r I got the following error Is this a model related error or possibly a tfdbg bug , zxvix From your error message it appears that you are using tf.while loop . Can you try setting its paralle iterations parameter to 1 and see if the error still happens There may be a bug in how tfdbg handles while loops with parallel iterations 1. I think it might be a GPU thing. The example below errors if run as python tf 8337 minimal.py but is fine is run as CUDA VISIBLE DEVICES 1 python tf 8337 minimal.py . The final error tends to change in a non deterministic fashion. I have also got Ubuntu 16.04.2 Source 12a9872 Bazel 0.4.4 CUDA 8.0 CUDNN 5.1.5 python 2.7.12 2 GPUs Titan X pascal Thanks for the info. Ah I see you have more than one GPUs. As a diagnostic step does this error occur if you run the code on a single GPU Yeah same thing Thanks for looking into this. I m seeing this error on CPU 1e33acb381430ed0004eb76b98181624af267b2c with parallel iterations 1 so it s not entirely a GPU thing jekbradbury It will be helpful if you could provide some code for reproducing this error on our and and let us know how reproducible time it happens this crash is. Thanks. My guess is that I ve done something dumb in the optimization routine I m getting NaNs which is why I tried the debugger . But here s the gist https gist.github.com jekbradbury 05e9f17776771f1f501919b7eff58c7f anyway it has happened all three times I ve tried running it with the debugger the error comes on the second use of run the first time the optimization routine happens . On v1.0 stable it hangs like in this issue https github.com tensorflow tensorflow issues 7774 . Thanks jekbradbury your code reproduces the issue on my machine CPU only . Also al626 s code also errors out on my GPU machine but works fine on CPU only. These may be different manifestations of the same issue. We ll look into it. FYI this bug should have been fixed by 7b410eb. I verified it works with the two reproduction code samples above. The fix will become available in the nightly artifacts on the next successful build. I am having the same problem getting NaNs when using dynamic rnn and the td debugger exiting with ValueError Causality violated in timing relations of debug dumps gradients rnn while rnn dnc cell head 0 strided slice 7 grad StridedSliceGrad StackPush 1 1501099455073575 these input s are not satisfied gradients rnn while rnn dnc cell head 0 strided slice 7 grad StridedSliceGrad RefEnter 1 0 Using tensorflow 1.2.0 with python3. 
8391,tf.lbeta error when fed with placeholder,Hi there seems to be a bug in tf.lbeta . , langmore are you the right person to take a look Yes I m the right person for this. concretevitamin I have an internal fix for this. I ll send to you once ready. thjashin Thanks for reporting A fix by langmore will be pushed upstream soon. Closing this. Thanks concretevitamin langmore Just a bit more there seems to be no static shape inference in tf.lbeta What in particular doesn t work The old code had some static inference see the tests . After this recent fix the same tests are still in place and the code is much simpler so I suspect there won t be any problems. 
8401,Tensorflow not working in Zeppelin 0.7.0,There seems to be some issue using Tensorflow in Zeppelin 0.7.0 and it throws this error NameError name interactive is not defined Another user reported the same issue http stackoverflow.com questions 42757433 tensorflow can not work with zeppelin on SO. The fix suggested there seems to be really hacky. The issue seems to be that in tf logging.py file . Anaconda3 Lib site packages tensorflow python platform tf logging.py the interactive variable is not triggering for Zeppelin. ,It looks the code meant to have interactive False before the block https github.com tensorflow tensorflow blob master tensorflow python platform tf logging.py L40 . Would you be willing to file a PR Sure thing Great thanks 
8481,Cannot import CudnnRNN from contrib.cudnn rnn in 1.0.1,For some reason all the init .py in python directory of contrib.cudnn rnn have been removed and it is impossible to import anything from there.,Just an update I have a fix for this but am still working on getting it in. https github.com tensorflow tensorflow commit 986d337e7da04de627218c12e20be1e3abbf6097 should address this. 
8504,Cannot reuse variables by tf.layers.conv2d,I am trying to make two conv layers share the same weights however it seems the API does not work. gives Update According to a post https stackoverflow.com questions 42862300 tensorflow reuse variable with tf layers conv2d the weights are already sharing with different layer names since the computation not sharing. However is it feasible to consider a better naming strategy so that it is easier to see from names that different layers are sharing the same weights , ebrevdo Thanks for your PR. I have a short question With the new behavior will fc2 reuse the weights from fc1 or from ker Because previously the weight sharing should be I accidentally introduced a bug. It should reuse the weights from fc1 but i accidentally made it reuse the weights from ker. i m sending an update to reverse this behavior. On Wed Mar 22 2017 at 12 52 AM Xingdong Zuo notifications github.com wrote ebrevdo https github.com ebrevdo Thanks for your PR. I have a short question with tf.variable scope new scope ker tf.get variable kernel 5 6 fc1 tf.layers.dense inputs tf.constant np.random.randn 3 4 5 units 3 reuse None fc2 tf.layers.dense inputs tf.constant np.random.randn 3 4 5 units 3 reuse True With the new behavior will fc2 reuse the weights from fc1 or from ker Because previously the weight sharing should be with tf.variable scope new scope fc1 tf.layers.dense inputs tf.constant np.random.randn 3 4 5 units 3 reuse None fc2 tf.layers.dense inputs tf.constant np.random.randn 3 4 5 units 3 reuse True You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 8504 issuecomment 288323108 or mute the thread https github.com notifications unsubscribe auth ABtim9CC9jizTqiQkcxXS 47Cao8h9IIks5roNNGgaJpZM4MgzyA . Fix should be in. On Wed Mar 22 2017 at 9 44 AM Eugene Brevdo ebrevdo gmail.com wrote I accidentally introduced a bug. It should reuse the weights from fc1 but i accidentally made it reuse the weights from ker. i m sending an update to reverse this behavior. On Wed Mar 22 2017 at 12 52 AM Xingdong Zuo notifications github.com wrote ebrevdo https github.com ebrevdo Thanks for your PR. I have a short question with tf.variable scope new scope ker tf.get variable kernel 5 6 fc1 tf.layers.dense inputs tf.constant np.random.randn 3 4 5 units 3 reuse None fc2 tf.layers.dense inputs tf.constant np.random.randn 3 4 5 units 3 reuse True With the new behavior will fc2 reuse the weights from fc1 or from ker Because previously the weight sharing should be with tf.variable scope new scope fc1 tf.layers.dense inputs tf.constant np.random.randn 3 4 5 units 3 reuse None fc2 tf.layers.dense inputs tf.constant np.random.randn 3 4 5 units 3 reuse True You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 8504 issuecomment 288323108 or mute the thread https github.com notifications unsubscribe auth ABtim9CC9jizTqiQkcxXS 47Cao8h9IIks5roNNGgaJpZM4MgzyA . 
8564, tf.test.compute gradient gives error when computation involves TensorArrays, If possible provide a minimal reproducible example We usually don t have time to read hundreds of lines of your code The gradient calculation sess.run grads works fine it is just something to do with how the gradients are being calculated within tf.test.compute gradients . Here is the stack trace , yuanbyu ebrevdo this test was introduced in an old CL 118251112 . Any idea What version of tensorflow are you using On Wed Mar 22 2017 at 12 18 PM drpngx notifications github.com wrote yuanbyu https github.com yuanbyu ebrevdo https github.com ebrevdo this test was introduced in an old CL 118251112 . Any idea You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 8564 issuecomment 288509134 or mute the thread https github.com notifications unsubscribe auth ABtim9jUS bGN2k87umxBTSHgFPnR2xEks5roXP gaJpZM4Mi g . This is on master commit af2ebdb827f0b07d8f7d2b76b061976a2a778cb0 What happens if you do z tf.identity y then use z in gradients and test gradients On Wed Mar 22 2017 at 1 11 PM Daniel Rasmussen notifications github.com wrote This is on master commit af2ebdb https github.com tensorflow tensorflow commit af2ebdb827f0b07d8f7d2b76b061976a2a778cb0 You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 8564 issuecomment 288523693 or mute the thread https github.com notifications unsubscribe auth ABtim o6vg7zj3qTcoT gD06pwE7MlL9ks5roYBYgaJpZM4Mi g . Hi all new user here I also experienced this problem also on master. I found that if you slice the stacked tensor then the gradient test passes like y a.stack with adjusted size in the compute gradient call tf.test.compute gradient x 5 y 1 5 ebrevdo tried the tf.identity this didn t resolve the issue. Hope this helps. Just double checked and this issue is still occurring in release 1.3.0. CC alextp This will be resolved when we move from TensorArray to TensorList under the covers cc alextp. alextp do you want to take this over and close it when everything s TensorList or should we put a special label on it drasmuss Is this issue resolved Please update here If it was not resolved already. Thanks Yes it looks like this has been resolved now as of TF 1.13 . drasmuss Thanks for confirming. I am closing the issue. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 8564 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 8564 No a 
8571,shape of state in the output of dynamic rnn is undetermined,short code then state.h and state.c should be of shape 72 56 but actually the first dim batch size is undetermined in program is it a bug ,CC ebrevdo for comments What is the shape of lookup ebrevdo does it matter let s say embedding matrix is of shape vocab size 90000 embed size 500 so lookup will be of shape 72 unkown 500 output should be 72 unknown 56 state.h should be 72 56 can you confirm lookup is shape 72 unknown 500 On Wed Mar 22 2017 at 7 37 PM Jie Zhou notifications github.com wrote ebrevdo https github.com ebrevdo does it matter let s say embedding matrix is of shape vocab size 90000 embed size 500 so lookup will be of shape 72 unkown 500 output should be 72 unknown 56 state.h should be 72 56 You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 8571 issuecomment 288600264 or mute the thread https github.com notifications unsubscribe auth ABtimzfXEmN2UUFq eea9GWs9sXt3nPQks5rodsDgaJpZM4MjTwm . ebrevdo What s the status of this issue I think this is fixed in tf 1.2... OP can you confirm On Jun 16 2017 1 40 PM Geoffrey Irving notifications github.com wrote ebrevdo https github.com ebrevdo What s the status of this issue You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 8571 issuecomment 309128560 or mute the thread https github.com notifications unsubscribe auth ABtim1bnJBGaW RNRGN4k3WMBwCEKMO9ks5sEugSgaJpZM4MjTwm . ebrevdo Hi I m having the same problem with the shape of the state being undetermined when using a dynamic rnn. I m using tensorflow version 1.2.1. outputs final state tf.nn.bidirectional dynamic rnn cell fw encoder cell cell bw encoder cell inputs encoder inputs embedded sequence length encoder inputs length dtype tf.float32 time major True Encoder inputs embedded is the following Tensor embedding lookup 0 shape 8 11 20 dtype float32 while final state is the following LSTMStateTuple c tf.Tensor bidirectional rnn fw fw while Exit 2 0 shape 20 dtype float32 h tf.Tensor bidirectional rnn fw fw while Exit 3 0 shape 20 dtype float32 LSTMStateTuple c tf.Tensor bidirectional rnn bw bw while Exit 2 0 shape 20 dtype float32 h tf.Tensor bidirectional rnn bw bw while Exit 3 0 shape 20 dtype float32 What about in tf 1.3 On Sep 3 2017 1 33 PM Alex Fabbri notifications github.com wrote Hi I m having the same problem with the shape of the state being undetermined when using a dynamic rnn. I m using tensorflow version 1.2.1. outputs final state tf.nn.bidirectional dynamic rnn cell fw encoder cell cell bw encoder cell inputs encoder inputs embedded sequence length encoder inputs length dtype tf.float32 time major True Encoder inputs embedded is the following Tensor embedding lookup 0 shape 8 11 20 dtype float32 while final state is the following LSTMStateTuple c tf.Tensor bidirectional rnn fw fw while Exit 2 0 shape 20 dtype float32 h tf.Tensor bidirectional rnn fw fw while Exit 3 0 shape 20 dtype float32 LSTMStateTuple c tf.Tensor bidirectional rnn bw bw while Exit 2 0 shape 20 dtype float32 h tf.Tensor bidirectional rnn bw bw while Exit 3 0 shape 20 dtype float32 You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 8571 issuecomment 326829438 or mute the thread https github.com notifications unsubscribe auth ABtimwjPP56wsPLfP8KHXoeQh cJ1W2dks5sew0EgaJpZM4MjTwm . I just tried it on tf 1.3 and it worked. Thanks Great 
8581,tf.read file doesn t support non ASCII characters in filename,Windows 2003 server R2 if input file names contain non ASCII characters such as Chinese exception will raise. Node ReadFile 17 ReadFile device job localhost replica 0 task 0 cpu 0 unstack 5 2 Caused by op ReadFile 17 defined at File train.py line 660 in module tf.app.run File C Anaconda3 lib site packages tensorflow python platform app.py line 44 in run sys.exit main sys.argv 1 flags passthrough File train.py line 657 in main train File train.py line 430 in train num preprocess threads num preprocess threads File input.py line 216 in distorted inputs num readers FLAGS.num readers File input.py line 256 in batch inputs image buffer tf.read file filename File C Anaconda3 lib site packages tensorflow python ops gen io ops.py line 203 in read file result op def lib.apply op ReadFile filename filename name name File C Anaconda3 lib site packages tensorflow python framework op def library.py line 768 in apply op op def op def File C Anaconda3 lib site packages tensorflow python framework ops.py line 2334 in create op original op self. default original op op def op def File C Anaconda3 lib site packages tensorflow python framework ops.py line 1226 in init self. traceback extract stack NotFoundError see above for traceback Can not get size for 01 057 14734860 91829.jpg u03f5 u0373 udcd5 u04b2 udcbb udcb5 udcbd u05b8 udcb6 udca8 udcb5 udcc4 udcce u013c udcfe udca1 udca3 Node ReadFile 17 ReadFile device job localhost replica 0 task 0 cpu 0 unstack 5 2 ,Right. We need to call the right function on windows. It would be nice if you could submit a PR. This is a bug in TensorFlow. It looks like we re using the ANSI versions of Windows filesystem functions e.g. in windows file system.cc 444 https github.com tensorflow tensorflow blob master tensorflow core platform windows windows file system.cc L444 we should use GetFileAttributesExW instead of GetFileAttributesExA see Microsoft s docs https msdn.microsoft.com en us library windows desktop aa364946 v vs.85 .aspx . I m working on a fix but unfortunately I don t have a good setup for testing Windows code so it might take a while before it s committed. For now I suggest using ASCII filenames if possible. Oh didn t see drpngx s comment If wangxianliang or someone else wants to spin up a patch that would be great. Maybe the bug is not related to the ANSI version of GetFileAttributesExA. I created a new project and used the code snippet of GetFileSize. The function can get the file attributes correctly whenever the input file name contains non ASCII characters or not. I wonder if we re getting the string properly utf8 encoded from python. Worth checking. On Mar 22 2017 11 16 PM Xianliang Wang notifications github.com wrote Maybe the bug is not related to the ANSI version of GetFileAttributesExA. I created a new project and used the code snippet of GetFileSize. The function can get the file attributes correctly whenever the input file name contains non ASCII characters or not. You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 8581 issuecomment 288627061 or mute the thread https github.com notifications unsubscribe auth AT SbVWqE5ksJtJWG67QcfAGdI5MsBZgks5rog47gaJpZM4MjzGo . Automatically closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks drpngx girving skye This bug can be fixed by changing TranslateName in windows file system.h from to The input fname is UTF 8 format however windows can t open utf8 encoded filename in c . another fix could change some of the python codes in tensorflow python lib io file io.py from to drpngx skye girving 
8612,make test graphs.py fails to open file in binary mode breaks in py3,The file tensorflow compiler aot tests make test graphs.py attempts to write binary data to files without opening the files in binary mode which raises errors in Python 3. This can be fixed by opening the files in binary mode i.e. I m not sure if making this change will break Python 2. I will try and submit a pull request later in the week. ,Thanks that sounds like a good PR to send Please mention this bug in the PR Fixes 8612 will make it auto close this issue . 
8624,Constant folding does not work across devices ,I ve been trying to understand tensorflow internals recently. I found in tensorflow core common runtime direct session.cc if I understand it correctly that constant folding only take place at L1051 https github.com tensorflow tensorflow blob ef56133461079f28b61b5a83a62685051408aadb tensorflow core common runtime direct session.cc L1051 after graph partitioning so constants won t propagate through device boundary. This is also evidenced by a simple experiment Resulting computation time graph is graph run https cloud.githubusercontent.com assets 10446514 24199102 62c291ae 0f43 11e7 8689 a595ded2c7ed.png Whereas placing all ops on GPU gives a fully shaded graph. Did I miss something Or is there any consideration not to run constant folding before partitioning the graph ,Right from your example it looks like it should have been folded all the way up to c 2 on cpu in the ideal case. If you re confident that might be a good PR to send. Also I m curious to know what happens with XLA enabled. I have tried enabling XLA with both session config and jit scope. With the session config approach nothing changed. With jit scope all nodes became shaded but even when a was fed so it is more likely an incompatibility between XLA and tracing than an indication that constant folding is working. Global constant folding has its own problem in that a full graph can only be run using a session while a partition graph requires just an executor. Therefore constant folding sounds more suitable as an JIT optimization to me. For example a session triggers constant folding at the second run then it identifies constant foldable tensors and add them to fetches and finally substitute them at the third run. Right. When xla is enabled all that is jit ed shows as a single node. So it may or may not be folded inside. On Mar 25 2017 8 19 AM Huazuo Gao notifications github.com wrote I have tried enabling XLA with both session config and jit scope. With the session config approach nothing changed. With jit scope all nodes became shaded but even when a was fed so it is more likely an incompatibility between XLA and tracing than an indication that constant folding is working. Global constant folding has its own problem in that a full graph can only be run using a session while a partition graph requires just an executor. Therefore constant folding sounds more suitable as an JIT optimization to me. For example a session triggers constant folding at the second run then it identifies constant foldable tensors and add them to fetches and finally substitute them at the third run. You are receiving this because you commented. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 8624 issuecomment 289218123 or mute the thread https github.com notifications unsubscribe auth AT SbbebDW61 pTcM9cp9tgmg3z2UEzlks5rpTBogaJpZM4MlL8n . Automatically closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks 
8715,RecordReader reads disk without buffer,tensorflow io RecordReader calls RandomAccessFile Read directly without go through IoBuffer. And RandomAccessFile Read will call pread 2 or ReadFile OS API. If there is an IoBuffer between them it could reduce a lot of syscalls. What related GitHub issues or StackOverflow threads have you found by searching the web for your problem None Environment info Operating System Windows 10 Installed version of CUDA and cuDNN please attach the output of ls l path to cuda lib libcud None if installed from source provide 1. The commit hash git rev parse HEAD c7b80d51da4fb6d51ea54a0bdf2601afa379d60c 2. The output of bazel version compiled by cmake and vs 2017 If possible provide a minimal reproducible example We usually don t have time to read hundreds of lines of your code None What other attempted solutions have you tried None Logs or other output that would be helpful ,That might be a good candidate for a PR if you re willing to contribute. skye Hi drpngx I ll take this. I just found the interface is a bit inconsistent. For compressed file it assumes that the file will be read sequentially and it will ignore the offset parameter in So if this class is designed for sequential reads I want to remove the offset argument and use ZlibInputStream for zipped file SnappyInputStream for snappy file RandomAccessInputStream for uncompressed file. Is it ok Automatically closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks 
8720,split not working correctly, NOTE Only file GitHub issues for bugs and feature requests. All other topics will be closed. For general support from the community see StackOverflow https stackoverflow.com questions tagged tensorflow . To make bugs and feature requests more easy to find and organize we close issues that are deemed out of scope for GitHub Issues and point people to StackOverflow. For bugs or installation issues please provide the following information. The more information you provide the more easily we will be able to offer help and advice. What related GitHub issues or StackOverflow threads have you found by searching the web for your problem Environment info Operating System Windows Installed version of CUDA and cuDNN please attach the output of ls l path to cuda lib libcud See 2 below If installed from binary pip package provide 1. A link to the pip package you installed How do I get this 2. The output from python c import tensorflow print tensorflow. version . C Users television2 Anaconda3 C cygwin64 home television2 nn hierarchy python c import tensorflow print tensorflow. version I c tf jenkins home workspace release win device gpu os windows tensorflow stream executor dso loader.cc 135 successfully opened CUDA library cublas64 80.dll locally I c tf jenkins home workspace release win device gpu os windows tensorflow stream executor dso loader.cc 135 successfully opened CUDA library cudnn64 5.dll locally I c tf jenkins home workspace release win device gpu os windows tensorflow stream executor dso loader.cc 135 successfully opened CUDA library cufft64 80.dll locally I c tf jenkins home workspace release win device gpu os windows tensorflow stream executor dso loader.cc 135 successfully opened CUDA library nvcuda.dll locally I c tf jenkins home workspace release win device gpu os windows tensorflow stream executor dso loader.cc 135 successfully opened CUDA library curand64 80.dll locally 1.0.0 ,I found a work around. in the tf.split line split out the whole thing. tf.split model input 4 6 axis 1 0 ekelsen this appears to be a bug. Fix in progress. To be clear the line model split tf.split model input 4 axis 1 0 shouldn t work it would need to be model split tf.split model input 10 axis 1 0 . You will now get an error either when the op is created if the dimensions are known or at runtime if they are not. You can specify an unknown value with 1 but you can t specify N 1 sizes if you expect N outputs. This got fixed by the referenced commit. 
8850,Error in running text classification character rnn.py,I am running the exact example given in the repo for text classification using rnn. I am getting the following error. TypeError Tensors in list passed to values of ConcatV2 Op have types int32 float32 that don t all match. Example https github.com tensorflow tensorflow blob master tensorflow examples learn text classification character rnn.py Would you please let me know how can I fix it. ,Please provide details about what platform you are using operating system architecture . Also include your TensorFlow version. We ask for this in the issue submission template because it is really difficult to help without that information. Thanks Thank you for the response. Following are the information you asked for Operating system GNU Linux Ubuntu 14.04.5 LTS Architecture x86 64 NVIDIA Driver Version 375.39 GeForce GTX 980 Cuda V8.0.61 tensorflow gpu 1.0.1 Thanks Can you see if there is a second python back trace that indicates where the op was created it should have such a label . This is the runtime backtrace which is not useful for finding where in the original source the problem arose. Thanks I can reproduce this. A simple fix but not complete one is here https github.com aselle tensorflow commit 163fd843b787e43f085383eca91dc8dd68d78809 After applying it it works fine for me basically just need to make the byte list float32 type instead of int32 type . There are several uses of deprecated functions in that example. Not sure if nealwu is planning to fix this as part of moving examples out of TensorFlow or if martinwicke has a better suggestion. I don t have any immediate plans to move this. It may make sense to do so at some point in the future though. Thanks after fixing sample code works with warning. martinwicke who would be best to fix these examples so they don t use the wrong apis. This is in the main tensorflow repo not even models so it would be good it if were canonical. jamieas this is an old character RNN demo would you be up for adapting it to use your RNN classes Hi Martin. Rui from my team is going to take a look at this issue. It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. Ought to be fixed. Please reopen if not. 
9012,BiasGradOp mistakenly put on CPU,NOTE Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow. You must complete this information or else your issue will be closed Have I written custom code as opposed to using a stock example script provided in TensorFlow custom code TensorFlow installed from source or binary from binary TensorFlow version 1.0.1 Bazel version if compiling from source N A CUDA cuDNN version CUDA 8.0 cuDNN v5.1 GPU Model and Memory GTX 1080 8GB Exact command to reproduce Here is a sample script in Python that can reproduce the problem Describe the problem clearly I am on Ubuntu 16.04. While running the above script despite the device has been specified to be GPU tensorflow still try to do the BiasGradOp on CPU and will cause an error because of the data format. If I change the implementation of lrelu to return tf.maximum leak x x then the problem goes away. Source Code Logs Here is the output from console ,This does indeed seem like a bug thanks for the nice reproducible example it really helps. Will dig into it some. asimshankar hey asim are you still working on this It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. Nagging Assigneee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. A member of the TensorFlow organization has replied after the stat awaiting tensorflower label was applied. I m very sorry for the delayed response. I believe this is fixed in TensorFlow 1.4.0 the same code snippet doesn t not error out . Please re open if I m mistaken. Once again apologies for the delay. 
9103,BUG tensorflow.placeholder shape does not serialize with protobuf, Profobuf serialization json attr dtype type DT FLOAT shape shape name x op Placeholder Tensorflow code x tf.placeholder tf.float32 shape None name x ,Could you elaborate on the bug here There is a known issue with the Placeholder op https github.com tensorflow tensorflow blob r1.1 tensorflow core ops array ops.cc L2751 where it cannot distinguish between an unknown and a scalar shape but it does serialize all other shapes correctly. There is some work underway to figure out if that bug can be fixed without requiring the PlaceholderV2 operation but all other shapes should be fine regardless. Could you elaborate on your concern here Sorry I copied the wrong line from Python as I was testing. When a placeholder of shape None 784 was serialized the corresponding element in the profobuf json serialization does not contain a shape attribute. This is the python code Select elements from the output json file Let me know if this is because of what you said earlier. Im using tensorflow gpu installed from pip3 on windows. asimshankar do you know if there s an issue tracking that work karpkarp Thanks for the sample code. It seems that if any of the dimensions are unknown is when we end up with an empty shape in the GraphDef which is broader than the problem PlaceholderV2 is going to address. I ll dig in a bit more. CC vrv Actually I m trying to change Placeholder itself so no new V2 is needed but this is precisely correct. We currently lose shape information when you serialize and deserialize partially known placeholder shapes. This is fixed in V2 which I am trying to backport to v1. It seems in array ops.py it sets a requirement for the shape to be fully defined with shape.is fully defined in the placeholder function. Any particular reason for this Does this mean that a placeholder of shape of None SomeNum will not be enforced In any case I removed the condition where the Placeholder shape has to be fully defined and the serialization issues are fixed. This does break placeholders with no defined shape so I added two additional function in python framework tensor shape.py python framework tensor shape python ops array ops.py JSON versions producer 21 node op Placeholder name x attr shape shape dim size 1 size 784 dtype type DT FLOAT op Placeholder name y attr shape shape dim size 1 size 10 dtype type DT FLOAT op NoOp name init 
9234,Segfaults NaN s in SVD,I m getting failures trying to run SVD on a particular matrix. The result is either all NaN s for u matrix or it s segfaults like below. To reproduce run this script in Python3 https github.com yaroslavvb stuff blob master svd test.py https github.com yaroslavvb stuff blob master svd test.py I can t see anything special about this matrix beside the fact that it s badly conditioned. IE I can perform SVD on this matrix in Mathematica fine https www.wolframcloud.com objects f16d71a7 cc47 4a3d b686 da440670eed3 rmlarsen , rmlarsen Do you want to take a look or triage further I m also experiencing this issue though only segfaults. np.linalg.svd works on the same matrices but calling with tf.py func is of course much slower than the native tf function. strubell I ve had good luck with SVD through numpy. Because SVD is O n 3 operation the O n 2 overhead of copying things between scipy TF becomes negligible for 1000x1000 mats or larger. Scipy mkl version of SVD is considerably faster than TF version due to multi threading and the few crashes I ve seen with scipy can be worked around by setting MKL NUM THREADS to lower number ie 15 You can do something like SvdWrapper https github.com yaroslavvb stuff blob dcc5e8d63ee0aaa7052d061e4fd155aabd2dac7f whitening util.py L816 to easily switch between TensorFlow and scipy versions of SVD example usage is in kfac example https gist.github.com yaroslavvb 3894d69491eb8db444fc6698d89cb48e rmlarsen maybe long term solution to speed correctness is to make gesdd available in TensorFlow Either new implementation or MKL version gesdd https software.intel.com en us mkl developer reference fortran gesdd language en . There s also gesvd https software.intel.com en us mkl developer reference c gesvd which may be more robust but also takes 3x longer I just reran the test in latest TensorFlow and it works if you use GPU. On CPU I still get NaN s there should be no NaNs yaroslavvb Cool Latest as in head or release 1.3 head it looks like GPU op was added recently strubell yaroslavvb FYI if you use py func on a CPU tensor there s no overhead to copy it back and forth between tf and numpy in most cases we only copy now if the tensor is not in column order and since numpy doesn t hold the gil during SVD you shouldn t see too much python overhead. alextp we no longer need to do numpy tricks to force alignment You only need to force alignment when going from numpy to tensorflow not the other way around. I assume when you take the SVD you re not sending the full U S and V matrices back but something smaller. On Tue Oct 10 2017 at 8 00 AM Yaroslav Bulatov notifications github.com wrote alextp https github.com alextp we no longer need to do numpy tricks to force alignment You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 9234 issuecomment 335499616 or mute the thread https github.com notifications unsubscribe auth AAATxV72ppi3AVEF pp7HIRv9c4iQs3mks5sq4Z4gaJpZM4M R0j . Alex BTW running this example under debug mode triggers a debug assert inside Eigen python external eigen archive Eigen src Core DenseCoeffsBase.h 180 Eigen DenseCoeffsBase Derived 0 CoeffReturnType Eigen DenseCoeffsBase Derived 0 operator Eigen Index const with Derived Eigen Ref Eigen Array long int 1 1 Eigen DenseCoeffsBase Derived 0 CoeffReturnType const long int Eigen Index long int Assertion index 0 index size failed. index is 1 full strack trace rmlarsen any update Just tried it out on with latest MKL compiled TensorFlow on Amazon instance and crash was not there https github.com yaroslavvb stuff blob master linalg benchmark launch tensorflow svd crash.py However I suspect that takes a different code path MKL Nagging Assignee rmlarsen It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. I started having this error tensorflow core kernels linalg svd op impl.h 110 Eigen BDCSVD failed with error code on Intel CPUs in 2.7 and in 2.9.1 as well I started having this error tensorflow core kernels linalg svd op impl.h 110 Eigen BDCSVD failed with error code on Intel CPUs in 2.7 and in 2.9.1 as well That would be a different issue. Perhaps this one 26842 
9312,Typo in seq2seq.attention wrapper.py,Hi I think there is a small typo in contrib.seq2seq.attention wrapper.py would someone like to check it code url https github.com tensorflow tensorflow blob master tensorflow contrib seq2seq python ops attention wrapper.py L471 I guess it should be probability fn rather than cell input fn to be checked. Thanks.,Yes could you send a PR to fix this CC ebrevdo yes please On Wed Apr 19 2017 at 9 57 AM drpngx notifications github.com wrote Yes could you send a PR to fix this CC ebrevdo https github.com ebrevdo You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 9312 issuecomment 295343911 or mute the thread https github.com notifications unsubscribe auth ABtim3R dvUJtttYzVdb zSrz70zJrTgks5rxjzugaJpZM4NBuTr . I have sent a PR please let me know if any questions. Merged. 
9424, minor bug tf1.0 compatibility script doesn t handle old batch matmul arguments,tf 1.0 merges batch matmul into matmul but the compatibility script forgot to rename the arguments for batch matmul what is needed is adj x transpose a adj y transpose b I would be happy to write a quick PR. References https www.tensorflow.org versions r0.12 api docs python https www.tensorflow.org api docs python tf matmul,Thanks for volunteering to improve TensorFlow. Please submit the PR and reference it here Apparently fixed in https github.com tensorflow tensorflow pull 9448. 
9439,small error in DynamicAttentionWrapper, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Not relevant OS Platform and Distribution e.g. Linux Ubuntu 16.04 Not relevant TensorFlow installed from source or binary Not relevant TensorFlow version use command below 1.1 Bazel version if compiling from source Not relevant CUDA cuDNN version Not relevant GPU model and memory Not relevant Exact command to reproduce Not relevant Describe the problem line 465 in tensorflow tensorflow contrib seq2seq python ops dynamic attention wrapper.py says if not callable cell input fn . I think it should say if not callable probability fn Source code logs None,The attention wrapper has been rewritten in master branch there is no dynamic attention wrapper.py any more. Anyway I think this typo was fixed in commit https github.com tensorflow tensorflow commit 0557e8f90c1a2f027f4561c70558c6c836138058. You can update your TensorFlow and check it. vrenkens thanks for submitting this but it looks like it is already fixed. Closing for now. Let us know if upgrading doesn t resolve your issues. 
9505,Check failed NDIMS dims 2 vs. 1 when I build a svm model,when I build a svm model with tf.learn it get error like this F tensorflow core framework tensor shape.cc 36 Check failed NDIMS dims 2 vs. 1 Asking for tensor of 2 dimensions from a tensor of 1 dimensions I have ask a question in stackoverflow It could be a issue according the reply http stackoverflow.com questions 43638488 check failed ndims dims 2 vs 1 when i build a svm model all the reproduce code here and the all error output text ,This question is better asked on StackOverflow http stackoverflow.com questions tagged tensorflow since it is not a bug or feature request. There is also a larger community that reads questions there. Thanks This seems like a bug to me though. burness Does a similar setup run successfully for other canned estimators This is definitely a bug either with one of the FeatureColumn processing ops or with the way the SVM optimizer is using them. I didn t trace it through completely with GDB to figure out what s wrong exactly probably equivalent effort to fixing the bug but the fact that this is required is indicative even if there s something wrong with the usage we need to do better than a CHECK failure. terrytangyuan LinearClassifier seems all right I will try other estimators later I can reproduce at head with a single real valued column Apparently the error is indicating that only vectors are supported for real valued columns. A similar CHECK failure pops up if the column is a matrix tf.zero 4 1 1 . So you can get your code working by adding a dummy second dimension to your real valued columns. We however need to work on our argument checking. Thank you for the report Hi petrosmol Could you take a look at this issue It looks like LinearClassifier accepts scalar real valued columns i.e. input with just a batch dimension . Do we just want to add a reshape here to keep behavior consistent Thanks for reporting I am looking into it. Will update this thread shortly. Update The problem is not with the estimator per se rather it is with the underlying optimizer SDCA which accepts only matrices 2 dimensional tensors for dense features. I am working on a fix. In the meantime a simple fix so that you are not blocked is to reshape your real valued columns so that they always have rank one. For instance in your input fn you can replace continuous cols k tf.constant df k .values for k in CONTINUOUS COLUMNS with continuous cols k tf.expand dims tf.constant df k .values 1 for k in CONTINUOUS COLUMNS petrosmol Thanks It seems ok now. And In a randomforest test case as follow It will get the error and I replace continuous cols k tf.constant df k .values for k in CONTINUOUS COLUMNS with continuous cols k tf.expand dims tf.constant df k .values 1 for k in CONTINUOUS COLUMNS It seems ok now It may need to enhance this reshape problem with the process of input fn It looks like thomascolthurst has added a reshape to TensorForest input which I assume just hasn t made it into a TensorFlow release yet. FYI I have submitted a change that should fix the problem for SVM and any other estimator that uses SDCA as an optimizer . You should be able to provide rank 1 tensors for 1 dimensional real valued columns. Let me know if you encounter any other problems petrosmol Thanks 
9550,ctc greedy decoder inconsistent with ctc beam search decoder,The following extract from the the ctc beam search decoder documentation seems to be misleading The ctc greedy decoder is a special case of the ctc beam search decoder with top paths 1 and beam width 1 but that decoder is faster for this special case . Instead the following results can be observed Decoding AA ctc blank AA using merge repeated True merge repeated False tf.nn.ctc beam search decoder top paths 1 beam width 1 A AA tf.nn.ctc greedy decoder AA AAAA To reproduce This is confusing and probably not intended. How to solve this Adapt the documentation or Adapt the ctc beam search decoder implementation to that of ctc greedy decoder or vice versa. Both directions would cover my use case AA this decision would depend on which of the other behaviors A or AAAA is needed and which of them could be dropped. System information OSX 12.4 TensorFlow 1.1.0 CPU from binary,I stumbled upon this today too. Why are the results not fully equivalent when ctc beam search decoder uses top paths 1 and beam size 1 Ping ebrevdo any comments I know there are a lot of issues assigned to you so could you reassign this one who may have time to investigate this Also encounter this problem as well. And test with this data set Logits 200.0 1.0 100.0 200.0 200.0 1.0 Which should give AA The greedy decoder is correct but the beam search decoder only give A And the beam search deocder work on this dataset log 0.4 log 0.6 log 0.4 log 0.6 Greedy decoder gives an empty output where the beam search decoder gives the true output A . Which is known would fail in best path greedy decoding. But they both fail on this dataset log 0.4 log 0.6 log 0.4 log 0.6 200.0 200.0 200.0 200.0 Where the true output is AA but both decoders give A . But I guess they fail in the different place beam search decoder seems unable to give the correct answer for AbA type path where greedy decoder failed in the first two logits. To reproduce System information Ubuntu 14.04 Tensorflow 1.3.0 CPU from pip install. same issue and look forward to solving that any updates having the same issue as well. I spent a good fraction of the day debugging this before coming to a somewhat obvious realization. If you read the Graves paper CTC beam decoding implicitly collapses repeated characters as part of calculating the optimal path i.e. AAA will contribute probability mass through the path A . So the correct CTC decoding behavior occurs when merge repeated False . In this case it DOES merge repeated characters. The merge repeated flag when true will merge repeated characters after characters have already been merged blank symbols removed. This parameter should simply be removed. As it stands now the behavior is extremely misleading. I m happy to provide a PR for this change if it s something a tensorflower would be amenable with. cc ebrevdo gunan Let s use the example from above AA ctc blank AA Expected behavior when merge repeated False AA ctc blank AA should give AAAA when merge repeated True AA ctc blank AA should give AA Only tf.nn.ctc greedy decoder is working as expected but tf.nn.ctc beam search decoder top paths 1 beam width 1 is not. ryanleary are you saying tf.nn.ctc beam search decoder top paths 1 beam width 1 merged symbols twice I think I might be able to help out in amending the decoder. I agree that merge repeated True should give AA is the correct behavior. What I m saying is that having a case where merge repeated False in a beam decoder doesn t make a whole lot of sense. Further the merging is somewhat fundamental to the way the algorithm handles summing probabilities across multiple paths. In the current implementation of the beam decoder if merge repeated False repeated characters are merged once. If merge repeated True they are merged again returning nonsensical results. I m advocating for removing the flag and the only behavior to be where repeated characters are merged but equivalent to the merge repeated False current functionality . following up on PR Since this is a backwards breaking API change I ll remove the argument for TF 2.0. Thank you all for the report detailed analysis. Nagging Assignee ebrevdo It has been 29 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Fixed by 21187. 
9560,tf.pow x y will freeze for negative integer y, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 and macOS 10.12.4 TensorFlow installed from source or binary binary TensorFlow version use command below 1.1.0 on both operation systems Bazel version if compiling from source not compiled from source CUDA cuDNN version CPU only on Windows while CUDA 8.0 on macOS GPU model and memory Nvidia Titan X 12GB Exact command to reproduce a short piece of Python code Python version 3.5.2 Anaconda on Windows 3.6.0 Anaconda on macOS Describe the problem tf.pow x y will freeze for negative integer y and of course integer x . It will not freeze for negative floating number y. Source code logs ,I ve confirmed and reproduced this thanks for reporting. I think this issue is related to issue 12156. Also related to PR 11852. I ve looked in the source code and the origin of this bug is in Eigen specifically src Core MathFunctions.h. They specialize their template for pow to the case of both parameters integers and provide an implementation which only works for positive y. The problem is the well known bug loop of shifting y to the left if y 0 then the bit sign extends and it continues indefinitely. Here is the code from Eigen However it would be difficult to fix this the template specifically requests that the return type is Scalar and not Real and changing this in Eigen might break things for other people. What I could do would be to change pow in cwise ops.h in tensorflow core to fix this bug. But then again tf.pow would start returning float for integers inputs. Is this acceptable I don t see any way to fix the bug without accepting that tf.pow should also return float for integer inputs. If you agree with this change I can fix the issue. It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. Nagging Assigneee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. PR 15607 is pending review. There are discussions about fixing it in Eigen vs. TensorFlow. Will take a look at Eigen. Nagging Assignee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. korepwx I think the issue is addressed in PR 15607 and this issues could be closed. 
9633,SIGSEGV with sparse add and broadcasting, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes enclosed below OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04 TensorFlow installed from source or binary binary via pip TensorFlow version use command below v1.0.0 65 g4763edf dirty 1.0.1 Bazel version if compiling from source N A using pip installation CUDA cuDNN version N A CPU only GPU model and memory none Exact command to reproduce Describe the problem Running the code above results in For lower values of nnz nnz 1 it finishes fine quite often. Source code logs See above., concretevitamin can you take a look at this Here s the stacktrace Did you modify anything else in the code The version shows dirty . yifeif av8ramit are the pip installs built from a dirty git repo I m taking a look. The results look the same with tf pip upgraded to v1.1.0 rc0 61 g1ec6ed5 1.1.0 . Thanks for the report tpet. I am submitting a fix that instead of segfaulting return a proper error status that requires both operands have matching shapes. This has always been the assumption in the SparseTensorDenseAddOp but was not enforced https github.com tensorflow tensorflow blob master tensorflow core kernels sparse tensor dense add op.cc L56 . The patch should show up in master within a day or two. Do you exactly require the functionality of sparse dense dense with dense to sparse broadcast If so I d like to mark this as contributions welcome the current kernels do not support this broadcast pattern . However if you can get away with sparse dense sparse with dense to sparse broadcast we already have sparse dense cwise add https github.com tensorflow tensorflow blob master tensorflow python ops sparse ops.py L316 that does this. Let us know and we can expose this function as a public method. concretevitamin I agree raising an exception is much better. The sparse dense dense with dense to sparse broadcast really seems not that much useful compared to sparse dense sparse with dense to sparse broadcast . Now I actually don t need this particular thing. My initial use case was a bit different. In the process of trying to get some reasonable behavior I happened to find the segfault and created this example. My use case is this D reduce sum a S where D and the result is dense 1 n2 n3 n4 S is sparse n1 n2 n3 n4 a is dense n1 1 1 1 and broadcasts to S. So far I hasn t been able to get to some reasonable performance with this. I m using a pip installation so it will take some time until it propagates down to me so feel free to close the issue if you think it is resolved. Thanks. Okay closing for now. For the reduce take a look at tf.sparse reduce sum and or tf.sparse reduce sum reduce . 
9708,tf.random crop exception after upgrading to tf1.1 from tf1.0,Please go to Stack Overflow for help and support http stackoverflow.com questions tagged tensorflow If you open a GitHub issue here is our policy 1. It must be a bug or a feature request. 2. The form below must be filled out. Here s why we have that policy TensorFlow developers respond to issues. We want to focus on work that benefits the whole community e.g. fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem rather than being redirected to Stack Overflow. System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.4 TensorFlow installed from source or binary binary TensorFlow version use command below v1.1.0 rc0 61 g1ec6ed5 1.1.0 Bazel version if compiling from source CUDA cuDNN version 8.0 5.1 GPU model and memory Tesla m40 12 gb Exact command to reproduce You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION Describe the problem The tf.random crop gives exception even when it receives input of valid size. assertion failed Need value.shape size got 224 224 3 224 224 3 Source code logs File .. foo.py line 1356 in pp augment aug tf.random crop aug crop size 0 crop size 1 aug dim3 File usr local anaconda3 envs tf1.1 lib python3.5 site packages tensorflow python ops random ops.py line 303 in random crop Need value.shape size got shape size File usr local anaconda3 envs tf1.1 lib python3.5 site packages tensorflow python ops control flow ops.py line 121 in Assert condition data summarize name Assert File usr local anaconda3 envs tf1.1 lib python3.5 site packages tensorflow python ops gen logging ops.py line 39 in assert summarize summarize name name File usr local anaconda3 envs tf1.1 lib python3.5 site packages tensorflow python framework op def library.py line 768 in apply op op def op def File usr local anaconda3 envs tf1.1 lib python3.5 site packages tensorflow python framework ops.py line 2336 in create op original op self. default original op op def op def File usr local anaconda3 envs tf1.1 lib python3.5 site packages tensorflow python framework ops.py line 1228 in init self. traceback extract stack InvalidArgumentError see above for traceback assertion failed Need value.shape size got 224 224 3 224 224 3 Node image filters train tower 0 random crop 1 Assert Assert Assert T DT STRING DT INT32 DT INT32 summarize 3 device job localhost replica 0 task 0 cpu 0 image filters train tower 0 random crop 1 All 29 image filters train tower 0 random crop 1 Assert Assert data 0 image filters train tower 0 random crop 1 Shape 31 image filters train tower 0 random crop 1 size 33 Node image filters train tower 0 DecodeRaw 1 93 Recv client terminated false recv device job localhost replica 0 task 0 gpu 0 send device job localhost replica 0 task 0 cpu 0 send device incarnation 1 tensor name edge 232 image filters train tower 0 DecodeRaw 1 tensor type DT UINT8 device job localhost replica 0 task 0 gpu 0 , girving the error message does look strange. Looking now. The code looks fine so far which is confusing. Ah the problem is that Assert truncates its arguments at 3 entries by default. The shapes are actually bigger and presumably shape size someone in the invisible bit. I ll fix the error message. saurabh203 Can you confirm that your tensors have rank 3 girving The tensors have the rank 3 only. But the issue seems like an OS related issue. The earlier run was erroneously reported as Ubuntu but was SLES 12. My apologies for the mistake. The code seems to run fine on the ubuntu machine that I tested today. I do have to make it work on the SLES too. Is there any other reason apart from Assert truncation which might cause this issue saurabh203 No idea why it would fail on SLES 12 only. Please let us know if you figure out the cause but I don t think we ll be able to debug it on our end without more information. 
9747, Tensor object has no attribute initializer after import from meta graph, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution Darwin Austins MBP 16.5.0 Darwin Kernel Version 16.5.0 Fri Mar 3 16 52 33 PST 2017 root xnu 3789.51.2 3 RELEASE X86 64 x86 64 Mac OS X 10.12.4 TensorFlow installed from source or binary binary TensorFlow version use command below v1.1.0 rc0 61 g1ec6ed5 1.1.0 Bazel version if compiling from source 0.4.5 CUDA cuDNN version None GPU model and memory None Exact command to reproduce Ref to Codes tensorflow import tf.VERSION 1.1.0 tf.GIT VERSION v1.1.0 rc0 61 g1ec6ed5 tf.COMPILER VERSION v1.1.0 rc0 61 g1ec6ed5 Sanity check array 1 dtype int32 Describe the problem After export and import a meta graph with uninitialized local variables You can not inittialize them with sess.run tf. local variables initializer cause TF do not register variable s proto function with key LOCAL VARIABLES and when export meta graph to protobuf source code can not find to proto function from repository. Source code logs As it show above in origin graph local variable collection is a list of tf.Variable but in the new graph is a list of tf.Tensor Work around Add following registration in your model core OR Ref to this PR https github.com tensorflow tensorflow pull 9674 , tf.local varliabe initializer might not be the right spelling. josh11b Looks like metagraph import isn t creating Variable objects. Any ideas why I m just facing the same problem. To be more precise I m using tf.metrics.accuracy https www.tensorflow.org api docs python tf metrics accuracy in my training script which uses local count variables internally. After saving and restoring the graph using tf.train.import meta graph https www.tensorflow.org api docs python tf train import meta graph I m not able so reuse the accuracy metric because the required call to tf.initialize local variables https www.tensorflow.org versions master api docs python tf initialize local variables fails as desribed austinzh. is there already any progress on this EDIT OS Ubuntu 16.04 TF Version r1.1 native pip Python 2.7 CPU only Same case for me. I m using tf.metrics.accuracy in my training process. When I try to restore the saved model and use the function accuracy again it fails Traceback most recent call last File usr local lib python3.4 dist packages tensorflow python client session.py line 1022 in do call return fn args File usr local lib python3.4 dist packages tensorflow python client session.py line 1004 in run fn status run metadata File usr lib python3.4 contextlib.py line 66 in exit next self.gen File usr local lib python3.4 dist packages tensorflow python framework errors impl.py line 466 in raise exception on not ok status pywrap tensorflow.TF GetCode status tensorflow.python.framework.errors impl.FailedPreconditionError Attempting to use uninitialized value accuracy count 1 Node accuracy AssignAdd 3 AssignAdd T DT FLOAT class loc accuracy count 1 use locking false device job localhost replica 0 task 0 cpu 0 accuracy count 1 accuracy ToFloat 2 During handling of the above exception another exception occurred Traceback most recent call last File main.py line 70 in module main File main.py line 46 in main data 0 input data.reshape 200 10 16 label aux 0 output data keep prob 0 config KEEP PROB File usr local lib python3.4 dist packages tensorflow python client session.py line 767 in run run metadata ptr File usr local lib python3.4 dist packages tensorflow python client session.py line 965 in run feed dict string options run metadata File usr local lib python3.4 dist packages tensorflow python client session.py line 1015 in do run target list options run metadata File usr local lib python3.4 dist packages tensorflow python client session.py line 1035 in do call raise type e node def op message tensorflow.python.framework.errors impl.FailedPreconditionError Attempting to use uninitialized value accuracy count 1 Node accuracy AssignAdd 3 AssignAdd T DT FLOAT class loc accuracy count 1 use locking false device job localhost replica 0 task 0 cpu 0 accuracy count 1 accuracy ToFloat 2 Caused by op accuracy AssignAdd 3 defined at File main.py line 70 in module main File main.py line 43 in main accuracy metrics update accuracy metrics tf.metrics.accuracy label operation one hot File usr local lib python3.4 dist packages tensorflow python ops metrics impl.py line 332 in accuracy updates collections name or accuracy File usr local lib python3.4 dist packages tensorflow python ops metrics impl.py line 267 in mean update count op state ops.assign add count num values File usr local lib python3.4 dist packages tensorflow python ops gen state ops.py line 75 in assign add use locking use locking name name File usr local lib python3.4 dist packages tensorflow python framework op def library.py line 763 in apply op op def op def File usr local lib python3.4 dist packages tensorflow python framework ops.py line 2327 in create op original op self. default original op op def op def File usr local lib python3.4 dist packages tensorflow python framework ops.py line 1226 in init self. traceback extract stack FailedPreconditionError see above for traceback Attempting to use uninitialized value accuracy count 1 Node accuracy AssignAdd 3 AssignAdd T DT FLOAT class loc accuracy count 1 use locking false device job localhost replica 0 task 0 cpu 0 accuracy count 1 accuracy ToFloat 2 sherrym Any ideas bsautermeister any solution for this problem I face the same problem as yours. AlbertXiebnu Unfortunately no. But I have not check if this has been fixed in TF 1.2. I try to check that in the next days... bsautermeister I have tried in TF1.2 seems that still not work. Same issue here for me it seems to be related to https github.com tensorflow tensorflow issues 1045. When I set num epochs to a value other than None in the file where I save the model I will get the Tensor object has no attribute initializer error in the file where I restore the model. TF 1.2 Python 3.6.1 Same issue here while restoring a graph and using string input producer to evaluate model first I get Attempting to use uninitialized value input producer limit epochs epochs Node input producer limit epochs CountUpTo CountUpTo T DT INT64 class loc input producer limit epochs epochs limit 1 device job localhost replica 0 task 0 cpu 0 input producer limit epochs epochs If to fix above issue I run tf.local variable intializer then I get Tensor object has no attribute initializer error on TF 1.2 Python 3.6.1 I did manage to find a work around upon restoring i removed queue runners and local variables from restored graph and it worked get default graph .clear collection queue runners get default graph .clear collection local variables gauravsindhwani can you still run the code successfully after you remove these two collections since they are actually part of the graph. I try your way but find the program is stuck after restoring. gauravsindhwani this approach seams working ok for me. I use streaming accuracy and it seems create many local variable accuracy which are no longer used. So remove local variables seems to be a good way to hack it. It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. Nagging Awaiting TensorFlower It has been 14 days with no activityand the awaiting tensorflower label was assigned. Please update the label and or status accordingly. A member of the TensorFlow organization has replied after the stat awaiting tensorflower label was applied. suharshs has been working on MetaGraphDef related stuff and might have some ideas. Nagging Awaiting TensorFlower It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. Nagging Awaiting TensorFlower It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. It works well on TF 1.4.1 it seems this issue have been fixed by https github.com tensorflow tensorflow commit 2d1823f7be0d65d4ee28b7c6ea2acaac01f1db61 . Thanks. Nagging Awaiting TensorFlower It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. 
9797,Bijector caching breaks when used with TransformedDistribution,Currently the caching that Bijector https github.com tensorflow tensorflow blob 7fa0cf39f854d5fdaaa19ad6425dfed02f5fea64 tensorflow python ops distributions bijector impl.py L114 objects do to avoid unnecessary calculations does not work when the bijector is used in a TransformedDistribution https github.com tensorflow tensorflow blob 7fa0cf39f854d5fdaaa19ad6425dfed02f5fea64 tensorflow python ops distributions transformed distribution.py L124 object. I believe the culprit is the reshaping that the distribution object does here https github.com tensorflow tensorflow blob 7fa0cf39f854d5fdaaa19ad6425dfed02f5fea64 tensorflow python ops distributions distribution.py L640 when we call Bijector.inverse on the output the Bijector object cannot tell that this is merely a reshaped version of what it calculated previously. My use case is to sample from a TransformedDistribution and then later calculate the log probability of that sample. This issue is particularly a problem when using a bijector whose inverse is numerically delicate in my case I m chaining together softplus bijectors and my own custom affine bijector . I m willing to work on a fix for this problem but I m not sure what the best way to do it is adding caching to the TransformedDistribution code might work but that seems like code duplication . System information OS Platform and Distribution Ubuntu 16.04 TensorFlow installed from Source TensorFlow version v1.1.0 rc2 773 g7fa0cf3 commit 7fa0cf39f854d5fdaaa19ad6425dfed02f5fea64 Bazel version 0.4.5, ebrevdo Can you comment Waiting for response as well deep bijections work slow for me My apologies for the slow response it looks like this issue was inadvertantly misrouted. Caching was fixed on Sept 29 and should be working in 1.4.0 rc0. https github.com tensorflow tensorflow commit 46cf6262476b1d058e43acacc2c15097cc7bbf5a If youre still seeing a problem please let me know. Closing as fixed. 
9855,Unexpected error at contrib.seq2seq s BeamSearchDecoder, System information OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOS TensorFlow installed from source or binary Source TensorFlow version use command below v1.1.0 rc2 773 g7fa0cf39f Bazel version if compiling from source 0.4.5 CUDA cuDNN version None GPU model and memory CPU Describe the problem I encountered an unexpected error at tf.contrib.seq2seq s BeamSearchDecoder when the beam width is smaller than number of vocabs . This is a part of beam search step operation in beam search decoder.py Since the shape of scores is batch size beam width vocab size the shape of scores flat is batch size vocab size at time step 0. However if k shape 1 in nn.top k operation it just throws InvalidArgumentError input must have at least k columns . Thus the code just throws an error and dies. I think code should be modified to handle cases where vocab size n beam width or at least throw an appropriate error message when the input is vocab size beam width . ,Nice catch Seems like the top k operation should be shielded with a min . Would be up for submitting a small PR OK I fixed the issue and made PR 9875 
9858,Function decode raw ignoring parameter little endian., little endian parameter in decode raw takes no effect. I had a look at the C source code tensorflow tensorflow core kernels decode raw op.cc you are just using reinterpret cast without any awareness of this parameter.,Would you be willing to send a PR I hope PR 9876 could be good. It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. Fixed in 9876. 
9902,Session run method s feed dict argument implicitly converts types, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 OSX 10.12.3 TensorFlow installed from source or binary binary TensorFlow version use command below 1.1.0 Bazel version if compiling from source NA CUDA cuDNN version None GPU model and memory None Exact command to reproduce see below Describe the problem Hi TensorFlow humans Thanks so much for making TensorFlow Right now if you feed a floating point array into a integral placeholder type it will be converted implicitly. To my knowledge most python operations will not implicitly convert. The implicit conversion potentially creates convenience but of course it also creates the opportunity for a hard to see bug. In my case I lost about 1 day to find this bug and experienced great sadness. That probably says more about me than it does about TF. Still my feeling is that it is a more sensible default to require the user to do the conversion explicitly. Alternatively perhaps it would be logical to log a warning to the user. Note that to my knowledge TF operations like tf.equal require both tensors to have the same type. So users might have a belief that that TF and Session run require them to be fairly explicit about types. If it would be likely to be accepted I am happy to write a patch for TF that warns the user when they feed a tensor of the wrong type. Thanks for reading this issue Source code logs Output ,I would say this is a bug silently turning floats into ints is surprising There was a similar one fixed by suharshs where he added some code here https github.com tensorflow tensorflow blob a5b1fb8e56ceda0ee2794ee05f5a7642157875c5 tensorflow python framework tensor util.py L391 to prevent python integers 2 31 being silently downcasted to int32. martinwicke is changing our implicit conversion rules in placeholders breaking our API guarantees I agree this is confusing and dangerous but in theory some may rely on this behavior... yaroslavvb the fix was actually by saxenasaurabh It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. Yeah this is likely to break people. We should add a warning if we can and we will change this behavior in 2.0. Nagging Assignee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. I will close this since we cannot do anything before 2.0. I noted this issue for 2.0. 
9931,Go SIGSEGV when using int32 instead of int64 and missing error in Resize functions, Problem In Go some operation causes a SIGSEGV when using an int32 instead of an int64 and I have reasons to believe that the same will happen when using float instead of double and vice versa . The Resize operations don t define the output shape correctly when the input is not a batch they just let the dimensions undefined instead of raising some errors. The tests below are commented so I hope that s enough to let you understand what the problems are. Source code logs System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Archlinux TensorFlow installed from source or binary source TensorFlow version use command below 1.1.0 rc2 Bazel version if compiling from source 0.4.5 CUDA cuDNN version cuda 8 cudnn 5.1 GPU model and memory GeForce GTX 1080 Exact command to reproduce go test ,Thanks for the report galeone there are multiple things going on here. 1. There was a bug in the underlying C API where it was suppressing errors during graph construction. That is fixed at head in 7d785f1e18af9d22d940f18aac6e8c9ffd268b22 so it will be available with the 1.2 release of the TensorFlow C API 2. The Scope type follows the builder pattern for graph construction. So while op.ResizeNearestNeighbor Scope ... https godoc.org github.com tensorflow tensorflow tensorflow go op ResizeNearestNeighbor doesn t return an error errors are collected in Scope.Err https godoc.org github.com tensorflow tensorflow tensorflow go op Scope.Err . This allows for more compact graph definitions see the package example for the op package https godoc.org github.com tensorflow tensorflow tensorflow go op ex package at the cost of forgetting to check the error. Setting aside comments on the merits of this design before using any of the returned tf.Outputs from the functions in the op package one is encouraged to check the error. So in line 30 of the snippet above for example I d suggest 3. Regardless the Go API should never end up with segfaults from the underlying C API in this case providing an invalid TF Operation pointer . So I m going to send a fix for that. Long story short A couple of fixes will ensure error messages panics that are more useful than the cryptic segfaults. Additionally it s good practice to check the error on the Scope object. Hope that helps will update this issue with the fix mentioned above . Comments thoughts welcome. Thank you for the suggestion on how to properly check for errors Is somewhere documented the Scope follows the builder pattern I guess it would be nice to put this in Scope s go doc. About the point 3 what is the problem Why it segfaults That op has a kernel registered to handle both types or the fix is to register the missing type galeone I m probably misusing the phrase builder pattern but yeah it does want you to check for errors. If you d like to contribute or suggest some changes to scope.go or other files to help improve documentation we d be more than happy to look at them. Regarding point 3 It isn t that the kernel isn t registered it s that the operation requires that the size argument be an int32 Tensor see godoc https godoc.org github.com tensorflow tensorflow tensorflow go op ResizeNearestNeighbor . Between 7d785f1e18af9d22d940f18aac6e8c9ffd268b22 and a fix I m about to make on the Go side you shouldn t see any segfaults in the C API but you may see a nil pointer dereference in Go because Scope.AddOperation will return nil and an error after the fix . Hope that helps the fix should be in sometime tomorrow I think I ll be happy to contribute In the next days as soon as I have time I m going to give a better look. However since every size argument of every function requires an int32 why the .Shape method returns a int64 I mean it seems not coherent. The documentation talks about int32 almost everywhere and this int64 just makes operations like the creation of a new tensor with a predefined shape like the noise in my previous example a problem because of the required casts. Isn t just better to use an int32 everywhere and thus avoid the problem I m pretty sure that no one will create a tensor with a dimension 2 32 1 I believe the use of int32 in many operations predates efforts to accept int64 as well see for example 91ce95d497ec2957535b2ce6a965cd8269d723e5 . So I think some of these ops need to be updated to accept int64 as well. In general having Tensors that reach 2 32 1 dimensions isn t unheard of especially when reshaping large batches of multi dimensional tensors. Alright thus instead of using int32 everywhere I suggest using int64 everywhere in order to maintain consistency. Or if not everywhere at leat use the same type for attributes that works togeather i.e. shape and dimensions as input and or output parameters should be both int64 . This issue should be resolved at head and with C library compiled from head so I m going to close this out. Contributions for making more operations accept int64 for shape similar to https github.com tensorflow tensorflow commit 91ce95d497ec2957535b2ce6a965cd8269d723e5 are welcome. galeone Hope that helps. Feel free to open a new issue if you run into more trouble. Thanks 
10403,contrib learn python learn io data feeder.py y is dict instead of x is dict at line 325,version r1.2 I was trying to create a regressor with X as dict and y as ndarray. Got error that at line mentioned in title saying ndarray object as no attribute items. When I checked the code it look like This x is dict in the last line should be y is dict Or do I have to send both as dict ,This question is better asked on StackOverflow http stackoverflow.com questions tagged tensorflow since it is not a bug or feature request. There is also a larger community that reads questions there. Thanks Our apologies. I think navya xx is correct that the x is dict on the last line should be a y is dict. No worries. I am glad that its been solved now.. Thanks.. I think this issue has been fixed in PR 12562 several weeks ago Yes thanks for reminding.. i will close this issue now.. Sorry.. I guess someone from admin has to close it.. I don t have the rights.. It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. Let me close this issue as it has been fixed. 
10641,bug BeamSearchDecoder should not assume that when time 0 beam will be full, code start from https github.com tensorflow tensorflow blob master tensorflow contrib seq2seq python ops beam search decoder.py L510 Correct me if I am wrong but I think this code is assuming that when time 0 the beam will be full. It is true when the vocabulary is big such as is the case in machine translation. but if the vocabulary is small the beam might won t be full when time 0 and might pose a problem. the value of next beam size in the code seems must be beam width or it will raise an error since next beam scores.set shape static batch size beam width which make next beam size math ops.minimum useless. I am trying to write a Pointer Network BeamSearch Decoder by modifying this source file. And the vocabulary is usually small so there is a possibility that when time 1 the beam won t be fully filled. I appreciate finally some one wrote a general BeamSeach decoder that will make my life easier. , ebrevdo could you please take a look it s beyond my expertise. This is indeed the assumption. Unfortunately it s unlikely to be fixed until after the 1.2 release and I m away this week. If you would like to send a PR to fix this by using the tf.shape on the input Tensor instead of assuming beam width and adding a unit test I can review when I return. On Jun 13 2017 10 28 AM Andrew Selle notifications github.com wrote ebrevdo https github.com ebrevdo could you please take a look it s beyond my expertise. You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 10641 issuecomment 308189704 or mute the thread https github.com notifications unsubscribe auth ABtim6kdt62nhk I7Ie5cDr45ccAy5MRks5sDsbEgaJpZM4N2l0D . ebrevdo It probably is not a good idea to push tensors with variant shape to TensorArray. Actually I think it s a good idea to just set log probs 1 weight width to negative infinity in initialize function. and of course set to a simple reshape I will add a test unit and test it if you think it s ok. It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. A member of the TensorFlow organization has replied after the stat awaiting tensorflower label was applied. Nagging Assignee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. 
10648,Segmentation Fault core dumped on exit from unit test that imports tf.contrib.rnn , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 14.04 TensorFlow installed from source or binary Source TensorFlow version use command below 1.0.0 Bazel version if compiling from source 0.4.4 CUDA cuDNN version 8.0 5110 GPU model and memory GeForce GTX 980 Ti Exact command to reproduce python m unittest discover s tests p example test.py see below for details Describe the problem I am running Tensorflow code from within the Python unittest module. The individual unit tests each of which train a specific architecture run successfully with an OK. However I observed that given the above described system configuration I receive a Segmentation Fault core dumped just before the program exits. And this leads to the overall test being marked as FAIL despite the individual tests passing . On further investigation I observed that the segmentation fault can be prevented from occurring by preventing the tensorflow contrib rnn python ops gru ops.so file from being loaded by the load op library function inside tensorflow.contrib.rnn.python.ops.gru ops Source code logs Say I have a folder my code that contains a tests folder where my unit tests are all located. The error can be reproduced by running an example test.py given below within the unittest module using the call python m unittest discover s tests p example test.py from within the my code directory. The file my code tests example tests.py contains the following Note that the error occurs only when both session is created and there is a reference to tf.contrib.rnn .,I m having the same problem my python code that uses TensorFlow will segfault when the python process terminates. I m also using TF 1.0.0 but haven t touched any of the source code. I am also seeing the problem ONLY when I use both tf.Session AND anything in tf.contrib.rnn. gunan do you have any ideas on this I am able to reproduce this. I was able to reproduce this in 1.0.1 but I found that in the nightly build it works fine so I d recommend upgrading to at least 1.2. I am using tensorflow gpu binary and could not reproduce this with v1.1.0 current PyPI version . Log is here https gist.github.com byronyi 54d30dd39f5420d883a1a26919e6ad7c . Yes I don t see the problem in v1.1 or higher. However our team is heavily using v1.0 and we won t be able to upgrade everything anytime soon.... For what it s worth I found a temporary fix I have a train method in which i have something like in which get model builds the model and fit trains the model The temporary fix I added was in train What s interesting is the session returned from tf.Session is never used anywhere because in fit I make a new session and use that session. Hopefully this helps anyone who s also stuck in v1.0 for the moment... This issue is automatically closed due to lack of activity. Please re open if this is still an issue for you. Thanks 
10673,Public head of master is failing windows CMAKE tests ,The following tests are failing for several of the public pull requests. I suggest that the current head of master would also fail in the same way. ,I ll revise that. Looking at the history of the Windows CMAKE tests I can see that this test has been failing on and off for quite a while didn t go back to the origin to find out who actually broke it yifeif could you take a look Flake was due to a new numpy version. Should be fixed with 10709 and gunan re downloaded numpy afterward. 
10675,Inner tf.device inherits device index when using wildcard index,TF version v1.2.0 rc0 735 gf48673b about one week ago workaround , mrry could you take a look please Yes this looks like a bug. There s no distinction in the DeviceSpec https github.com tensorflow tensorflow blob master tensorflow python framework device.py L24 class between and not present which are both represented using None for the device index property. The resulting behavior is not very intuitive and it would be nice to fix this but I m not sure how heavily is used in device strings and it s possible that fixing it could break existing users. asimshankar would you decide if this is worth fixing josh11b Would it make sense to consider this fix as part of the tf.device changes in 2.0 This is a stale issue. Please check the issue with latest TensorFlow. If the issue still persists in the newer version of TF please feel free to reopen it by providing details about the issue and a standalone code to reproduce the issue. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 10675 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 10675 No a 
10728,tensorflow.contrib.keras.python.keras.models throwing errors for a valid keras code, System information cat etc issue Linux parikshit XPS L322X 4.4.0 79 generic 100 Ubuntu SMP Wed May 17 19 58 14 UTC 2017 x86 64 x86 64 x86 64 GNU Linux VERSION 16.04.2 LTS Xenial Xerus VERSION ID 16.04 VERSION CODENAME xenial are we in docker No compiler c Ubuntu 5.4.0 6ubuntu1 16.04.4 5.4.0 20160609 Copyright C 2015 Free Software Foundation Inc. This is free software see the source for copying conditions. There is NO warranty not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. uname a Linux parikshit XPS L322X 4.4.0 79 generic 100 Ubuntu SMP Wed May 17 19 58 14 UTC 2017 x86 64 x86 64 x86 64 GNU Linux check pips numpy 1.13.0 protobuf 3.3.0 tensorflow 1.2.0rc1 check for virtualenv False tensorflow import tf.VERSION 1.2.0 rc1 tf.GIT VERSION v1.2.0 rc0 24 g94484aa tf.COMPILER VERSION v1.2.0 rc0 24 g94484aa Sanity check array 1 dtype int32 env LD LIBRARY PATH is unset DYLD LIBRARY PATH is unset nvidia smi . tf env collect.sh line 105 nvidia smi command not found cuda libs Keras version 2.0.5 with Tensorflow backend Using PyCharm Community edition 2017.1.3 as editor Describe the problem I was trying to implement a toy example for One Shot Siamese paper Gregory Koch etc. using Keras and found difference in behaviour errors between tensorflow.contrib.keras.python.keras.models i.e using Tensorflow s contrib library for Keras and keras.models i.e Keras library with tensorflow backend . Here we have to train two separate CNNs with tied weights and tensorflow contrib library for keras is throwing errors for valid Keras code. Please refer to the code below for difference in behaviour error Source code using tensorflow contrib lib for keras Output Error tensorflow contrib lib for keras Source code using Keras library with tensorflow backend Output Error Keras library with tensorflow backend , fchollet could you please take a look. Thanks Thanks for the bug report. I ve sent out a CL to fix it. It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. A member of the TensorFlow organization has replied after the stat awaiting tensorflower label was applied. Nagging Assignee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee fchollet It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee fchollet It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee fchollet It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee fchollet It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. 
10741, go bug in Shape.size for dim NumDimensions, System information This does not matter. Describe the problem go when dim equals s.NumDimensions the function should return 1 instead it panics. Source code logs In shape.go https github.com tensorflow tensorflow blob master tensorflow go shape.go L62 Shape.Size method func s Shape Size dim int int64 if dim 0 dim s.NumDimensions return 1 should be func s Shape Size dim int int64 if dim 0 dim s.NumDimensions return 1 ,Thanks for the report please specify the filename link is even better in the future to make it clear what you are talking about. Thanks 
10747,Striding behaviour different between caffe and tensorflow, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 14.04 TensorFlow installed from source or binary Binary pip install TensorFlow version use command below v1.2.0 rc2 21 g12f033d 1.2.0 Bazel version if compiling from source N A CUDA cuDNN version 8.0 GPU model and memory Tesla P100 SXM2 16MB Exact command to reproduce python caffe to tf test.py Describe the problem Caffe convolution produce different results then tensorflow with the same parameters. This has something to do with striding the attached test fails with striding equal to 2 and succeeds with striding equal to 1. Source code logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem. x.zip https github.com tensorflow tensorflow files 1079200 x.zip ,Could you show the weights and tensors outputs from this program I don t have both TensorFlow and Caffe installed. If the tensor is too big to show can you try to create a very small example like on a 3x3 image with a 2x2 filter test and output.zip https github.com tensorflow tensorflow files 1081662 test and output.zip Here is a zip file with the random weights random biases caffe output and TF output in .npy format. How hard is this to repro on a very small example i.e. 1x4x4x1 image and 1 channel in 1 channel out filter of size 2 or 3 That might make it clearer what the problem is. I can t do much smaller but here is a 1x16x16x1 with a kernel size 7 test and data.zip https github.com tensorflow tensorflow files 1081842 test and data.zip P.S. You can run a docker image of caffe from here https github.com BVLC caffe tree master docker then simply pip install tensorflow this might be the issue https stackoverflow.com questions 42924324 tensorflows asymmetric padding assumptions amnonh uw Center Padding is happening in Gemmlowp is this you wanted. tensorflow core kernels quantized conv ops.cc What we re doing here is trying to copy and fill the im2col buffer as efficiently as possible using functions to set or duplicate values en masse. We know we don t have to worry about vertical edges because we dealt with that case above so we just need to handle filters that overlap the left or right edges. Here s what that looks like left zero count center copy count right zero count filter image filter in x origin 0 input width in x end In reality it s unlikely that a filter patch will be wider than an input but this shows all the edge cases. We use memset to set the left and right sections to zeroes and memcpy to copy over the input data for the center. These are preferred to std fill and std copy because they re much faster on Android. It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. Nagging Assignee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee aselle It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee aselle It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee aselle It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. reedwm are you still looking at padding strategies in tf Not really but yzhwang recently fixed a padding bug. I m going to assume the issue is fixed since that was the only known padding bug amnonh uw please reopen if that s not the case in 1.7. 
10749,Numpy.fft.fft2 gives different result than tf.fft2d ,as mentioned in the issue 6401 the tf.fft2d gives different result compared to np.fft.fft2 . Is there a reason for this Note numpy gives proper fourier transform after np.fft.fftshift and I have taken care of that in my code. The differences are not visible here but the mean squared error is significant. image1 https user images.githubusercontent.com 18488880 27207200 84637a88 5202 11e7 9d49 d22cf9a8057d.png image2 https user images.githubusercontent.com 18488880 27207198 8462f2b6 5202 11e7 88c2 7114b36c696a.png image3 https user images.githubusercontent.com 18488880 27207199 8462fe50 5202 11e7 8d7b bca01bba063b.png image4 https user images.githubusercontent.com 18488880 27207631 f2a41284 5205 11e7 91a6 336d75755787.png Second image is the fft using tensorflow and third one is using numpy. You can see the difference in the corners. The fourth image is the difference between the two images times 10. tf has some features while numpy does not I am working on an application which uses fft in backpropagation and thus it is of absolute importance that the fft in numpy are same as fft by tf. My question is Why is there a difference and how can I get the same fft as numpy ,I am not sure if this is related to this issue 10735 Can you give any indication about the relative error between the two versions How much did you magnify the difference image aselle Hey sorry for late response. I magnified the difference by a factor of 10. I realized that numpy uses float64 whereas tf uses float32. I could not find a way to use float64 in tf creates a complex128 which is not supported by tf.fft2d . Could this be a possible problem The difference kind of seems huge to consider floating point accuracy as the reason. We use cuFFT to compute fft s so it is a property of the cufft. You could look at its documentation to see if it has any insight or google for cufft vs fftw . Also you could see whether numpy.fft when run with floats matches better. This is still an issue. I am not sure how much it will matter in the long run if we are training a network around it anyway... but it is quite shocking that the error is so high. This also makes debugging hard because TF s FFT is undoubtedly not doing what we think it should be. Perhaps cuFFT s algorithm is not numerically stable The error is there even for 1D but it is much much smaller. This makes me think that the 2D implementation is not optimal with a lot of fill in or non optimal number of operations. If someone can point me to the code maybe we can debug suggest a fix without changing too much. Some python code to verify error using complex64 for both numpy and tf i.e. based on float32 Doing some prints in my session sirgogo can you please post the source to fft2D np and fft2D tf in your example Are your ops running on GPU or CPU have you verified that with tf.ConfigProto log device placement True FWIW NumPy is the oracle used for all of the FFT unit tests https github.com tensorflow tensorflow blob master tensorflow python kernel tests fft ops test.py . The tolerance on those checks is 1e 4 at the moment which is pretty high but it s because we re using a one size fits all bound against Eigen s TensorFFT CPU and cuFFT GPU . Please also note that the FFT unit tests include gradient tests that verify the numerical gradient matches the symbolic gradient. Due to differences in the floating point hardware across your CPU and GPU the results between NumPy and cuFFT will differ by some amount for an identical sequence of floating point operations. Additionally NumPy uses fftpack for its FFTs so whether your ops are running on CPU or GPU it s a different FFT implementation from TensorFlow s. That s not to say there isn t a bug in TensorFlow s invocation of these FFT implementations or the implementations themselves Here are code pointers Eigen TensorFFT https bitbucket.org eigen eigen src 699b6595fc471456896fb27193c8ca51389b7850 unsupported Eigen CXX11 src Tensor TensorFFT.h at default fileviewer file view default TensorFlow FFT kernels https github.com tensorflow tensorflow blob master tensorflow core kernels fft ops.cc . Used to invoke either Eigen TensorFFT or cuFFT. cuFFT documentation http docs.nvidia.com cuda cufft index.html StreamExecutor invocation of cuFFT https github.com tensorflow tensorflow blob master tensorflow stream executor cuda cuda fft.cc TensorFlow FFT gradient definitions https github.com tensorflow tensorflow blob master tensorflow python ops spectral grad.py What is your expectation here e.g. would you be happy if np.isclose returned True in your example That would imply you need something like rtol 1e 5 atol 1e 8 rryan thank you for the info and links I will take a look shortly. Sorry for the delay I was out of town server went down etc etc. You can run the example with the following simple definitions I have updated that post with this I am running with tensorflow GPU. This is the only way I was able to get any of the tf.fft like functions to work. It complains if you are using the CPU only version at least for 2D . Is this sufficient to assume the ops are running on the GPU I can try the log device placement setting. Yes I would expect that the maximum relative error needs to be lower. That would probably make me most folks happy. Thanks for your insight into this. Just to clarify you are saying this is an issue with Eigen s TensorFFT cuFFT and not something that would be fixed by compiling from source Hi using tensorflow to do some signal processing and the fft problem has appeared. I did some tests and hope they are helpful. I also created a repository with the example signal where the fft really goes wrong using the cpu error after fft is 1e 14 . With a gpu it looks good. Code https github.com rassibassi fftTensorflow unix ubuntu cpu scientific linux hpc cluster cpu scientific linux hpc cluster gpu Tesla K80 with cudnn v5.1 Thanks very much for the helpful repro Rassibassi I can also reproduce the issue. This suggests an issue with Eigen TensorFFT or the TensorFlow plumbing that uses it. Interestingly if I trim your 66528 tensor to length 65536 power of 2 the results match numpy to 1e 8 but I get high error at length 65535. I get similar results when I trim the input tensor to length 32768 32767 but not when I trim to 16384 16383. Maybe suggests something is overflowing We ll take a look. cc rmlarsen Hi I ve gotten high errors also running one dimensional tf.rfft on CPU over large signals 280000 samples . On the other hand after trimming the tensor length to the next power of two the results are quite better in terms of error as pointed out by rryan . We think we tracked the issue or part of the issue down to this code in Eigen that computes twiddle factors for non power of 2 FFTs via Bluestein s algorithm. https bitbucket.org eigen eigen src 034b6c3e101792a3cc3ccabd9bfaddcabe85bb58 unsupported Eigen CXX11 src Tensor TensorFFT.h at default fileviewer file view default TensorFFT.h 230 249 We ll submit a fix to Eigen then update the version of Eigen that TensorFlow uses. https bitbucket.org eigen eigen pull requests 356 disable use of recurrence for computing I m not sure if this is the same issue Sushobhan04 is seeing though the subject of this issue . Sushobhan04 are you using power of 2 transforms Nagging Awaiting Response It has been 14 days with no activityand the awaiting response label was assigned. Is this still an issue As of 67ff23036e093343a7ea02e17f36f2b02eaae740 the fix in https bitbucket.org eigen eigen pull requests 356 disable use of recurrence for computing has been applied to TensorFlow. Hej Thanks for fixing the issue. It seems like there is still an issue with the CPU FFT. It s more subtle but after calling ifft fft signal several times the error becomes apparent. Take a look at the following script. I updated https github.com rassibassi fftTensorflow with the new script in run2.py Output CPU Output GPU Nagging Awaiting Response It has been 14 days with no activityand the awaiting response label was assigned. Is this still an issue Nagging Awaiting Response It has been 14 days with no activityand the awaiting response label was assigned. Is this still an issue Nagging Awaiting Response It has been 14 days with no activityand the awaiting response label was assigned. Is this still an issue It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue Rassibassi I m working on adding complex128 support to the FFT ops and I ran your code snippet on CPU with the new complex128 support. It looks like a good bit of the difference comes down to floating point precision but there is still a difference in magnitude of error between our Eigen CPU kernels and NumPy s FFTPACK based implementation sirgogo running your code snippet using complex128 FFTs the error is 1.57009245868e 16 . Under complex64 the error is 5.1619136559e 08 . rryan Thanks this sounds promising. One thing I was wondering perhaps unrelated to this issue is some of us are seeing different results on TF CPU vs TF GPU computation even for networks with only simple operations like Add Multiply MatMul reduce sum reduce max etc. I need to come up with a good set of tests to expose this but do you have any insight on why this might be the case Different results make sense since the computation is being run by different hardware implementations of IEEE 754. Also the implementation of reductions across CPU and GPU can be quite different. I ve heard that Eigen on CPU can produce non deterministic results when computing e.g. reductions using a thread pool. One way to eliminate Eigen CPU non determinism as a factor in testing is to set intra op parallelism threads https github.com tensorflow tensorflow blob master tensorflow core protobuf config.proto L273 to 1 in your ConfigProto. Differences can also arise from software bugs though so please do file issues ideally with reproducible test cases for places where you think the difference might be a bug. rryan Thanks Looking forward to complex128 FFT functionality what about running the examples with complex128 on a GPU Sushobhan04 Hi According to the second that you have made you used fftshift in tensorflow. Am I right Can you please share tensorflow implementation of fftshift I added complex128 FFT support for CPU and GPU in https github.com tensorflow tensorflow commit 8f0a90b711480c12716d1a3b1094cc8b34939f2d. It ll be part of TensorFlow 1.9. RFFT IRFFT support is on the way but a little more complicated so I split it into a separate commit. It has been 21 days with no activity and the awaiting response label was assigned. Is this still an issue It has been 36 days with no activity and the awaiting response label was assigned. Is this still an issue We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks 
10755,Weird behavior of tf.train.Saver, System information Linux Ubuntu 14.04 TensorFlow installed from binary TensorFlow version v1.0.0 rc2 CUDA 8.0 CuDNN 5.1 Tesla K80 12GB Describe the problem I have a problem with the tf.train.Saver specifically with the max to keep argument. If I create a Saver with max to keep set to let s say 3 and use the saver.save function to save my model in the current directory it keeps all files and doesn t delete the old ones after 3 or more are created. If I set the path where to save the model to a different location it works just fine. See also my stackoverflow question https stackoverflow.com questions 44458947 tensorflow keeps all files how to prevent that Source code logs Creates for the numbers 1 to 10 each 3 files testfile 1.data 00000 of 00001 testfile 1.index testfile 1.meta This code works as expected and deletes all the old files and I only end up with the numbers 8 to 10 ,Could you take a look concretevitamin please Seems relevant to I O layer. Adding rohan100jain. There is another bug if path to save contains double slash e.g. logdir model . In this case old checkpoints are not removed as well. Quite sure that https github.com tensorflow tensorflow commit 3e6c638727c3274908a7c9c6bbf4474c014511fe would have fixed the original issue... If you update your tf version to the latest one it works.. The case is a little more complex since in some file systems paths have s in them... so I d recommend just not having them in the path rohan100jain python open works ok with double slash for me it s expected to have same behaviour with default bash python semantics of path. I have experienced the same problem when using brackets in the name of the saving folder. Sorry for not following up on this but the original issue has been resolved. So I guess this can be closed. The best way to use the function is to hand over os.path.abspath filename . It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. 
10766,MultivariateNormalDiag probability gradient fails, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOS 10.12.5 TensorFlow installed from source or binary source TensorFlow version use command below v1.2.0 rc2 0 gce1d6ec49 1.2.0 rc2 Bazel version if compiling from source 0.5.1 homebrew CUDA cuDNN version N A GPU model and memory N A Exact command to reproduce python test diag.py Describe the problem The gradients for the tensorflow.contrib.distributions.MultiVariateNormalDiag throw an error. This might be related to 10149. Notice that the tensorflow.contrib.distributions.Normal works as expected so I think this is a bug in the gradient implementation of the MultiVarateNormalDiag. Source code Output ,Also fails on the released version of tensorflow v1.2.0 built from source. Also fails when gradients calculated for loss w.r.t. weights drawn from MultivariateNormalDiag in my case with the help of contrib.bayesflow.stochastic tensor.StochasticTensor . Using Normal appears to be a slower workaround. I had a similar problem as jakedailey1 which is why I distilled this example down from a Mixture Density Network with a likelihood based loss function. This turns out to be an issue with tf.reduce prod whereby gradients fail upon reduction over negative indices. I ll get an internal fix started. Wow Just when I thought I was stuck I find there s a solution already on the way. Nice work D At least it looks like the same issue with gradient back propagation as here on paste bin https pastebin.com M4suZ68K How is it coming along It looks like a fix has been put together here https github.com tensorflow tensorflow pull 11019 . I can t try this out right now but perhaps you can just grab the latest files and it will work. I can confirm that ea79ba4 mentioned above fixes the issue. I cherry picked it to v1.2.1 to verify. The fix is available on master. I upgraded my Tensorflow to v1.2.1. But I am still facing the same error message when I am trying to train a model to learn an appropriate sigma value for the Gaussian filter. Am I doing something wrong My system is Win10. I installed Tensorflow through Anaonda. Here is my code. Here are the error messages. v1.2.1 does not have the fix. The fix is on master. You need to cherry pick the commit I mentioned above ea79ba41be89d76180305b14d26d5c2f256d4c08 onto your v1.2.1 branch. Oh now I see Thank you so much ktarplee 
11005, Bug tf version 1.2 dropout layer reject tensor type rate,version 1.2 The following code or will fail with following error The same code run successfully on tf version 1.1.0 I check the documents are not changed https www.tensorflow.org api docs python tf contrib layers dropout https www.tensorflow.org api docs python tf contrib layers dropout keep prob A scalar Tensor with the same type as x. The probability that each element is kept. It accepts tensor so I guess this should be a bug. , abelxie Thanks for filing the issue I ve reproduced the problem and agree that this looks like a bug. fchollet Can you take a look or suggest someone to fix this Also marking contributions welcome in case anyone in the community would like to contribute to fix this. The error seems caused by using Python min and max on a Tensor. I also get he error in TF 1.2 but it is not in the current master as of c98dab03f0de724da81dac8218757207996d1505 Do you know if we can expect an updated official version soon or do we need to build ourselves simonkamronn Thanks for tracking down the PR that fixed this av8ramit Probably knows the schedule for TF 1.3. In the meantime you ll either need to pull the nightly releases or build from source yourself. simonkamronn the next official release will be sometime in July. Please pull the nightly release for a fix Thanks for bringing this to our attention. Thanks for the info av8ramit Closing this out since the underlying problem has already been fixed. 
11016,map func of tf.contrib.data.Dataset.map gets dict keys instead of values when the nested structure of Dataset is dict, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 TensorFlow installed from source or binary source TensorFlow version use command below b 0.5.0 12520 g1111e06d9 1.2.0 rc2 Bazel version if compiling from source 0.4.5 CUDA cuDNN version 8.0 6 GPU model and memory Describe the problem If the nested structure of Dataset is dict MapDataset will call map func nested args https github.com tensorflow tensorflow blob 1111e06d9cd691cbdfcb67cf9f234a504f4e0f6d tensorflow contrib data python ops dataset ops.py L1463 and pass the keys of nested args instead of components in the dataset to map func . It seems that nested args or nested args.values need to be passed to map func so that map func could transform the elements in the dataset. Source code logs ,I tried reproducing and I think that Dicts are not supported as inputs in general a tf.contrib.data.Dataset.from tensors a 1 b 2 Traceback most recent call last File stdin line 1 in module File usr local lib python2.7 dist packages tensorflow contrib data python ops dataset ops.py line 460 in from tensors return TensorDataset tensors File usr local lib python2.7 dist packages tensorflow contrib data python ops dataset ops.py line 864 in init for i t in enumerate nest.flatten tensors File usr local lib python2.7 dist packages tensorflow python framework ops.py line 676 in convert to tensor as ref False File usr local lib python2.7 dist packages tensorflow python framework ops.py line 741 in internal convert to tensor ret conversion func value dtype dtype name name as ref as ref File usr local lib python2.7 dist packages tensorflow python framework constant op.py line 113 in constant tensor conversion function return constant v dtype dtype name name File usr local lib python2.7 dist packages tensorflow python framework constant op.py line 102 in constant tensor util.make tensor proto value dtype dtype shape shape verify shape verify shape File usr local lib python2.7 dist packages tensorflow python framework tensor util.py line 462 in make tensor proto supported type. type values values TypeError Failed to convert object of type type dict to Tensor. Contents a 1 b 2 . Consider casting elements to a supported type. My tensorflow version was 1.2.0 though. It seems that f3f53e8b394bdcaddc707f7bde8dcc98a73531e7 adds support for dict as nested structures of Dataset . I built master branch from source. This is definitely a bug. Thanks for catching it I have a fix in the works. It seemed that the parameter padded shapes for tf.contrib.data.Dataset.padded batch can t be dict too. Thanks That turned up when I was testing the fix. 
11017,Tfdbg does not work with Coordinator QueueRunners, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Mint 18 TensorFlow installed from source or binary Binary pip TensorFlow version use command below v1.2.0 rc2 21 g12f033d 1.2.0 Bazel version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Exact command to reproduce N A Describe the problem The Tensorflow debugger does not seem to be working with Queues data never seems to be fetched by the QueueRunner threads be it from a file using tf.TFRecordReader and tf.parse single example or preloaded using tf.train.slice input producer . Instead the coordinator.should stop is True right away. This is only the case after wrapping the session in a tf.python.debug.LocalCLIDebugWrapperSession . The example should make things clearer. Moreover another error occurs at coordinator.join threads . I am aware of the FAQ entry on Threads https www.tensorflow.org programmers guide debugger but that does not explain why the data fetching threads would not be working. Source code logs To make it easiest to replicate I simply took the example on working with preloaded data https github.com tensorflow tensorflow blob master tensorflow examples how tos reading data fully connected preloaded.py and wrapped the session in there with the debugger. I uploaded the gist with two lines added to https gist.github.com rubenvereecken 079cdf1abc76866714ff6f752167481d file fully connected preloaded debug py L92. To reproduce run the file. Once you drop in the debugger run once. It then exits. The full output is below The stacktrace is about coord.join threads but this is only possible because coord.should stop never seems to be False which would indicate there is data to load. Without the added debugger lines the example simply works.,cc mrry rubenvereecken Thanks for reporting this issue. We are aware of it and will push a fix to it soon. caisq thank you so much I look forward to it. rubenvereecken While you wait for the fix I want to ask you whether you are trying to debug the data input queues or the training operation on the main thread. If the latter there is a workaround for that. caisq Ah actually the former but I d be working my way towards the latter. Is there a way to debug these training ops while still using data fed from queues Either way could you point me at the workaround Much appreciated rubenvereecken The workaround is based on the assumption that the train op runs on the Python main thread while the data queues run on the child threads which should usually be the case. You can just use the thread name filter kwarg of the wrapper s constructor to limit the debugging to the train op. This is talked about in the FAQ. Doing this doesn t change the source of the input data. They still come from the queues they just don t break into the TFDBG UI when they run. Oops. I may have given incomplete suggestion. In your code at https gist.github.com rubenvereecken 079cdf1abc76866714ff6f752167481d file fully connected preloaded debug py L92 make sure that your wrapped Session object is used only to run the train op. You can do something like this Move the line sess tf debug.LocalCLIDebugWrapperSession sess after line 100. That makes sure that when the data input queue ops are created the make callable method of the original session not that of the wrapped session is called. The wrapped session doesn t have a make callable method which was recently added. This was the root cause of the issue you are seeing. tada For the user of TF Slim The usage of thread name filter 
11091,tf.nn.elu incorrect second derivative, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 14.04 TensorFlow installed from source or binary binary TensorFlow version use command below 1.2.0 Bazel version if compiling from source N A CUDA cuDNN version CUDA 8.0 cuDNN 6.0 GPU model and memory GTX 1080 Ti 11GB Exact command to reproduce see below tf.nn.elu gives incorrect second derivatives Consider the graph y 2 elu x . We ll be evaluating at x 1 We can evaluate first derivatives with automatic differentiation This lines up with the analytic answer y 2e x However for the second derivative Whoops this doesn t look right Analytically the derivative is y 2e x . Evaluated at x 1 this is 0.7357588 Workaround Just in case anyone else needs to work around this until it s fixed Looks like second derivatives work with that.,Possibly related to 7403 alextp do you know why this might be anishathalye is it possible to make a simpler repro script that you can share Can you use the tensorflow gradient checker to test the gradient of your particular graph See https github.com tensorflow tensorflow blob master tensorflow python ops gradient checker.py and usages of it in the tensorflow tests. Okay so I tried the following I have a network y x where y contains no tf.gradient ops and I checked 1. tf.check numerics tf.gradients y x 0 result is ok 2. tf.test.compute gradient error x ... y ... result is 983 is that okay in any case for training first derivatives seem to work 3. tf.check numerics tf.gradients tf.gradients y x 0 x 0 result is ok 4. tf.test.compute gradient error x ... tf.gradients y x 0 ... result is 1522423936 Finite differences probably isn t producing great results because y is a fairly big graph. But still having a maximum error of 1e9 seems kind of large. What do you suggest looking into next This is really hard to debug without having access to the full graph. What I d do if I did have access to the full graph would be bisect it removing chunks of graph at a time until the error in second derivative goes down to see if there s some part of the code which is less numerically stable than it should be. On Wed Jul 5 2017 at 3 17 PM Anish Athalye notifications github.com wrote Okay so I tried the following I have a network y x where y contains no tf.gradient ops and I checked 1. tf.check numerics tf.gradients y x 0 result is ok 2. tf.test.compute gradient error x ... y ... result is 983 is that okay in any case for training first derivatives seem to work 3. tf.check numerics tf.gradients tf.gradients y x 0 x 0 result is ok 4. tf.test.compute gradient error x ... tf.gradients y x 0 ... result is 1522423936 Finite differences probably isn t producing great results because y is a fairly big graph. But still having a maximum error of 1e9 seems kind of large. What do you suggest looking into next You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 11091 issuecomment 313241964 or mute the thread https github.com notifications unsubscribe auth AAATxdLiF38xxI13fIJsHsvl7rooT44Cks5sLAuTgaJpZM4OHLlJ . Alex Ok I have some more evidence indicating that there is in fact a bug. Also I can share the full graph with you. I don t want to post it publicly so I ve sent you an email with this additional information. I found the bug I updated the original post. The commit from last week should have fixed this. 
11138,Hang when fitting tensorflow learn model which contains crossed sparse column with columns and the data is loaded with pandas input fn, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Run on Databricks GPU cluster Ubuntu 16.04.1 LTS TensorFlow installed from source or binary pip installed tensorflow gpu TensorFlow version use command below v1.2.0 rc2 21 g12f033d 1.2.0 Bazel version if compiling from source n a CUDA cuDNN version CUDA Version 8.0 cuDNN Version 5.1 for CUDA 8.0 GPU model and memory Exact command to reproduce Describe the problem When building Linear models with the tensorflow learn libraries the program hangs if the pandas input function tensorflow.estimator.inputs.pandas input fn is used and a crossed column tensorflow.contrib.layers.crossed column is constructed from a sparse column with . This issue is not seen in the tensorflow tutorials as they use constant tensor s to input the data which does not scale to large datasets. This also only occurs if crossing sparse column with columns bucketized continuous columns do not cause a hang. The final line of output before the code hangs is INFO tensorflow Create CheckpointSaverHook. Source code logs Minimum working example of bug with example of code that works and code that breaks ,I tried running your script inside the latest nightly docker image and got NameError name real valued column is not defined So I tried patching it with from tensorflow.contrib.layers import real valued column and got some errors that suggest that wasn t the right thing to do WARNING tensorflow Rank of input Tensor 1 should be the same as output rank 2 for column. Will attempt to expand dims. It is highly recommended that you resize your input as this behavior may change. I ll go look for the v1.2.0 rc2 image but is there any chance that your script is missing a piece I am sorry I missed the include for real valued column I accidentally removed it when simplifying the code to share as you say simply adding the line from tensorflow.contrib.layers import real valued column fixes this. I will edit the original to fix this. The warnings you see are warnings not errors and as such are not expected to prevent the algorithm from correctly training. These are also related to the use of the pandas input fn I believe but I have not managed to fully understand their source when investigating. They do not prevent the successful running of the above code with the crossed columns removed and I believe they are unrelated to this problem. I have run this script on multiple systems and encountered the same issue so I don t believe it is missing anything. Please also note there are two model.fit lines at the bottom the script will not complete successfully if both these are uncommented I provided both to demonstrate a working case and a not working case. In my running I have found that the bug in the first prevents the running of the second but in the case the first is commented out the second will run. Also if the line feature columns sparse cat1 sparse cat2 cross bb cross cc cross bc is changed to feature columns sparse cat1 sparse cat2 cross bb then the first will run and it will crash on the second if left uncommented. In your testing did the script complete successfully or did it hang as I saw in my testing You simply state you saw a warning message not what happened after the warning message. ispirmustafa are you able to take a look or reassign to someone who can xiejw could you please take a look it may be related to pandas input fn . bensowden is this still an issue Nagging Assignee xiejw It has been 59 days with no activity and this issue has an assignee. Please update the label and or status accordingly. 
11192,why use x is dict to check y ,In tensorflow contrib learn python learn learn io data feeder.py On L325 self. y None if y is None else dict k check array v v.dtype for k v in list y.items if x is dict else check array y y.dtype I happened to have a implementation where x is a dict but y is a numpy array so I got an error. I wonder why we do not use y is dict here ,This question is better asked on StackOverflow http stackoverflow.com questions tagged tensorflow since it is not a bug or feature request. There is also a larger community that reads questions there. Thanks Ok thanks. I do think it is a bug though. A wrong variable is used here. Sorry I missed that. I agree. 10403 looks like the same problem with a little more context about the x y confusion. 
11471,XLA crash on Wasserstein GAN, System information OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04 TensorFlow installed from source or binary TensorFlow installed from source TensorFlow version use command below v1.2.0 1382 g708cbaf 1.2.0 Python version Python 3.5.2 Bazel version if compiling from source CUDA cuDNN version CUDA 8 cudnn 6 GPU model and memory 2x1080 Ti 12 Gb Describe the problem When I m running this code https github.com Randl WassersteinGAN.tensorflow with XLA I get the following error message Without XLA it works OK. ,I assumed the problem might be with memory but for smaller batches I get the same error Second part with free desn t appear each time I don t know what it depends on hawkinsp XLA crash reported with free invalid size . We are seeing similar crash with ArrayFire which is doing just like XLA JIT . nvidia driver 375.51 seems more stable and we dont see crash with it... TBC The crash in cuModuleLoadDataEx fatBinaryCtl Compile cuda CallJitEntryPoint also disappears with nvidia drivers 384.59. There seems to be a background bug in nvidia driver sources that is randomly effective according to the version of the drivers. The bugtracker of nvidia is not public so hard to say if they are aware of that bug. I got the similar or the same error when turning on XLA jit for the WDL example https github.com tensorflow tensorflow blob r1.0 tensorflow examples learn wide n deep tutorial.py https github.com tensorflow tensorflow blob r1.0 tensorflow examples learn wide n deep tutorial.py I tried to use a newer stable version of LLVM in workspace.bzl But the problem still exist I am wondering if there is something wrong in the LLVM IR emitter of the XLA code It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. Sorry I didn t notice this was assigned to me and I m not working on this code at the moment. Justin I think this may have been fixed recently Yes this should be fixed by 6605938c280590ee981470abe87386396cf0e438. I m pretty confident this will fix the problem so closing this issue. But please reopen if you all see this problem after this commit. 
11548,Different behavior of tf.extract image patches and tf.nn.conv2d for certain padding stride filter size combinations,Hi I am trying to implement something using tf.extract image patches and ran into some troubles that made clear tf.extract image patches handles some combinations of padding filter size and stride differently than tf.nn.conv2d . Since tf.extract image patches is conceptually a part of a convolution operation I think this might be unintended behavior. Specifically I implemented a manual version of a convolution operation using tf.extract image patches and tested it like this This test fails for some combinations of padding filter size and stride. I think it has to do with the fact that tf.extract image patches tries to center patches if possible as discussed in this 1 stackoverflow question. 1 https stackoverflow.com questions 40731433 understanding tf extract image patches for extracting patches from an image System information Ubuntu 16.04. Python 2.7.12 tensorflow version 1.2.1 installed via pip CPU only ,Yangzihao can you verify this is fixed The reason for the failure might also be due precision issues instead of padding issues. Nagging Assignee reedwm It has been 149 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Closing because I assume this is fixed by a change Yangzihao made. Please reopen if that is not the case. 
11638,Probably wrong implementation for tf.layers.max pooling1d when data format channels first ,In function call of class Pooling1D when the input data format channels first it should transform input tensor from N C H to N C H W batch size channels height width meaning that we should expand dimension on the last dimension. However in the code we use inputs array ops.expand dims inputs 1 expanding on the second dimension and transforming from N C H to N 1 C H . Then the pool shape and strides are looking at the third dimension which is not consistant with our expand dims inputs 1 used before. I think the code should be changed to inputs array ops.expand dims inputs 1 and return array ops.squeeze outputs 1 . Using 1 will expand and squeeze on the last dimension transforming from N C H to N C H 1 and then doing pool shape and strides on the third dimension. Source Code , zhangyaobit could you take a quick look Indeed looks not right to me. fchollet could you take a look I agree with ZekunZh. The expand dims in Pooling1D should transform NHC to NHWC for channels last with expand dims inputs 2 NCH to NCHW for channels first with expand dims inputs 1 not 1 Marked as contribution welcome. Anyone is welcome to implement the fix proposed by ZekunZh How about contributing your idea ZekunZh Thanks for your invitation taehoonlee I have proposed my solution in my first post. To be clearer my suggestion of code for call function of Pooling1D class I will make my pull request very soon. Please remove the assignee as this issue is inviting external contributions. Otherwise remove the contributions welcome label. Thank you. Please remove the assignee as this issue is inviting external contributions. Otherwise remove the contributions welcome label. Thank you. Please remove the assignee as this issue is inviting external contributions. Otherwise remove the contributions welcome label. Thank you. The issue is already fixed refer to the codes https github.com tensorflow tensorflow blob master tensorflow python layers pooling.py L74 . Closed through 15500 
11676,Error in tf.contrib.layers.batch norm when explicitly assigned on gpu, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 14.04 TensorFlow installed from source or binary source TensorFlow version use command below 1.2.1 Python version 3.4.3 Bazel version if compiling from source 0.5.2 CUDA cuDNN version 8.0 6.0 GPU model and memory Tesla P100 16 gb Describe the problem Batch norm layer fails with an error when explicitly assigned to be run on gpu and zero debias moving mean is False. Interesting that I m getting this error only when is training is a placeholder passing just True doesn t reproduce the error . If commented line is used instead zero debias moving mean True the code also works without any error. Source code logs The following code could be used to reproduce the issue The error log , zhangyaobit do you have any ideas Perhaps using fused batch norm This is caused by pred array ops.reshape pred 1 in smart select in tensorflow python layers normalization.py. pred is a scalar Tensor of boolean type. However the GPU kernel of reshape doesn t support boolean type because of the type constraint here https github.com tensorflow tensorflow blob master tensorflow core kernels reshape op.cc L31 . vrv do you have some ideas why boolean is not supported fchollet sguada as pred is a scalar could this reshape be replaced by something else No idea why it isn t supported perhaps it could be registered. People only add registrations as they are needed. Adding REGISTER GPU KERNEL bool can fix the problem. Thanks taehoonlee for the fix 
11692,A bug of tf.reduce logsumexp with inf , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux CentOS 7 TensorFlow installed from source or binary binary TensorFlow version use command below 1.2.0 Python version 2.7.13 Bazel version if compiling from source CUDA cuDNN version 8.0 5.1.3 GPU model and memory Tesla K40m 11439MiB Exact command to reproduce python c import tensorflow as tf print tf.Session .run tf.reduce logsumexp float inf Describe the problem The doc of tf.reduce logsumexp says it Computes log sum exp elements across dimensions of a tensor . However it does not when the tensor is inf . Source code logs prints prints ,The tf.reduce logsumexp source code currently is I have written my own tf reduce logsumexp to fix the bug fastturtle could you take a look This looks like a reasonable fix would you be willing to submit it as a pull request aselle Sure I will try it. 
11856,tf.variables initializer seems broken., System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes. OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 14.04 TensorFlow installed from source or binary The nightly wheel. TensorFlow version use command below The nightly build that went out at 2AM on 7 28 2017 https ci.tensorflow.org view Nightly job nightly matrix linux gpu TF BUILD IS OPT OPT TF BUILD IS PIP PIP TF BUILD PYTHON VERSION PYTHON2 label gpu linux Python version 2.7.13 Bazel version if compiling from source 0.5.3 CUDA cuDNN version 8.0 GPU model and memory GTX 1080 Exact command to reproduce To reproduce run this snippet in the python interpreter Describe the problem Running the snippet above yields an exception It seems like tf.variables initializer is broken it does not initialize the variables passed to it. This has caused a TensorBoard test summary test to fail today TensorBoard runs tests on nightly TensorFlow . https travis ci.org tensorflow tensorboard jobs 258655621 The test had been passing yesterday on nightly built on 7 27 . ,It seems like indenting print sess.run result one to the right does away with the exception. Like this However it doesn t seem like that was necessary yesterday. And it would be nice if that were not necessary to save one level of indentation for most of the function . Actually ... indeed that won t work because mostly likely I m not running the session within a control dependency block. This also fails on 1.2.1 version of TensorFlow. The reason is that v 1 is relying on Variable read node which is created outside of tf.control dependencies block created during tf.Variable construction in this line https github.com tensorflow tensorflow blob faf7c32ab27dad24d2d806a16d1371ecb4671fc8 tensorflow python ops variables.py L330 and therefore doesn t have proper control dependency. The following makes it pass. cc alextp result tf.assign v v.read value 1 Also swapping the variables with resource variables using get variable ... use resource True to construct the variables makes this issue go away. On Fri Jul 28 2017 at 12 11 PM Yaroslav Bulatov notifications github.com wrote This also fails on 1.2.1 version of TensorFlow. Theory v 1 is relying on Variable read node which is created outside of tf.control dependencies block and therefore doesn t have proper control dependency. The following makes it pass. cc alextp https github.com alextp result tf.assign v v.read value 1 You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 11856 issuecomment 318738286 or mute the thread https github.com notifications unsubscribe auth AAATxTXVWhHcR4WVrlkj0HlefurKvgpMks5sSjJhgaJpZM4Om9Cy . Alex I think I still want a tf.Variable instead of a resource one because the test uses a ScatterAdd op here https github.com tensorflow tensorboard blob 02f97383951f57dacb462fb6335f461f32ac7141 tensorboard plugins pr curve summary.py L162 And the scatter add seems to err when passed resource variables Is there a way to include that Variable read op in the control dependency list Thank you There s a resource scatter add op which can be used instead. On Fri Jul 28 2017 at 1 11 PM Chi Zeng notifications github.com wrote I think I still want to use a tf.Variable instead of a resource one because the test uses a ScatterAdd op here https github.com tensorflow tensorboard blob 02f97383951f57dacb462fb6335f461f32ac7141 tensorboard plugins pr curve summary.py L162 And the scatter add seems to err when passed resource variables Traceback most recent call last File usr home agent007 .cache bazel bazel agent007 e5cf4ac2d3d80f539d85c4d9419a6bd7 bazel sandbox 68 677344569147791 execroot org tensorflow tensorboard bazel out local fastbuild bin tensorboard plugins pr curve summa ry test.runfiles org tensorflow tensorboard tensorboard plugins pr curve summary test.py line 48 in test1Class num thresholds 10 File usr home agent007 .cache bazel bazel agent007 e5cf4ac2d3d80f539d85c4d9419a6bd7 bazel sandbox 68 677344569147791 execroot org tensorflow tensorboard bazel out local fastbuild bin tensorboard plugins pr curve summa ry test.runfiles org tensorflow tensorboard tensorboard plugins pr curve summary.py line 165 in op tp buckets v bucket indices true labels use locking True File usr home agent007 anaconda2 lib python2.7 site packages tensorflow python ops gen state ops.py line 212 in scatter add name name File usr home agent007 anaconda2 lib python2.7 site packages tensorflow python framework op def libr ary.py line 328 in apply op op type name name keywords File usr home agent007 anaconda2 lib python2.7 site packages tensorflow python framework op def libr ary.py line 641 in apply op helper e.g. a tf.Variable op type name input name TypeError ScatterAdd Op requires that input ref be a mutable tensor e.g. a tf.Variable Is there a way to include that Variable read op in the control dependency list Thank you You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 11856 issuecomment 318750405 or mute the thread https github.com notifications unsubscribe auth AAATxaWsTbZ1nJ27MWGHaXpFmphdCPM8ks5sSkB gaJpZM4Om9Cy . Alex It seems like resource scatter add is only supported internally for now I don t see it in the tf. namespace and the test is open source. I want to spend some time to fix this and move the read node into the dependency chain. Could someone please give me some guidance My alternative would be to rewrite a project to avoid scatter add. That is a relatively big task. Furthermore after a variable is initialized it seems reasonable for a user to assume that they can read from and update the variable. chihuahua replacing the line with tf.assign v v.read value 1 will refer to the proper read node. The issue is that the tensor is retrieved from snapshot in AsTensor https github.com tensorflow tensorflow blob faf7c32ab27dad24d2d806a16d1371ecb4671fc8 tensorflow python ops variables.py L367 which is initialized during variable construction. For a more sweeping fix one might modify AsTensor to check whether you are inside any active context managers for control dependencies and if so create a new read node read value rather than referring to the snapshot. That might get complicated ie what if someone is inside control dependency context referring to variable in a long loop That explodes the graph I think it should be safe to always call read value when you reference the variable. Indeed I do this in resource variables which do not have this bug and for which a scatter add operation exists resource scatter add . I do not understand what s the remaining issue here. On Sun Jul 30 2017 at 9 12 AM Yaroslav Bulatov notifications github.com wrote chihuahua https github.com chihuahua replacing the line with tf.assign v v.read value 1 will refer to the proper read node. The issue is that the tensor is retrieved from snapshot in AsTensor https github.com tensorflow tensorflow blob faf7c32ab27dad24d2d806a16d1371ecb4671fc8 tensorflow python ops variables.py L367 which is initialized during variable construction. One might modify AsTensor to check whether you are inside any active context managers for control dependencies and if so create a new read node read value rather than referring to the snapshot. That might get complicated ie what if someone is inside control dependency context referring to variable in a long loop That explodes the graph You are receiving this because you were assigned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 11856 issuecomment 318911787 or mute the thread https github.com notifications unsubscribe auth AAATxf0HNbhFj4pMoZpNRxDLTJTDa Kzks5sTKtjgaJpZM4Om9Cy . Alex Does calling read value append to the graph invalidate graph cache Yes On Mon Jul 31 2017 at 9 17 AM Yaroslav Bulatov notifications github.com wrote Does calling read value append to the graph invalidate graph cache You are receiving this because you were assigned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 11856 issuecomment 319118922 or mute the thread https github.com notifications unsubscribe auth AAATxRDSm3qNOfY VGyhQLf5WjUQbiYYks5sTf4ygaJpZM4Om9Cy . Alex Is this issue solved irrelevant chihuahua IMHO this is a counter intuitive property of graph based system rather than a bug you always need to create a new graph node if you want constraints enforced by surrounding context managers to be enforced. In your case v reuses previously created graph node which doesn t know about your desired execution order whereas v.read value will create a new node which respects the control dependency SGTM. Thank you I think it might be worth clarifying this in the tf.variable initializer docs. Otherwise it might be a bit unclear to some folks to use v.read value . And indeed I have worked around this issue by avoiding scatter add entirely. 
11868,keras resnet50 example yields different predictions than in stand alone keras, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No using example from Keras documentation here https keras.io applications classify imagenet classes with resnet50 OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac OS X 10.12.6 TensorFlow installed from source or binary Binary CPU Version TensorFlow version use command below v1.2.0 5 g435cdfc 1.2.1 Python version 2.7 OS X system version Bazel version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Exact command to reproduce Describe the problem The above code yields the following output However the same code run using Stand alone Keras yields this Note to reproduce under stand alone Keras substitute this code for the imports at the top Also note that you need the elephant.jpg file in the working directory to reproduce. You can find that file here https github.com rstudio keras blob master docs articles elephant.jpg ,Observed the same behavior with v1.3.0 rc1 447 g4b50313 1.2.1 rc1 I have noticed this issue for long. See https github.com tensorflow tensorflow issues 11160 https github.com tensorflow tensorflow issues 11160 . The predefined weights seem to be essentially random in tf.contrib.keras and issues do not seem to get much attention. After some time trying to port my keras models to tf.contrib.keras I have moved to tf slim models api which is excellent. fchollet could you take a look please. Thanks aselle I won t have time to look into these issues for at least a week or so. Can we find someone else Currently every single issue related to Keras is routed to me personally which isn t scalable. There is no significant change in the resnet script but several tf keras layers have been refactored in here https github.com tensorflow tensorflow commit 35253fa89c5f8af25e3d84f76980729569091a6c . Thus I tested the four individual layers mainly used in the resnet and found something weird in Conv2D as follows The results are I examined each of the layers in ResNet50 and found a slight difference between BatchNormalization layers of keras and tf keras. Would you please check this comment aselle fchollet The results are While the gamma is defined first in original keras the beta is done first in tf keras. This order is important because the load weights of keras API checks shapes of symbolic and numpy tensors according to the order. In order words pretrained variables are stored as gamma beta format but tf keras reads them as beta gamma without error due to the same shape. The load weights doesn t check tensor names. I listed up two concise solutions as follows Swapping the two self.add variable calls for gamma and beta in tensorflow python layers normalization.py I think this might be the best if the swapping doesn t break someone s code. Adding self.trainable weights self.trainable weights 1 after layer building in tensorflow contrib keras python keras layers normalization.py This seems to be a patchwork but no one s codes will break. Which one of these looks better This has been fixed last week and will soon appear in GitHub. I just checkout the latest tree with git fetch a git checkout master git pull But I still cannot find the fix for issue 11868. Can you please double check if it was merged to GitHub Thanks counterexample 13562 is visible. 
11902, tensorflow python kernel tests denormal test test failure on ppc64le error Arrays are not equal , System information OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04 ppc64le TensorFlow installed from source or binary Installed from source TensorFlow version use command below v1.2.1 0 gb4957ff 1.2.1 Python version Python 2.7.5 Bazel version if compiling from source 0.4.5 2017 07 13 037b9b9 CUDA cuDNN version NA GPU model and memory NA Exact command to reproduce bazel test test output errors tensorflow python kernel tests denormal test Describe the problem Following code returning incorrect results https github.com tensorflow tensorflow blob v1.2.1 tensorflow python kernel tests denormal test.py L36 L44 I have printed the values of flush.eval and np.zeros shape see below flush.eval o p 1.17549463108e 39 np.zeros shape o p 0.0 Getting failure 1.17549463108e 39 vs expected 0.0 . I am not able to understand why we are getting flush.eval o p 1.17549463108e 39 and not 0.0 . But I tried changing the comparison function from assertAllEqual to assertAllClose and test is passing successfully. Is it OK to merge this changes or I would like to hear comment on this.Thanks This test passing successfully in TF1.0.1 without any changes see relevant code https github.com tensorflow tensorflow blob v1.0.1 tensorflow python kernel tests denormal test.py L31 L48 Looks like this test is disabled in TF1.0.1 Source code logs ,I have done some investigation on this looks like we need to add Power specific code in https github.com tensorflow tensorflow blob v1.2.1 tensorflow core platform denormal.cc L43 L73 I was discussing about this test failure on power porting mailing list power port tune lists.linux.ibm.com see below Hi I am trying to port the TensorFlow code to Power I m looking at some code which uses certain Intel intrinsics and want to know if there are equivalent Power equivalents The code I m trying to port uses these intrinsics MM GET FLUSH ZERO MODE MM GET DENORMALS ZERO MODE MM SET FLUSH ZERO MODE MM SET DENORMALS ZERO MODE MM SET FLUSH ZERO MODE Would there be corresponding intrinsics on Power Just for reference the functions I m trying to port are these https github.com tensorflow tensorflow blob v1.2.1 tensorflow core platform denormal.cc L43 L73 ScopedFlushDenormal ScopedFlushDenormal andScopedFlushDenormal ScopedFlushDenormal Any suggestions would help Thanks Sandip And I got following reply This is not an intrinsic it is mode and a nonstandard not IEEE754 conforming one at that. Some embedded machines perform slight better in this mode but POWER servers do not. So for POWER you do not need this and should simply disable it. For now we flush denormals only on SSE 3. Other architectures such as ARM can be added as needed. ifdef DENORM USE INTRINSICS So find out where DENORM USE INTRINSICS is define and undef it. drpngx gunan girving mrry Is it OK to disable or don t execute this test on power Sgtm On Jul 31 2017 11 58 PM sandipmgiri notifications github.com wrote I was discussing about this test failure on power porting mailing list power port tune lists.linux.ibm.com and they mentioned as follows MM GET FLUSH ZERO MODE MM GET DENORMALS ZERO MODE MM SET FLUSH ZERO MODE MM SET DENORMALS ZERO MODE This is not an intrinsic it is mode and a nonstandard not IEEE754 conforming one at that. Some embedded machines perform slight better in this mode but POWER servers do not. So for POWER you do not need this and should simply disable it. drpngx https github.com drpngx irving naml.us derek.murray gmail.com Is it OK to disable this test on power You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 11902 issuecomment 319284836 or mute the thread https github.com notifications unsubscribe auth AT SbfArRQalyQG2xvf5LBioWPYlLoWuks5sTsx4gaJpZM4OoGhK . Also SGTM If I recall correctly we also have a check in there to conditionally skip this test anyway. Fixed in PR https github.com tensorflow tensorflow pull 11939 
11970,Error when using dataset.map with num threads None, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes code attached below OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 TensorFlow installed from source or binary binary TensorFlow version use command below 1.2.1 Python version 3.5 Bazel version if compiling from source CUDA cuDNN version 8 5.1 GPU model and memory GeForce GTX 1080 8GB Exact command to reproduce just run the script below Describe the problem When using the tf.contrib.data API and applying a function to a dataset via dataset.map my function num threads 2 the following error occurs TypeError Input output buffer size of ParallelMapDataset Op has type int32 that does not match expected type of int64. Please note that num threads 2 is necessary to cause the error. From what I can see in MapDataset make dataset resource file dataset ops.py the if self. num threads is None triggers the call to gen dataset ops.parallel map dataset which then raises the error. I suspect a cast from int32 to int64 got lost somewhere in that function. Source code logs Full traceback ,Thank you for pointing that out. This bug has been fixed in TensorFlow 1.3.0 rc1. 
12003,top k doesn t order lower index first if two elements are equals, Bug If two elements are equal is not ordering them by index value. Reproducing the error Here is the command I am running And here is the output for Tensorflow 1.3.0 rc0 compared to version 1.2.1 Setup Running ,I was able to reproduce in version 1.3.0 rc0 and rc1 confirmed it changed from version 1.2 Opening internal bug. Thanks for the detailed report with simple repro example. Stable sort was not part of the API guarantee we changed it for the increased performance of unstable sort. The documentation claims sort stability https www.tensorflow.org api docs python tf nn top k poxvoculi thanks for pointing that out. We ll have to revert to stable sort. I believe this should be fixed now or in the very next push. Great thanks 
12011,Quantized graph not running with commit bb88ec7ecc4dc7ba72548a5115fb86e20b14de5b,OS Ubuntu 16.04 64bits Android Version 7.1 Nougat NDK Version android ndk r12b commit bb88ec7ecc4dc7ba72548a5115fb86e20b14de5b Author Alan Yee alyee ucsd.edu Date Mon Jul 24 22 46 38 2017 0700 LOG Earlier this error was not getting reported. Thanks,Did you confirm this works just before the commit and fails just after I was using 1.2 version. later I pulled to that commit id I am not sure from which commit id this error is happening. thanks Can you post a small reproducible case Quantize the inception v3 alexnet and try running benchmark app. cwhipkey Can you see what s going on with the RoundToSteps Op This is an old bug in the quantize graph.py script. We used to have a RoundToSteps op but it was removed but the script still inserts it. The quantize graph.py script is now obsolete though since it s been replaced by the Graph Transform Tool s quantize nodes rule https github.com tensorflow tensorflow tree master tensorflow tools graph transforms quantize nodes It is confusing though and I need to officially deprecate quantize graph.py and add a logging message point people to the right tool to use. petewarden any quick fix to get rid of this error It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. Nagging Assignee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Since the recommended fix is to use the more up to date script closing this one for now. I m also hoping we ll have some improved documentation on quantization soon. 
12022,Error exporting TensorForestEstimator model for serving, Problem I am trying to host a TensorForestEstimator model on Google Cloud s ML Engine. Everything works right but at the very end the model fails to export with stack trace Based on that trace I m thinking the error is in the make export strategy function with default output alternative key None . So what I did is set default output alternative key default but then got the error So this shows that there are no output alternatives and my model is single headed. Here is the code This seems like a bug but I could be wrong. System information ,We ran into this issue as well. I don t think your problem is related to default output alternative key being None rather it looks like TensorForestEstimator currently populates model ops.predictions only if a keys column is provided 1 . Without model ops.predictions populated default outputs in saved model export will lack a default key 2 . As a result this will make the model end up with no heads and eventually the export will fail. TLDR Populating keys column in TensorForestEstimator should fix your problem. That being said it s definitely a bug if an optional argument is really required for the model to function properly. So I would leave this issue open. As a side I did not really understand why requiring keys name 3 to be part of the incoming feature columns list after all it s not necessary a feature and then pop it 1 https github.com tensorflow tensorflow blob master tensorflow contrib tensor forest client random forest.py L235 2 https github.com tensorflow tensorflow blob master tensorflow contrib learn python learn utils saved model export utils.py L217 3 https github.com tensorflow tensorflow blob master tensorflow contrib tensor forest client random forest.py L157 Should also mention that the workaround populating key column only helps with tensorflow 1.3 . It looks like upgrading to tensorflow 1.3 fixed the issue. I did not have to even populate keys name the graph was saved as normal after training evaluation. However I need a fix for tensorflow 1.2 since I am using Google Cloud ML Engine and 1.2 is the highest version supported. caisq are you able to take a look justinshapiro Are you able to resolve it now Any temporary wayout It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. Nagging Awaiting TensorFlower It has been 14 days with no activityand the awaiting tensorflower label was assigned. Please update the label and or status accordingly. Nagging Awaiting TensorFlower It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. Nagging Awaiting TensorFlower It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. Nagging Awaiting TensorFlower It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. Nagging Awaiting TensorFlower It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. Nagging TensorFlower It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. Nagging Assignee skye It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. I m assuming this is resolved now. Please comment or reopen if I m mistaken. 
12059,tfdbg does not work with sparse tensors, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 14.04 TensorFlow installed from source or binary binary TensorFlow version use command below v1.2.0 5 g435cdfc 1.2.1 Python version Python 2.7.6 Bazel version if compiling from source CUDA cuDNN version 8.0.61 5.1.10 GPU model and memory Nvidia TITAN X Pascal 12G Exact command to reproduce python sparse debug.py debug Describe the problem There seems to be a bug using tensorflow debugger with sparse tensors. Below is just a simple example it fails when run with or without the debug option. It works when LocalCLIDebugWrapperSession line is removed. This prevents the use of the debugger while using sparse placeholders unless I m missing something. This issue https github.com tensorflow tensorflow issues 6110 also reports the same error but isn t related to tfdbg. Source code logs sparse debug.py Traceback ,Thanks for reporting this issue. I can reproduce it at HEAD. Will work on it. Why is it closed The above example gives me the same error. I am using Python 3.5.2. Below is what shows after running tf env collect.sh cat etc issue Linux test machine 4.4.0 79 generic 100 Ubuntu SMP Wed May 17 19 58 14 UTC 2017 x86 64 x86 64 x86 64 GNU Linux VERSION 16.04.3 LTS Xenial Xerus VERSION ID 16.04 VERSION CODENAME xenial are we in docker No compiler c Ubuntu 5.4.0 6ubuntu1 16.04.4 5.4.0 20160609 Copyright C 2015 Free Software Foundation Inc. This is free software see the source for copying conditions. There is NO warranty not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. uname a Linux test machine 4.4.0 79 generic 100 Ubuntu SMP Wed May 17 19 58 14 UTC 2017 x86 64 x86 64 x86 64 GNU Linux check pips numpy 1.13.1 protobuf 3.4.0 tensorflow 1.3.0 tensorflow tensorboard 0.1.6 check for virtualenv True tensorflow import tf.VERSION 1.3.0 tf.GIT VERSION v1.3.0 rc2 20 g0787eee tf.COMPILER VERSION v1.3.0 rc2 20 g0787eee Sanity check array 1 dtype int32 env LD LIBRARY PATH is unset DYLD LIBRARY PATH is unset nvidia smi . tf env collect.sh line 105 nvidia smi command not found cuda libs xiaohan2012 I can confirm that. Sorry about the inconvenience. Will push a fix soon. This is a regression related to https github.com tensorflow tensorflow commit 73a8755cdfd7bd74d626eee35cc58a3ddb2198e6 diff d04e2893c412aa1a1765a1c8b53158a6 Sorry I don t understand. Is that a fix No. That was just a note about why this bug occurred again. Stay tuned for a fix. is it fixed lnisre yes. The referred commit above should have fixed this issue. This fix is available in the latest python pip release of TensorFlow. 
12205,BUG TypeError in DNNClassifier.eval when using same name for feature in feature engineering fn, Describe the problem If we use the same key to replace a feature tensorflow might throw TypeError when evaluating eg When features is dict it is a mutable object. Hence the bug is caused by evaluate method which runs feature engineering fn again see code https github.com facaiy tensorflow blob c7b80d51da4fb6d51ea54a0bdf2601afa379d60c tensorflow contrib learn python learn estimators estimator.py L1165 . I ll open a PR later. ,
12372,tf.contrib.data.Dataset does not correctly handle nested dictionaries, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 14.04 TensorFlow installed from source or binary binary TensorFlow version use command below v1.3.0 rc2 20 g0787eee Python version 2.7 Bazel version if compiling from source CUDA cuDNN version GPU model and memory Exact command to reproduce Describe the problem tf.contrib.data.Dataset objects do not correctly deal with nested dictionary structures. When using a dataset with a nested dictionary the inner dictionaries are replaced with the first tensor in that inner dictionary and following tensors are restored for incorrect keys. This is not an issue with tf.contrib.framework.nest only with datasets which appear to instead use tensorflow.contrib.data.python.util.nest . The particular difference causing the bug appears to be https github.com tensorflow tensorflow blob r1.3 tensorflow python util nest.py L279 vs. https github.com tensorflow tensorflow blob r1.3 tensorflow contrib data python util nest.py L184 Source code logs ,Thanks for pointing out this bug and helping to narrow down the cause We re working on a fix presently. 
12396,tf Dataset with tf.py func doesn t work as the tutorial says, System information Ubuntu 16.04 TF 1.3 released version CUDA 8.0 CUDNN 6.0 Python 3.5 Describe the problem Dataset with tf.py func doesn t work as the dataset tutorial says in https www.tensorflow.org programmers guide datasets Source code logs minimum code to reproduce the problem. ,Thanks for reporting this problem. It s definitely a bug and we re working on a fix. In the meantime you can work around it with the following tweak The problem stems from treating the list returned by tf.py func as an invitation to auto stack the returned tensors into a single tensor. Converting it to tuple prevents that conversion from being attempted. The root cause is that the interpretation of lists in a Dataset nested structure changed between TF 1.2 and 1.3.... Thanks for pointing out this trick. It works now when converting it to tuple. 1 
12414,tf Dataset Iterator console flood when using CUDA builds, System information Have I written custom code Yes OS Platform and Distribution Manjaro linux Arch Linux repo TensorFlow installed from source or binary binary TensorFlow version 1.3.0 Python version 3.6.2 Bazel version CUDA cuDNN version cuda 8.0.61 cudnn 7.0.1 cudnn6 6.0.21 GPU model and memory Nvidia 1080 GTX 8GB Exact command to reproduce The problem When using a tensorflow wheel built with cuda support my app prints the following warning message at the end of a training epoch 2017 08 19 14 01 18.214060 W tensorflow core framework op kernel.cc 1192 Out of range End of sequence Node IteratorGetNext IteratorGetNext output shapes 132 output types DT FLOAT DT INT64 DT INT64 DT INT64 device job localhost replica 0 task 0 cpu 0 Iterator The code trains a seq2seq model and I assume the message gets printed somewhere downstream of seq2seq.dynamic decode. The message still gets printed even when the NN cells are not wrapped with a tf.contrib.rnn.DeviceWrapper with device field indicating a GPU only works fine on non cuda builds. All of this happens while the code is protected with the try except statements Now the only cheeky thing is that I am using the binaries from Arch Linux repositories but these are far from being dodgy. python tensorflow https www.archlinux.org packages community x86 64 python tensorflow python tensorflow cuda https www.archlinux.org packages community x86 64 python tensorflow cuda The build script https git.archlinux.org svntogit community.git tree trunk PKGBUILD h packages tensorflow This problem was in tensorflow 1.2 and persists in tensorflow 1.3. Also tested on a laptop without dedicated gpu but same OS and packages works fine. ,this is a side effect of using tf.contrib.data iterators with small batches as you are doing for inference . mrry any way to disable the logging in a configurable way nothing to do with dynamic rnn . Hi ebrevdo by small batches you refer to the size of most batches or the size of the last one The warning is there in training too for any batch size apparently including the case when the batch size evenly divides the training examples. I thought it has something to do with dynamic decode because simply iterating through the dataset and printing it to the console for example works fine. How is the dataset defined I think there s an issue in the 1.3 release when you have a Dataset.map that contains a tf.py func that ultimately raises StopIteration ... essentially it gets reported as an errors OutOfRange and logged rather than end of sequence true which exits silently. I recently noticed fixed that bug in an internal branch and it should appear in the nightlies soon. parse function is a Python function does tf.py func get called internally The problem also existed in TF 1.2 CUDA build only What s the source of parse function parse function takes an Example and an int applies tf.parse example returns a tuple of one tf.float32 and three tf.int64 the context and the features Do you get the logged message at every run call or only every once in a while On Aug 19 2017 10 36 AM georgesterpu notifications github.com wrote parse function takes an Example and an int applies tf.parse example returns a tuple of one tf.float32 and three tf.int64 the context and the features You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 12414 issuecomment 323537053 or mute the thread https github.com notifications unsubscribe auth ABtimyk5kfuqOjYeuXaKV0Vs53xuJeyjks5sZx0AgaJpZM4O8Vau . I get it at the end of each training epoch. If using BeamSearchDecoder with attention it gets printed multiple times depending on the beam width. One workaround that I could find is stopping the loop after the last batch if I know in advance their number instead of using the infinite loop and stopping on exception Here is the content of my parse function couldn t make it work as a tf.py func though with the tuple trick indicated by mrry I can confirm the same issue on a couple different Ubuntu versions and some unknown Linux distro Python 3.5 and 3.6 TensorFlow 1.2 and 1.3 cudnn 5.1 and cudnn 6 and on Telsa P100 GeForce GTX 1070 and Pascal Titan X. I get the message at the end of each epoch on both training and dev set. Here s my dataset definition I use a batch size of 8 and shuffle the training set but not the dev set but the warning shows up in all cases . What happens to your data once it is fetched and parsed by the iterator How much do you need Here s how it starts Never mind I made a wrong assumption. Looking at the first comment of mrry the issue might be fixed by now in tf nightly 1.4. I d be willing to test it out I m not sure what the git model is that s used for TF but if I compile from master and test that will that suffice Maybe try pip install tf nightly in a python 3.5 environment first On 20 September 2017 at 19 49 Aditya Bhargava notifications github.com wrote I d be willing to test it out I m not sure what the git model is that s used for TF but if I compile from master and test that will that suffice You are receiving this because you authored the thread. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 12414 issuecomment 330944089 or mute the thread https github.com notifications unsubscribe auth AFvUy5NVYYdUGT27gEMpRICMnviwrgVEks5skV4igaJpZM4O8Vau . This issue is specific to the GPU builds and the TF README.md says the nightlies are CPU builds only has that changed I might not be the right person to answer this but you are correct in my opinion my mistake. Please try compiling from sources. On 20 September 2017 at 20 44 Aditya Bhargava notifications github.com wrote This issue is specific to the GPU builds and the TF README.md says the nightlies are CPU builds only has that changed You are receiving this because you authored the thread. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 12414 issuecomment 330958913 or mute the thread https github.com notifications unsubscribe auth AFvUyxWHvrV01K6gEzSeArThvl3RYE2mks5skWsAgaJpZM4O8Vau . I just cloned master yesterday and compiled that and gave it a shot but the error still shows up for me with that latest build. Not sure how it was for georgesterpu but I actually get multiple copies of the message On my dev set which is smaller I just get one I meet the same problem and fix by flowing. tr data Dataset.from tensor slices train images train labels .repeat .batch 100 Just add .repeat . fumihwh That won t work for me as I need to know when the epoch is done. rightaditya how about this with .repeat . I know your problem now and use following to avoid it. If I do not use this I got same error log with yours. I have just cloned the master repo and compiled from sources. This time with cuDNN 7 Warning still there fumihwh Is your dataset size divisible by your batch size Mine isn t so I d probably have to ceil the division and make sure to use float division if using python2 . Still it s not an ideal solution given the way the API seems to have been designed. fumihwh Is the .repeat function only suitable for training I suppose it s not suitable for testing as I only want to evaluate once will the make one shot iterator only run the data set once even if my dataset size is NOT divisible by my batch size My test seems that the evaluation would go infinite loops when using .repeat . I encountered the same error as described by previous users during testing but not in training. During evaluation there are thousands of Out of range End of sequence errors I guess the number of errors are the same as the number of my evaluation samples . But correct evaluation results are still printed out after those errors and the program did not crash and can still continue training. Anyone knows the reason and how to fix it Thank you. I used estimator and the input functions are def input fn mode Input function which provides a single batch for train or eval. dataset tf.contrib.data.Dataset.from tensor slices filenames mode if mode tf.estimator.ModeKeys.TRAIN dataset dataset.shuffle buffer size SHUFFLE BUFFER dataset dataset.flat map tf.contrib.data.TFRecordDataset if mode tf.estimator.ModeKeys.TRAIN dataset dataset.repeat dataset dataset.map lambda value dataset parser value mode num threads FLAGS.map threads output buffer size FLAGS.batch size if mode tf.estimator.ModeKeys.TRAIN dataset dataset.shuffle buffer size SHUFFLE BUFFER iterator dataset.batch FLAGS.batch size .make one shot iterator images labels iterator.get next return images labels My model function is DEVICE LIST gpu 0 gpu 1 def imagenet model fn features labels mode Our model fn for ResNet to be used with our Estimator. tf.summary.image images features max outputs 6 with tf.device cpu 0 split batch tf.split features len DEVICE LIST split labels tf.split labels len DEVICE LIST if mode tf.estimator.ModeKeys.TRAIN global step tf.train.get or create global step Multiply the learning rate by 0.1 at 30 60 120 and 150 epochs. boundaries int batches per epoch epoch for epoch in 30 60 120 150 values INITIAL LEARNING RATE decay for decay in 1 0.1 0.01 1e 3 1e 4 learning rate tf.train.piecewise constant tf.cast global step tf.int32 boundaries values Create a tensor named learning rate for logging purposes. tf.identity learning rate name learning rate tf.summary.scalar learning rate learning rate optimizer tf.train.MomentumOptimizer learning rate learning rate momentum MOMENTUM tower grads tower cross entropy tower reg loss tower preds with tf.variable scope tf.get variable scope for dev idx device device features device labels in enumerate zip DEVICE LIST split batch split labels with tf.device device with tf.name scope device d dev idx logits network inputs device features is training mode tf.estimator.ModeKeys.TRAIN tf.get variable scope .reuse variables tower pred classes tf.argmax logits axis 1 probabilities tf.nn.softmax logits name softmax tensor tower preds.append tower pred cross entropy tf.losses.softmax cross entropy logits logits onehot labels device labels tower cross entropy.append cross entropy reg loss FLAGS.weight decay len DEVICE LIST tf.add n tf.nn.l2 loss v for v in tf.trainable variables tower reg loss.append reg loss loss cross entropy reg loss if mode tf.estimator.ModeKeys.TRAIN grads optimizer.compute gradients loss tower grads.append grads predictions classes tf.concat p classes for p in tower preds axis 0 probabilities tf.concat p probabilities for p in tower preds axis 0 if mode tf.estimator.ModeKeys.PREDICT return tf.estimator.EstimatorSpec mode mode predictions predictions cross entropy tf.add n tower cross entropy tf.identity cross entropy name cross entropy tf.summary.scalar cross entropy cross entropy reg loss tf.add n tower reg loss tf.summary.scalar reg loss reg loss loss cross entropy reg loss tf.summary.scalar total loss loss accuracy tf.metrics.accuracy tf.argmax labels axis 1 predictions classes metrics accuracy accuracy if mode tf.estimator.ModeKeys.TRAIN tf.identity accuracy 1 name train accuracy tf.summary.scalar train accuracy accuracy 1 Batch norm requires update ops to be added as a train op dependency. update ops tf.get collection tf.GraphKeys.UPDATE OPS with tf.control dependencies update ops grads average gradients tower grads train op optimizer.apply gradients grads global step global step else train op None return tf.estimator.EstimatorSpec mode mode predictions predictions loss loss train op train op eval metric ops metrics I get flooded the same way using today s tf nightly gpu wheel binary on Ubuntu 16.04 Python 2.7 cudnn 6. I get the same warning with tensorflow 1.3 I am experiencing this with tensorflow gpu on Windows 10 Python 3.6.1 cudnn64 6 using tf.slim. Still need to verify this but it seems the message only comes up when training for the first time without a checkpoint. The dataset size is divisible by the batch size. Same problem with TF1.3 Same problem. I got a bunch of error logs at the end of each epoch. Very annoying. 2017 10 31 18 59 22.473230 W tensorflow core framework op kernel.cc 1192 Out of range End of sequence Node IteratorGetNext IteratorGetNext output shapes 120 120 3 120 120 1 output types DT FLOAT DT UINT8 device job localhost replica 0 task 0 cpu 0 Iterator Same problem with TF 1.4 built from source CUDA 9.0 cuDNN 7.0 Ubuntu 16.04 Python 3.5 
12436,zeros like doesn t fully respect the optimize argument, The definition of zeros like https github.com tensorflow tensorflow blob master tensorflow python ops array ops.py L1463 is We can see that if the shape of tensor is already known the optimize parameter is ignored which is inconsistent with the documented behavior https www.tensorflow.org api docs python tf zeros like .,I think optimize flag should be respected. Added a PR 12459 for it. benoitsteiner could you please update this issue upon concluding review of 12459 Thanks 
12739,TF 1.3 keras TimeDistributed wrapper issue rnn got an unexpected keyword argument input length ,When I use TimeDistributed wrapper from keras I m getting unexpected keyword argument input length System Info Windows 10 TF 1.3.0 Python 3.5 Code Exception Traceback most recent call last File D PlayGround Git Coding2Fun Blog DeepLearning Quora NLP model.py line 103 in module nn.train input fn train input fn steps 100 File D Programs Anaconda envs tensorflow lib site packages tensorflow python estimator estimator.py line 241 in train loss self. train model input fn input fn hooks hooks File D Programs Anaconda envs tensorflow lib site packages tensorflow python estimator estimator.py line 630 in train model model fn lib.ModeKeys.TRAIN File D Programs Anaconda envs tensorflow lib site packages tensorflow python estimator estimator.py line 615 in call model fn model fn results self. model fn features features kwargs File D PlayGround Git Coding2Fun Blog DeepLearning Quora NLP model.py line 38 in model fn q1 tf.contrib.keras.layers.TimeDistributed layers.Dense EMBEDDING DIM activation relu q1 File D Programs Anaconda envs tensorflow lib site packages tensorflow contrib keras python keras engine topology.py line 396 in call output super Layer self . call inputs kwargs File D Programs Anaconda envs tensorflow lib site packages tensorflow python layers base.py line 450 in call outputs self.call inputs args kwargs File D Programs Anaconda envs tensorflow lib site packages tensorflow contrib keras python keras layers wrappers.py line 208 in call unroll False TypeError rnn got an unexpected keyword argument input length , fchollet It looks like a bug fix to wrappers.py https github.com tensorflow tensorflow blob r1.3 tensorflow contrib keras python keras layers wrappers.py made in https github.com tensorflow tensorflow commit fa83a270c943317da6b07a3d093c224be0827bd9 might need to be cherry picked onto r1.3. This has been fixed in the new release. 
13190,TF AddGradients gradients returns wrong result when multiple outputs specified, System information Darwin Mac Admin.local 15.6.0 Darwin Kernel Version 15.6.0 Thu Jun 23 18 25 34 PDT 2016 root xnu 3248.60.10 1 RELEASE X86 64 x86 64 Mac OS X 10.11.6 Describe the problem Hi. I ve added a unit test for TF AddGradients API see code below which is similar to this python test https github.com tensorflow tensorflow blob ca3bc0f1c2f917cf6e7c49d58f5ec604a9af9367 tensorflow python ops gradients test.py L337 In the test I provide two outputs y 0 x 0 2 y 1 y 0 8 where input x 0 3. According to the documentation https github.com tensorflow tensorflow blob 03619fab3f4dd6f28b67418455a953b0fccdd9bf tensorflow c c api.h L1018 result should be calculated by formula d y 1 y 2 ... dx 1 and be equal to 17502 but the API prints 6. What am I missing Thanks. Source code logs , suharshs skye Mind taking a look This indeed seems strange. I can reproduce this in python as well along with some other examples to try to narrow it down. It seems that building up x 8 by using intermediate tensors via multiplication results in errors. Curiously using tf.pow y0 vs y0 y0 y0 y0 results in different gradients when paired with y0. I don t know the issue yet but will take another look tomorrow. Dorokhov please take another look. I was mistaken in my previous comments. I went ahead and deleted my old comments so as to not cause more confusion Huh looks like I was using a old version of tensorflow. The issue in python was resolved here https github.com tensorflow tensorflow commit fb56fc90167c3919cb59f753f233ef2a41469cb2 diff 7d33d81b07b3cb1679ed1b011a66e447 I will send a similar fix for C . Thanks. 
13258,small mistake,https github.com tensorflow tensorflow blob b46340f40fe5e2ec9bfcd385b07cfb914055fb51 tensorflow contrib distributions python ops mvn full covariance.py L48 this should be only y not y 2, vsaase Can you fill a template https github.com tensorflow tensorflow blob master ISSUE TEMPLATE.md with good explanation of the pointed mistake along with the wiki links or substantial proof I can create PR for this one It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue Please update the label and or status accordingly. It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue Please update the label and or status accordingly. Nagging Awaiting Response It has been 14 days with no activityand the awaiting response label was assigned. Is this still an issue Nagging Awaiting Response It has been 14 days with no activityand the awaiting response label was assigned. Is this still an issue Nagging Awaiting Response It has been 14 days with no activityand the awaiting response label was assigned. Is this still an issue Nagging Awaiting Response It has been 14 days with no activityand the awaiting response label was assigned. Is this still an issue It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue vsaase is right. Thanks for pointing out this mistake. We will include the fix in the next release. It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue It has been 28 days with no activity and the awaiting response label was assigned. Is this still an issue It has been 43 days with no activity and the awaiting response label was assigned. Is this still an issue 
13289,Bug SDCAModel missbehave bug when mixing sparse and dense features, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Up to date Arch Linux TensorFlow installed from source or binary Binary TensorFlow version use command below 1.3.0 Python version 3.6 Bazel version if compiling from source CUDA cuDNN version GPU model and memory Exact command to reproduce Any toy problem using https www.tensorflow.org api docs python tf contrib linear optimizer SdcaModel SDCAModel that mixes sparse and dense features. Describe the problem It s a bug. When using both sparse and dense features in SDCAModel https www.tensorflow.org api docs python tf contrib linear optimizer SdcaModel the linear predictions method output a tensor of wrong shape. This happens because the sparse results and dense results ranks are inconsistent in the current version and summing them trigger an unexpected broadcast. https github.com tensorflow tensorflow pull 13279 fixes the bug. ,It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. Nagging Assignee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Fixed by https github.com tensorflow tensorflow pull 13279 
13336,Cannot assign a device for operation save ShardedFilename 1 when exporting custom Estimator, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux ubuntu254 4.2.0 42 generic 49 14.04.1 Ubuntu SMP Wed Jun 29 20 22 11 UTC 2016 x86 64 x86 64 x86 64 GNU Linux TensorFlow installed from source or binary pip TensorFlow version use command below tf.VERSION 1.3.0 Python version 2.7.6 Bazel version if compiling from source CUDA cuDNN version release 8.0 V8.0.44 6.0 GPU model and memory GTX Titan Black 6 GB GTX 1060 6 GB Exact command to reproduce run estimator CNN.py Describe the problem Continued from this discussion https groups.google.com a tensorflow.org forum searchin discuss export discuss XHABHQG5l2I jZBvc0 NBgAJ . I want to export a custom Estimator multiple CNN layer CTC loss in multi GPU setting derived from cifar 10 multi GPU example using export savemodel . But i encountered this error InvalidArgumentError see above for traceback Cannot assign a device for operation save ShardedFilename 1 Could not satisfy explicit device specification device GPU 0 because no supported kernel for GPU devices is available. this should not occured in 1.3.0 please see discussion Source code logs environment tf env.txt https github.com tensorflow tensorflow files 1336111 tf env.txt full error trace error.txt https github.com tensorflow tensorflow files 1336110 error.txt source code TF bug.zip https github.com tensorflow tensorflow files 1336108 TF bug.zip , ispirmustafa isaprykin Looks like the sharded saver on GPU bug is back. Sharded savers have a bunch of string processing ops and cannot live on GPUs. If they re forced onto GPUs by collocation they break in this way. I haven t looked at all the code yet so I m not positive yet but I think we have to fix Saver. UPDATE I have tried to export another custom Estimator Recurrent CNN in the same machine trained with single GPU with latest tensorflow 1.4.0 and export it successfully. However if I add with tf.device device GPU 0 to the model function the same error occurs even if the training just use a single GPU. This sounds something that likely got fixed here https github.com tensorflow tensorflow commit bf05a2eef97863fc78778bcde5987f93af8a7598 . I think this should go away if you try the latest source version https www.tensorflow.org install install sources or wait for 1.5. It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. Sure tensorflowbutler. I m pretty sure it s fixed in that commit I pointed to. Hello it seems that my confirmation reply didn t posted correctly and i just realized that now I m sorry. as isaprykin suggested building from source was working. However when I building latest tensorflow serving from source with GPU support this exact error show up again. from what i know tensorflow serving will pull latest tensorflow nighly so this fix will have been merged right I use different machine when i try this It only use a single GTX 1070 this time. is it because I load a 2 GPU model in a single GPU machine I don t know if this is a tensorflow issue or serving issue. I will create new issue in a appropriate repo if needed. Thank You. dieka13 when do you encounter that error When you run export savedmodel Or when you load it into tensorflow serving martinwicke when I load it into tensorflow serving. no problem when running export savedmodel now. I also run into another similar device assignment issue in 17149 Then I believe this is best filed as an issue to tensorflow serving. nfiedel FYI. This is key the key log message Could not satisfy explicit device specification device GPU 0 because no supported kernel for GPU devices is available . The binary you are trying to run most likely does not link in the GPU kernels. The supported build targets tensorflow model server model servers BUILD and pre built binaries installable via apt get currently support CPU with and without AVX but not yet GPU. If you want to try adding a tensorflow gpu model server that is probably the next step in sorting this out. You are also welcome to file a feature request under serving. Thanks nfiedel but i used tensorflow serving built with is that still the case dieka13 Am not 100 certain but pretty sure you need both the config as well as ops to be linked in. dieka13 I also encounter the same error Cannot assign a device for operation save SaveV2 Could not satisfy explicit device specification device GPU 0 because no supported kernel for GPU devices is available. Is there any solution as of now 
13351,TensorFlow variable initializers broken,Three symptoms observed with models after upgrading tensorflow 1. must feed placeholder error or 2. things hang with 100 utilization inside python build initializer expr 3. things succeed but sess.run call takes 100x slower than before I believe this is due to this commit https github.com tensorflow tensorflow commit 07adc2ea910de715d31e16a019fcbcccb575e931 Because the following work around restores good behavior Here s a self contained repro https github.com yaroslavvb stuff blob master tf initializer bug report.py It works fine in tensorflow 1.2 or with the fix in latest version is throws Sorry I didn t make the repro smaller I lost motivation after finding the quick fix ,cc benoitsteiner alextp TimSalimans who is also affected Just tested with tf nightly and it s still broken. I believe this bug is also responsible for deadlock during data dependent init in tim s models. To reproduce If you comment out the top block which essentially reverts https github.com tensorflow tensorflow commit 07adc2ea910de715d31e16a019fcbcccb575e931 then things work again and you see following output asimshankar josh11b It s this issue again. Apparently it broke recently. I didn t know that. There is an internal bug for this which is being worked on I agree with your analysis that https github.com tensorflow tensorflow commit 07adc2ea910de715d31e16a019fcbcccb575e931 is the culprit and that introduced likely multiple bugs. The description of that commit when a variable w is initialized with the value of another variable v make sure that the initializer of v is run first. would have been fine but the actual code change instead redirects references to v in w s initializer to v s initial value graph. This has at least a couple of problems there are some bugs in the code e.g. if there is a while loop it can follow the cycle in the graph forever if v was already initialized and has since been updated w should get the new value not the original value Ideally we would rewrite the logic to be less buggy scan w s initializer init w for references to variables being careful to avoid visiting nodes in a graph cycle more than once replace w s initialize op with if v has not been initialized run v s initialization op once that is done run w w s initialization expression without changes in particular without the rewrite to use v s initial value I m starting to push through changes to fix this. The first one is a refactoring to make subsequent changes easier as well as avoiding infinite recursion when the initial value contains cycles note that it also changes the name of build initiazier expr to try guard against uninitialized depenencies to clarify what it does . I ll now attempt to address more edge cases. Here s what I believe is a minimal reproduction of the bad behaviour in this case The transformation done by build initializer expr makes v1 depend on v0.initialized value which is a cond of the form Because p isn t actually created inside false fn it will always be evaluated when the cond is which is inconsistent with the intended behaviour. If you were to run that session.run v1.initializer statement and feed p it would work but we shouldn t expect the user to do that . This adds to the list of edge cases that I m aware of where build initializer expr behaves poorly 1 This issues of extraneous propagation of placeholders. 2 Cyclic initializers now fixed by having it ignore those graphs . 3 When TF functions are involved. For example 4 With most stateful ops. For example 5 If you have an initializer where there is an intermediate op X which takes an argument that must be a reference type and that argument is a variable. This algorithm will turn the reference type into a value type and cause an error. For example 6 Not a bug as such but the implementation will produce an exponentially large number of nodes given a diamond pattern dependency graph now fixed by memoizing 7 The implementation completely ignores control dependencies. Any new ops created simply don t have them. 8 The implementation completely ignores control flow contexts. Any new ops created based on ops in a control flow context aren t added to that context. 9 The implementation will only be able to find a Variable s initialized value method if that variable is present in the GLOBAL VARIABLES or LOCAL VARIABLES collection. This is a weird inconsistency to the end user. It s even possible it could find the wrong Variable although you d have to be working really hard to get it to do that . 10 The implementation relies on a hard coded set of known variable ops. If new variable ops are added it won t recognize them without manual intervention. 11 Even if the caller explicitly passes in a .initialized value to a Variable s constructor the build initializer expr will probably create a bunch of new nodes in the graph anyway. 12 It currently doesn t even do anything when the initial value depends on a ResourceVariable. There are probably even more that I m not currently aware of. I ll start working through these in the coming days. I m working under the assumption that we can t just remove this transformation since there will already be a significant number of graphs which depend on it. That means most of the work will be to whitelist and or blacklist the set of initial value subgraphs for which we can safely apply the transformation. Working this feature into existing client code is complicated I wonder if it would be much simpler to add it as new functionality. Since dependent initialization has never worked in TF this can be a new op like VariableV5 which can eventually replace Variable. The example here https gist.github.com yaroslavvb d592394c0cedd32513f8fbb87ca05938 worked for our code in OpenAI and the idea there was to make variable reads have control dependency on safe initializer . Safe initializer is an assign op that only runs when variable has not been initialized. PS motivation for the breaking commit is https github.com tensorflow tensorflow issues 4920 djvisentin I think this is fixed right Are there any updates on this I just tried depending variables e.g. which did not work. After initialization sess.run var gave me the correct values but sess.run var.initialized value raised an error because of missing some placeholder . Well as a workaround I just convert the variable to a numpy array and initialize using the numpy array it feels as inelegant as it looks but it s the simplest way I ve found Hoeze That s not pretty but simple. Nevertheless imho this should be solved. A dependency tree initializing variables would be much more pretty. Nagging Assignee djvisentin It has been 152 days with no activity and this issue has an assignee. Please update the label and or status accordingly. If you use tf.function in tf v2 this will not be a problem so this is how we plan on fully resolving this issue. 
13430,Unstable numerics in MvNormal KL,https github.com tensorflow tensorflow blame 0cfb16e025b3d20e8c8aca431fc0887814817c44 tensorflow contrib distributions python ops mvn linear operator.py L302 This line should look like this It yields NaN in grad KL q p if q p , jvdillon langmore Mind taking a look Thanks for alerting me. I will take a look Fixed. Should hit master tomorrow or the day after. Thanks for your suggested fix Not sure why linalg.norm doesnt work that seems like the real bug. It doesn t work because gradient is unstable function and graph is not simplified 
13431,Windows nightly build Dataset.from generator fails with pyfunc error, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 windows 7 TensorFlow installed from source or binary pip TensorFlow version 1.4.0 dev20170929 Python version 3.5.2 Bazel version if compiling from source CUDA cuDNN version GPU model and memory Exact command to reproduce see below Describe the problem As described in the SO question https stackoverflow.com q 46511328 281545 the code fails with That s a problem in windows nightly installing the nightly on an Ubuntu machine works EDIT Maybe related to https github.com tensorflow tensorflow issues 8196 ,Yes I think is a result of np.array x returning a different array type when x is a Python int on Windows and Linux. If I remember correctly on Windows the array will have type np.int32 and on Linux the array will have type np.int64 . This behavior in Dataset.from generator is inherited from tf.py func which performs the NumPy conversion automatically as discussed in 8196 . To write platform independent code that handles this case you should explicitly wrap the return value in a NumPy array. On second thoughts... I think this is really easy to fix. I m working on a patch. 
13506,tf.image.pad to bounding box crashes when passed bounds with dtype int64, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 TensorFlow installed from source or binary from pip in virtualenv TensorFlow version use command below v1.3.0 rc2 20 g0787eee 1.3.0 Python version 3.5.2 default Nov 17 2016 17 05 23 n GCC 5.4.0 20160609 Bazel version if compiling from source CUDA cuDNN version GPU model and memory Exact command to reproduce Description Passing arguments of type int64 to tf.image.pad to bounding box triggers a crash of the python interpreter. This is a bug because the type required by tf.image.pad to bounding box not documented anywhere and just causes a crash with a cryptic error message. Sources Logs The following snippet crashes the whole python interpreter with a core dump. import tensorflow as tf i tf.constant 0 0 3 3 dtype tf.int64 img tf.ones 1 1 1 dtype tf.float32 sess tf.Session sess.run tf.image.pad to bounding box img i 0 i 1 i 2 i 3 And leaves the following 2017 10 05 13 51 24.789715 F tensorflow core framework tensor.cc 493 Check failed dtype expected dtype 9 vs. 3 ,this is bad it shouldn t crash Python even if type is wrong The crash is caused by the fact that int64 was never registered in the kernel of tf.image.pad to bounding box . Instead the int64 of the Tpadding is directly used as int32 without appropriate processing. A PR 13517 has been created to address the crash. Thanks for debugging and the PR yongtang Marking as Contributions Welcome though it seems the contribution has already been sent 
13536,BeamSearchDecoder incorrectly truncates results when used with dynamic decode, System information irrelevant for this bug Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 Any TensorFlow installed from source or binary binary TensorFlow version use command below v1.3.0 rc2 20 g0787eee 1.3.0 Python version Python 3.5.2 Continuum Analytics Inc. Bazel version if compiling from source N A CUDA cuDNN version irrelevant GPU model and memory irrelevant Exact command to reproduce irrelevant Describe the problem tf.contrib.seq2seq.BeamSearchDecoder incorrectly truncates some of the results because the same index was previously used for a beam member that ended at a earlier step. The root of the problem is that the while loop body in dynamic decode assumes that sequences are independent and will finish only once. In the same time BeamSearchDecoder creates a tree like structure where a beam index can be reused in a later step for a state that originates from a different parent index. This causes the decoding loop to sometimes record the wrong sequence length for a beam member. Then this wrong sequence length is passed to BeamSearchDecoder.finalize which returns a truncated sequence. Source code logs I use the following code to workaround the problem. This causes the right sequence to be returned but still the length returned by dynamic decode is wrong. , ebrevdo Can you take a look I see that you wrote the seq2seq library. I wanted to submit a fix but I don t see how to correct this problem without changing some of the library s public inteface. Seems ok to update the BeamSearchDecoder.finalize to use final state.lengths instead of sequence lengths looks like this fixes a couple of other open issues. We could consider having finalize return new updated sequence lengths to decode dynamic as well. Thanks for catching this Could you send a PR with the fix and a unit test that catches it Will look into submitting a fix. Sorry I ve been meaning to make a PR last week but never got to it. No problem. We re evaluating your change internally. On Sat Oct 14 2017 7 17 PM bdaskalov notifications github.com wrote Sorry I ve been meaning to make a PR last week but never got to it. You are receiving this because you were assigned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 13536 issuecomment 336681065 or mute the thread https github.com notifications unsubscribe auth ABtimxRoy93pElky6ZtzF ctLOu6Khocks5ssWs8gaJpZM4PxBUM . The problem is deeper and the solution requires some additional changes. I ll try to submit something in the next couple days. Could anyone tell me when this bug was fixed. I couldn t find it in the release notes. Thank you ebrevdo It was first released in TensorFlow 1.5. guillaumekln Thank you for the info 
13827,Tensorflow 1.3 tf.constant with dtype float32 float64 float16 may have inconsistent behavior., System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04 with docker running gcr.io tensorflow tensorflow latest TensorFlow installed from source or binary NA TensorFlow version use command below v1.3.0 rc2 20 g0787eee 1.3.0 Python version 2.7 Bazel version if compiling from source NA CUDA cuDNN version NA GPU model and memory NA Exact command to reproduce Describe the problem A tensorflow constant with None in array with dtype float32 float64 seem to throw an error. However if they are first wrapped by a numpy array none is accepted and turned into NaN. This behavior seems inconsistent., CC MarkDaoust That s unfortunate. Would you like to contribute a fix I d love to take a stab at contributing. From a brief glance i m wondering if the convert to tensor function may be culprit. Glad to take any pointers as well. So here is what I ve found after looking through the code. This error is propagated from tensorflow version 0.5.0 to 1.4.0 rc0 tf.float16 None values pass without any problems because they are not checked in TF TO IS OK https github.com tensorflow tensorflow blob r1.3 tensorflow python framework tensor util.py L273 L291 . None values are thus not stopped by the AssertCompatible https github.com tensorflow tensorflow blob r1.3 tensorflow python framework tensor util.py L376 call where they continue onward to be transformed into nan values by np.array https github.com tensorflow tensorflow blob r1.3 tensorflow python framework tensor util.py L377 For float32 and float64 types None values are caught by the AssertCompatible https github.com tensorflow tensorflow blob r1.3 tensorflow python framework tensor util.py L294 L303 and since they are not an instance of compat.real types https github.com tensorflow tensorflow blob r1.3 tensorflow python framework tensor util.py L243 numbers.Real np.integer np.floating a TypeError is raised. So while the following produces an error Using explicit nan will not cause an error Since the convert to tensor abstraction states the following are equivalent https github.com tensorflow tensorflow blob r1.3 tensorflow python framework ops.py L575 L578 I believe that the below should be equivalent as well Given what has been found I would like to 1. Update FilterFloat https github.com tensorflow tensorflow blob r1.3 tensorflow python framework tensor util.py L240 L243 to allow for None values. 2. Add tf.float16 into the TF TO IS OK https github.com tensorflow tensorflow blob r1.3 tensorflow python framework tensor util.py L273 L291 dictionary with the FilterFloat function 3. Add a test into ops test.py https github.com tensorflow tensorflow blob r1.3 tensorflow python framework ops test.py named testConvertToTensorFloatNoneValue Please let me know if this is a good plan assuming nothing breaks. Thanks Ouwen This is the best kind of bug report. I m not an expert on this part of TensorFlow but this all looks pretty reasonable. Please send a PR after you fix that missing self . Appreciate it MarkDaoust here is the PR https github.com tensorflow tensorflow pull 13834 It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue Please update the label and or status accordingly. This is fixed by 13834. 
13980,NadamOptimizer throws InvalidArgumentError incompatible shapes , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes but this issue can be replicated by changing one line in the example script word2vec basic.py OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux CentOS 7 MacOS High Sierra TensorFlow installed from source or binary binary TensorFlow version use command below v1.4.0 rc0 21 g1e25994 1.4.0 rc1 . I also know someone who encountered this in the 1.3 release. Python version 2.7.10 2.7.14 Bazel version if compiling from source n a CUDA cuDNN version 8.0 6.0 GPU model and memory Titan X 12gb Exact command to reproduce Change line 190 in tensorflow examples tutorials word2vec word2vec basic.py to optimizer tf.contrib.opt.NadamOptimizer 1.0 .minimize loss . Run. Describe the problem When I attempt to train a model with tf.controb.opt.NadamOptimizer TensorFlow crashes with an InvalidArgumentError Incompatible shapes 105 vs. 50000 which is thrown from the optimizer in my code it s when trying to apply updates to the word embedding table. It looks like it s trying to apply sparse updates to the embeddings using dense operations causing a shape mismatch or something along those lines. AdamOptimizer and LazyAdamOptimizer work fine. Source code logs Traceback can be found here https gist.github.com strubell 37897084888252989750b58260f76ff5 . The error is easily replicable in the example script word2vec basic.py . , alisidd can you look into this skye sorry for the delayed response. Yes I can look into it. Nagging Assignee rohan100jain It has been 276 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee rohan100jain It has been 291 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Seems like there is a PR that should fix this issue. Lets wait for that to go in. strubell For tf.contrib.opt.NadamOptimizer you would need to modify more code base to make it function. In this case you can use optimizer tf.train.AdamOptimizer 0.01 .minimize loss which should work and serve the similar purpose. 
14116,py func cannot handle Chinese string correctly,tensorflow 1.3.0 I write a simple code to split and concatenate utf8 string. However I found that only English string works well on python 2.7. script logs python 2.7 success for English failed for Chinese. python 3.5.2 both failed. ,Your example will work if the u prefixes are removed from strings. An important thing to note is that tf.py func needs to turn Python strings into tensors before calling the func. We have a byte string tensor but no unicode tensor. Things could still probably be improved on our end. It s likely we re forgetting to call compat.as bytes somewhere in our py func implementation which is causing Python s default behavior of crash and burn if not ASCII to shine through. Assigning to panyx0718 who might be able to help us. Thanks for your quick reply jart . The u prefixes is intended to show that py func cannot handle unicode string correctly. I indeed find that byte string always works well However the workground really burdens users. It could be better if py func can handle unicode string as well. Thank you. I agree facaiy. While there is a workaround this issue has still been triaged as a bug. Once we find the specific lines that need to be updated it should be a relatively straightforward fix. I believe this kind of polishing is worth doing because I can see how this could lead to mild frustration. I believe everyone should have positive experiences with TensorFlow and I m grateful that you took the time to bring this to our attention. It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. Closing as this is resolved feel free to reopen if problem persists. 
14143,Passing clear devices True to SavedModelBuilder.add meta graph and variables loses function definitions,The SavedModelBuilder inadvertently strips all function definitions from the MetaGraphDef when clear devices True . See this gist for a repro thanks eggie5 for finding it https gist.github.com eggie5 f0838a1289ca851aa5a72593575b7f06 The bug appears to stem from export meta graph which builds a GraphDef by selective field copying when certain options such as clear devices True are passed. Passing clear devices False enables the code to succeed. However it breaks compatibility between tf.data and potentially other code and SavedModel so we should find a sustainable fix. sukritiramesh Can you please take a look Thanks ,
14243,SpaceToDepthGrad and DepthToSpaceGrad are not aware of data format, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 TensorFlow installed from source or binary binary TensorFlow version use command below v1.4.0 rc1 11 g130a514 1.4.0 Python version 3.5 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version CUDA 8.0 cuDNN 6 GPU model and memory Exact command to reproduce tf.depth to space and tf.space to depth support data format NCHW on GPU. However SpaceToDepthGrad https github.com tensorflow tensorflow blob r1.4 tensorflow python ops array grad.py L626 and DepthToSpaceGrad https github.com tensorflow tensorflow blob r1.4 tensorflow python ops array grad.py L633 are not aware of data format . Maybe they would need to propagate op.get attr data format . Source code logs ,Thanks for the report this indeed seems to be an oversight in https github.com tensorflow tensorflow commit 4d69d0408da946096163ee1d8ea068ae6698ae9d which introduced data format . yzhwang wujingyue Could you take a look 
14533,Tensorflow Website XSRF Token missing or incorrect,The error occurs when I read current TF API documentation. My default lang is set to Russian but it does not matter and has influence only on the bottom menu . But when I m trying to change the language to English or Chinese I get the following error message XSRF Token missing or incorrect Is it ok or this is the TF Website bug Windows 10 Firefox 56.0.2 x64 1setlang https user images.githubusercontent.com 7415950 32767632 4ab78b0e c925 11e7 8cff 6843af74ef71.png 2xsrf https user images.githubusercontent.com 7415950 32767633 4ada17c8 c925 11e7 8e82 4f2bd322350a.PNG , MarkDaoust is this your territory and can you please advise or reassign This one is over my head maybe wolffg knows who to send it to. It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. I can t repro this on my machine. You probably shouldn t be given the option for Russian as we have no Russian pages. I would expect that to be a bug possibly transient. Is it still happening Do you still have the option for Russian If so which page is it appearing on This seems like a bug as evidenced by curl H Accept Language ru https www.tensorflow.org . It s now been brought to the attention of the right people internally in b 70993883. We can hopefully have this fixed sometime soon but I m going to close this issue out just to get it out of triage queue. Thank you for bringing this to our attention. 
14542, Model object has no attribute container nodes , Problem Output Environment System Ubuntu 16.04 Tensorflow gpu bin v1.4.0 rc1 11 g130a514 1.4.0 , model to dot does not work since commit 3599fd4. A minor fix seems works. 
14776,tf.keras.estimator.estimator from model does not respect options set in RunConfig, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 TensorFlow installed from source or binary binary TensorFlow version use command below tf.VERSION 1.4.0 tf.GIT VERSION v1.4.0 rc1 11 g130a514 Python version 2.7.12 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version 8.0.61 6.0.21 GPU model and memory NVIDIA Tesla M60 8 GB Exact command to reproduce See Below Describe the problem When trying to use an estimator that is derived from and training with setting in the of does not cause the settings to be respected when passed to the estimator from model function. For example setting does not decrease the memory allocated to the process on the GPU similarly setting continues to allocate all of the memory and does not allow memory growth. I also tested this with the canned estimator and the settings were applied as expected when the RunConfig was passed to the estimator. Below is code to demonstrate this issue. Source code logs Minimal example runs to completion and trains successfully. But changing the GPUOptions settings does not cause the GPU memory to be utilized as expected , fchollet yifeif May be related to 14504 yifeif could you please take a look. ispirmustafa any idea The run config is passed in directly when creating the keras version of the Estimator. Do we need to pass these configurations anywhere else It should not be related with Keras code. It should work since it is handled within Estimator. droidicus could you please print est keras.config and est keras.config.cluster spec.as dict Sure thing here is the output Code to reproduce same as the source in the origional issue above but with print statements after the creation of the keras estimator and full log output are avaliable here https gist.github.com droidicus 146532eacf88ed57538bb41a8fc7da4b It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. Gentle ping this is still an issue for me. Hi shivaniag Estimator sends the given session config directly to the Session constructor. Could you please assign somebody who is more familiar with tf.Session and it s handling of GPU settings FYI I ve checked the keras.model to estimator. It s sending the config properly to tf.estimator.Estimator. tf.estimator.Estimator sends that config to tf.train.SessionManager calls which uses it as a constructor argument to tf.Session. Just as an FYI while this is still a problem in TFv1.5rc0 we were able to do the following as a workaround for now by setting the default session manually the memory fraction is respected yifeif fchollet is there any place within underlying Keras code that uses default session while building the graph train op ... A member of the TensorFlow organization has replied after the stat awaiting tensorflower label was applied. Nagging Assignee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Sorry for the delay. This https github.com tensorflow tensorflow blob master tensorflow python keras impl keras estimator.py L147 is how model to estimator create its model fn. ispirmustafa anything you see with session that should be done differently Thanks Also tried print out estimator.config. dict and estimator.config.cluster spec. dict for canned estimator custom estimator and keras converted estimator and I m seeing the same results est keras.config. dict save checkpoints secs 600 session config gpu options per process gpu memory fraction 0.5 keep checkpoint max 5 task type worker is chief True cluster spec tensorflow.python.training.server lib.ClusterSpec object at 0x7f8e3ebcabd0 save checkpoints steps None keep checkpoint every n hours 10000 service None num ps replicas 0 tf random seed None master num worker replicas 1 task id 0 log step count steps 100 model dir tmp tmp8jvdNt save summary steps 100 est keras.config.cluster spec. dict cluster def cluster spec Nagging Assignees yifeif shivaniag ispirmustafa It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. A fix has been submitted internally and should make to master tomorrow. Thanks Fantastic thanks 
14818,Error when setting model dir for tf.keras.estimator.estimator from model , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 TensorFlow installed from source or binary binary TensorFlow version use command below tf.VERSION 1.4.0 tf.GIT VERSION v1.4.0 rc1 11 g130a514 Python version 2.7.12 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version 8.0.61 6.0.21 GPU model and memory NVIDIA Tesla M60 8 GB Exact command to reproduce See Below Describe the problem When trying to use an estimator that is derived from and training with setting either in the or in causes a to be thrown. If the model dir is not set then a tmp directory is used as expected and the training is completed successfully. I also tested this with the canned estimator and the settings were applied as expected when the RunConfig was passed to the estimator or the model dir passed to the estimator directly. Below is code to demonstrate this issue. Source code logs Minimal example NotFoundError is thrown Error generated , fchollet yifeif May be related to 14504 and or 14776 yifeif could you please take a look at this. Thanks for reporting the issue droidicus. This should be fixed with 14354. 
14819,Keras Dropout support masking gets reset to False, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac OS X 10.12.6 TensorFlow installed from source or binary binary TensorFlow version use command below v1.4.0 rc1 11 g130a514 1.4.0 Python version 3.6.1 Bazel version if compiling from source n a GCC Compiler version if compiling from source n a CUDA cuDNN version n a GPU model and memory n a Exact command to reproduce see below You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION Describe the problem Describe the problem clearly here. Be sure to convey here why it s a bug in TensorFlow or a feature request. The Keras Dropout layer constructor tensorflow python keras impl keras layers core.py sets support masking True and then calls its super constructor which sets it back to False. Other layers defined in that module appear to set support masking True after the super constructor call. Source code logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem. ,Sounds like a mistake cc fchollet https github.com tensorflow tensorflow blob ab0fcaceda001825654424bf18e8a8e0f8d39df2 tensorflow python keras impl keras layers core.py L107 L113 Correct that s a bug self.supports masking True should be after the call to the parent s constructor. I can work on it. I ll fix it later. Fine. It would be better if you can add a corresponding test case here https github.com tensorflow tensorflow blob ab0fcaceda001825654424bf18e8a8e0f8d39df2 tensorflow python keras impl keras layers core test.py L38 okay facaiy please check if unit test added is valid. Thanks 
14913, BUG argparse Argument Parser is not working in nightly build, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 TensorFlow installed from source or binary pip install tf nightly gpu TensorFlow version use command below v1.3.0 rc1 5211 gab0fcac 1.5.0 dev20171125 Python version 3.5.3 Bazel version if compiling from source n a GCC Compiler version if compiling from source n a CUDA cuDNN version 9.0 7.0 GPU model and memory Titan Xp 12Gb Exact command to reproduce python test.py myarg buzz Describe the problem Nightly build is not handing correctly arguments passed to the script. The arguments are parsed correctly in the official 1.4 version. Source code ,2652704b576adc16b4d735f651cea1024e88b72e changed the behavior of tf.app.run to look at sys.argv and attempt to parse the flags using absl.flags . As a workaround you can pass an empty list of args to tf.app.run martinwicke yilei This seems like it might be a breaking change for existing users of argparse . Should we find a way to make this optional before it gets into a release Yeah that is a problem. yilei we do need to maintain backwards compatibility for the behavior of tf.app. . Maybe we need to extend the wrapper to be ugly. We should also deprecate tf.app and tf.flags altogether. They are included for historical reasons TF has no business containing those modules. Looks like tf.app.run needs to continue ignoring unknown flags i.e. unfix 11195 However does the example code work before the change tf.app.run will call main with positional arguments but here the main function has no arguments and it s called manually. Did you mean the following def main argv parser argparse.ArgumentParser description argsparse test for tensorflow nightly build parser.add argument myarg type str help fizz or buzz default fizz args parser.parse args if name main tf.app.run I m asking since I m wondering if the positional arguments passed to main by tf.app.run is important or not. If it s not important we can simply swallow the absl.flags parsing exception in tf.app.run and pass sys.argv 1 to main . Otherwise we need to add an extra API to absl.flags . yilei yes the example code works with tensorflow 1.4. I call main manually just to show that the arguments are parsed by python. I wouldn t call it in a usual usecase. I don t think I would want any arguments from tf.app.run . mrry Unfortunately the work around doesn t work This however works and this doesn t The correct workaround should be tf.app.run argv sys.argv 1 not tf.app.run argv sys.argv 1 yilei you are right but surprisingly it works in both cases. Hmm I think it worked in your example since sys.argv 1 is myarg buzz myargv is treated as the program name and it only tries to parse buzz which only contains positional arguments. dantkz I tried your example in 1.4 but I got this yilei Oops my tensorflow 1.4 example had def main argv sorry. The bug is still there with def main argv . Sorry for the bug in the workaround. Perhaps the following would work to ensure that the executable name is preserved It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. I believe this has been fixed with https github.com tensorflow tensorflow commit f38f92eb369b9cbb12b2c8bd0006d7fa1c64c5c0 
14916,tf.nn.softmax input dim 1 name None argument dim cannot take negative index other than 1 ,I expect that in the function tf.nn.softmax input dim 1 name None the argument dim can take dim either positive or negative. However it seems the only negative index can be taken is 1 ,Added a PR 14920 for the fix. yongtang thanks for looking into fixing this. 
14932,tf.squared difference does not work with complex dtypes, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes the provided minimal example OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 TensorFlow installed from source or binary binary tensorflow gpu TensorFlow version use command below 1.4.0 Python version 3.6 CUDA cuDNN version 8 6 GPU model and memory GeForce GTX 980 4GB Describe the problem tf.squared difference does not work with complex dtypes tf.complex64 and tf.complex128 although the documentation says so https www.tensorflow.org api docs python tf squared difference. Source code logs Minimal not working example The error output ,Added a PR 14939 for the fix. 
14985,tf.nn.fractional max pool output have same batch size when feed with different input batch size, Describe the problem tf.nn.fractional max pool output have same batch size when feed with different input batch size. Attached is test code I write. 2 different input is feed in with different batch size outputs get same batch size. pool test.py.txt https github.com tensorflow tensorflow files 1516498 pool test.py.txt code result shape of input a 3 32 32 3 shape of output a 3 21 21 3 shape of input b 4 32 32 3 shape of output b 3 21 21 3 System information cat etc issue Linux c 1080u 4.10.0 40 generic 44 16.04.1 Ubuntu SMP Thu Nov 9 15 37 44 UTC 2017 x86 64 x86 64 x86 64 GNU Linux VERSION 16.04.3 LTS Xenial Xerus VERSION ID 16.04 VERSION CODENAME xenial are we in docker No compiler c Ubuntu 5.4.0 6ubuntu1 16.04.5 5.4.0 20160609 Copyright C 2015 Free Software Foundation Inc. This is free software see the source for copying conditions. There is NO warranty not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. uname a Linux c 1080u 4.10.0 40 generic 44 16.04.1 Ubuntu SMP Thu Nov 9 15 37 44 UTC 2017 x86 64 x86 64 x86 64 GNU Linux check pips numpy 1.13.3 numpydoc 0.7.0 check for virtualenv False tensorflow import Traceback most recent call last File string line 1 in module ModuleNotFoundError No module named tensorflow env LD LIBRARY PATH usr local cuda 8.0 lib64 DYLD LIBRARY PATH is unset nvidia smi Thu Nov 30 11 55 40 2017 NVIDIA SMI 384.90 Driver Version 384.90 GPU Name Persistence M Bus Id Disp.A Volatile Uncorr. ECC Fan Temp Perf Pwr Usage Cap Memory Usage GPU Util Compute M. 0 GeForce GTX 108... Off 00000000 01 00.0 On N A 0 51C P8 21W 280W 860MiB 11169MiB 9 Default Processes GPU Memory GPU PID Type Process name Usage 0 1060 G usr lib xorg Xorg 542MiB 0 1540 G compiz 315MiB cuda libs usr local cuda 8.0 doc man man7 libcudart.7 usr local cuda 8.0 doc man man7 libcudart.so.7 usr local cuda 8.0 targets x86 64 linux lib libcudart static.a usr local cuda 8.0 targets x86 64 linux lib libcudart.so.8.0.61 cat etc issue Linux c 1080u 4.10.0 40 generic 44 16.04.1 Ubuntu SMP Thu Nov 9 15 37 44 UTC 2017 x86 64 x86 64 x86 64 GNU Linux VERSION 16.04.3 LTS Xenial Xerus VERSION ID 16.04 VERSION CODENAME xenial are we in docker No compiler c Ubuntu 5.4.0 6ubuntu1 16.04.5 5.4.0 20160609 Copyright C 2015 Free Software Foundation Inc. This is free software see the source for copying conditions. There is NO warranty not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. uname a Linux c 1080u 4.10.0 40 generic 44 16.04.1 Ubuntu SMP Thu Nov 9 15 37 44 UTC 2017 x86 64 x86 64 x86 64 GNU Linux check pips numpy 1.13.3 protobuf 3.5.0.post1 tensorflow gpu 1.4.0 tensorflow tensorboard 0.4.0rc3 check for virtualenv False tensorflow import tf.VERSION 1.4.0 tf.GIT VERSION v1.4.0 rc1 11 g130a514 tf.COMPILER VERSION v1.4.0 rc1 11 g130a514 Sanity check array 1 dtype int32 env LD LIBRARY PATH usr local cuda 8.0 lib64 DYLD LIBRARY PATH is unset nvidia smi Thu Nov 30 11 56 18 2017 NVIDIA SMI 384.90 Driver Version 384.90 GPU Name Persistence M Bus Id Disp.A Volatile Uncorr. ECC Fan Temp Perf Pwr Usage Cap Memory Usage GPU Util Compute M. 0 GeForce GTX 108... Off 00000000 01 00.0 On N A 0 51C P0 80W 280W 860MiB 11169MiB 0 Default Processes GPU Memory GPU PID Type Process name Usage 0 1060 G usr lib xorg Xorg 542MiB 0 1540 G compiz 315MiB cuda libs usr local cuda 8.0 doc man man7 libcudart.7 usr local cuda 8.0 doc man man7 libcudart.so.7 usr local cuda 8.0 targets x86 64 linux lib libcudart static.a usr local cuda 8.0 targets x86 64 linux lib libcudart.so.8.0.61 , balconychy Thanks for the clear bug description Looks like weiranzhao wrote the code and josh11b might have reviewed it. Can one of you take a look at this For convenience here s the repro program that balconychy created It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and or status accordingly. Thanks for the detailed example. I am working on a fix. Will update the thread when it is checked in to master branch. balconychy This should have been fixed. I checked this with nightly build at tf nightly 1.6.0.dev20180109. 
15019,Tensorflow how to save restore tf.data.Dataset ,I made a model with tf.data.Dataset as a data IO function then i exported the graph and tried to restore it with meta graph file But it failed and following error messages occurred. I think that tf.data.Dataset made a C object instead of python queue used before. And the graph def only has a C object handler reference so the graph def alone without real C object can t load complete graph. How can I load a executable graph with tf.data.Dataset Or is it impossible for now In short all the tensorflow graphs without tf.data.Dataset work when i add following codes. But the graphs with tf.data.Dataset make a error message above ,Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case or leave them as N A Thanks. Have I written custom code OS Platform and Distribution TensorFlow installed from TensorFlow version Bazel version CUDA cuDNN version GPU model and memory Exact command to reproduce I have a similar issue I m trying to load a trained model for inference. In the code snippet below features 0 is the name of one of the tensors returned by dataset iterator. Is it possible to directly feed the tensor without having to initialize the iterator Code Error taehyunkim1527 Can you share a complete reproducible example of the problem I was unable to reproduce the problem with the code fragment in your example although it was possible to reproduce it by adding arguments to tf.train.export meta graph . I am in the process of fixing the latter problems but it would be great to confirm that the fix works for your issue too. suryasumukh I think you re running into a different problem. You certainly can feed values over the tensors returned from Iterator.get next but you need to ensure that you feed all of the tensors returned by the iterator that might be used in the run call. If you continue to have problems with this please post a question on Stack Overflow. mrry When i use that series of code of exporting and importing in tensorflow benchmarks s ImageNet tasks I experienced this error message. I tried to implement a simple code to make the same error message today but it failed. I think it is not a problem of tf.data.Dataset and let me debug again. Thank you for your kindness. Thanks for confirming that the problem doesn t arise with that exact code I have a change in the pipeline that will make this path work in more cases e.g. when using clear devices True or a scope prefix so I ll reopen this issue until it lands. mrry Hi I m who opened this question formerly. And i found what was the problem. if we give arg clear devices as True for the function export scoped meta graph of python framework meta graph.py It starts to make empty graph def and copy previous node defs newly. In this process this code does not consider about graph def.library . But the information about map function resides in graph def.library suryasumukh In order to retrain the pre trained model the initializer of data iterator can be declared as a tf.operation with a name while training for the first time. Then it can be sess.run with the name and fed with training data. sess.run Data itr init feed dict Model in 0 train X Model out 0 train Y suryasumukh In order to retrain the pre trained model the initializer of data iterator can be declared as a tf.operation with a name while training for the first time. Then it can be sess.run with the name and fed with training data. sess.run Data itr init feed dict Model in 0 train X Model out 0 train Y Not the point 
15178,Dataset.from generator doesn t play nice with feature column.categorical column with vocabulary list, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux 4.4.0 98 generic 121 14.04.1 Ubuntu SMP Wed Oct 11 11 54 55 UTC 2017 x86 64 x86 64 x86 64 GNU Linux TensorFlow installed from source or binary binary TensorFlow version use command below v1.4.0 rc1 11 g130a514 1.4.0 Python version Python 2.7.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Exact command to reproduce curl L o predict not working.py https drive.google.com uc authuser 0 id 1KHCNOfJOpEFSLzzJqjtP6oy0EvuLvETE export download python predict not working.py Describe the problem I have the same code in two versions one using Dataset.from generator https drive.google.com open id 1KHCNOfJOpEFSLzzJqjtP6oy0EvuLvETE and one using Dataset.from tensor slices https drive.google.com open id 1BFmIEnpuRWPeghUfeBD4225BXZqvLcmS . To me it looks like the created Datasets have the exact same content but feature column.categorical column with vocabulary list doesn t work with the from generator one which ought to be a bug right Source code logs Sources are in links above.,It looks like the problem stems from feature column.categorical column with vocabulary list requiring a defined rank for its input. By default Dataset.from generator does not know anything about the shapes of the tensors that it generates but you can work around this by passing output shapes when you create it. The following input function worked for me Ah good workaround thanks ispirmustafa could you take a look It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. This is fixed at head. 
15239,No gradient defined for op Pow, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 x64 TensorFlow installed from source or binary binary TensorFlow version use command below 1.4 Bazel version N A Python version None CUDA cuDNN version None GPU model and memory None Exact command to reproduce Describe the problem It seems there is no gradient defined for the Pow operation in the C API. I am actually transferring this issue from https github.com migueldeicaza TensorFlowSharp issues 187. Similar to the case of Select 14845 it seems there is also no gradient for the Pow operation in the C API., cesarsouza I have created a PR 15245 to resolve the issue. Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case or leave them as N A Thanks. Bazel version Huge thanks facaiy Also I ve just noticed that I used the wrong title for the issue probably it seemed confusing at first. Sorry about it and thanks again for the extremely fast PR Is this issue fixed now. Because i get the same error with 1.9 C C Tensorflow is useles if you want to build your model. The PR has been merged so I think the issue is fixed. Could you give an example 
15410,Calling tf.contrib.lite.toco convert results in global name tempfile is not defined error,Please go to Stack Overflow for help and support https stackoverflow.com questions tagged tensorflow Related SO post https stackoverflow.com questions 47645056 tensorflow lite toco python apl nameerror name tempfile is not defined If you open a GitHub issue here is our policy 1. It must be a bug or a feature request. 2. The form below must be filled out. 3. It shouldn t be a TensorBoard issue. Those go here https github.com tensorflow tensorboard issues . Here s why we have that policy TensorFlow developers respond to issues. We want to focus on work that benefits the whole community e.g. fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem rather than being redirected to Stack Overflow. System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Followed example on Tensorflow Lite documentation OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOS High Sierra TensorFlow installed from source or binary Source TensorFlow version use command below v1.3.0 rc1 5910 ge2174cc943 1.4.0 Python version 2.7.10 Bazel version if compiling from source bazel release 0.5.4 homebrew GCC Compiler version if compiling from source Apple LLVM version 9.0.0 clang 900.0.34.1 CUDA cuDNN version N A CPU only GPU model and memory N A Exact command to reproduce You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION Describe the problem Describe the problem clearly here. Be sure to convey here why it s a bug in TensorFlow or a feature request. Opened python on terminal and ran Resulting output is As a sanity check I ran the following Output was var folders hd 7yc9wgwj5wvd43 d94b4m47m0000gn T tmpxxdYMY Source code logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.,Same here. It also throws this error with Python 3.6.3 Anaconda custom 64 bit I believe the bug has been fixed in master branch could you test it on tf nightly https github.com tensorflow tensorflow installation Hello facaiy I am using the master branch nightly version . But the problem is still there at least for high sierra Sorry about this. It seems it has to do with the interface sealing in tensorflow where we try to prevent exposing symbols unrelated to the interfaces . You can work around this with the following code I ll make a more permanent fix soon. Thanks for the workaround It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. Nagging Assignee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. I tried your way but its giving error. . . Kindly help. . . tf. version 1.6.0rc1 and Ubuntu ERROR is as follows Converted 2 variables to const ops. Traceback most recent call last File tf.py line 94 in module tflite model tf.contrib.lite.toco convert sess.graph def x mxc File home siteurl anaconda3 envs osrco lib python3.6 site packages tensorflow contrib lite python lite.py line 212 in toco convert input data.SerializeToString File home siteurl anaconda3 envs osrco lib python3.6 site packages tensorflow contrib lite python lite.py line 134 in toco convert protos stdout stderr RuntimeError TOCO failed see console for info. b 2018 03 01 14 40 13.614956 I tensorflow contrib lite toco import tensorflow.cc 1171 Converting unsupported operation VariableV2 n2018 03 01 14 40 13.615016 I tensorflow contrib lite toco import tensorflow.cc 1171 Converting unsupported operation Assign n2018 03 01 14 40 13.615059 I tensorflow contrib lite toco import tensorflow.cc 1171 Converting unsupported operation VariableV2 n2018 03 01 14 40 13.615083 I tensorflow contrib lite toco import tensorflow.cc 1171 Converting unsupported operation Assign n2018 03 01 14 40 13.615118 I tensorflow contrib lite toco import tensorflow.cc 1171 Converting unsupported operation Log n2018 03 01 14 40 13.615212 I tensorflow contrib lite toco import tensorflow.cc 1171 Converting unsupported operation DynamicStitch n2018 03 01 14 40 13.615283 I tensorflow contrib lite toco import tensorflow.cc 1171 Converting unsupported operation Reciprocal n2018 03 01 14 40 13.615341 I tensorflow contrib lite toco import tensorflow.cc 1171 Converting unsupported operation BroadcastGradientArgs n2018 03 01 14 40 13.615384 F tensorflow contrib lite toco import tensorflow.cc 973 Check failed GetBoolAttr node transpose b false 1 vs. 0 nAborted core dumped n None CODE is as follows import tensorflow as tf import pandas as pd import numpy as np import tempfile import subprocess tf.contrib.lite.tempfile tempfile tf.contrib.lite.subprocess subprocess from tensorflow.python.tools import freeze graph from tensorflow.python.tools import optimize for inference lib print tf. version data pd.read csv iris.data names f1 f2 f3 f4 f5 s np.asarray 1 0 0 ve np.asarray 0 1 0 vi np.asarray 0 0 1 data f5 data f5 .map Iris setosa s Iris versicolor ve Iris virginica vi print data data data.iloc np.random.permutation len data print data data data.reset index drop True training data trainFeats data.ix 0 105 f1 f2 f3 f4 temp data f5 trainlabels temp 0 106 y tf.placeholder tf.float32 shape 106 3 weight and bias m tf.Variable tf.zeros 4 3 x tf.placeholder tf.float32 shape 106 4 name Input c tf.Variable tf.zeros 3 mxc tf.nn.softmax tf.matmul x m c name output loss tf.reduce mean tf.reduce sum y tf.log mxc reduction indices 1 train step tf.train.AdamOptimizer 0.01 .minimize loss sess tf.InteractiveSession init tf.initialize all variables sess.run init tf.contrib.tflite.Convert sess.graph def x mxc number of interations epoch 2000 for step in range epoch print sess.run train step loss feed dict x trainFeats y t for t in trainlabels.as matrix testData data.ix 130 f1 f2 f3 f4 testDataInFrormat testData.reshape 1 4 print sess.run tf.argmax mxc feed dict x testDataInFrormat tf.train.write graph sess.graph def pbtxtFiles savegraph.pbtxt as text True tf.train.Saver .save sess pbtxtFiles model.ckpt MODEL NAME iris input graph path pbtxtFiles savegraph.pbtxt checkpoint path pbtxtFiles model.ckpt input saver def path input binary False output node names output restore op name save restore all filename tensor name save Const 0 output frozen graph name pbtxtFiles frozen model MODEL NAME .pb output optimized graph name pbtxtFiles optimized inference model MODEL NAME .pb clear devices True freeze graph.freeze graph input graph path input saver def path input binary checkpoint path output node names restore op name filename tensor name output frozen graph name clear devices output graph def optimize for inference lib.optimize for inference sess.graph def Input an array of the input node s output an array of output nodes tf.float32.as datatype enum tflite model tf.contrib.lite.toco convert sess.graph def x mxc open wow.tflite w .write tflite model IRIS.DATA is this 5.1 3.5 1.4 0.2 Iris setosa 4.9 3.0 1.4 0.2 Iris setosa 4.7 3.2 1.3 0.2 Iris setosa 4.6 3.1 1.5 0.2 Iris setosa 5.0 3.6 1.4 0.2 Iris setosa 5.4 3.9 1.7 0.4 Iris setosa 4.6 3.4 1.4 0.3 Iris setosa 5.0 3.4 1.5 0.2 Iris setosa 4.4 2.9 1.4 0.2 Iris setosa 4.9 3.1 1.5 0.1 Iris setosa 5.4 3.7 1.5 0.2 Iris setosa 4.8 3.4 1.6 0.2 Iris setosa 4.8 3.0 1.4 0.1 Iris setosa 4.3 3.0 1.1 0.1 Iris setosa 5.8 4.0 1.2 0.2 Iris setosa 5.7 4.4 1.5 0.4 Iris setosa 5.4 3.9 1.3 0.4 Iris setosa 5.1 3.5 1.4 0.3 Iris setosa 5.7 3.8 1.7 0.3 Iris setosa 5.1 3.8 1.5 0.3 Iris setosa 5.4 3.4 1.7 0.2 Iris setosa 5.1 3.7 1.5 0.4 Iris setosa 4.6 3.6 1.0 0.2 Iris setosa 5.1 3.3 1.7 0.5 Iris setosa 4.8 3.4 1.9 0.2 Iris setosa 5.0 3.0 1.6 0.2 Iris setosa 5.0 3.4 1.6 0.4 Iris setosa 5.2 3.5 1.5 0.2 Iris setosa 5.2 3.4 1.4 0.2 Iris setosa 4.7 3.2 1.6 0.2 Iris setosa 4.8 3.1 1.6 0.2 Iris setosa 5.4 3.4 1.5 0.4 Iris setosa 5.2 4.1 1.5 0.1 Iris setosa 5.5 4.2 1.4 0.2 Iris setosa 4.9 3.1 1.5 0.1 Iris setosa 5.0 3.2 1.2 0.2 Iris setosa 5.5 3.5 1.3 0.2 Iris setosa 4.9 3.1 1.5 0.1 Iris setosa 4.4 3.0 1.3 0.2 Iris setosa 5.1 3.4 1.5 0.2 Iris setosa 5.0 3.5 1.3 0.3 Iris setosa 4.5 2.3 1.3 0.3 Iris setosa 4.4 3.2 1.3 0.2 Iris setosa 5.0 3.5 1.6 0.6 Iris setosa 5.1 3.8 1.9 0.4 Iris setosa 4.8 3.0 1.4 0.3 Iris setosa 5.1 3.8 1.6 0.2 Iris setosa 4.6 3.2 1.4 0.2 Iris setosa 5.3 3.7 1.5 0.2 Iris setosa 5.0 3.3 1.4 0.2 Iris setosa 7.0 3.2 4.7 1.4 Iris versicolor 6.4 3.2 4.5 1.5 Iris versicolor 6.9 3.1 4.9 1.5 Iris versicolor 5.5 2.3 4.0 1.3 Iris versicolor 6.5 2.8 4.6 1.5 Iris versicolor 5.7 2.8 4.5 1.3 Iris versicolor 6.3 3.3 4.7 1.6 Iris versicolor 4.9 2.4 3.3 1.0 Iris versicolor 6.6 2.9 4.6 1.3 Iris versicolor 5.2 2.7 3.9 1.4 Iris versicolor 5.0 2.0 3.5 1.0 Iris versicolor 5.9 3.0 4.2 1.5 Iris versicolor 6.0 2.2 4.0 1.0 Iris versicolor 6.1 2.9 4.7 1.4 Iris versicolor 5.6 2.9 3.6 1.3 Iris versicolor 6.7 3.1 4.4 1.4 Iris versicolor 5.6 3.0 4.5 1.5 Iris versicolor 5.8 2.7 4.1 1.0 Iris versicolor 6.2 2.2 4.5 1.5 Iris versicolor 5.6 2.5 3.9 1.1 Iris versicolor 5.9 3.2 4.8 1.8 Iris versicolor 6.1 2.8 4.0 1.3 Iris versicolor 6.3 2.5 4.9 1.5 Iris versicolor 6.1 2.8 4.7 1.2 Iris versicolor 6.4 2.9 4.3 1.3 Iris versicolor 6.6 3.0 4.4 1.4 Iris versicolor 6.8 2.8 4.8 1.4 Iris versicolor 6.7 3.0 5.0 1.7 Iris versicolor 6.0 2.9 4.5 1.5 Iris versicolor 5.7 2.6 3.5 1.0 Iris versicolor 5.5 2.4 3.8 1.1 Iris versicolor 5.5 2.4 3.7 1.0 Iris versicolor 5.8 2.7 3.9 1.2 Iris versicolor 6.0 2.7 5.1 1.6 Iris versicolor 5.4 3.0 4.5 1.5 Iris versicolor 6.0 3.4 4.5 1.6 Iris versicolor 6.7 3.1 4.7 1.5 Iris versicolor 6.3 2.3 4.4 1.3 Iris versicolor 5.6 3.0 4.1 1.3 Iris versicolor 5.5 2.5 4.0 1.3 Iris versicolor 5.5 2.6 4.4 1.2 Iris versicolor 6.1 3.0 4.6 1.4 Iris versicolor 5.8 2.6 4.0 1.2 Iris versicolor 5.0 2.3 3.3 1.0 Iris versicolor 5.6 2.7 4.2 1.3 Iris versicolor 5.7 3.0 4.2 1.2 Iris versicolor 5.7 2.9 4.2 1.3 Iris versicolor 6.2 2.9 4.3 1.3 Iris versicolor 5.1 2.5 3.0 1.1 Iris versicolor 5.7 2.8 4.1 1.3 Iris versicolor 6.3 3.3 6.0 2.5 Iris virginica 5.8 2.7 5.1 1.9 Iris virginica 7.1 3.0 5.9 2.1 Iris virginica 6.3 2.9 5.6 1.8 Iris virginica 6.5 3.0 5.8 2.2 Iris virginica 7.6 3.0 6.6 2.1 Iris virginica 4.9 2.5 4.5 1.7 Iris virginica 7.3 2.9 6.3 1.8 Iris virginica 6.7 2.5 5.8 1.8 Iris virginica 7.2 3.6 6.1 2.5 Iris virginica 6.5 3.2 5.1 2.0 Iris virginica 6.4 2.7 5.3 1.9 Iris virginica 6.8 3.0 5.5 2.1 Iris virginica 5.7 2.5 5.0 2.0 Iris virginica 5.8 2.8 5.1 2.4 Iris virginica 6.4 3.2 5.3 2.3 Iris virginica 6.5 3.0 5.5 1.8 Iris virginica 7.7 3.8 6.7 2.2 Iris virginica 7.7 2.6 6.9 2.3 Iris virginica 6.0 2.2 5.0 1.5 Iris virginica 6.9 3.2 5.7 2.3 Iris virginica 5.6 2.8 4.9 2.0 Iris virginica 7.7 2.8 6.7 2.0 Iris virginica 6.3 2.7 4.9 1.8 Iris virginica 6.7 3.3 5.7 2.1 Iris virginica 7.2 3.2 6.0 1.8 Iris virginica 6.2 2.8 4.8 1.8 Iris virginica 6.1 3.0 4.9 1.8 Iris virginica 6.4 2.8 5.6 2.1 Iris virginica 7.2 3.0 5.8 1.6 Iris virginica 7.4 2.8 6.1 1.9 Iris virginica 7.9 3.8 6.4 2.0 Iris virginica 6.4 2.8 5.6 2.2 Iris virginica 6.3 2.8 5.1 1.5 Iris virginica 6.1 2.6 5.6 1.4 Iris virginica 7.7 3.0 6.1 2.3 Iris virginica 6.3 3.4 5.6 2.4 Iris virginica 6.4 3.1 5.5 1.8 Iris virginica 6.0 3.0 4.8 1.8 Iris virginica 6.9 3.1 5.4 2.1 Iris virginica 6.7 3.1 5.6 2.4 Iris virginica 6.9 3.1 5.1 2.3 Iris virginica 5.8 2.7 5.1 1.9 Iris virginica 6.8 3.2 5.9 2.3 Iris virginica 6.7 3.3 5.7 2.5 Iris virginica 6.7 3.0 5.2 2.3 Iris virginica 6.3 2.5 5.0 1.9 Iris virginica 6.5 3.0 5.2 2.0 Iris virginica 6.2 3.4 5.4 2.3 Iris virginica 5.9 3.0 5.1 1.8 Iris virginica Kindly help MR. aselle Kindly help MR. facaiy Kindly help MR. leandroBorgesFerreira Kindly help MR. javierluraschi Kindly help MR. tensorflowbutler above code is not giving any output. . . wow.tflite is not even created. . . Please help. . . Nagging Assignee aselle It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue import tensorflow as tf img tf.placeholder name img dtype tf.float32 shape 1 64 64 3 val img tf.constant 1. 2. 3. tf.constant 1. 4. 4. out tf.identity val name out with tf.Session as sess tflite model tf.contrib.lite.toco convert sess.graph def img out open converteds model.tflite wb .write tflite model Traceback most recent call last File stdin line 2 in module File Library Python 2.7 site packages tensorflow contrib lite python lite.py line 198 in toco convert input data.SerializeToString File Library Python 2.7 site packages tensorflow contrib lite python lite.py line 91 in toco convert protos with tempfile.NamedTemporaryFile as fp toco NameError global name tempfile is not defined Any Update with this issue. I am facing same problem. The tempfile issue is fixed in the newest code but it wasn t released with TensorFlow 1.7.0 yet. You can choose to Do the workaround https github.com tensorflow tensorflow issues 15410 issuecomment 352189481 mentioned above. Use the TensorFlow nightly build pip uninstall tensorflow pip install tf nightly Build TensorFlow from source code https www.tensorflow.org install install sources Upgrade to the next TensorFlow release when it happens in the future. I m closing this bug as the original issue was fixed. If you see other model conversion issue please create a bug separately. Thanks 
15425,TypeError broadcast takes 1 positional argument but 2 were given, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow no OS Platform and Distribution e.g. Linux Ubuntu 16.04 TensorFlow installed from source or binary source TensorFlow version use command below b v1.3.0 rc1 6044 g0b80606 1.4.0 2 days ago Python version 3.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Exact command to reproduce Source code logs I ran tensorflow benchmarks and got the following error. The reason is that the invocation of nccl.broadcast is different from its signature https github.com tensorflow tensorflow blob 17e725c0558581cba19bd6c409698b2c3f88efe5 tensorflow contrib all reduce python all reduce.py L748 https github.com tensorflow tensorflow blob 17e725c0558581cba19bd6c409698b2c3f88efe5 tensorflow contrib nccl python ops nccl ops.py L173 L182 Problems still exists in current HEAD., chsigg contrib all reduce needs to be updated due to the contrib nccl API change made in https github.com tensorflow tensorflow commit ca8af1d0dbb605087a4f8ae076188f2b9a26b1ba which broke some benchmarks. cc poxvoculi 15642 same problem Nagging Awaiting TensorFlower It has been 14 days with no activityand the awaiting tensorflower label was assigned. Please update the label and or status accordingly. nccl.broadcast got changed without updating its uses. The immediate problem has been fixed in the internal build and should roll out with the next update. poxvoculi thanks very much how do i get this fix now one more question contrib all reduce not support embedding lookup that has IndexedSlice gradients can you give some advice The fix is easy you can patch your own client now if you re eager. In all reduce.py remove these lines dst devices per worker devices w 1 send op dst tensors nccl.broadcast level 2 output w dst devices NOTE need control dependency to ensure send op executes with ops.control dependencies send op with ops.device per worker devices w 0 dst tensors.insert 0 array ops.identity level 2 output w down values w dst tensors and replace with these lines dst tensors with ops.device per worker devices w 0 broadcast src nccl.broadcast array ops.identity level 2 output w for d in per worker devices w with ops.device d dst tensors.append array ops.identity broadcast src down values w dst tensors I m not expert at NCCL but I wouldn t be surprised if it only works on dense tensors of numeric types. IndexedSlices are a sparse representation right poxvoculi thanks i had already tried same fix with you before this issue. yesterday i tried your code but failed again at same error use tf benchmark variable update distributed all reduce all reduce spec nccl xring UnimplementedError see above for traceback This op should be replaced during graph optimization. caused by File python2.7 site packages tensorflow contrib all reduce python all reduce.py line 781 in build nccl then ring return build nccl hybrid input tensors red op upper level f File python2.7 site packages tensorflow contrib all reduce python all reduce.py line 749 in build nccl hybrid broadcast src nccl.broadcast array ops.identity level 2 output w File python2.7 site packages tensorflow contrib nccl python ops nccl ops.py line 187 in broadcast return gen nccl ops.nccl broadcast input tensor shape tensor.shape File python2.7 site packages tensorflow contrib nccl ops gen nccl ops.py line 98 in nccl broadcast NcclBroadcast input input shape shape name name which i guess rewrite graph maybe happen in nccl send recv op. IndexedSlices are a sparse representation right Yes excuse me for that i sayed that nccl in the issue above actually it s contrib all reduce module i fix the issue content error now thanks. Hi All I meet the same issue. Need your help. Thanks. Agoniii can you post detail thanks anpark I run tf cnn benchmarks with the commands variable update distributed all reduce all reduce spec nccl xring . My problem is almost as same as yours. I ve run into this UnimplementedError as well with 1.5. A small repro It looks like the registration of the optimization pass at this line never happens https github.com tensorflow tensorflow blob bf1fad214febef6af5c101d8f953d0109c46dfbb tensorflow contrib nccl kernels nccl rewrite.cc L270 This may be because nccl ops.so doesn t include kernels nccl rewrite.cc https github.com tensorflow tensorflow blob bf1fad214febef6af5c101d8f953d0109c46dfbb tensorflow contrib nccl BUILD L24 Unfortunately Bazel throws a fit when I try to add the source file to this target. chsigg can you clarify as to whether this test program is using contrib nccl in the anticipated way benbarsdell Great i add the same issue in tensorflow benchmarks repo but tfboyd closed it and ask me to run with tf1.5. Hope anyone can help Nagging Assignee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignees chsigg poxvoculi It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignees chsigg poxvoculi It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Reassigning to zheng xq Seems to be fixed already. ppwwyyxx which version do you use Have you tested it ok There might be other issues but at least my original one seems fixed. As I can run tensorflow benchmarks with TF1.6. failed again at same error use tf benchmark variable update distributed all reduce all reduce spec nccl xring TF1.6 ppwwyyxx what s your params 
15611, saved model cli.py bug fix ,In file python tools saved model cli.py at function def print tensor info tensor info The first line should be print dtype value key for key value in types pb2.DataType.items tensor info.dtype Not be print dtype types pb2.DataType.keyss tensor info.dtype because tensor info.dtype is an Integer which is the value of types not the index of type values .,It s not necessary to do that types pb2.DataType is not python dict . The order of EnumTypeWrapper.keys is not arbitrary. See the definition of EnumTypeWrapper.keys and self. enum type.values is something like When I run tfdbg The type of v1 is DT INT REF and saved model cli.py throws exception. Yes my mistake for REF type the value is not the same as its index. Can you add a minimum repuducible code and its error log to better describe this problem This is a simple example to show the usage of saved model cli Then run saved model cli in shell The result is I change tensorflow python tools saved model cli. print tensor info like this By the way I use python3.6 ubuntu 16.04 tensorflow 1.4 Thanks for the supplement. I am not sure who is responsible for this module friendly ping yifeif drpngx CC sukritiramesh Thanks huaxz1986 Will fix this internally 
15766,tf.assert equal raises incorrect traceback in Eager mode, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04.1 LTS TensorFlow installed from source or binary pip binary TensorFlow version use command below 1.5.0 dev20171227 Python version 3.5.0 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version None GPU model and memory None Exact command to reproduce python main.py Describe the problem In eager mode tf.assert equal only shows in traceback message when two inputs are different. However in graph mode it does show different values in the message. Source code logs Eager Mode Traceback Graph Mode Traceback ,Thanks for the report. iganichev could you take a look The fix is in review. In the meantime you can use summarize 3 parameter to assert equals . Thank you very much 
15882,tfdbg error Dump root directory does not exist with empty fetches, System information Have I written custom code yes OS Platform and Distribution Linux Ubuntu 16.04 TensorFlow installed from binary pip install TensorFlow version tensorflow import tf.VERSION 1.4.1 tf.GIT VERSION v1.4.0 19 ga52c8d9 tf.COMPILER VERSION v1.4.0 19 ga52c8d9 Sanity check array 1 dtype int32 Python version 2.7.12 CUDA cuDNN version cuda libs usr local cuda 9.0 targets x86 64 linux lib libcudart static.a usr local cuda 9.0 targets x86 64 linux lib libcudart.so.9.0.176 usr local cuda 9.0 doc man man7 libcudart.so.7 usr local cuda 9.0 doc man man7 libcudart.7 usr local cuda 8.0 targets x86 64 linux lib libcudart.so.8.0.61 usr local cuda 8.0 targets x86 64 linux lib libcudart static.a usr local cuda 8.0 doc man man7 libcudart.so.7 usr local cuda 8.0 doc man man7 libcudart.7 GPU model and memory GeForce GTX 1080 8114MiB Exact command to reproduce see code below Describe the problem LocalCLIDebugWrapperSession.run does not behave like tf.Session.run if there are no fetches. The dump directory will never be created and it crashes with an IOError . For me this issue occured in a situation like this where actually everything was restored from the checkpoint and not initialized from checkpoint was empty. This code runs fine with an ordinary tf.Session but crashed with tfdbg. It took me some time to track down the issue. If it s not too hard to fix it would be nice to keep other users from the same pain maybe just speculating 13604 crashes for the same reason Source code logs ,Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case or leave them as N A Thanks. Bazel version Dear tensorflowbutler it s Bazel version N A . The original poster has replied to this issue after the stat awaiting response label was applied. Thanks for reporting the issue jgefele . We are working on a fix which will let tfdbg s tf.Session wrappers bypass Session.run calls with effectively empty fetches. Let me know if you have any further questions. 
15891,Dependencies of tensors created within a tf.while loop might not be executed, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes. See test case below. OS Platform and Distribution e.g. Linux Ubuntu 16.04 macOS Sierra Version 10.12.6 16G1114 TensorFlow installed from source or binary Both. I have compiled TensorFlow at 136697ecdc64b5171522fb7f89cfe51a02f0f1c1 with my small change in PR 15823. I have also tried using the pip package. TensorFlow version use command below v1.4.0 19 ga52c8d9b01 1.4.1 pip package Python version 2.7.10 Bazel version if compiling from source 0.9.0 homebrew GCC Compiler version if compiling from source Apple LLVM version 8.1.0 clang 802.0.42 CUDA cuDNN version CUDA 9.0.176 mac cuDNN 9.0 osx x64 v7 GPU model and memory NVIDIA GeForce GT 750M with 2048 MB device memory CUDA Compute Capability 3.0 Exact command to reproduce python repro.py .. where repro.py contains the test case to reproduce listed below. Describe the problem Here is my test case Part I is basic setup. I create two random 10 times 10 matrices and compute their singular values pre singular values of A 5.65906715 4.9420261 4.40626739 3.73506125 2.70703249 2.57429488 1.73387162 1.16000494 0.58836563 0.39101954 singular values of B 7.0283055 4.65840063 4.48502098 3.25319445 2.94667168 2.74267484 1.86004291 1.6626967 0.63884034 0.27131664 pre Part II shows usage of control dependencies to guarantee that A has been assigned to A var before the singular values of A var are computed. The output from this part is pre computed s 5.65906715 4.9420261 4.40626739 3.73506125 2.70703249 2.57429488 1.73387162 1.16000494 0.58836563 0.39101954 computed A dep 11 pre This is the expected result for Part II. In Part III I have introduced use of a tf.while loop . Now tf.svd is returning the singular values of B pre computed s 7.0283055 4.65840063 4.48502098 3.25319445 2.94667168 2.74267484 1.86004291 1.6626967 0.63884034 0.27131664 computed A dep 10 pre This is not the expected result for Part III. I expect that the singular values of A would be printed. In Part IV based on reading https github.com tensorflow tensorflow issues 4663 issuecomment 336609536 I switched to using ResourceVariable . However the output is still the same the singular values of B pre computed s 7.0283055 4.65840063 4.48502098 3.25319445 2.94667168 2.74267484 1.86004291 1.6626967 0.63884034 0.27131664 computed A dep 9 pre This is not the expected result for Part IV. I expect that the singular values of A would be printed. It appears the issue is that tf.control dependencies on tensors created by tf.while loop might not execute the tensors own dependencies. This used to work okay around TensorFlow 1.1 if I recall correctly . While searching for a previous report of this issue I found 6087 which appears related in that the sample code there has a tf.while loop that creates tensors with dependencies. When I run the sample code I consistently get result 10. This is an unexpected result in my opinion. What is happening is that update x runs exactly once so for each of the 5 loop iterations x has the value 2. I tried rewriting the sample code to use a ResourceVariable but the output is the same , ebrevdo Do you have any thoughts on this or know who would know it best Can you write a much smaller minimal failing test oh wait i see you did Try passing parallel iterations 1 to your while loop call. I just now tried adding parallel iterations 1 to my adapted 6087 test case but this didn t fix the problem. As for the first test case you can take Parts I and III separately to reproduce the issue Alternatively take Parts I and IV separately. computed s is the vector of singular values of B whereas I am expecting that it will be the singular values of A. A member of the TensorFlow organization has replied after the stat awaiting tensorflower label was applied. CC alextp can you take a look Can you add a print tf.get default graph .as graph def so I can understand how we re generating the wrong graph i.e. are control dependencies missing mangled or ignored because of weird variable stuff I created a gist for the graph def https gist.github.com dtrebbien f917cb2891e0b141b9fa6323a3c55239 Here is the exact modified test case that I used to print the graph def By the way is there a utility that can graphically display this Looking at the graph def it looks like no node has control input init A var op . Contrasting that with the following working script which does not use a tf.while loop the increment A dep y node corresponding to the const second argument to the increment A dep tf.add op has control input init A var op Here is an excerpt from the working script s graph def The non working graph s while increment A dep y node has control input while Identity but not init A var op . Ok so this is a real bug. asimshankar who do you think we should assign this to There s a bug somewhere in the control dependency processing logic of WhileContext somewhere around here most likely https github.com tensorflow tensorflow blob ba64f5334d4bba31d22c30e09a96f806ea0e2f7e tensorflow python ops control flow ops.py L2414 skye was looking into something similar I think. skye let me know if I m mistaken and will try to find someone else. 
16076,Using self.test session in setUp in tf.test.TestCase runs setUp but not tearDown, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes see below OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04 TensorFlow installed from source or binary From binary with pip TensorFlow version use command below v1.4.1 Python version 2.7.13 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version 8.0 GPU model and memory 1080Ti Exact command to reproduce python mwe.py Describe the problem https github.com tensorflow tensorflow blob 1cc4ec4c5a08cadae87fa02222c5b5c3e81dedbb tensorflow python framework test util.py L874 If you use tf.test.TestCase.test session in setUp there will be one skipped test for which setUp is run but not tearDown . I believe this has to do with tf.TestCase.test session trying to take care of automatic test discovery on the line above but I m not sure. Source code logs In the following MWE mwe.py after running this test there will be a tmp.txt file left in the current directory. MWE import tensorflow as tf import os import unittest class FooTest tf.test.TestCase def setUp self with open tmp.txt w as f f.write Hello with self.test session as sess pass def tearDown self os.unlink tmp.txt def testExample self self.assertEqual 1 1 if name main unittest.main ,Please provide details about what platform you are using operating system architecture . Also include your TensorFlow version. Also did you compile from source or install a binary Make sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in the Github new issue template https github.com tensorflow tensorflow issues new . We ask for this in the issue submission template because it is really difficult to help without that information. Thanks I have updated the issue with the new issue template. martinwicke can you comment or refer further please Looks like a bug. It is a bug. Realistically since it doesn t seem to be hurting too badly not sure when someone will look at it but I d love a PR with a fix. A member of the TensorFlow organization has replied after the stat awaiting tensorflower label was applied. This should be addressed by 9962eb5e84b15e309410071b06c2ed2d6148ed44 which deprecates test session in favor of new methods session and cached session . 
16100,Exception when not providing optional parameter frequency skip in TimeFreqLSTMCell, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes see below OS Platform and Distribution TensorFlow installed from pip3 install user tensorflow gpu TensorFlow version 1.4.1 Python version 3.5.2 CUDA 8.0 GPU NVidia Titan X Describe the problem Using a TimeFreqLSTMCell in a dynamic rnn or static rcnn without providing the optional parameter frequency skip results in an exception The line which throws this exception is https github.com tensorflow tensorflow blob 8b78c23c161c9d0bec462d5f4c73f0fca413bc8b tensorflow contrib rnn python ops rnn cell.py L474 L475 frequency skip has it s default value None here. Maybe the default should be changed to 1 Source code logs Sadly I am not allowed to share my full source code. However this is how I create the RNN layers ,After reading the code I agree that this is a bug. Don t know who knows this most friendly ping cy89 
16152,DeprecationWarning from inspect.getargspec , inspect.getargspec is deprecated in Python 3 https docs.python.org 3 library inspect.html inspect.getargspec I solved the problem in keras like this https github.com keras team keras pull 7035 System information Using tensorflow as a keras backend keras 2.1.2 Linux Ubuntu 16.04 installed from conda version 1.3.0 python 3.6.4 Describe the problem We recently switched from theano to tensorflow and this warning message is filling up my test output. Source code logs , martinwicke FYI frexvahi thanks for reporting this. Would you be willing to submit a PR to fix this since you already didd this in Keras I could submit a PR but getting someone to sign the corporate CLA would be harder. The original poster has replied to this issue after the stat awaiting response label was applied. I fixed it on my end. I will try to submit a pull request today or tomorrow with the fix. Just a quick update I submitted a pull request that should fix the issue. Can you say Fixes 16152 in the description for your PR That way this issue will be closed when your PR is submitted. A quick update. I believe the issue has been fixed in r1.7 since ever since I started using it I haven t had any of the warnings. Thanks I m still hitting this Tensorflow 1.7.0 and 1.8.0 rc1. Python 3.6.5. Afraid I don t have an easy reproducer to hand will update if I get one. Reproducer below tested on 1.8.0 rc1. Was hard to make a reproducer because the warning did not show up unless I had warnings.filterwarnings error which I use to make it easier to find the sources of warnings. Full stack trace below. details details Tensorflow 1.9.0 dev20180427 Added PR 19199 as an attempt for the fix. I am still hitting this warning with the following versions 1.8.0 1.9.0 1.10.0 1.10.1 1.11.0 rc2 Had so many of these on 1.8 that i could not longer see my test output. Updated to 1.10 and now i only have a few dozen of these warnings left. Yeah still seeing this in 1.11.0 rc0 Can we reopen any updates I think the issue has been resolved in 22517 now. Still getting this issue with 1.11.0 and Python 3.6 me too. me too I see it in 1.12 too on Python 3.6 I see it in 1.12 too on Python 3.6 the same to you Resolved in 22517 so I believe it will appear in the next release TF 1.13 or 2.0 . 
16160,tf.contrib.lookup.HashTable kv initializer does not work in eager mode.,Please go to Stack Overflow for help and support https stackoverflow.com questions tagged tensorflow If you open a GitHub issue here is our policy 1. It must be a bug or a feature request. 2. The form below must be filled out. 3. It shouldn t be a TensorBoard issue. Those go here https github.com tensorflow tensorboard issues . Here s why we have that policy TensorFlow developers respond to issues. We want to focus on work that benefits the whole community e.g. fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem rather than being redirected to Stack Overflow. System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 TensorFlow installed from source or binary TensorFlow version use command below Python version Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Exact command to reproduce You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION v1.5.0 rc0 9 gf9472619f6 1.5.0 rc1 Describe the problem Can not use hashtable in eager mode. Source code logs , alextp please comment take a look. thanks for the fix. But now And if I comment this out give this On Wed Jan 17 2018 at 11 45 PM xiaoyun wu notifications github.com wrote thanks for the fix. But now File torch textline.py line 16 in table.init.run AttributeError NoneType object has no attribute run just do table.init.run which uses a session if you re in graph mode. And if I comment this out dataset dataset.map lambda tkns table.lookup tkns give this Traceback most recent call last File Library Frameworks Python.framework Versions 3.6 lib python3.6 site packages tensorflow python framework op def library.py line 510 in apply op helper preferred dtype default dtype File Library Frameworks Python.framework Versions 3.6 lib python3.6 site packages tensorflow python framework ops.py line 1022 in internal convert to tensor ret conversion func value dtype dtype name name as ref as ref File Library Frameworks Python.framework Versions 3.6 lib python3.6 site packages tensorflow python eager function.py line 100 in convert to graph tensor return constant op.constant value.numpy File Library Frameworks Python.framework Versions 3.6 lib python3.6 site packages tensorflow python framework ops.py line 677 in numpy raise ValueError Resource handles are not convertible to numpy. ValueError Resource handles are not convertible to numpy. You seem to have found a bug with the eager dataset integration. For cleanliness can you file another issue Alex it is the same code that still failing... I do not get the reason for file another issue. Ok. I have a fix which is getting sent through to github soon. On Fri Jan 19 2018 at 2 46 AM xiaoyun wu notifications github.com wrote it is the same code that still failing... I do not get the reason for file another issue. You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 16160 issuecomment 358930504 or mute the thread https github.com notifications unsubscribe auth AAATxXxWWFryD dej Bb0PbjrQAKBrQ0ks5tMHJ8gaJpZM4RfwC7 . Alex 
16161,tf.case raising IllegalArgumentError None of the conditions evaluated as True when used with Dataset, System information OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 64bit TensorFlow installed from source or binary via pip TensorFlow version use command below 1.4.0 Python version 3.5 When I use tf.case within a tf.data.Dataset I get an IllegalArgumentError None of the conditions evaluated as True . However when I use the same code without the Dataset it works fine. Furthermore if I understand the error message correctly it already tells me that one condition evaluated to true see the end of the first line The following code reproduces the issue I also found this question http www.programfaqs.com faq tensorflow case error invalid argument assertion failed none of the conditions evaluated as true which seems to be the same problem., mrry could you please take a look It looks like the bug has already been fixed I ran your code with TF 1.5.0rc1 and it works as expected. I also confirmed that it failed with TF 1.4. Feel free to reopen if you continue to have problems after upgrading. Can you suggest a work around for 1.4 
16238, tensorflow contrib gan losses impl test fails with AssertionError , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 s390x TensorFlow installed from source or binary Source TensorFlow version use command below v1.4.1 Python version 2.7.12 Bazel version if compiling from source 0.7.0 GCC Compiler version if compiling from source 5.4.0 CUDA cuDNN version No GPU GPU model and memory NA Exact command to reproduce bazel test c opt tensorflow contrib gan losses impl test Describe the problem One of the sub test test stable global norm unchanged fails on s390x with AssertionError 110.709068 110.709084 0.000010 Seems like a minor difference so I tried changing the tolerance slightly as below with this the test is passing. Is it ok to create a PR with this change Could you please share your thoughts on this. Source code logs , yifeif any reason loosening this bound would not be acceptable redirect to joel shor who owns tf.contrib.gan. Joel do you mind taking a look Thanks 
16261,Tensorflow Debug tfdbg ValueError with combined loss functions., System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes. Included OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 Home x64 1709 TensorFlow installed from source or binary binary pip3 TensorFlow version use command below 1.4.0 Python version 3.5.2 Exact command to reproduce See source code. Describe the problem When attempting to combine two loss functions tfdbg fails to properly grab the gradients due to a ValueError when executing run. Source code logs consolelog.txt https github.com tensorflow tensorflow files 1649424 consolelog.txt Log of the error. not working.txt https github.com tensorflow tensorflow files 1649426 not working.txt Code example that generates the error. working.txt https github.com tensorflow tensorflow files 1649427 working.txt Code example where the two loss functions are split that does not generate the error. , caisq could you PTAL. Thanks zazabar Thanks for taking the time to report this issue and creating the example code for reproduction. However I can t reproduce the issue using your not working.txt on a GPU machine that I have. See screenshot of the successful tfdbg run below tfdbg bug fail bug works https user images.githubusercontent.com 16824702 35258674 141b22aa ffcf 11e7 8f65 62bc792f6b27.png The tfdbg run passed 5 out of the 5 times that I ran it. This was the case for the latest nightly version of tensorflow gpu 1.5 . I didn t try 1.4 though too painful to downgrade CUDA from 9 to 8... p I wonder whether this has to do with GPU type or OS details. Mine is a GeForce GTX 1050 Ti. My OS is Ubuntu 16.04. I haven t tried Windows though. Maybe this is a Windows specific issue. If you could try a Linux machine that would be great. I can also try a Windows machine with GPU later. Hey caisq As you indicated when I ran it on a Linux machine Mint it did not run into any problems. So perhaps this is just a Windows issue. Is there anything else that I can supply that might help fish out what the culprit is Nagging Assignee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee caisq It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee caisq It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee caisq It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee caisq It has been 15 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee caisq It has been 15 days with no activity and this issue has an assignee. Please update the label and or status accordingly. 
16294,ScipyOptimizer SLSQP supporting callback,The callback is deprecated when SLSQP method in scipy optimizer is selected see here https github.com tensorflow tensorflow blob 04b5c75aae4bdbdac7c713714a369f9b360daf70 tensorflow contrib opt python training external optimizer.py L400 . Actually SLSQP does support callback so in the above linked file could be removed. The following example shows that SLSQP do support callback. , mjwen thank you for reporting this issue. Would you be interested in submitting a pull request that implements the change that you suggest tatianashp Sure. The pull request has been submitted at 16312 https github.com tensorflow tensorflow pull 16312 . Great Thank you. 
16320,BUG py func don t support unicode string results for python2, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Y OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac 10.11 TensorFlow installed from source or binary source TensorFlow version use command below master Python version 2.7 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Exact command to reproduce N A Describe the problem When I investigated 14116 I found that py func converts unicode strings result to bytes only for Python3 while raise an exception for Python 2. I m curious why we don t the same thing for Python 2. Source code logs script logs ,Moreover py func runs well for English even though unicode string while Chinese failed. I don t know whether the inconsistent result is intentional. 16322 is proposed to fix the issue. Thank you for noticing this issue. As you already submitted PR that fixes it I am closing the issue. Thank you for noticing this issue. As you already submitted PR that fixes it I am closing the issue. 
16358,Request for updating keras datasets files to r1.5, System information executes Keras sample code imdb fasttext.py https github.com keras team keras blob master examples imdb fasttext.py Windows 7 TensorFlow installed from binary TensorFlow version 1.5.0rc0 Python version 3.5.1 Describe the problem Keras sample program does not work. There is a bug for numpy arange method wrong usage. Need to fix from arrange to arange This issue is already solved on master branch. not in 1.5.0rc1 Would you update these source codes Source code logs Error messages are follows C Users sakaia work tensorflow keras python imdb fasttext.py Loading data... Traceback most recent call last File imdb fasttext.py line 75 in module x train y train x test y test imdb.load data num words max features File C Program Files Python35 lib site packages tensorflow python keras imp l keras datasets imdb.py line 77 in load data indices np.arrange len x train AttributeError module numpy has no attribute arrange Following are just checking np.arrange not np.arange git branch r1.5 grep rn np.arrange tensorflow python keras impl keras datasets boston housing.py 51 indices np.arrange len x tensorflow python keras impl keras datasets reuters.py 76 indices np.arrange len xs tensorflow python keras impl keras datasets imdb.py 77 indices np.arrange len x train tensorflow python keras impl keras datasets imdb.py 82 indices np.arrange len x test git branch grep rn np.arrange This line is intentionally blank ,Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case or leave them as N A Thanks. Have I written custom code OS Platform and Distribution Bazel version CUDA cuDNN version GPU model and memory Exact command to reproduce Part of parameters are already written I add it Bazel version NOT used since it is binary CUDA cuDNN version NOT used GPU model and memory NOT used Exact command to reproduce get file from https github.com keras team keras blob master examples imdb fasttext.py edit import path from keras to tensorflow.python.keras like follows execute python imdb fasttext.py Thanks for letting us know about the typo. It will be fixed in 1.6 release once it comes out. Please work around it till then. 1 for this... had to hack my installed version to get this working... As for me I am doing like follows as workaround For this workaround it needs to install original keras separately. Ref. https github.com tensorflow tensorflow issues 16532 here is how i got the same error ERROR Traceback most recent call last File home ahmed Code deep learning with python classifying movie reviews.py line 5 in module train data train labels test data test labels imdb.load data num words 10000 File usr local lib python3.6 dist packages tensorflow python keras impl keras datasets imdb.py line 77 in load data indices np.arrange len x train AttributeError module numpy has no attribute arrange you should change from from tensorflow.python.keras.datasets import imdb to from keras.datasets import imdb because of this issue in tensorflow keras package. original keras not tensorflow keras works fine. 
16481,Container localhost does not exist.,Hi I upgraded from 1.5.0 rc1 to the current master branch and I started receiving the following error It s hard to reproduce this error but a summary of the context is that I have a lookup table op inside a dataset map operator and I get this error when I try to execute the corresponding iterator GetNext op. I m looking for information in how to parse and debug this error. I never explicitly set any containers for my variables or lookup tables i.e. leave them to the default value an empty string . Were there any changes introduced recently that could result in this error Note that this happens with my Scala API but not with the Python API and so it may be that I haven t updated something in my code. I just don t really know where to look for this. Thanks ,I think this could be a knock on effect from 9f4118d00fa9eb85f81a4eb3f96a5583ae5afcdc which modifies and in principle simplifies how DT RESOURCE tensors are captured inside a Dataset transformation such as Dataset.map . It sounds like there is some disagreement between the code that creates the lookup table and the op that performs the lookup itself. Could you share the code for creating the table and the Dataset.map cc rohan100jain mrry I d love to share the code but it s written in Scala using my API and it might be hard to get familiar with the codebase. I can do so if you think that can help. However in order to give you a summary the lookup table is created right before being used in the dataset map. It is also initialized prior to being used by calling GetNext . What kind of disagreement could cause such an error Is there an easy way to debug Also is localhost the default container used when not providing and container Hmm what you re doing doesn t sound like it s wrong because that s effectively what the Python version would be doing too. And the recent change should make it less likely to see a NotFound error because it changes the function implementation to share the same ResourceMgr which defines the namespace for resource names and containers whereas before there was an explicit copying and rewriting step for captured resource handles. Could you possibly create a minimal example that exhibits the problem in both Python and Scala and share the code for these If you could also capture the text format GraphDef it might be possible to inspect the graphs and find the discrepancy. Also is localhost the default container used when not providing and container That s right. Are you using the container attr in your code It should still work but it s possible there s a bug there.... mrry I can look into creating a minimal example this week but I have found some information that might be useful. It has to do with creating functions and capturing control dependencies of these functions. Currently functions are created in a separate graphs and tensors that are used as inputs are captured and replaced with placeholders to be fed later. What about control dependencies that some of the new ops might have on ops defined outside the function For example I have a dataset map op that uses a lookup table but I want the iterator initializer op for that dataset to have a control dependency on the lookup table initializer op of the outer graph. How should I go about handling that Currently I get an error of the form Source ... not in the main body where source refers to the source of the control edge. I think that s what s causing the error because I cannot properly initialize the iterator. mrry Actually never mind even if I work around this initialization issue and manage to run all the initializer ops fine I still get the same error once I get to invoke GetNext . I ll try to create a minimal example for this but in the meantime do you have any idea of what could be causing the container to not be found Shouldn t the localhost container always exist given that it s the default one Note also that if I put a breakpoint in my Scala code this error keeps being printed over and over again indefinitely as if it keeps being thrown from within some loop running in another thread. mrry Given that the lookup table initializer op and the iterator initializer op run without any issues is there any chance the resource manager is cleared reset afterwards in between the session runs Is there any tips you might have on how to debug this sort of error mrry I m wondering if this has to do with me using the C API TF GraphToFunction method. I m using it to create the function provided to the dataset map. The lookup table which is used by the function is replaced with a placeholder of type RESOURCE . The initialized lookup table resource tensor is then passed as input to the created function op. Could this be causing the problem It was all working fine with version 1.5.0 rc1. mrry I finally resolved this. I was accidentally setting the shared name of the iterator I was creating. I m not sure why this caused the problem but without setting the shared name everything works fine. Do you know why that could have happened I m actually curious. Thanks for tracking that down I think this is a legitimate bug introduced in https github.com tensorflow tensorflow commit 9f4118d00fa9eb85f81a4eb3f96a5583ae5afcdc. That change modifies most iterators to use the same Device FunctionLibraryRuntime and ResourceMgr as the op that created them which enables the resource capturing logic to be simplified because handles are valid in both the caller and the callee. However when the shared name is set the iterator might outlive the FunctionLibraryRuntime used by the op that created it. The new code identifies this case and creates its own private FunctionLibrary Device and ResourceMgr that will outlive the caller. Unfortunately this means that the handles from the caller and no longer valid in the callee which manifests as a NotFoundError . Thanks for the promising lead on this bug We ll try to get a fix in soon. mrry Thanks for tracking this down and fixing it Thanks for digging into it and making it much easier to fix Hi mrry eaplatanios can you please take a look if the issue 50801 is related to this in any way Thanks in advance 
16829,tf.contrib.estimator.replicate model fn fails when a trainable variable doesn t have gradient,tf.contrib.estimator.replicate model fn fails when the gradient of a trainable variable is None. The error messages are ,Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case or leave them as N A Thanks. Have I written custom code OS Platform and Distribution TensorFlow installed from TensorFlow version Bazel version CUDA cuDNN version GPU model and memory Exact command to reproduce isaprykin x10000year Hi I fixed this yesterday and the fix is coming. Thanks for reporting. I ll link the commit when it s available and then close the issue. I hope you can re built or take the nightly build to take advantage of the fix. Thank you 
17284,tensor valued seeds in tf.data API can result in nondeterministic results, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 17.10 TensorFlow installed from source or binary Source TensorFlow version use command below v1.5.0 11 g4588350f20 1.5.0 Python version 3.6.3 Bazel version if compiling from source 0.11.0 GCC Compiler version if compiling from source 7.2.0 CUDA cuDNN version N A GPU model and memory N A Exact command to reproduce See code below Describe the problem The tf.data API allows requires seeds to be provided that are tf.tensor s. This is an issue when the graph level seed has been set to 0 and the provided op level seed tensor takes on a value of 0. As noted in the comments in the code for tf.get seed a 0 0 seed is problematic because the C ops assume this means nondeterminism. Of course when a user is specifying these seeds they re expecting deterministic behaviour. Unfortunately tf.get seed only checks for this issue for the case where the seeds are ints not tensors. See the code below for an example. I would have been happy to submit a PR for this but I have no idea where the fix should be for this bug. As I m not especially familiar with the code base it s not apparent whether it s even possible to have the code for tf.get seed to check the value of a tensor seed. If not I m guessing the tf.data API would need to provide the checks. Source code logs The following code reproduces the bug for me The result I get is I would expect the first and second runs to produce the exact same sequence though the particular sequence might differ from environment to environment. If I change the feed dict to provide a value of 0 for seed tensor for both runs I get the expected result If I change the shuffle call to take a value of 0 directly i.e. ...shuffle 10 seed 0 I get the expected result as well For the record I encountered this issue because I m using an Iterator via Iterator.from structure and when I use that iterator on a Dataset.shuffle I get the same order for each epoch. To get around this I provided the epoch number as a seed to Dataset.shuffle with the first epoch being epoch 0. In my case I can avoid this bug by just starting the epoch count at 1 but it took me a while to track down and there may be other cases where the API is used in a similar way that would be problematic for those expecting deterministic results., mrry Can you look at this I m a bit confused by the code example. Why would you expect the shuffled order to be the same when you feed two different values for seed tensor It might make sense to rewrite the seed as tf.get seed does when the runtime value is 0. Is that what you re suggesting mrry Ugh sorry that s my mistake. Both fed values should be 0. I copied and pasted the code I was using to test it and had changed one of them but not the other. I ve edited the code to reflect my intention. It might make sense to rewrite the seed as tf.get seed does when the runtime value is 0. Is that what you re suggesting Yes exactly. the tf.get seed check is insufficient if tensor valued seeds are allowed as it only does an identity check against 0 . OK that does seem like a bug and the fix isn t too difficult. The gist is to set seed tf.where seed 0 tf.constant 2 31 1 dtype tf.int64 seed when seed is a tf.Tensor . Watch this space for a fix in the next few days. mrry I d be willing to submit a PR for this if no one s already working on it. Thanks for offering I ve already got a fix under review so it should be merged soon. 
17360,C api use of op Attrs methods in gradients,The generated op Attrs struct returns new instances on its chainable methods and doesn t change the original object. https github.com tensorflow tensorflow blob d7d7f4eea5f3a2e63e12c803d02f56726c0e0513 tensorflow cc framework cc op gen.cc L701 There are a few related issues e.g. https github.com tensorflow tensorflow blob 6fdb9ad1baf7686a75f9e660178f7ac595e7fc2e tensorflow cc gradients nn grad.cc L164 where the code assumes the underlying object is being mutated and the parameters don t actually pass through. I guess there might be a couple of ways forward depending on how Tensorflow prefers the C API 1. Decide the Attrs chaining methods mutate the underlying object and fix the code generation. 2. Decide the Attrs chaining methods return new instances and fix the uses. Suggestions Fwiw if option 2 it might be nice to add TF MUST USE RESULT to the generated API. Unfortunately a long standing bug in gcc https gcc.gnu.org bugzilla show bug.cgi id 38172 means this may be unreliable as an actual error across versions of gcc that contributors may use. cc suharshs keveman , kbsriram thanks for pointing out the bug. I am not a fan of mutable state so my natural inclination is towards option 2. Adding TF MUST USE RESULT to the generated API would simply point out the existing erroneous uses as compiler errors. keveman thanks for the note Sgtm don t have a strong opinion on this. But would like to make progress on fixes and for option 2 as you note add a bit of a compile time safety net for new code. Tentatively predicated on option 2 suggestions on next steps If someone is already on it awesome otherwise I can sign up for a pull request on this. 
17419,possible issue of tensorflow.keras not handling shape correctly, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Win10 1709 x64 TensorFlow installed from source or binary binary from PyPI TensorFlow version use command below 1.6.0 Python version 3.6.4 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 9.0 7.0 GPU model and memory GTX1060 6GB Exact command to reproduce Describe the problem i believe this is a bug that tensorflow.keras not handling the shape correctly self.input shape is provided by me which is 7514 1 in this case. My code runs fine with keras but get this error with tensorflow.keras Source code logs ,The bug description is a little unclear. It seems like you think the error is L623 in topology.py but the actual ValueError message is different than what that line produces. Are you sure that is the line that is causing the error Hi I have found out the cause of issue. The issue is in tensorflow.keras the shape cannot be a numpy array while keras can. The issue is gone now if I explicitly convert self.input shape to a list I do think this is a bug Here is an example script to reproduce the issue For Keras which will run without error For tensorflow.keras with the error Assigning to Francois to take a look at it. fchollet May I take a look at this or have you already solved this This would be my first issue.. alphaCTzo7G This issue can still be reproduced with the latest TF1.7 so the issue is not solved. The workaround is I simply convert the numpy array to python list. Ok.. thanks for reporting henrysky. If its ok with fchollet I will take a look at it and create a PR I have tested the above code runs without error in TF1.12.0 so I will close it 
17484,contrib.boosted trees.estimator batch modules are not available in 1.6.0, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow no OS Platform and Distribution e.g. Linux Ubuntu 16.04 macOS Sierra TensorFlow installed from source or binary source TensorFlow version use command below 1.6.0 Python version 2.7 Bazel version if compiling from source 0.11.0 CUDA cuDNN version not relevant GPU model and memory not relevant Exact command to reproduce trying to run the example in https github.com tensorflow tensorflow blob master tensorflow contrib boosted trees examples mnist.py but get an error ModuleNotFoundError No module named tensorflow.contrib.boosted trees.estimator batch.estimator tried to install the latest version with pip and from source on python2.7 and python3.6 but still end up with the same error. this is the content of the boosted trees estimator batch folder in my installation custom export strategy.pyc dnn tree combined estimator.py init .py dnn tree combined estimator.pyc init .pyc trainer hooks.py custom export strategy.py trainer hooks.pyc ,I can reproduce this issue. Soroush sshrdp . Can you take a look at it I got the same error This should be fixed in the more recent releases of tensorflow. Nagging Assignee rohan100jain It has been 136 days with no activity and this issue has an assignee. Please update the label and or status accordingly. I just confirmed that as of tf 1.11 this works now. Closing issue. 
17631,Eager mode in multithreaded environment v1.6 generates All graphs are building functions and no eager context was previously active , The problem Scope initialization assumes that the context stack must be in the same stacktrace thread. while in some cases the user might use the eager context from a different thread. For example the code below fails in version 1.6 but not in 1.5. Also in version 1.6 it does not fail if foo is called from main thread or if enable eager execution is called from the child thread. Source code logs exception System information Have I written custom code no OS Platform and Distribution Linux Ubuntu 16 TensorFlow installed from binary TensorFlow version v1.6.0 0 gd2e24b6039 1.6.0 Python version 3.5 ,Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case or leave them as N A Thanks. Bazel version CUDA cuDNN version GPU model and memory Exact command to reproduce Bazel version if compiling from source N A not compiling from source GCC Compiler version if compiling from source N A not compiling from source CUDA cuDNN version N A not using GPU GPU model and memory N A not using GPU Exact command to reproduce python and the file name of code above Thanks for the report this shouldn t happen will take a look. CC akshayka since this seems to be related to the change to use init scope Yes thanks for report I ll investigate further. uyerushalmidop We ve fixed this bug at head which is packaged on pip as tf nightly. The fix will be included in version 1.7 of TensorFlow. Do you need a fix immediately If so I can provide you with one with the caveat that the fix uses a private API which will be removed in a future release. Thanks for the quick response. No need for an immediate fix. For now I have a workaround until 1.7 is out. 
17720,map and batch tensor shape does not match value of tensor in the same way that calling map and batch individually does, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Happens with stock code OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu Server 17.10.1 TensorFlow installed from source or binary Source TensorFlow version use command below b v1.6.0 0 gd2e24b6039 1.6.0 Python version 3.6 Bazel version if compiling from source 0.11.0 GCC Compiler version if compiling from source gcc Ubuntu 6.4.0 8ubuntu1 6.4.0 20171010 CUDA cuDNN version 9.1 7.0.5 GPU model and memory not applicable Exact command to reproduce not applicable When I create a tf.data.Dataset from tfrecord files that utilizes a call to map to parse the tfrecord file and a call to batch to batch the dataset I am able to filter out the last small batch utilizing a straight forward call to filter . This same function does not work correctly when utilizing the combined map and batch function. The filter function in question is The reason this does not work is because every tensor passed through tf.shape when utilizing map and batch has the same shape even though the contents of the tensor does not. This is not the case when executing map and batch separately the last batch has a shape returned from tf.shape that correctly matches the shape of the value. My real world example has 7153 batches in 1 epoch using map and batch separately I am returned 7152 batches that have a shape returned from tf.shape of 128 and the final 1 batch returned as 70 in this case the filter function works correctly. When I swap out this configuration with map and batch I am returned 7153 batches with shape returned from tf.shape of 128 in this case my filter function does not work and my estimator throws an exception because it receives a batch of 70 when it was expecting a batch of 128 . I would very much like to use map and batch because it takes 1 3 the time of map and batch separately. Here is an example script When executed with the following parameters you should see the output To be clear the execution with the use broken map and batch shows a 104 at the end this is because that batch was not filtered out you can recreate this by using the following code to generate a tfrecord file And you can execute in this way Any help on this issue is greatly appreciated.,It looks like this has been broken since 8693bf519399495cedd91293ec82b492ea401f6f which did not update the Python shape inference logic when adding the ability for map and batch to return a smaller final batch. Sorry for the inconvenience and we ll have a fix for this soon. In the meantime you can fix by using tf.placeholder with default to inhibit the shape inference dataset dataset.apply tf.contrib.data.map and batch map func parse fn batch size tf.placeholder with default tf.constant 128 dtype tf.int64 num parallel batches 28 cc jsimsa To clarify it should be tf.placeholder with default needs shape passed to it is it correct to pass that in as in this situation I don t get a problem and it fixes the issue thank you Right that should work. 
17932,tf.contrib.data.bucket by sequence length fails for nested Dataset element,Hello everyone I just tried the new function to group variable length inputs for the dataset API namely tf.contrib.data.bucket by sequence length for a small Estimator Model. I implemented the input fn such that it returns a dataset where each element is a tuple feature dict label https www.tensorflow.org get started premade estimators create input functions . However when I run it I get following exception Traceback most recent call last ... File home leo anaconda3 lib python3.6 site packages tensorflow python data ops dataset ops.py line 960 in apply dataset transformation func self File home leo anaconda3 lib python3.6 site packages tensorflow contrib data python ops grouping.py line 198 in apply fn window size func window size fn File home leo anaconda3 lib python3.6 site packages tensorflow python data ops dataset ops.py line 960 in apply dataset transformation func self File home leo anaconda3 lib python3.6 site packages tensorflow contrib data python ops grouping.py line 90 in apply fn window size func File home leo anaconda3 lib python3.6 site packages tensorflow contrib data python ops grouping.py line 239 in init self. make key func key func input dataset File home leo anaconda3 lib python3.6 site packages tensorflow contrib data python ops grouping.py line 289 in make key func self. key func.add to graph ops.get default graph File home leo anaconda3 lib python3.6 site packages tensorflow python framework function.py line 488 in add to graph self. create definition if needed File home leo anaconda3 lib python3.6 site packages tensorflow python framework function.py line 321 in create definition if needed self. create definition if needed impl File home leo anaconda3 lib python3.6 site packages tensorflow python framework function.py line 338 in create definition if needed impl outputs self. func inputs File home leo anaconda3 lib python3.6 site packages tensorflow contrib data python ops grouping.py line 279 in tf key func ret key func nested args TypeError element to bucket id takes 1 positional argument but 2 were given Here is a link https github.com tensorflow tensorflow blob master tensorflow contrib data python ops grouping.py L143 to the function. Here is a code snipped to reproduce the error My Env Specs are logged in tf env.txt https github.com tensorflow tensorflow files 1838947 tf env.txt Thanks in advance , rsepassi Can you take a look please cc ebrevdo FYI. Yes looking into it. On Thu Mar 22 2018 at 9 02 PM Derek Murray notifications github.com wrote rsepassi https github.com rsepassi Can you take a look please cc ebrevdo https github.com ebrevdo FYI. You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 17932 issuecomment 375505400 or mute the thread https github.com notifications unsubscribe auth ABEGW jtSsBu9OcEFUz8J3yEIi4AdEo5ks5thEmIgaJpZM4S3plo . This is consistent with the behavior of all callables in tf.data with tuple inputs. The function should unpack args https github.com tensorflow tensorflow blob master tensorflow python data ops dataset ops.py L1769 is used to determine whether inputs to a user supplied callable should be unpacked i.e. called as fn args and is True if the inputs are a tuple . If the elements of a Dataset are tuples as they are in the example in this thread then callables must expect the arguments to be passed unpacked. For example here s a Dataset where the elements are tuples and a call to map fails when the map fn expects only 1 element. We may want to update this behavior but that s where it stands right now. Fix right now would be to use a dict going in and apply a map to turn into a tuple afterwards. mrry should we look into updating the behavior of Dataset callables rsepassi Right that example should fail and we can t change this even if we wanted to. It works if you instead do dataset.map lambda x y x y . I think there might be a missing should unpack args in the bucket by sequence length code because making the equivalent change to lhlmgr s program doesn t fix things i.e. ...still yields the same error TypeError element to bucket id takes exactly 1 argument 2 given Yup found the issue. Have a fix out. bucket by sequence length was not correctly handling tupleized elements. 
17985,name scope problem of tensorflow python layers normalization.py , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 64bit TensorFlow installed from source or binary Binary TensorFlow version use command below 1.7.0 rc1 GPU version from pip Python version 3.6.2 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 9.0 GPU model and memory GTX 1080Ti 11GB Exact command to reproduce See below Describe the problem I found this problem when I am using tf.layers.batch normalization . For example the following code Will generate the following computation graph bn problem https user images.githubusercontent.com 8580553 37874558 ae939870 3063 11e8 82f1 72349b32f369.png While what I expect is the variables gamma beta moving mean etc. should all inside the name scope layer1 batch normalization but not batch normalization So I refer to the code tensorflow python layers normalization.py https github.com tensorflow tensorflow blob r1.7 tensorflow python layers normalization.py Around line 301 361 I found image https user images.githubusercontent.com 8580553 37874572 041a6dbe 3064 11e8 98b7 860e7dc157eb.png It seems that here the name scope was cleaned in order to support the reuse option. But this should only affect moving mean and moving variance not beta and gamma . However as shown above all of them was affected. In fact if we add print self. scope.partitioner just before line 302 we will found that self. scope.partitioner is always None . Summary 1. self. scope is probably not set correctly. 2. Suggestion I want an option to tell the program I will never reuse the weights thus do not clean the name scope of moving mean and moving variance . Otherwise the graph in tensorboard is too messy...... ,Is there anyone can solve this problem fchollet could you please take a look Thank you for the bug report. I believe the issue is resolved for layers from tf.keras.layers at HEAD. You could try with tf.keras.layers.BatchNormalization with the nightly release. Unfortunately we cannot touch the variable names of the of layers from tf.layers since this would affect existing checkpoints. But going forward we re recommending the usage of tf.keras.layers instead for consolidation. Automatically closing this out since I understand it to be resolved but please let me know if I m mistaken.Thanks 
18094, tf.keras.estimator. create ordered io casts everything to floatx which breaks non floatx inputs, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Debian 3.16.36 TensorFlow installed from source or binary Installed via pip TensorFlow version use command below v1.6.0 0 gd2e24b6039 1.6.0 Python version 2.7.9 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version n a GPU model and memory n a Exact command to reproduce Requires significant code let me know if necessary Describe the problem This is kind of a simple issue with using Keras models as Tensorflow Estimators. I unfortunately need to do this awkward conversion in order to use SageMaker which is even more awkwardly behind by two versions of Tensorflow. Which is fun. Basically I have a Keras model that expects a tf.string input dtype which is then passed through to a Lookup layer for some text embeddings. This works fine as a Keras model and works fine if I extract the input layers myself and connect them into an Estimator. However if I go to create an estimator from the model using model to estimator I run into this code path https github.com tensorflow tensorflow blob r1.6 tensorflow python keras impl keras estimator.py L80 This conversion then causes the model to break further down the line. I m not sure why this float cast occurs but this commit https github.com tensorflow tensorflow commit 4c86ece040cb96ea689f5c0d084b6959274eab91 diff 69effda952f96b36c8015cc1a3462d65 seems to imply that Keras models are meant to only take floatx input which doesn t really seem right. Would not doing this cast break anything If so is there a way to use a non float32 input with Keras models that need to be converted to Estimators Thanks Source code logs Here s the exact traceback for the issue I can provide code if absolutely necessary but it d take some work to get to a minimal reproduction.,Looking at the history of this file using other types were never tested. We should use https www.tensorflow.org api docs python tf is numeric tensor before casting fchollet Can you take a look at this I confirm this bug. cc lenlen yifeif what do you think about this issue Could it be a simple fix I opened a PR https github.com tensorflow tensorflow pull 18104 as a starting point. 
18180,Eager tf.size does not respect out type , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 TensorFlow installed from source or binary binary TensorFlow version use command below 1.7.0 Python version 2.7.12 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Exact command to reproduce Describe the problem As per the documentation of tf.size https www.tensorflow.org api docs python tf size the dtype of the returned tensor should default to tf.int32 and it can be optionally overridden by providing an out type argument. However in the snippet above tf.size returns a tf.int64 tensor and in a related StackOverflow question https stackoverflow.com questions 49604969 gradient error occurred when calculate two embeddings on eager mode the tf.size used by GatherGrad is resulting in a float64 tensor. Long story short this is a buggy discrepancy between eager execution and graph construction. Likely introduced in commit https github.com tensorflow tensorflow commit 47ea851d3faf029d5b23ee70cb3b96bad0128324 CC alextp ,
18260, BUG 1.8 prefetch to device doesn t work with eager Iterators, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow YES Minimal non working example can be seen below. OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 TensorFlow installed from source or binary pip install tf nightly gpu 05.04.2018 TensorFlow version use command below 1.8.0.dev20180329 Python version Python 3.6.3 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version CUDA 9 CuDNN 7.1 GPU model and memory 2x GTX 1080 8GB Exact command to reproduce Describe the problem Even when applying prefetch to device as the last operation it throws the following error logs ,Is this related to the migration of eager mode switch in 1.7 I believe they are available outside contrib now. byronyi which specific migration do you mean tfe.Iterator Because I used the migrated switch... i also tried same error. This is currently broken and I have a fix in review but here s a temporary workaround that should work yeah that works. Doesn t have the performance benefits of prefetch to device though... It is supposed to have the same benefits... the tfe.Iterator has an on device buffer internally when it s placed on a non CPU device. Are you observing different performance In my current Implementation I see no difference between putting the Iterator on GPU or not. In both cases my GPU spends a decent amount of time copying data though that might be for different batches GPU Compute load stays the same. Doesn t mean there is no difference though. Might just be bound somewhere else. I ll have to try disabling eager mode using prefetch to device and see if it s faster then... Update I checked without my model code just putting some dummy GPU op in place and that is significantly faster with the Iterator on GPU. Thanks for confirming Here s how to check if anyone s interested details p summary Click to expand summary p details Hi guys I facing that error.How can I do that WARNING tensorflow From Library Frameworks Python.framework Versions 3.6 lib python3.6 site packages tensorflow contrib learn python learn datasets base.py 198 retry from tensorflow.contrib.learn.python.learn.datasets.base is deprecated and will be removed in a future version. Instructions for updating Use the retry module or similar alternatives. 
18317, bug FtrlOptimizer with l2 shrinkage regularization strength is incorrect,The result of accum in FtrlOptimizer with l2 shrinkage regularization strength seems incorrect. Test code My result is OS Platform and Distribution OS Ubuntu 16.04 Tensorflow version 1.7.0 CPU i7 4790 TensorFlow installed from pypi According to comments of apply ftrl v2 in gen training ops.py grad with shrinkage should be 0.1 2 0.5 1.0 1.1 accum new should be 0.0 1.1 1.1 1.21 so accum should be 1.21 but the result of accum from the test code is 3.61 ,Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case or leave them as N A Thanks. Have I written custom code OS Platform and Distribution TensorFlow installed from Bazel version CUDA cuDNN version GPU model and memory Exact command to reproduce rohan100jain Any update Have I written custom code No. OS Platform and Distribution OS Ubuntu 16.04 CPU i7 4790 TensorFlow installed from pypi TensorFlow version 1.7.0 Bazel version N A CUDA cuDNN version N A GPU model and memory N A Exact command to reproduce According to comments of apply ftrl v2 in gen training ops.py grad with shrinkage should be 0.1 2 0.5 1.0 1.1 accum new should be 0.0 1.1 1.1 1.21 so accum should be 1.21 but the result of accum from the test code is 3.61 The root cause is when this line https github.com tensorflow tensorflow blob 15a9de7763aa736e97d7e636d9487c0dde372d4b tensorflow core kernels training ops.cc L217 or this line https github.com tensorflow tensorflow blob 15a9de7763aa736e97d7e636d9487c0dde372d4b tensorflow core kernels training ops.cc L224 update value the is also changed although it is defined before those lines. So when update accum the accum new is not the expected one. From this paper https www.eecs.tufts.edu dsculley papers ad click prediction.pdf the should be the sum of gradient square in equation 2 . So looks like the update of the is not correct. yangli2 to take a look Nagging Assignee rohan100jain It has been 80 days with no activity and this issue has an assignee. Please update the label and or status accordingly. https github.com tensorflow tensorflow commit 20d5683b826be03776978af3b8108fc3b5dc9cb8 diff fee33779233fb6ff133bbb93c7830880 should have fixed the bug 
18355,Cannot return string from tf.data map function, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 TensorFlow installed from source or binary binary TensorFlow version use command below 1.6.0 Python version 2.7.12 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version N A GPU model and memory N A Exact command to reproduce see below Describe the problem The map method of tf.data does not allow returning a string because no implicit conversion to a tensor is made. Instead an error AttributeError str object has no attribute get shape is raised. According to mrry this is a bug introduced in TF 1.5. For a reference see the comments at https stackoverflow.com questions 49668252 returning strings in tf data dataset map method this is also the Q A that originally noted this problem. Source code logs To reproduce Error trace , jsimsa I think this regression crept in at 82fa1e1ae5b2f8af642979fafb1cab455db1882f which added SparseTensor support because the new logic assumes that all return values from a map function are either tf.Tensor or tf.SparseTensor whereas the old logic had a conversion pass. Perhaps some variant of convert to tensor or sparse tensor is needed here Nagging Assignee jsimsa It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. This has been fixed in https github.com tensorflow tensorflow commit 5a53c9b54d8781032ebf2cf26f93da3b2a33d1e4 
18437,Converting TensorFlow frozen and inference model to lite fails with Check failed array has shape , Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac OS X High Sierra TensorFlow installed from source or binary Source TensorFlow version use command below v1.7.0 rc1 1018 g7a0def60d4 1.7.0 rc1 Python version 3.5.5 Bazel version if compiling from source 0.10.1 GCC Compiler version if compiling from source Apple LLVM version 9.1.0 clang 902.0.39.1 CUDA cuDNN version NA GPU model and memory NA Exact command to reproduce bazel bin tensorflow contrib lite toco toco input file frozen.pb input format TENSORFLOW GRAPHDEF output format TFLITE output file frozen.lite input array Inputs Input output array Outputs Prediction input shape 1 28 28 1 inference type FLOAT Error Message F tensorflow contrib lite toco tooling util.cc 831 Check failed array has shape I have created a TensorFlow model which classifies MNIST image data. The model is then frozen and optimized for inference. These models I can benchmark as well. But when converting to TensorFlow lite the toco command fails with the above mentioned error message. I have summarized the graph as well for both frozen and inference model. The frozen model has the same size of 1 28 28 1 but the inference has a size of None . I am using a placeholder when creating the model for the Input. , andrehentz could you take a look That s a strange error. Could you make sure the input and output arrays have the correct names in your command line The Input and Output arrays have the correct name. I m using tf.gather in my graph which I guess might be causing the problem as an unsupported node. It looks like this code has been changed recently so I can t pinpoint exactly where the CHECK is. It seems to be this line https github.com tensorflow tensorflow blob master tensorflow contrib lite toco tooling util.cc L841 That indicates a problem with the model or a unreasonable assumption by TOCO. Yeah that s what I was guessing. But I read the docs and it said tf.gather was not supported so I guess it won t be able to convert it. I just wanted to double check does the input the lite models need to be a fixed number e.g. the model is saved as Input 28 28 1 but for lite conversion does it need to be 1 28 28 1 Thank you for looking into this issue. Support for tf.gather has been added recently. You can resize the input tensor before inference. Call ResizeInputTensor . I have been having a similar issue. All of the operations are supported but conversion to .tflite fails because the input tensor has an input dimension of type None to allow an arbitrary number of input datapoints From the tflite dev guide https www.tensorflow.org versions master mobile tflite devguide you can convert your model to FlatBuf like so However if our input can be of arbitrary size e.g. our input x is of shape None 784 then we get Error TypeError int returned non int type NoneType . I posted a SO question https stackoverflow.com questions 50028868 tensorflow lite toco convert for arbitrary sized input tensor but have no answers yet. A M not WE could be How would one convert a model like this to the .tflite format Nagging Assignee andrehentz It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee andrehentz It has been 29 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee andrehentz It has been 30 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee andrehentz It has been 45 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee andrehentz It has been 60 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee andrehentz It has been 75 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee andrehentz It has been 90 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Wheest this seems like a separate issue. You should use a fixed batch size. In the future you should create a new issue. I think andrehentz resolved the original issue so closing this now. 
18455,eager mode fails with custom operator, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes I have written a custom tensorflow operator OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 TensorFlow installed from source or binary binary wheel TensorFlow version use command below v1.7.0 3 g024aecf414 1.7.0 Python version python 2.7.12 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Exact command to reproduce Custom operator is defined here https github.com ska sa montblanc blob f168870a9756f31c8639ff0c0219ce16af68e612 montblanc impl rime tensorflow rime ops phase op cpu.h L44 L122 but I doubt you ll want to look through that. This produces the following stack trace Describe the problem Eager mode does not work with a custom operator. Source code logs See above,Thanks for the report this was indeed an oversight. akshaym has a fix for this that should be merged soon. This should be fixed with 18504. Thanks for the report. Thanks akshaym. Will this go into the 1.8 release sjperkins Alas the fix was made after the 1.8 branch was cut so this is not included there. It will be included in 1.9. 
18512,NoOp replacement in DependencyOptimizer misses some control dependency conversions, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Debian testing buster TensorFlow installed from source or binary binary TensorFlow version use command below v1.7.0 3 g024aecf414 1.7.0 Python version 2.7.14 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Exact command to reproduce Run Python code below with environment variable TF CPP MIN VLOG LEVEL 1 Describe the problem When DependencyOptimizer OptimizeNode converts a node to a NoOp it only converts the first input from each input node to a control dependencies. Multiple inputs from the same node can cause a NoOp to be created with non control inputs. In the VLOG output below the IdentityN node has inputs Placeholder and Placeholder . I haven t tried it but perhaps when this condition https github.com tensorflow tensorflow blob 92e6c3e4f5c1cabfda1e61547a6a1b268ef95fa5 tensorflow core grappler optimizers dependency optimizer.cc L214 is false the input should be deleted like in the control input case https github.com tensorflow tensorflow blob 92e6c3e4f5c1cabfda1e61547a6a1b268ef95fa5 tensorflow core grappler optimizers dependency optimizer.cc L204 . Note that the bad NoOp gets deleted in the small example here because that was the easiest way to display the problem but the real code I was running the NoOp remains in the graph to cause trouble later. This bug is pretty sneaky a NoOp with non control inputs can cause GraphConstructor methods to fail which in turn silently prevents graph optimization see here https github.com tensorflow tensorflow blob 92e6c3e4f5c1cabfda1e61547a6a1b268ef95fa5 tensorflow core common runtime graph execution state.cc L396 . The fallback original graph runs fine. I only went investigating because I saw output like this The cause was an error message from GraphConstructor MakeEdge called here https github.com tensorflow tensorflow blob 92e6c3e4f5c1cabfda1e61547a6a1b268ef95fa5 tensorflow core graph graph constructor.cc L1008 . It appears node and node def did not agree on how many inputs there were to the node node thought zero which is where that strange DataType enum value came from reading uninitialized memory here https github.com tensorflow tensorflow blob 92e6c3e4f5c1cabfda1e61547a6a1b268ef95fa5 tensorflow core graph graph constructor.cc L1156 . I wasn t able to reproduce that exact failure in a tiny example I believe the example here causes optimization to fail here https github.com tensorflow tensorflow blob 92e6c3e4f5c1cabfda1e61547a6a1b268ef95fa5 tensorflow core graph graph constructor.cc L442 . It would also be nice to see a warning when graph optimization fails. Source code logs Code to reproduce the problem Relevant log output ,Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case or leave them as N A Thanks. Bazel version CUDA cuDNN version GPU model and memory rmlarsen Can you take a look at this This is fixed with latest version of TF 1.15.0 dev20190821 . Feel free to reopen if have any issues. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 18512 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 18512 No a 
18588,XLA implementation of FFT on CPU pulls in tf core framework, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 macos 10.11 TensorFlow installed from source or binary source TensorFlow version use command below commit 63c6562df68ade3a03481874a71b536a4e02b6f5 master as of April 15 2018 Python version n a Bazel version if compiling from source n a GCC Compiler version if compiling from source n a CUDA cuDNN version n a GPU model and memory n a Exact command to reproduce bazel build config opt tensorflow compiler xla service cpu runtime fft . It works but see below. Describe the problem Short version The CPU implementation of the XLA FFT operation appears to pull in tensorflow Tensor and ... TensorShape as a dependency via tensorflow core framework . Long version The FFT implementation comes in three flavors real to complex complex to real and complex to complex. The first two flavors involve allocating a temporary buffer for an intermediate step in the computation. This is currently achieved by creating a tensorflow Tensor object. This requires linking against tensorflow core framework . This feels like a bug or at least unintentional and undesirable. For instance every other op listed in tensorflow compiler xla service cpu BUILD besides runtime fft depends only on framework lite . My understanding re allocating temporary space was that at least for the AOT compiler not sure about JIT there were specific temporary buffers set aside and that allocation should work through that system not by just letting malloc run wild. Is that understanding correct Aside Eigen s FFT op internally calls malloc regardless of FFT flavor which likewise bypasses the AOT temporary buffers. Is that ok Is anyone currently working on this Has anyone noticed anything awry If I were to try fixing this myself would anyone have any suggestions on how to allocate a temporary buffer in an XLA friendly way I thought one possibility would be writing an Algebraic Simplifier pass to rewrite real to complex and complex to real flavors in terms of the complex to complex flavor which doesn t need to create a tf Tensor but that s my only idea. Source code logs References tensorflow compiler xla service cpu BUILD https github.com tensorflow tensorflow blob master tensorflow compiler xla service cpu BUILD tensorflow compiler xla service cpu runtime fft impl.h https github.com tensorflow tensorflow blob master tensorflow compiler xla service cpu runtime fft impl.h ,Update I went ahead and fixed it. If it s ok I ll open a PR after I run the unit tests . It still allocates a buffer on the fly so the memory profiling tools won t catch it but it ll do it with an Eigen Tensor instead of a tf Tensor so it won t need to link in core framework . I also added a proper option for single threaded FFT status quo assigns the multi threaded version even when single threaded is requested . Nagging Assignee sanjoy It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Fixed in 18685 
18598,tensorflow 1.8.0rc0 tf.compat.as str returns bytes for python3 since 20180409,The issue appeared first in tf nightly 1.8.0.dev20180409 but is now present in tensorflow 1.8.0rc0 . Reproduce steps Is expected to always print True . But gets , annarev When I trace through this code in Python 3 it does indeed look like tf.compat.as str is mapped to this function https github.com tensorflow tensorflow blob 3128b43eb0bf37ac3c49cb22a6e1789d8ea346e8 tensorflow python util compat.py L48 L68 However if I do from tensorflow.python.util import compat compat.as str maps to as text via presumably this appropriate redirection https github.com tensorflow tensorflow blob 3128b43eb0bf37ac3c49cb22a6e1789d8ea346e8 tensorflow python util compat.py L93 L97 Is it possible that the tf export decorator is rebinding the symbol incorrectly From what I can tell it has the correct behavior in Python 2 but should have different behavior in Python 3. mrry annarev I think the tf export could be explicitly called based on python 2 or 3 to address the issue. The following diff might work Added a PR 18601 for the fix. Please take a look to see if it fixes the issue. Can we get this merged in 1.8 https github.com tensorflow tensorflow blob r1.8 tensorflow python util compat.py andresusanopinto I added a PR 18662 against r1.8 to carry the fix. As 1.8.0 release is very close not sure if the fix could be applied in time though. Thanks yongtang 
18642,Keras models fit method not converging with eager execution, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes see below. OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04 TensorFlow installed from source or binary Binary with pip3 TensorFlow version use command below v1.7.0 3 g024aecf414 1.7.0 Python version 3.5.2 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version 9.0.176 7.0.5 GPU model and memory GTX 1080 Armor MSI with 8 GB VRAM Exact command to reproduce Run program below Describe the problem After enabling eager execution method tf.keras.models.Sequential.fit doesn t seem to converge as the computed loss doesn t go down stays around 9 to 11 also method fit doesn t report the requested metric accuracy it doesn t print the metric to console and does not return it in the History object. After disabling eager execution the same optimization converges as loss goes down to around 1 also method fit correctly reports the requested metric accuracy both to console and in the returned History object. Source code logs ,Thank you for the bug report. We have a fix for this issue that will show up on GitHub soon. 
19186, tf.keras Bug with Stateful Metrics Fit Generator, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04 TensorFlow installed from source or binary Source TensorFlow version use command below 1.8.0 Python version 2.7 Bazel version if compiling from source n a GCC Compiler version if compiling from source n a CUDA cuDNN version n a GPU model and memory n a Exact command to reproduce n a The stateful metrics integration with fit generator is not working. This can be demonstrated by taking the Keras metrics tests from the latest release and changing the imports to TensorFlow.keras see https github.com keras team keras blob master tests keras metrics test.py In addition the progress bar is not working with fit generator. fchollet ,I believe the equivalent tensorflow PR of https github.com keras team keras pull 9446 is missing. But this is beyond my capability at the time being I have to get approval for each open source contribution... The two key issues 1. does not get called when is used 2. Progress bar issues. See keras team keras 9446 fchollet Can you take a look at this Thanks for the report. This has been fixed internally a few days ago. It appears the fix is not yet on GitHub but it should appear in the next few days. 
19214,for loop not working in tf.contrib.autograph,The following example for autograph is not working see my system info below If I remove the for loop it works fine. With the loop it fails with python3 version Python 3.5.2 python3 c import tensorflow as tf print tf.GIT VERSION tf.VERSION v1.8.0 0 g93bc2e2072 1.8.0 cat etc issue Linux 5a7b67a966f3 4.9.87 linuxkit aufs 1 SMP Wed Mar 14 15 12 16 UTC 2018 x86 64 x86 64 x86 64 GNU Linux VERSION 16.04.3 LTS Xenial Xerus VERSION ID 16.04 VERSION CODENAME xenial are we in docker Yes compiler c Ubuntu 5.4.0 6ubuntu1 16.04.9 5.4.0 20160609 Copyright C 2015 Free Software Foundation Inc. This is free software see the source for copying conditions. There is NO warranty not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. uname a Linux 5a7b67a966f3 4.9.87 linuxkit aufs 1 SMP Wed Mar 14 15 12 16 UTC 2018 x86 64 x86 64 x86 64 GNU Linux check pips check for virtualenv False tensorflow import tf.VERSION 1.8.0 tf.GIT VERSION v1.8.0 0 g93bc2e2072 tf.COMPILER VERSION v1.8.0 0 g93bc2e2072 Sanity check array 1 dtype int32 env LD LIBRARY PATH is unset DYLD LIBRARY PATH is unset nvidia smi . tf env collect.sh line 105 nvidia smi command not found cuda libs ,Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case or leave them as N A Thanks. Have I written custom code OS Platform and Distribution TensorFlow installed from TensorFlow version Bazel version CUDA cuDNN version GPU model and memory Exact command to reproduce Sorry for the delay This is a known issue we still need to properly handle the cases where a code block in this case the body of the for loop doesn t calculate any numeric values. For now you can work around this limitation by having the loop perform some computation on an existing variable like for instance Side note we also need to make sure the argument passed to range is a tensor otherwise the loop will be statically unrolled. We re still debating whether to keep this behavior because it s been a source of confusion. Quick update this now works in TF2 with tf.function https www.tensorflow.org versions r2.0 api docs python tf function which uses a stable subset of autograph Final update the loop uses the type to decide whether to run as a Python or TF loop. So if the iterated is a Tensor then the loop will run in TF. Otherwise it stays in Python. Examples We found that this is the way that s most consistent with other instances like for instance overloaded operators e.g. a b runs in TF if either a or b is a tensor . 
19274,tf.matching files fails when there is a file with wrong permissions in a subdirectory , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu18.04 TensorFlow installed from source or binary pip installed binary TensorFlow version use command below 1.8 Python version 3.6 Bazel version if compiling from source NA GCC Compiler version if compiling from source NA CUDA cuDNN version NA GPU model and memory NA Exact command to reproduce see below Describe the problem This simple script fails if some dir contains a file without read access somewhere in subfolders of some dir . So for instance if I don t have permission to read some dir subfolder filebelongingtosomeoneelse the script fails while I have permissions for subfolder . Strange thing is that neither read access needed to know its filename nor it is actually will be listed in the output so it should be skipped during scanning . ,I can verify the issue. Created a PR 19307 for the fix. yongtang PR should address this issue. Will follow up on that. Please remove the assignee as this issue is inviting external contributions. Otherwise remove the contributions welcome label. Thank you. Please remove the assignee as this issue is inviting external contributions. Otherwise remove the contributions welcome label. Thank you. 
19333,Bug in EluGradGrad, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes see below OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 TensorFlow installed from source or binary binary latest pip tf nightly TensorFlow version use command below v1.8.0 1674 gd8fac4cb80 1.9.0 dev20180515 Python version Python 3.6.3 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version CUDA Version 9.1.85 GPU model and memory GeForce GTX 970 Exact command to reproduce See script attached Describe the problem There seems to be an issue with EluGradGrad that I have uncovered. Please see the post here https github.com renmengye tensorflow forward ad issues 2 issuecomment 389321546 Source code logs Running this script will demonstrate the incorrect values The solution is to edit the implementation of EluGradGrad here https github.com tensorflow tensorflow blob f318765ad5a50b2fbd7cc08dd4ebc249b3924270 tensorflow python ops nn grad.py L364 . I ve created a pull request that references this issue.,Nagging Assignee tatianashp It has been 15 days with no activity and this issue has an assignee. Please update the label and or status accordingly. zero impact Thank you for submitting PR 19334 
19480,Manually placing operations in eager execution raises FailedPreconditionError., System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux TensorFlow installed from source or binary Binary TensorFlow version use command below tensorflow 1.6 Python version python3.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version V9.0.176 GPU model and memory NVIDIA Corporation GV100GL Tesla V100 SXM2 16GB rev a1 Exact command to reproduce Describe the problem I have a model that I believe was not automatically being placed onto an available GPU. I then placed this part of the computation inside a with device block. This schematically looks like The error is thrown during the loss calculation step. Source code logs 2018 05 22 13 57 11.963021 I tensorflow core platform cpu feature guard.cc 140 Your CPU supports instructions that this TensorFlow binary was not compiled to use AVX2 FMA 2018 05 22 13 57 12.324280 I tensorflow core common runtime gpu gpu device.cc 1212 Found device 0 with properties name Tesla V100 SXM2 16GB major 7 minor 0 memoryClockRate GHz 1.53 pciBusID 0000 04 00.0 totalMemory 15.78GiB freeMemory 15.36GiB 2018 05 22 13 57 12.324584 I tensorflow core common runtime gpu gpu device.cc 1312 Adding visible gpu devices 0 2018 05 22 13 57 12.623548 I tensorflow core common runtime gpu gpu device.cc 993 Creating TensorFlow device job localhost replica 0 task 0 device GPU 0 with 14878 MB memory physical GPU device 0 name Tesla V100 SXM2 16GB pci bus id 0000 04 00.0 compute capability 7.0 Traceback most recent call last File tf registration continuous.py line 619 in module cProfile.run main file File share software user open python 3.6.1 lib python3.6 cProfile.py line 16 in run return pyprofile. Utils Profile .run statement filename sort File share software user open python 3.6.1 lib python3.6 profile.py line 55 in run prof.run statement File share software user open python 3.6.1 lib python3.6 cProfile.py line 95 in run return self.runctx cmd dict dict File share software user open python 3.6.1 lib python3.6 cProfile.py line 100 in runctx exec cmd globals locals File string line 1 in module File tf registration continuous.py line 615 in main accuracy runtime final loss run registration directory output hparams run.name hparams hparams run dataset path dataset path save figs False File tf registration continuous.py line 525 in run registration rm.register save summaries save figs make animation save figs File tf registration continuous.py line 190 in register num images to optimize params self.hparams.num images to optimize params File tf registration continuous.py line 105 in single registration step self.eif.warp scale num images for loss calculation File home groups bmacint Ultrasound timing elastic image field.py line 118 in warp field image.initialize translation File home groups bmacint Ultrasound timing field image.py line 87 in initialize translation self.translation warp points tf.tile self.translation tf.newaxis tf.newaxis File share software user open py tensorflow 1.6.0 py36 lib python3.6 site packages tensorflow python ops array ops.py line 828 in SliceHelperVar return slice helper var. AsTensor slice spec var File share software user open py tensorflow 1.6.0 py36 lib python3.6 site packages tensorflow python ops resource variable ops.py line 741 in AsTensor return self.value File share software user open py tensorflow 1.6.0 py36 lib python3.6 site packages tensorflow python ops resource variable ops.py line 572 in value return self. read variable op File share software user open py tensorflow 1.6.0 py36 lib python3.6 site packages tensorflow python ops resource variable ops.py line 655 in read variable op self. dtype File share software user open py tensorflow 1.6.0 py36 lib python3.6 site packages tensorflow python ops gen resource variable ops.py line 304 in read variable op attrs attrs ctx ctx name name File share software user open py tensorflow 1.6.0 py36 lib python3.6 site packages tensorflow python eager execute.py line 66 in quick execute six.raise from core. status to exception e.code message None File string line 2 in raise from tensorflow.python.framework.errors impl.FailedPreconditionError Error while reading resource variable 152 from Container eager execution 0 . This could mean that the variable was uninitialized. Invalid argument Trying to access resource located in device job localhost replica 0 task 0 device CPU 0 from device job localhost replica 0 task 0 device GPU 0 Op ReadVariableOp ,I have been able to reproduce this issue with a simple minimal example The error is similar to the one quoted above 2018 05 22 15 31 14.700544 I tensorflow core platform cpu feature guard.cc 140 Your CPU supports instructions that this TensorFlow binary was not compiled to use AVX2 FMA 2018 05 22 15 31 14.995858 I tensorflow core common runtime gpu gpu device.cc 1212 Found device 0 with properties name Tesla K80 major 3 minor 7 memoryClockRate GHz 0.8235 pciBusID 0000 84 00.0 totalMemory 11.92GiB freeMemory 11.85GiB 2018 05 22 15 31 14.996414 I tensorflow core common runtime gpu gpu device.cc 1312 Adding visible gpu devices 0 2018 05 22 15 31 15.471576 I tensorflow core common runtime gpu gpu device.cc 993 Creating TensorFlow device job localhost replica 0 task 0 device GPU 0 with 11492 MB memory physical GPU device 0 name Tesla K80 pci bus id 0000 84 00.0 compute capability 3.7 Device mapping job localhost replica 0 task 0 device GPU 0 device 0 name Tesla K80 pci bus id 0000 84 00.0 compute capability 3.7 2018 05 22 15 31 15.717080 I tensorflow core common runtime direct session.cc 297 Device mapping job localhost replica 0 task 0 device GPU 0 device 0 name Tesla K80 pci bus id 0000 84 00.0 compute capability 3.7 tf.Variable Variable 0 shape 2 dtype float32 numpy array 2. 3. dtype float32 Traceback most recent call last File minimal failing case.py line 16 in module print a File share software user open py tensorflow 1.6.0 py36 lib python3.6 site packages tensorflow python ops variables.py line 239 in repr ops.numpy text self.read value is repr True File share software user open py tensorflow 1.6.0 py36 lib python3.6 site packages tensorflow python ops resource variable ops.py line 677 in read value value self. read variable op File share software user open py tensorflow 1.6.0 py36 lib python3.6 site packages tensorflow python ops resource variable ops.py line 655 in read variable op self. dtype File share software user open py tensorflow 1.6.0 py36 lib python3.6 site packages tensorflow python ops gen resource variable ops.py line 304 in read variable op attrs attrs ctx ctx name name File share software user open py tensorflow 1.6.0 py36 lib python3.6 site packages tensorflow python eager execute.py line 66 in quick execute six.raise from core. status to exception e.code message None File string line 2 in raise from tensorflow.python.framework.errors impl.FailedPreconditionError Error while reading resource variable 2 from Container eager execution 0 . This could mean that the variable was uninitialized. Invalid argument Trying to access resource located in device job localhost replica 0 task 0 device CPU 0 from device job localhost replica 0 task 0 device GPU 0 Op ReadVariableOp alextp PTAL. Thanks akshaym can you take a look at this Something which is supposed to force colocate the read variable op with the handle is not force colocating it. The colocation code is in https github.com tensorflow tensorflow blob 09b562c8f21a8f9fcae71cbb5b97434c06ba1ca9 tensorflow core common runtime eager execute.cc L432 but some condition must be missed. Hi Noahyt Thanks for the report. I m able to reproduce this on 1.6.0 but unable to reproduce on the latest version of tensorflow 1.8 so its been fixed already. It d be great if you could upgrade your installation of tensorflow Thanks I m noticing a simiilar problem with TF 2.0 nightly 2.0.0 dev20190228 which enforces Eager mode. The following minimal code produces error I ve been able to fix this in 3 ways 1 use tensorflow.compat.v1.disable eager execution 2 remove the Conv2D layer 3 remove the batch size and epochs arguments from the .fit call the behavior seems weird can anyone explain what s going on I ve also opened a separate issue 26244 
19496,Segfault with rpc ops in eager mode, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04 TensorFlow installed from source or binary binary pip TensorFlow version use command below 1.8 Python version 3.6 Bazel version if compiling from source NA GCC Compiler version if compiling from source NA CUDA cuDNN version NA GPU model and memory NA Exact command to reproduce see below Describe the problem Rpc ops rpc try rpc are not working in eager mode. See minimal examples below. Source code logs Non eager version works Eager version does not results in segfault , jsimsa Mind taking a look will do This was just fixed internally and will be pushed to master in the next sync. 
19497,NHWC convolution sometimes incorrectly considered NCHW, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 TensorFlow installed from source or binary binary TensorFlow version use command below 1.7.0 Python version 3.5.2 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 9.0.176 7.0.5 GPU model and memory GeForce GTX 1050 Ti PCIe SSE2 4GB Exact command to reproduce custom script Describe the problem When using dilated conv2d and bias add the conv2d is incorrectly considered NCHW so the normal dilation format is not accepted. Strangely when there is no bias add the problem does not happen. Source code logs This gives ERROR tensorflow.python.framework.errors impl.InvalidArgumentError Current implementation does not yet support dilations in the batch and depth dimensions. Node Conv2D Conv2D T DT FLOAT data format NCHW dilations 1 4 4 1 padding VALID strides 1 1 1 1 use cudnn on gpu true device job localhost replica 0 task 0 device GPU 0 Conv2D 0 TransposeNHWCToNCHW LayoutOptimizer filter read Interestingly this works as intended ,I tried with v1.8.0 and it works fine. Maybe the issue has been resolved Hi Thanks for trying I ll check out the 1.8 version tomorrow at work and then confirm the results to you. Cheers It is fixed in 1.8. Thank you. It was good only in tf 1.8 CPU version but in the GPU version it is still erroneous. yongtang please confirm zheng xq Can you take a look This seems to be a layout optimizer issue. Adding Yao. Thanks for reporting the bug. I have submitted a fix which will be pushed to TensorFlow github and reflected in nightly build in a day or two it will also be part of the next release. More info on nightly build https github.com tensorflow tensorflow Hi I am also using 1.7 version and facing the same issue. Is there any fix for it 
19551,Cannot use AdagradOptimizer with MirroredStrategy, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 TensorFlow installed from source or binary Binary TensorFlow version use command below v1.8.0 0 g93bc2e2072 1.8.0 Python version 3.6.3 64 bit Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 9.0 6.0 GPU model and memory Nvidia GTX 1080 8 GB Exact command to reproduce python train model.py Describe the problem It took me a while to work out what was going on but it seems that tf.train.AdagradOptimizer has some specific implementation detail that causes an error when used with MirroredStrategy. I did a spot check with GradientDescentOptimizer and RMSPropOptimizer and they both appear to work in my environment. I m happy to use a different optimizer as a workaround but I thought at the very least this might save others some time hunting down the cause of the error Source code logs This is almost exactly copied from the example at https github.com tensorflow tensorflow tree master tensorflow contrib distribute except for the choice of optimizer import tensorflow as tf def model fn features labels mode layer tf.layers.Dense 1 logits layer features if mode tf.estimator.ModeKeys.PREDICT predictions logits logits return tf.estimator.EstimatorSpec mode predictions predictions loss tf.losses.mean squared error labels labels predictions tf.reshape logits if mode tf.estimator.ModeKeys.EVAL return tf.estimator.EstimatorSpec mode loss loss if mode tf.estimator.ModeKeys.TRAIN train op tf.train.AdagradOptimizer 0.2 .minimize loss return tf.estimator.EstimatorSpec mode loss loss train op train op def input fn features tf.data.Dataset.from tensors 1. .repeat 100 labels tf.data.Dataset.from tensors 1. .repeat 100 return tf.data.Dataset.zip features labels distribution tf.contrib.distribute.MirroredStrategy config tf.estimator.RunConfig train distribute distribution classifier tf.estimator.Estimator model fn model fn config config classifier.train input fn input fn Log output 2018 05 25 15 30 12.300908 I tensorflow core platform cpu feature guard.cc 140 Your CPU supports instructions that this TensorFlow binary was not compiled to use AVX2 FMA 2018 05 25 15 30 14.231174 I tensorflow core common runtime gpu gpu device.cc 1356 Found device 0 with properties name GeForce GTX 1080 major 6 minor 1 memoryClockRate GHz 1.898 pciBusID 0000 03 00.0 totalMemory 7.93GiB freeMemory 7.81GiB 2018 05 25 15 30 14.557081 I tensorflow core common runtime gpu gpu device.cc 1356 Found device 1 with properties name GeForce GTX 1080 major 6 minor 1 memoryClockRate GHz 1.898 pciBusID 0000 81 00.0 totalMemory 7.93GiB freeMemory 7.81GiB 2018 05 25 15 30 14.557174 I tensorflow core common runtime gpu gpu device.cc 1435 Adding visible gpu devices 0 1 2018 05 25 15 30 15.198082 I tensorflow core common runtime gpu gpu device.cc 923 Device interconnect StreamExecutor with strength 1 edge matrix 2018 05 25 15 30 15.198134 I tensorflow core common runtime gpu gpu device.cc 929 0 1 2018 05 25 15 30 15.198142 I tensorflow core common runtime gpu gpu device.cc 942 0 N N 2018 05 25 15 30 15.198145 I tensorflow core common runtime gpu gpu device.cc 942 1 N N 2018 05 25 15 30 15.198488 I tensorflow core common runtime gpu gpu device.cc 1053 Created TensorFlow device job localhost replica 0 task 0 device GPU 0 with 7542 MB memory physical GPU device 0 name GeForce GTX 1080 pci bus id 0000 03 00.0 compute capability 6.1 2018 05 25 15 30 15.324510 I tensorflow core common runtime gpu gpu device.cc 1053 Created TensorFlow device job localhost replica 0 task 0 device GPU 1 with 7543 MB memory physical GPU device 1 name GeForce GTX 1080 pci bus id 0000 81 00.0 compute capability 6.1 WARNING tensorflow Using temporary folder as model directory tmp tmpqglycjzk 2018 05 25 15 30 15.455314 I tensorflow core common runtime gpu gpu device.cc 1435 Adding visible gpu devices 0 1 2018 05 25 15 30 15.455414 I tensorflow core common runtime gpu gpu device.cc 923 Device interconnect StreamExecutor with strength 1 edge matrix 2018 05 25 15 30 15.455423 I tensorflow core common runtime gpu gpu device.cc 929 0 1 2018 05 25 15 30 15.455427 I tensorflow core common runtime gpu gpu device.cc 942 0 N N 2018 05 25 15 30 15.455431 I tensorflow core common runtime gpu gpu device.cc 942 1 N N 2018 05 25 15 30 15.455615 I tensorflow core common runtime gpu gpu device.cc 1053 Created TensorFlow device device GPU 0 with 7542 MB memory physical GPU device 0 name GeForce GTX 1080 pci bus id 0000 03 00.0 compute capability 6.1 2018 05 25 15 30 15.455720 I tensorflow core common runtime gpu gpu device.cc 1053 Created TensorFlow device device GPU 1 with 7543 MB memory physical GPU device 1 name GeForce GTX 1080 pci bus id 0000 81 00.0 compute capability 6.1 Traceback most recent call last File train model.py line 62 in module classifier.train input fn input fn File home aeiq .local share virtualenvs echoiq 308 HW9bcZ4A lib python3.6 site packages tensorflow python estimator estimator.py line 363 in train loss self. train model input fn hooks saving listeners File home aeiq .local share virtualenvs echoiq 308 HW9bcZ4A lib python3.6 site packages tensorflow python estimator estimator.py line 841 in train model return self. train model distributed input fn hooks saving listeners File home aeiq .local share virtualenvs echoiq 308 HW9bcZ4A lib python3.6 site packages tensorflow python estimator estimator.py line 884 in train model distributed self.config File home aeiq .local share virtualenvs echoiq 308 HW9bcZ4A lib python3.6 site packages tensorflow python training distribute.py line 756 in call for each tower return self. call for each tower fn args kwargs File home aeiq .local share virtualenvs echoiq 308 HW9bcZ4A lib python3.6 site packages tensorflow contrib distribute python mirrored strategy.py line 254 in call for each tower coord.join threads File home aeiq .local share virtualenvs echoiq 308 HW9bcZ4A lib python3.6 site packages tensorflow python training coordinator.py line 389 in join six.reraise self. exc info to raise File home aeiq .local share virtualenvs echoiq 308 HW9bcZ4A lib python3.6 site packages six.py line 693 in reraise raise value File home aeiq .local share virtualenvs echoiq 308 HW9bcZ4A lib python3.6 site packages tensorflow python training coordinator.py line 297 in stop on exception yield File home aeiq .local share virtualenvs echoiq 308 HW9bcZ4A lib python3.6 site packages tensorflow contrib distribute python mirrored strategy.py line 248 in call for each tower self merge args merge kwargs File home aeiq .local share virtualenvs echoiq 308 HW9bcZ4A lib python3.6 site packages tensorflow python training optimizer.py line 671 in distributed apply self. create slots var list File home aeiq .local share virtualenvs echoiq 308 HW9bcZ4A lib python3.6 site packages tensorflow python training adagrad.py line 66 in create slots with ops.colocate with v File home aeiq .pyenv versions 3.6.3 lib python3.6 contextlib.py line 81 in enter return next self.gen File home aeiq .local share virtualenvs echoiq 308 HW9bcZ4A lib python3.6 site packages tensorflow python framework ops.py line 4186 in colocate with for gradient with self.colocate with op ignore existing File home aeiq .pyenv versions 3.6.3 lib python3.6 contextlib.py line 81 in enter return next self.gen File home aeiq .local share virtualenvs echoiq 308 HW9bcZ4A lib python3.6 site packages tensorflow python framework ops.py line 4239 in colocate with op internal convert to tensor or indexed slices op as ref True .op File home aeiq .local share virtualenvs echoiq 308 HW9bcZ4A lib python3.6 site packages tensorflow python framework ops.py line 1262 in internal convert to tensor or indexed slices value dtype dtype name name as ref as ref File home aeiq .local share virtualenvs echoiq 308 HW9bcZ4A lib python3.6 site packages tensorflow python framework ops.py line 1104 in internal convert to tensor ret conversion func value dtype dtype name name as ref as ref File home aeiq .local share virtualenvs echoiq 308 HW9bcZ4A lib python3.6 site packages tensorflow contrib distribute python values.py line 243 in tensor conversion assert not as ref AssertionError ,I got the same problem when using the MovingAverageOptimizer The error with some context Thanks for bringing this to our attention. We will look into why AdagadOptimizer and MovingAverageOptimizer don t work. Hi All I m getting this same Error with Adagrad the default for pre canned tf.estimarot.DNNClassifier Here s my definitions Below is the ERROR message Nagging Assignee anj s It has been 15 days with no activity and this issue has an assignee. Please update the label and or status accordingly. optimizer1 tf.train. GradientDescentOptimizer learning rate FLAGS.tnet lr optimizer1 tf.contrib.estimator.TowerOptimizer optimizer1 optimizer2 tf.train. GradientDescentOptimizer learning rate FLAGS.mnet lr optimizer2 tf.contrib.estimator.TowerOptimizer optimizer2 tnet variables get model variables tnet mnet variables get model variables mnet train op1 slim.learning.create train op loss optimizer1 variables to train tnet variables summarize gradients True train op2 slim.learning.create train op loss optimizer2 variables to train mnet variables summarize gradients True train op tf.group train op1 train op2 Above two GradientDescentOptimizers do not work either. 
19588,Using Dataset api with Estimator in MirroredStrategy Non DMA safe string tensor error, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 centos TensorFlow installed from source or binary pip install tensorflow gpu TensorFlow version use command below 1.8.0 Python version Bazel version if compiling from source GCC Compiler version if compiling from source 4.8.5 CUDA cuDNN version 9.0 GPU model and memory GeForce GTX 1080Ti 4 Exact command to reproduce Describe the problem Using mutilple gpu by MirroredStrategy Get Non DMA safe string tensor may not be copied from to a GPU. error Source code logs image https user images.githubusercontent.com 12636388 40601170 a716ad26 6286 11e8 9fb1 7b5d1b6c6545.png image https user images.githubusercontent.com 12636388 40601198 c0ad1f36 6286 11e8 956c 3023fcc86292.png image https user images.githubusercontent.com 12636388 40601204 c6ecdaf8 6286 11e8 96cd 588ef276f5b4.png image https user images.githubusercontent.com 12636388 40601209 cbc60c16 6286 11e8 8dd0 6b54df3ab8fe.png ,I meet the same problem skye in using object detection apis Can you provide code to repro the problem Rohan Priya I m guessing this is what happens when a tf.string tensor goes through prefetch to devices but unclear whether it should be handled in the client program e.g. by splitting out strings from the prefetched dataset or in the FunctionBufferingResource e.g. by allowing some outputs to be host memory only . def get inputs mode csv file batch size label list preprocess iterator initializer hook IteratorInitializerHook def inputs is training mode estimator.ModeKeys.TRAIN ds tf.data.TextLineDataset csv file .skip 1 def classification parse line line columns img label img name label tf.decode csv line record defaults assume every pic is rgb image decoded tf.image.decode png tf.read file img name channels 3 image preprocess image decoded image image preprocessing fn image image size image size return image label cpu num multiprocessing.cpu count ds ds.apply tf.contrib.data.map and batch classification parse line batch size batch size num parallel batches cpu num ds ds.prefetch None iterator ds.make initializable iterator iterator initializer hook.iterator initializer func lambda sess sess.run iterator.initializer return ds return iterator initializer hook inputs distribution tf.contrib.distribute.MirroredStrategy config tf.estimator.RunConfig model dir args.model dir tf random seed 912 save summary steps args.save summary steps save checkpoints steps args.save interval steps keep checkpoint max 5 get num replicas train distribute distribution session config session config classifier tf.estimator.Estimator my model config config params params for epoch in range args.num epochs logger.info Starting epoch d d epoch 1 args.num epochs classifier.train train ds hooks train ds hook classifier.evaluate val ds hooks val ds hook Nothing special its just common code with MirroredStrategy. If replace MirroredStrategy with OneDeviceStrategy everything went well Nagging Assignees rohan100jain guptapriya It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. As Derek mentioned currently we don t have a mechanism for specifying if some outputs should be in host memory and we assume to a large extent that they d be in device memory. Strings can t be in device memory hence the bug. I shall work on having a dynamic method of identifying which outputs should be allocated on the host device. Stay tuned for a fix in a bit. rohan100jain can this issue be closed now that your fix has been merged 
19626,FailedPreconditionError Table already initialized when use Feature Column in Eager mode, Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Latest Mac OS TensorFlow installed from source or binary binary TensorFlow version use command below 1.8.0 Python version 3.6.5 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Exact command to reproduce see below Describe the problem Describe the problem clearly here. Be sure to convey here why it s a bug in TensorFlow or a feature request. Source code logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem. ,Maybe alextp has some idea on how to avoid re initialization of lookup tables in eager mode. Seems embedding column is not supported in eager mode 68ffc85450d328cf9e1323dd0021c6671110c5fb. How about indicator column and categorical column Are they supported in eager mode rohan100jain you were working on this This is also the case when using multiple tf.contrib.lookup.HashTable tf.contrib.lookup.KeyValueTensorInitializer keys values 1 calls in Eager mode even when assigning different names in each method. I m working on a solution to this problem. I d estimate a couple of weeks before it gets out. rohan100jain Thanks I see the error when using a simple vocab table.lookup operation. Is there any workaround this to run it in eager mode rohan100jain Any updates from your side Closing for now as I do not have time to reproduce it. 
19838,optimize for inference lib.optimize for inference produces an invalid graph, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes. OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 TensorFlow installed from source or binary Source. TensorFlow version use command below 1.8.0 Python version 2.7.12 Bazel version if compiling from source 0.13.0 GCC Compiler version if compiling from source gcc version 5.4.0 20160609 Ubuntu 5.4.0 6ubuntu1 16.04.9 CUDA cuDNN version 9.0.176 7.0.5.15 GPU model and memory 1080 Ti Exact command to reproduce Describe the problem optimize for inference lib.optimize for inference produces an invalid graph for the graph generated by the above script. The returned GraphDef cannot be imported ,I met the same problem when i use the .pb produced by quantize graph same problem with latest tensorflow optimized it using optimize for inference works error After using optimized for inference bin the input shape changed. tf version 1.9 I observed a similar problem. The following code works perfectly well with use optimize for inference False but gives the error if I use the optimize for inference routine. The code The problem is either related to the use of the boolean variable or the conditional. I observed it both in Tensorflow 1.11.0 Python 3.7 and 1.8.0 Python 3.6 . For now my solution is to just not use the optimize for inference script as it doesn t benefit me that much in my use.... I met the same problem when i use the .pb produced by quantize graph Hi kasyoukin I also met the same issue when use the .pb produced by quantize graph. Did you find the solution I met exactly the same problem and it seems like that the problem is caused by tf.cond . Same problem looks like the dropout node. I was also trying to use an optimized graph. It looks like I can used an unoptimized graph just fine. I could reproduce the issue with TF1.13.1. Thanks Here is the error log In my opinion it caused by tf.cond but without any solutions unless remove this node. Facing the a similar issue when using LSTM in my model. Any leads so far ValueError NodeDef expected inputs do not match 1 inputs specified Op name Const signature output dtype attr value tensor attr dtype type NodeDef node lstm 1 while add y Const output shapes dtype DT INT32 value Tensor type int32 shape values 1 lstm 1 while Switch 1 I met the same problem do not match 1 inputs specified... yesterday. The reason seem to be related tf.graph util.remove training nodes or this function wrapped in tensorflow.python.tools . I am unable to repro this in 1.14 for the examples posted by yegord and dvicini. Please reopen with a repro if this is still not working for you. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 19838 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 19838 No a I am unable to repro this in 1.14 for the examples posted by yegord and dvicini. Please reopen with a repro if this is still not working for you. What about Tf 1.13.1 entity tagging bidirectional rnn fw fw cond fw multi rnn cell cell 0 basic lstm cell MatMul Switch expects to be colocated with unknown node entity tagging bidirectional rnn fw multi rnn cell cell 0 basic lstm cell kernel read when use quantize tools cause this problem solved this problem by delete graph util.remove training nodes function reproduct the it 1. train a model by tf models official transformer 2. do freeze graph for the model which created at step 1 3. try to import the graph def which created at step 2 I m having the same issue. For me it seems to occur in . When I pass the bool to the call operator the error dissapears. Any idea on why this happens use tensorflow 1.14.0 Same issue with TF 1.13.1 caused by batch norm. ValueError NodeDef expected inputs do not match 1 inputs specified Op name Const signature output dtype attr value tensor attr dtype type NodeDef node import model conv1 batch norm cond Const Disappear with TF 1.14.0 
20021, BUG TensorFlow crashed when using tf.cast in dataset s map function,Please go to Stack Overflow for help and support https stackoverflow.com questions tagged tensorflow If you open a GitHub issue here is our policy 1. It must be a bug a feature request or a significant problem with documentation for small docs fixes please send a PR instead . 2. The form below must be filled out. 3. It shouldn t be a TensorBoard issue. Those go here https github.com tensorflow tensorboard issues . Here s why we have that policy TensorFlow developers respond to issues. We want to focus on work that benefits the whole community e.g. fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem rather than being redirected to Stack Overflow. System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 TensorFlow installed from source or binary pip TensorFlow version use command below 1.8.0 GPU Python version 3.6.4 Bazel version if compiling from source No GCC Compiler version if compiling from source No CUDA cuDNN version 9.0 7 GPU model and memory GPU GPU 0 GeForce GTX 1080 Ti x2 RAM 16GB Exact command to reproduce No You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION Describe the problem When I using TensorFlow eager execution with gpu I want to read some data so I use tf.data.Dataset . Then I want to pre process the data before training then I use dataset.map and tfe.py func . Also I use dataset.prefetch 1 to read data with pipelining Everything is fine so far. But when I using tf.cast in map function the program will crash. Source code logs run the code output 2018 06 14 13 29 48.784627 I tensorflow core platform cpu feature guard.cc 140 Your CPU supports instructions that this TensorFlow binary was not compiled to use AVX2 AVX512F FMA 2018 06 14 13 29 48.925618 I tensorflow core common runtime gpu gpu device.cc 1356 Found device 0 with properties name GeForce GTX 1080 Ti major 6 minor 1 memoryClockRate GHz 1.582 pciBusID 0000 17 00.0 totalMemory 10.92GiB freeMemory 10.45GiB 2018 06 14 13 29 49.063957 I tensorflow core common runtime gpu gpu device.cc 1356 Found device 1 with properties name GeForce GTX 1080 Ti major 6 minor 1 memoryClockRate GHz 1.582 pciBusID 0000 65 00.0 totalMemory 10.92GiB freeMemory 10.44GiB 2018 06 14 13 29 49.064088 I tensorflow core common runtime gpu gpu device.cc 1435 Adding visible gpu devices 0 1 2018 06 14 13 29 49.449500 I tensorflow core common runtime gpu gpu device.cc 923 Device interconnect StreamExecutor with strength 1 edge matrix 2018 06 14 13 29 49.449552 I tensorflow core common runtime gpu gpu device.cc 929 0 1 2018 06 14 13 29 49.449562 I tensorflow core common runtime gpu gpu device.cc 942 0 N Y 2018 06 14 13 29 49.449570 I tensorflow core common runtime gpu gpu device.cc 942 1 Y N 2018 06 14 13 29 49.449834 I tensorflow core common runtime gpu gpu device.cc 1053 Created TensorFlow device job localhost replica 0 task 0 device GPU 0 with 10108 MB memory physical GPU device 0 name GeForce GTX 1080 Ti pci bus id 0000 17 00.0 compute capability 6.1 2018 06 14 13 29 49.543343 I tensorflow core common runtime gpu gpu device.cc 1053 Created TensorFlow device job localhost replica 0 task 0 device GPU 1 with 10107 MB memory physical GPU device 1 name GeForce GTX 1080 Ti pci bus id 0000 65 00.0 compute capability 6.1 1 84561 segmentation fault core dumped python dataset eager bug.py,Akshay I ve run a few versions of this and think it could be a bug in tfe.py func so could you please take a look Here are some notes The crash only happens 1 when a GPU is available and 2 when Dataset.prefetch is used. The crash doesn t seem to depend on using tf.cast . Replacing fn with lambda tf.constant 1. 2. 3. tf.constant 4. 5. 6. gives the same failure. The crashing stack suggests that we are treating an EagerTensor that is actually in GPU memory as if it were in host memory When Dataset.prefetch is used the tf.cast kernel runs on GPU. When the Dataset.prefetch is removed the tf.cast kernel runs on CPU. The main difference between these two cases is that when Dataset.prefetch is used the body of the py func will run on a background thread so maybe some thread local state isn t configured Even though the tf.cast kernel runs on GPU when Dataset.prefetch is used the TensorHandle for its result has nullptr for its device which appears to be why EagerTensor numpy thinks it is safe to copy. CC asimshankar since the TensorHandle device method has comments with TODOs against his name and so he might know where the skeletons are . Thanks for the notes Derek. I ll dig in further. Thanks for the report DHZS and investigation mrry . While we dig into an appropriate fix a quick workaround would be to explicitly include a with tf.device CPU 0 in your py func or just add a .cpu to the outputs to ensure that the returned tensors are in CPU. So something like Thank you akshayka it works for me. By the way I have a related issue https github.com tensorflow tensorflow issues 19945 maybe it s helpful for you to find the bug. 
20073,Can t load hdf5 files when using tf.keras.SequentialModel.add and ModelCheckpoint ,Bug. Compiled Tensorflow 1.8 and 1.9rc0 both fail. Linux Mint 18.2. Python 3.5.2 CUDA 9.0 CuDNN 7.0 Have I written custom code I ve attached a file OS Platform and Distribution Linux Mint 18.2 TensorFlow installed from source TensorFlow version 1.8 and 1.9rc0 Bazel version 0.13.1 CUDA cuDNN version 9.0 7.0 GPU model and memory Nvidia 1080Ti Exact command to reproduce run the attached script. When using tf.keras.SequentialModel to build a network and using tf.keras.callbacks.ModelCheckpoint to save the model weights during training the saved hdf5 file cannot be loaded into the same model. I believe the same problem was observed in keras 1.0 See here https github.com keras team keras issues 2281 And reportedly fixed here https github.com keras team keras commit 1206120d1084cbe45dc2876f002cb572a97e3844 I ve attached a minimal script which will readily reproduce the problem here save bug.txt https github.com tensorflow tensorflow files 2107894 save bug.txt The output error I receive when attempting to load the model weights into the same model which I saved them from is ,Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case or leave them as N A Thanks. Have I written custom code OS Platform and Distribution TensorFlow installed from TensorFlow version Bazel version CUDA cuDNN version GPU model and memory Exact command to reproduce updated description fchollet Can you take a look at this Got the same error while trying to load saved Sequential model. Tensorflow built from r1.9 branch yesterday. I get a similar error message in this notebook https github.com Hvass Labs TensorFlow Tutorials blob master 19 Hyper Parameters.ipynb which worked perfectly with TensorFlow v. 1.4.0 and Keras v. 2.0.8 tf. I have just upgraded to TensorFlow 1.9 and Keras 2.1.6 tf and now I get an error. I have done an internet search and found many people experiencing different variations of this bug. This is the complete error message I get when calling load model in Keras I have a similar error with TF 1.9 and some models containing Lambda and Conv2D layers but other models seem to load correctly The weights are in the HDF5 file and are saved correctly at https github.com tensorflow tensorflow blob r1.9 tensorflow python keras engine saving.py L145 I added logs in the load save but when loading from https github.com tensorflow tensorflow blob r1.9 tensorflow python keras engine saving.py L716 they appear empty I guess it comes from the fact that the model is not initialized with placeholder weights when creating at https github.com tensorflow tensorflow blob r1.9 tensorflow python keras engine saving.py L229 so the loader assumes that the layers don t have weights Same for me tf.keras models saved and loaded fine when I added a reshape layer they still saved but I can t load them anymore. Model model.add tf.keras.layers.Reshape 76832 Of course this is just flattening the input and someone can argue that tf.keras.layers.Flatten does the same thing but this is just needed because of a TensorRT Bug so reshape is my only option here. Hope that somebody can take a closer look into that. This is still happening with tf 1.11.0 keras 2.1.6 tf. Repro This errors out with It appears to work without the InputLayer e.g. In the working case model.layers 0 .get config has batch input shape None 8 8 1 In the non working case batch input shape is absent. For those still struggling here is my workaround instead of saving whole model i just save and load weights. Obviously in this case you need exact same model defined in learning and inference code. E.g. This is fixed with latest tf nightly build version 1.15.0 dev20190808 . Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 20073 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 20073 No a 
20215,Tensorflow does not work, System information I used stock example script provided in TensorFlow Windows 7 64 bit installed tensorflow pip install tensorflow upgrade TensorFlow version 1 8 0 Python version 3.6.5 Exact command to reproduce import tensorflow Describe the problem When i run in python import tensorflow i receive failed to load native tensorflow runtime Source code logs Source import tensorflow as tf hello tf.constant Hello TensorFlow sess tf.Session print sess.run hello Full traceback Traceback most recent call last File C Python36 lib site packages tensorflow python pywrap tensorflow internal.py line 14 in swig import helper return importlib.import module mname File C Python36 lib importlib init .py line 126 in import module return bootstrap. gcd import name level package level File frozen importlib. bootstrap line 994 in gcd import File frozen importlib. bootstrap line 971 in find and load File frozen importlib. bootstrap line 955 in find and load unlocked File frozen importlib. bootstrap line 658 in load unlocked File frozen importlib. bootstrap line 571 in module from spec File frozen importlib. bootstrap external line 922 in create module File frozen importlib. bootstrap line 219 in call with frames removed ImportError DLL load failed with error code 1073741795 During handling of the above exception another exception occurred Traceback most recent call last File C Python36 lib site packages tensorflow python pywrap tensorflow.py line 58 in module from tensorflow.python.pywrap tensorflow internal import File C Python36 lib site packages tensorflow python pywrap tensorflow internal.py line 17 in module pywrap tensorflow internal swig import helper File C Python36 lib site packages tensorflow python pywrap tensorflow internal.py line 16 in swig import helper return importlib.import module pywrap tensorflow internal File C Python36 lib importlib init .py line 126 in import module return bootstrap. gcd import name level package level ModuleNotFoundError No module named pywrap tensorflow internal During handling of the above exception another exception occurred Traceback most recent call last File K Python testTensorflow.py line 5 in module import tensorflow as tf File C Python36 lib site packages tensorflow init .py line 24 in module from tensorflow.python import pywrap tensorflow pylint disable unused import File C Python36 lib site packages tensorflow python init .py line 49 in module from tensorflow.python import pywrap tensorflow File C Python36 lib site packages tensorflow python pywrap tensorflow.py line 74 in module raise ImportError msg ImportError Traceback most recent call last File C Python36 lib site packages tensorflow python pywrap tensorflow internal.py line 14 in swig import helper return importlib.import module mname File C Python36 lib importlib init .py line 126 in import module return bootstrap. gcd import name level package level File frozen importlib. bootstrap line 994 in gcd import File frozen importlib. bootstrap line 971 in find and load File frozen importlib. bootstrap line 955 in find and load unlocked File frozen importlib. bootstrap line 658 in load unlocked File frozen importlib. bootstrap line 571 in module from spec File frozen importlib. bootstrap external line 922 in create module File frozen importlib. bootstrap line 219 in call with frames removed ImportError DLL load failed with error code 1073741795 During handling of the above exception another exception occurred Traceback most recent call last File C Python36 lib site packages tensorflow python pywrap tensorflow.py line 58 in module from tensorflow.python.pywrap tensorflow internal import File C Python36 lib site packages tensorflow python pywrap tensorflow internal.py line 17 in module pywrap tensorflow internal swig import helper File C Python36 lib site packages tensorflow python pywrap tensorflow internal.py line 16 in swig import helper return importlib.import module pywrap tensorflow internal File C Python36 lib importlib init .py line 126 in import module return bootstrap. gcd import name level package level ModuleNotFoundError No module named pywrap tensorflow internal Failed to load the native TensorFlow runtime. See https www.tensorflow.org install install sources common installation problems for some common reasons and solutions. Include the entire stack trace above this error message when asking for help. ,Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case or leave them as N A Thanks. Have I written custom code OS Platform and Distribution TensorFlow installed from Bazel version CUDA cuDNN version GPU model and memory Have I written custom code NO OS Platform and Distribution Win7 64bit AMD TensorFlow installed from pip install upgrade tensorflow Bazel version NO BAZEL CUDA cuDNN version NO CUDA GPU model and memory NO GPU https github.com tensorflow tensorflow issues 18530 recommended to install tensorflow 1.5 i installed without GPU It Works I am experiencing same problem in current TF 1.9. Downgrading to TF1.5 helps but since it is old version it is missing some features I need. Please reopen the issue. janstrelka Assuming your issue is the same as what was talked about in 18530 then this means that the CPU you re running on does not support AVX instructions. Since TensorFlow 1.6 the release binaries require CPUs with AVX support. For older CPUs you ll have to either build from source or use binaries built by others in the community e.g. https github.com yaroslavvb tensorflow community wheels asimshankar Understand. Thank you. 
20373,kernel tests cwise ops test fails on AVX512 systems, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 TensorFlow installed from source or binary source TensorFlow version use command below v1.8.0 4021 g4292085 1.9.0 rc0 Python version Python 2.7.12 Bazel version if compiling from source 0.15.0 GCC Compiler version if compiling from source gcc Ubuntu 5.4.0 6ubuntu1 16.04.9 5.4.0 20160609 CUDA cuDNN version n a GPU model and memory n a Exact command to reproduce bazel test config opt tensorflow python kernel tests cwise ops test Describe the problem The tensorflow python kernel tests cwise ops test test fails on AVX512 machines. Reproducing the problem is simple. Just run the unit test. The test is failing as there s a bug in Eigen s AVX512 implementation of the psqrt functions which incorrectly compute the sqrt of negative numbers as 0 instead of NaN. There s a pull request pending on Eigen that fixes the issue. https bitbucket.org eigen eigen pull requests 412 fix avx512 implementations of psqrt diff A similar issue was fixed in Eigen s AVX2 implementations of psqrt a couple of years ago. Source code logs ,Nagging Assignee rohan100jain It has been 15 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee rohan100jain It has been 30 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Patch has been merged into Eigen. The issue will be fixed when tensorflow updates the version of Eigen it uses. rohan100jain This bug is still reproducible in tensorflow. Fixing it requires an update to the version of Eigen used in tensorflow so I think the bug should probably remain open. 
20379,tf.layers.conv3d throws an error when using channels first and None size for the input shape, System information Have I written custom code Yes OS Platform and Distribution Windows 10 Home version 1803 TensorFlow installed from Installed by running pip install tensorflow gpu TensorFlow version use command below b v1.8.0 0 g93bc2e2072 1.8.0 Python version 3.5 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version CUDA v9.0 cuDNN v7.1 GPU model and memory GeForce GTX1080 8GB Exact command to reproduce see the Code To Reproduce The Bug section Bug Description When calling tf.layers.conv3d with data format channels first and the input shape None 1 3 None None tensorflow throws the error TypeError unsupported operand type s for int and NoneType This error appears to be thrown from this line https github.com tensorflow tensorflow blob f202958ee2d5177a474e3d107fdbf0c83174d099 tensorflow python keras layers convolutional.py L205 according to Visual Studio s python debugger. Code To Reproduce The Bug Calling the BuggedCode function will throw the error while calling WorkingCode will work perfectly fine. Full Traceback , fchollet it looks like you might have introduced the code that manipulates outputs 4d in the case where channels first is True. Would you PTAL I have a pull request for this in https github.com tensorflow tensorflow pull 21610 This is still a problem also for channels last where if I pass a data type with shape None 10 None None 1 line 1088 in keras layers convolutional.py drops an error outputs 4d array ops.reshape outputs outputs shape 0 outputs shape 1 outputs shape 2 outputs shape 3 outputs shape 4 TypeError unsupported operand type s for int and NoneType Might be fixed upstream but this is still a problem in 1.13.1 . throws This is fixed with tf nightly version 1.15.0 dev20190726 . Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 20379 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 20379 No a 
20451,with quantized training model PC ok but tf lite failed., System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Y OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux 4.17.2 1 ARCH SMP TensorFlow installed from source or binary source TensorFlow version use command below b v1.8.0 3238 g52bf2fe0f6 1.9.0 rc0 Python version 3.6.5 Bazel version if compiling from source 0.14.1 non git GCC Compiler version if compiling from source gcc 7.1 CUDA cuDNN version cuda 9.2 cuDNN 7.1 GPU model and memory 16G Exact command to reproduce I trained label image on mobilenetv2 backbone with quantization and everything works well on PC. Then I tried to convert it to tf lite even the converting processing is well down no error no unsupported ops but when I finally ran it I got tensorflow contrib lite kernels conv.cc 260 real multiplier 1.0 was not true This the log tensors size 174 nodes size 66 inputs 1 input 0 name input 0 MobilenetV2 Conv Conv2D Fold bias 128 2 0.0408043 0 1 MobilenetV2 Conv Relu6 100352 3 0.0235285 0 2 MobilenetV2 Conv weights quant FakeQuantWithMinMaxVars 864 3 0.0408043 121 3 MobilenetV2 Conv 1 Conv2D Fold bias 5120 2 0.00108845 0 4 MobilenetV2 Conv 1 Relu6 20480 3 0.0235285 0 5 MobilenetV2 Conv 1 weights quant FakeQuantWithMinMaxVars 409600 3 0.00680513 119 6 MobilenetV2 Logits AvgPool 1280 3 0.0235285 0 7 MobilenetV2 Logits Conv2d 1c 1x1 BiasAdd 3 3 0.174401 120 8 MobilenetV2 Logits Conv2d 1c 1x1 Conv2D bias 12 2 2.91482e 05 0 9 MobilenetV2 Logits Conv2d 1c 1x1 weights quant FakeQuantWithMinMaxVars 3840 3 0.00123885 125 10 MobilenetV2 Logits Squeeze 3 3 0.174401 120 11 MobilenetV2 Logits Squeeze shape 8 2 0 0 12 MobilenetV2 Predictions Reshape 1 3 3 0.00390625 0 13 MobilenetV2 expanded conv depthwise Relu6 100352 3 0.0235285 0 14 MobilenetV2 expanded conv depthwise depthwise Fold bias 128 2 0.00805492 0 15 MobilenetV2 expanded conv depthwise weights quant FakeQuantWithMinMaxVars 288 3 0.342348 165 16 MobilenetV2 expanded conv project Conv2D Fold bias 64 2 0.00091254 0 17 MobilenetV2 expanded conv project add fold 50176 3 0.354141 130 18 MobilenetV2 expanded conv project weights quant FakeQuantWithMinMaxVars 512 3 0.0387845 150 19 MobilenetV2 expanded conv 1 depthwise Relu6 75264 3 0.0235285 0 20 MobilenetV2 expanded conv 1 depthwise depthwise Fold bias 384 2 0.00060018 0 21 MobilenetV2 expanded conv 1 depthwise weights quant FakeQuantWithMinMaxVars 864 3 0.0255087 109 22 MobilenetV2 expanded conv 1 expand Conv2D Fold bias 384 2 0.00352435 0 23 MobilenetV2 expanded conv 1 expand Relu6 301056 3 0.0235285 0 24 MobilenetV2 expanded conv 1 expand weights quant FakeQuantWithMinMaxVars 1536 3 0.00995183 126 25 MobilenetV2 expanded conv 1 project Conv2D Fold bias 96 2 0.000607076 0 26 MobilenetV2 expanded conv 1 project add fold 18816 3 0.294347 131 27 MobilenetV2 expanded conv 1 project weights quant FakeQuantWithMinMaxVars 2304 3 0.0258018 151 28 MobilenetV2 expanded conv 10 depthwise Relu6 18816 3 0.0235285 0 29 MobilenetV2 expanded conv 10 depthwise depthwise Fold bias 1536 2 0.000711485 0 30 MobilenetV2 expanded conv 10 depthwise weights quant FakeQuantWithMinMaxVars 3456 3 0.0302393 140 31 MobilenetV2 expanded conv 10 expand Conv2D Fold bias 1536 2 0.000350018 0 32 MobilenetV2 expanded conv 10 expand Relu6 18816 3 0.0235285 0 33 MobilenetV2 expanded conv 10 expand weights quant FakeQuantWithMinMaxVars 24576 3 0.00174141 149 34 MobilenetV2 expanded conv 10 project Conv2D Fold bias 384 2 0.000179579 0 35 MobilenetV2 expanded conv 10 project add fold 4704 3 0.150007 128 36 MobilenetV2 expanded conv 10 project weights quant FakeQuantWithMinMaxVars 36864 3 0.00763239 134 37 MobilenetV2 expanded conv 11 add 4704 3 0.149921 125 38 MobilenetV2 expanded conv 11 depthwise Relu6 28224 3 0.0235285 0 39 MobilenetV2 expanded conv 11 depthwise depthwise Fold bias 2304 2 0.0013777 0 40 MobilenetV2 expanded conv 11 depthwise weights quant FakeQuantWithMinMaxVars 5184 3 0.0585545 92 41 MobilenetV2 expanded conv 11 expand Conv2D Fold bias 2304 2 0.000252474 0 42 MobilenetV2 expanded conv 11 expand Relu6 28224 3 0.0235285 0 43 MobilenetV2 expanded conv 11 expand weights quant FakeQuantWithMinMaxVars 55296 3 0.00168308 133 44 MobilenetV2 expanded conv 11 project Conv2D Fold bias 384 2 0.000203408 0 45 MobilenetV2 expanded conv 11 project add fold 4704 3 0.102331 126 46 MobilenetV2 expanded conv 11 project weights quant FakeQuantWithMinMaxVars 55296 3 0.00864519 141 47 MobilenetV2 expanded conv 12 add 4704 3 0.213277 145 48 MobilenetV2 expanded conv 12 depthwise Relu6 28224 3 0.0235285 0 49 MobilenetV2 expanded conv 12 depthwise depthwise Fold bias 2304 2 0.00214226 0 50 MobilenetV2 expanded conv 12 depthwise weights quant FakeQuantWithMinMaxVars 5184 3 0.0910496 173 51 MobilenetV2 expanded conv 12 expand Conv2D Fold bias 2304 2 0.000217147 0 52 MobilenetV2 expanded conv 12 expand Relu6 28224 3 0.0235285 0 53 MobilenetV2 expanded conv 12 expand weights quant FakeQuantWithMinMaxVars 55296 3 0.00144841 139 54 MobilenetV2 expanded conv 12 project Conv2D Fold bias 384 2 0.000596996 0 55 MobilenetV2 expanded conv 12 project add fold 4704 3 0.170068 144 56 MobilenetV2 expanded conv 12 project weights quant FakeQuantWithMinMaxVars 55296 3 0.0253734 149 57 MobilenetV2 expanded conv 13 depthwise Relu6 9216 3 0.0235285 0 58 MobilenetV2 expanded conv 13 depthwise depthwise Fold bias 2304 2 0.00034101 0 59 MobilenetV2 expanded conv 13 depthwise weights quant FakeQuantWithMinMaxVars 5184 3 0.0144935 90 60 MobilenetV2 expanded conv 13 expand Conv2D Fold bias 2304 2 0.000297714 0 61 MobilenetV2 expanded conv 13 expand Relu6 28224 3 0.0235285 0 62 MobilenetV2 expanded conv 13 expand weights quant FakeQuantWithMinMaxVars 55296 3 0.0013959 122 63 MobilenetV2 expanded conv 13 project Conv2D Fold bias 640 2 0.000194787 0 64 MobilenetV2 expanded conv 13 project add fold 2560 3 0.144367 122 65 MobilenetV2 expanded conv 13 project weights quant FakeQuantWithMinMaxVars 92160 3 0.00827876 139 66 MobilenetV2 expanded conv 14 add 2560 3 0.150496 130 67 MobilenetV2 expanded conv 14 depthwise Relu6 15360 3 0.0235285 0 68 MobilenetV2 expanded conv 14 depthwise depthwise Fold bias 3840 2 0.0010313 0 69 MobilenetV2 expanded conv 14 depthwise weights quant FakeQuantWithMinMaxVars 8640 3 0.0438318 148 70 MobilenetV2 expanded conv 14 expand Conv2D Fold bias 3840 2 0.000355036 0 71 MobilenetV2 expanded conv 14 expand Relu6 15360 3 0.0235285 0 72 MobilenetV2 expanded conv 14 expand weights quant FakeQuantWithMinMaxVars 153600 3 0.00245926 111 73 MobilenetV2 expanded conv 14 project Conv2D Fold bias 640 2 0.000174049 0 74 MobilenetV2 expanded conv 14 project add fold 2560 3 0.0903757 130 75 MobilenetV2 expanded conv 14 project weights quant FakeQuantWithMinMaxVars 153600 3 0.00739738 137 76 MobilenetV2 expanded conv 15 add 2560 3 0.300834 122 77 MobilenetV2 expanded conv 15 depthwise Relu6 15360 3 0.0235285 0 78 MobilenetV2 expanded conv 15 depthwise depthwise Fold bias 3840 2 0.00129889 0 79 MobilenetV2 expanded conv 15 depthwise weights quant FakeQuantWithMinMaxVars 8640 3 0.0552048 110 80 MobilenetV2 expanded conv 15 expand Conv2D Fold bias 3840 2 0.000226284 0 81 MobilenetV2 expanded conv 15 expand Relu6 15360 3 0.0235285 0 82 MobilenetV2 expanded conv 15 expand weights quant FakeQuantWithMinMaxVars 153600 3 0.00150359 99 83 MobilenetV2 expanded conv 15 project Conv2D Fold bias 640 2 0.000805016 0 84 MobilenetV2 expanded conv 15 project add fold 2560 3 0.226103 131 85 MobilenetV2 expanded conv 15 project weights quant FakeQuantWithMinMaxVars 153600 3 0.0342145 139 86 MobilenetV2 expanded conv 16 depthwise Relu6 15360 3 0.0235285 0 87 MobilenetV2 expanded conv 16 depthwise depthwise Fold bias 3840 2 0.0040495 0 88 MobilenetV2 expanded conv 16 depthwise weights quant FakeQuantWithMinMaxVars 8640 3 0.17211 201 89 MobilenetV2 expanded conv 16 expand Conv2D Fold bias 3840 2 0.000576843 0 90 MobilenetV2 expanded conv 16 expand Relu6 15360 3 0.0235285 0 91 MobilenetV2 expanded conv 16 expand weights quant FakeQuantWithMinMaxVars 153600 3 0.00191748 125 92 MobilenetV2 expanded conv 16 project Conv2D Fold bias 1280 2 0.000119162 0 93 MobilenetV2 expanded conv 16 project add fold 5120 3 0.159945 146 94 MobilenetV2 expanded conv 16 project weights quant FakeQuantWithMinMaxVars 307200 3 0.0050646 130 95 MobilenetV2 expanded conv 2 add 18816 3 0.376629 129 96 MobilenetV2 expanded conv 2 depthwise Relu6 112896 3 0.0235285 0 97 MobilenetV2 expanded conv 2 depthwise depthwise Fold bias 576 2 0.00397992 0 98 MobilenetV2 expanded conv 2 depthwise weights quant FakeQuantWithMinMaxVars 1296 3 0.169153 51 99 MobilenetV2 expanded conv 2 expand Conv2D Fold bias 576 2 0.00106746 0 100 MobilenetV2 expanded conv 2 expand Relu6 112896 3 0.0235285 0 101 MobilenetV2 expanded conv 2 expand weights quant FakeQuantWithMinMaxVars 3456 3 0.00362654 142 102 MobilenetV2 expanded conv 2 project Conv2D Fold bias 96 2 0.000610138 0 103 MobilenetV2 expanded conv 2 project add fold 18816 3 0.342911 133 104 MobilenetV2 expanded conv 2 project weights quant FakeQuantWithMinMaxVars 3456 3 0.0259319 129 105 MobilenetV2 expanded conv 3 depthwise Relu6 28224 3 0.0235285 0 106 MobilenetV2 expanded conv 3 depthwise depthwise Fold bias 576 2 0.000397524 0 107 MobilenetV2 expanded conv 3 depthwise weights quant FakeQuantWithMinMaxVars 1296 3 0.0168954 126 108 MobilenetV2 expanded conv 3 expand Conv2D Fold bias 576 2 0.00108431 0 109 MobilenetV2 expanded conv 3 expand Relu6 112896 3 0.0235285 0 110 MobilenetV2 expanded conv 3 expand weights quant FakeQuantWithMinMaxVars 3456 3 0.00287898 107 111 MobilenetV2 expanded conv 3 project Conv2D Fold bias 128 2 0.000396253 0 112 MobilenetV2 expanded conv 3 project add fold 6272 3 0.20811 126 113 MobilenetV2 expanded conv 3 project weights quant FakeQuantWithMinMaxVars 4608 3 0.0168414 109 114 MobilenetV2 expanded conv 4 add 6272 3 0.250818 134 115 MobilenetV2 expanded conv 4 depthwise Relu6 37632 3 0.0235285 0 116 MobilenetV2 expanded conv 4 depthwise depthwise Fold bias 768 2 0.00236945 0 117 MobilenetV2 expanded conv 4 depthwise weights quant FakeQuantWithMinMaxVars 1728 3 0.100705 79 118 MobilenetV2 expanded conv 4 expand Conv2D Fold bias 768 2 0.000398864 0 119 MobilenetV2 expanded conv 4 expand Relu6 37632 3 0.0235285 0 120 MobilenetV2 expanded conv 4 expand weights quant FakeQuantWithMinMaxVars 6144 3 0.0019166 151 121 MobilenetV2 expanded conv 4 project Conv2D Fold bias 128 2 0.000489459 0 122 MobilenetV2 expanded conv 4 project add fold 6272 3 0.200968 132 123 MobilenetV2 expanded conv 4 project weights quant FakeQuantWithMinMaxVars 6144 3 0.0208029 147 124 MobilenetV2 expanded conv 5 add 6272 3 0.276968 127 125 MobilenetV2 expanded conv 5 depthwise Relu6 37632 3 0.0235285 0 126 MobilenetV2 expanded conv 5 depthwise depthwise Fold bias 768 2 0.00202895 0 127 MobilenetV2 expanded conv 5 depthwise weights quant FakeQuantWithMinMaxVars 1728 3 0.086234 63 128 MobilenetV2 expanded conv 5 expand Conv2D Fold bias 768 2 0.000362468 0 129 MobilenetV2 expanded conv 5 expand Relu6 37632 3 0.0235285 0 130 MobilenetV2 expanded conv 5 expand weights quant FakeQuantWithMinMaxVars 6144 3 0.00144514 120 131 MobilenetV2 expanded conv 5 project Conv2D Fold bias 128 2 0.000435147 0 132 MobilenetV2 expanded conv 5 project add fold 6272 3 0.205061 128 133 MobilenetV2 expanded conv 5 project weights quant FakeQuantWithMinMaxVars 6144 3 0.0184945 128 134 MobilenetV2 expanded conv 6 depthwise Relu6 9408 3 0.0235285 0 135 MobilenetV2 expanded conv 6 depthwise depthwise Fold bias 768 2 0.000267618 0 136 MobilenetV2 expanded conv 6 depthwise weights quant FakeQuantWithMinMaxVars 1728 3 0.0113742 126 137 MobilenetV2 expanded conv 6 expand Conv2D Fold bias 768 2 0.000528109 0 138 MobilenetV2 expanded conv 6 expand Relu6 37632 3 0.0235285 0 139 MobilenetV2 expanded conv 6 expand weights quant FakeQuantWithMinMaxVars 6144 3 0.00190675 128 140 MobilenetV2 expanded conv 6 project Conv2D Fold bias 256 2 0.00033262 0 141 MobilenetV2 expanded conv 6 project add fold 3136 3 0.182997 123 142 MobilenetV2 expanded conv 6 project weights quant FakeQuantWithMinMaxVars 12288 3 0.0141369 135 143 MobilenetV2 expanded conv 7 add 3136 3 0.182534 120 144 MobilenetV2 expanded conv 7 depthwise Relu6 18816 3 0.0235285 0 145 MobilenetV2 expanded conv 7 depthwise depthwise Fold bias 1536 2 0.00131166 0 146 MobilenetV2 expanded conv 7 depthwise weights quant FakeQuantWithMinMaxVars 3456 3 0.0557479 130 147 MobilenetV2 expanded conv 7 expand Conv2D Fold bias 1536 2 0.000259114 0 148 MobilenetV2 expanded conv 7 expand Relu6 18816 3 0.0235285 0 149 MobilenetV2 expanded conv 7 expand weights quant FakeQuantWithMinMaxVars 24576 3 0.00141595 134 150 MobilenetV2 expanded conv 7 project Conv2D Fold bias 256 2 0.000434401 0 151 MobilenetV2 expanded conv 7 project add fold 3136 3 0.150338 112 152 MobilenetV2 expanded conv 7 project weights quant FakeQuantWithMinMaxVars 24576 3 0.0184628 129 153 MobilenetV2 expanded conv 8 add 3136 3 0.182103 121 154 MobilenetV2 expanded conv 8 depthwise Relu6 18816 3 0.0235285 0 155 MobilenetV2 expanded conv 8 depthwise depthwise Fold bias 1536 2 0.000993556 0 156 MobilenetV2 expanded conv 8 depthwise weights quant FakeQuantWithMinMaxVars 3456 3 0.0422278 135 157 MobilenetV2 expanded conv 8 expand Conv2D Fold bias 1536 2 0.000266144 0 158 MobilenetV2 expanded conv 8 expand Relu6 18816 3 0.0235285 0 159 MobilenetV2 expanded conv 8 expand weights quant FakeQuantWithMinMaxVars 24576 3 0.00145805 128 160 MobilenetV2 expanded conv 8 project Conv2D Fold bias 256 2 0.000281633 0 161 MobilenetV2 expanded conv 8 project add fold 3136 3 0.122043 130 162 MobilenetV2 expanded conv 8 project weights quant FakeQuantWithMinMaxVars 24576 3 0.0119699 127 163 MobilenetV2 expanded conv 9 add 3136 3 0.200997 130 164 MobilenetV2 expanded conv 9 depthwise Relu6 18816 3 0.0235285 0 165 MobilenetV2 expanded conv 9 depthwise depthwise Fold bias 1536 2 0.000984803 0 166 MobilenetV2 expanded conv 9 depthwise weights quant FakeQuantWithMinMaxVars 3456 3 0.0418558 151 167 MobilenetV2 expanded conv 9 expand Conv2D Fold bias 1536 2 0.000224455 0 168 MobilenetV2 expanded conv 9 expand Relu6 18816 3 0.0235285 0 169 MobilenetV2 expanded conv 9 expand weights quant FakeQuantWithMinMaxVars 24576 3 0.00123257 123 170 MobilenetV2 expanded conv 9 project Conv2D Fold bias 256 2 0.000418117 0 171 MobilenetV2 expanded conv 9 project add fold 3136 3 0.157535 127 172 MobilenetV2 expanded conv 9 project weights quant FakeQuantWithMinMaxVars 24576 3 0.0177707 145 173 input 37632 3 1 128 number of inputs 1 number of outputs 1 tensorflow contrib lite kernels conv.cc 260 real multiplier 1.0 was not true. I just add tf.contrib.quantize.create training graph input graph tf.get default graph quant delay FLAGS.quan delay in training and tf.contrib.quantize.create eval graph in evaluation compared to the official version. Is there any extra processes I need to care Anyone help ,Same situation with MobilenetV1 that is used as a backbone for separate model. Model was trained and then freezed with tf.contrib.quantize.create training graph and tf.contrib.quantize.create eval graph. Backbone then was separated and converted with flag allow nudging weights to use fast gemm kernel. My tensorflow version is 1.9.0 rc2. My thoughts so far in the source code for TFLite there is 2 quantization routins QuantizeMultiplierGreaterThanOne and QuantizeMultiplierSmallerThanOneExp defined here. May be one can just add logic in tensorflow contrib lite kernels conv.cc to handle various real multiplier If i won t be able to find out anything better then i will probably give it a try and report here. Thanks for reporting this. This seems to be an issue with convs and fully connected layers that was fixed for DepthwiseConvs in this commit https github.com tensorflow tensorflow commit 3a17101171d3e51fcba2189d09416c5106bfe4ac diff ca0f46c80fd3cf1f2040be6147a2d8cf We likely need a similar fix for Conv and FC as well. Nagging Assignee liyunlu0618 It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. liyunlu0618 change should resolve it will be available in the next nightly. suharshs Thanks.It works now. 
20452,Keras TimeDistributed with Input and no batch size fails,OS Windows 10 Ubuntu 16.04 tested Tensorflow 1.8 Python 3.6.5 The following code shows the problem. When using Input with a batch size everything works fine but without it it fails. I assumed that when setting the batch size in batch shape to None or using shape instead that the batch size would then be dynamic. ,Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case or leave them as N A Thanks. Have I written custom code OS Platform and Distribution TensorFlow installed from TensorFlow version Bazel version CUDA cuDNN version GPU model and memory Exact command to reproduce sleighsoft I could not reproduce the issue. With the code snippet that you have shared here is the output I got Are you still seeing the issue I have this issue when running it under tensorflow gpu 1.8.0 . I tried it with the most recent tf nightly 1.10.0.dev20180609 and there it works. Thank you for checking. Closing the issue now as it seems to have been fixed. I have not tried it with the GPU version. It is an assumption that it is fixed there as well Please feel free to re open the issue if you see it again. 
20514,No OpKernel was registered to support Op QuantizedReluX with these attrs, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 TensorFlow installed from source or binary binary using conda TensorFlow version use command below 1.8.0 Python version 3.5.2 CUDA cuDNN version 8.0 7.0 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A GPU model and memory Quadro M2000M 4GB Exact command to reproduce Tried to use quantized relu x and it doesn t work I have already used quantized conv2d and quantized max pool both of them works fine but have only problem when I use quantized relu x Describe the problem Other two quantized kernels QuantizedRelu and QuantizedRelu6 seem to have defined but QuantizedReluX appears to be missing although three of them are registered together in core ops nn ops.cc file Source code logs Error message that I am getting is as follows InvalidArgumentError see above for traceback No OpKernel was registered to support Op QuantizedReluX with these attrs. Registered devices CPU GPU Registered kernels no registered kernels ,Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case or leave them as N A Thanks. Bazel version I have already added it 3 days ago.. Please try to have a look at it Added a PR 20700 for QuantizedReluX kernel. 
20516,Cannot restore variables with Checkpoint because keys do not align, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Win10 TensorFlow installed from source or binary binary TensorFlow version use command below tf nightly 1.10.0.dev20180609 Python version 3.6.5 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Exact command to reproduce I get the error that is thrown here https github.com tensorflow tensorflow blob r1.9 tensorflow python training checkpointable util.py L633 I cannot provide code to reproduce it. But basically what happens is I have a class that inherits from Checkpointable . It assigns all variables to itself to make them checkpointable. It also assigns an optimizer to itself. I then save the model and restore it. When calling .assert consumed on the load status object of restore dir session it throws an error because some key does not match. The variable it tries to restore is actually in the saved checkpoint it just has a different key then the one it gets from enumerate self. checkpoint.object graph proto.nodes . This describes it as good as I can. Sorry for not being able to share the code. I tried to reproduce it but so far I cannot. I believe it is a bug because I call ckpt.save and immediately after it ckpt.restore and I get the exception. Output of self. checkpoint.object by proto id.keys at line 631. You can see that some keys are missing idk why but theses are the ones I need to restore. Output of util. serialize object graph self.checkpoint None after restore before assert consumed ,I assume this is something for allenlavoie I found the culprit. Double assigning a variable. Here BasicLSTMCell . It works if I double assign a tf.get variable though. Seems like this https github.com tensorflow tensorflow blob r1.9 tensorflow python training checkpointable base.py L593 does not override clear all previous state. Edit Also does not work for tf.layers.Dense and probably others. Huh good point looks like the by name lookup isn t being updated when the dependency is replaced. Thank you for the report Happy to help 
20521,prefetch to device not working with SparseTensor when fetching to GPU, System information Have I written custom code yes OS Platform and Distribution Linux Ubuntu 14.04 TensorFlow installed from source TensorFlow version 1.9 Python version 2.7 Bazel version 0.12.0 GCC Compiler version 4.8.4 CUDA cuDNN version 9.1 GPU model and memory GTX 1080 8114MiB Exact command to reproduce Any sparse tensor being prefetched to GPU will work here this is just one short example Describe the problem Bug resulting from prefetch to device with the following conditions device is GPU dataset element contains a sparse tensor Iterator functions appropriately only when you try to use the data do you run into the error. see log for error Also as a side note which may or may not be relevant I ve found that even a single sparse tensor in an element will spoil any other nice kind and dense tensors in the pack. So if in the previous example I were to do the following I would still run into an error Log Traceback most recent call last File sparse bunk.py line 17 in module sess.run val normal File local lib python2.7 site packages tensorflow python client session.py line 877 in run run metadata ptr File local lib python2.7 site packages tensorflow python client session.py line 1100 in run feed dict tensor options run metadata File local lib python2.7 site packages tensorflow python client session.py line 1272 in do run run metadata File local lib python2.7 site packages tensorflow python client session.py line 1291 in do call raise type e node def op message tensorflow.python.framework.errors impl.InternalError No unary variant device copy function found for direction 1 and Variant type name tensorflow Tensor Node FunctionBufferingResourceGetNext FunctionBufferingResourceGetNext output types DT VARIANT DT INT32 device job localhost replica 0 task 0 device GPU 0 FunctionBufferingResource Thank you,Have I written custom code yes OS Platform and Distribution Linux Ubuntu 14.04 TensorFlow installed from source TensorFlow version 1.9 Python version 2.7 Bazel version 0.12.0 GCC Compiler version 4.8.4 CUDA cuDNN version 9.1 GPU model and memory Tesla K40 K80 I have been getting a similar error when I use MirroredStrategy for training across multiple devices. Here is the stack trace for it Stacktrace File train.py line 113 in module tf.app.run File home weinman virtualenv tf b2fe2a874 local lib python2.7 site packages tensorflow python platform app.py line 125 in run sys.exit main argv File train.py line 110 in main classifier.train input fn lambda get input stream File home weinman virtualenv tf b2fe2a874 local lib python2.7 site packages tensorflow python estimator estimator.py line 376 in train loss self. train model input fn hooks saving listeners File home weinman virtualenv tf b2fe2a874 local lib python2.7 site packages tensorflow python estimator estimator.py line 1141 in train model return self. train model distributed input fn hooks saving listeners File home weinman virtualenv tf b2fe2a874 local lib python2.7 site packages tensorflow python estimator estimator.py line 1366 in train model distributed saving listeners File home weinman virtualenv tf b2fe2a874 local lib python2.7 site packages tensorflow python estimator estimator.py line 1449 in train with estimator spec loss mon sess.run estimator spec.train op estimator spec.loss File home weinman virtualenv tf b2fe2a874 local lib python2.7 site packages tensorflow python training monitored session.py line 583 in run run metadata run metadata File home weinman virtualenv tf b2fe2a874 local lib python2.7 site packages tensorflow python training monitored session.py line 1059 in run run metadata run metadata File home weinman virtualenv tf b2fe2a874 local lib python2.7 site packages tensorflow python training monitored session.py line 1150 in run raise six.reraise original exc info File home weinman virtualenv tf b2fe2a874 local lib python2.7 site packages tensorflow python training monitored session.py line 1135 in run return self. sess.run args kwargs File home weinman virtualenv tf b2fe2a874 local lib python2.7 site packages tensorflow python training monitored session.py line 1207 in run run metadata run metadata File home weinman virtualenv tf b2fe2a874 local lib python2.7 site packages tensorflow python training monitored session.py line 987 in run return self. sess.run args kwargs File home weinman virtualenv tf b2fe2a874 local lib python2.7 site packages tensorflow python client session.py line 877 in run run metadata ptr File home weinman virtualenv tf b2fe2a874 local lib python2.7 site packages tensorflow python client session.py line 1100 in run feed dict tensor options run metadata File home weinman virtualenv tf b2fe2a874 local lib python2.7 site packages tensorflow python client session.py line 1272 in do run run metadata File home weinman virtualenv tf b2fe2a874 local lib python2.7 site packages tensorflow python client session.py line 1291 in do call raise type e node def op message tensorflow.python.framework.errors impl.InternalError No unary variant device copy function found for direction 1 and Variant type name tensorflow Tensor Node FunctionBufferingResourceGetNext FunctionBufferingResourceGetNext output types DT FLOAT DT STRING DT INT32 DT VARIANT device job localhost replica 0 task 0 device GPU 0 FunctionBufferingResource Node GroupCrossDeviceControlEdges 0 train OptimizeLoss global norm gradient norm tags 766 Recv client terminated false recv device job localhost replica 0 task 0 device CPU 0 send device job localhost replica 0 task 0 device GPU 0 send device incarnation 1 tensor name edge 1235 ... norm tags tensor type DT FLOAT device job localhost replica 0 task 0 device CPU 0 Caused by op u FunctionBufferingResourceGetNext defined at File train.py line 113 in module tf.app.run File home weinman virtualenv tf b2fe2a874 local lib python2.7 site packages tensorflow python platform app.py line 125 in run sys.exit main argv File train.py line 110 in main classifier.train input fn lambda get input stream File home weinman virtualenv tf b2fe2a874 local lib python2.7 site packages tensorflow python estimator estimator.py line 376 in train loss self. train model input fn hooks saving listeners File home weinman virtualenv tf b2fe2a874 local lib python2.7 site packages tensorflow python estimator estimator.py line 1141 in train model return self. train model distributed input fn hooks saving listeners File home weinman virtualenv tf b2fe2a874 local lib python2.7 site packages tensorflow python estimator estimator.py line 1241 in train model distributed input fn model fn lib.ModeKeys.TRAIN File home weinman virtualenv tf b2fe2a874 local lib python2.7 site packages tensorflow python estimator estimator.py line 1011 in get features and labels from input fn return estimator util.parse input fn result result File home weinman virtualenv tf b2fe2a874 local lib python2.7 site packages tensorflow python estimator util.py line 111 in parse input fn result result iterator.get next File home weinman virtualenv tf b2fe2a874 local lib python2.7 site packages tensorflow contrib distribute python values.py line 619 in get next data list self. iterator.get next name name File home weinman virtualenv tf b2fe2a874 local lib python2.7 site packages tensorflow contrib distribute python prefetching ops v2.py line 124 in get next self.output types self.output classes name name File home weinman virtualenv tf b2fe2a874 local lib python2.7 site packages tensorflow contrib data python ops gen dataset ops.py line 351 in function buffering resource get next output types output types name name File home weinman virtualenv tf b2fe2a874 local lib python2.7 site packages tensorflow python framework op def library.py line 787 in apply op helper op def op def File home weinman virtualenv tf b2fe2a874 local lib python2.7 site packages tensorflow python util deprecation.py line 432 in new func return func args kwargs File home weinman virtualenv tf b2fe2a874 local lib python2.7 site packages tensorflow python framework ops.py line 3212 in create op op def op def File home weinman virtualenv tf b2fe2a874 local lib python2.7 site packages tensorflow python framework ops.py line 1702 in init self. traceback self. graph. extract stack pylint disable protected access InternalError see above for traceback No unary variant device copy function found for direction 1 and Variant type name tensorflow Tensor Node FunctionBufferingResourceGetNext FunctionBufferingResourceGetNext output types DT FLOAT DT STRING DT INT32 DT VARIANT device job localhost replica 0 task 0 device GPU 0 FunctionBufferingResource Node GroupCrossDeviceControlEdges 0 train OptimizeLoss global norm gradient norm tags 766 Recv client terminated false recv device job localhost replica 0 task 0 device CPU 0 send device job localhost replica 0 task 0 device GPU 0 send device incarnation 1 tensor name edge 1235 ... norm tags tensor type DT FLOAT device job localhost replica 0 task 0 device CPU 0 End of Stacktrace My guess is that the underlying issue should be the same. I do not make any explicit calls to prefetch to device Insert GPU here with sparse tensors anywhere in my code. Hence the issue probably lies with references to prefetching ops under the hood. I think I m experiencing the same error here. After staring at it for a while I realized that the direction HOST TO DEVICE 1 the recv device job localhost replica 0 task 0 device CPU 0 and the send device job localhost replica 0 task 0 device GPU 0 don t seem to be in agreement. Perhaps I m misunderstanding what s going on here but maybe it s a simple issue related to getting the direction of the copy correct rohan100jain Any thoughts on the above Nagging Assignee rohan100jain It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. So mrry added some support recently that makes this go away. I ran the same piece of code with tf nightly gpu and seems to work. https github.com tensorflow tensorflow commit 02ae1e2e781b8e049d1fc1ab7b52f6ee7edb4423 diff 3d32c23cf21d0adac13fdd28576dd6f9 Please test it and let me know if there is still a problem. Everything I ve tested appears to work. Thank you 
20545,Bug in tflite benchmark model, The compilation output is The problem is in https github.com tensorflow tensorflow blob b2fe2a874bade4782aaca5c44bf29e7ff6c39200 tensorflow contrib lite profiling profile summarizer.cc L86 details.name string profiling string should be details.name std string profiling string ,I sent a PR https github.com tensorflow tensorflow pull 20330 for this couple days ago freedomtan Thank you for fixing the bug. 
20619,ReduceLROnPlateau with native optimizer TFOptimizer object has no attribute lr ,Please go to Stack Overflow for help and support https stackoverflow.com questions tagged tensorflow If you open a GitHub issue here is our policy 1. It must be a bug a feature request or a significant problem with documentation for small docs fixes please send a PR instead . 2. The form below must be filled out. 3. It shouldn t be a TensorBoard issue. Those go here https github.com tensorflow tensorboard issues . Here s why we have that policy TensorFlow developers respond to issues. We want to focus on work that benefits the whole community e.g. fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem rather than being redirected to Stack Overflow. System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 macOS 10.12.6 TensorFlow installed from source or binary Binary pip TensorFlow version use command below 1.9.0rc2 Python version Python 3.6.4 Anaconda custom x86 64 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Exact command to reproduce You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION Describe the problem Using a native optimizer AdamOptimizer I can t get ReduceLROnPlateau to work but it does work using an optimizer from tf.keras.optimizers. Only TF native optimizers are supported in Eager mode so right now I just don t use ReduceLROnPlateau while in eager mode but I thought this should be reported. Thank you. Source code logs ,I suspect the problem is coming from here https github.com keras team keras blob master keras callbacks.py L1037 where it asked the optimizer for lr. On the other hand native AdamOptimizer uses lr https github.com tensorflow tensorflow blob r1.8 tensorflow python training adam.py L94 I wonder if we could make a check in the callback if it s keras optimizer adjust self.lr if it s tf optimizer adjust self. lr. else raise error. as I was checking tf native optimizers mostly use 1 self. lr 2 self. learning rate. Maybe we could consider unifying them. Review is completed and still working on implementation. It appears to be more challenging in distributed versions. Any progress Yes. We re near to finish and stay tuned. I faced a very similar problem but while using SGD optimizer. The problem for me was in the file condaEnv Lib site packages tensorflow python keras callbacks.py specifically in function on epoch begin . I changed the code I know it is bad to this It works for me. I still have this problem when using from tensorflow.contrib.opt import AdamWOptimizer and from tensorflow.keras.callbacks import ReduceLROnPlateau with tensorflow gpu 1.12.0 Similar issue with LearningRateScheduler . I think this may solve the problem https github.com uber horovod issues 42. This is my code chuong98 I get this any ideas Could not interpret optimizer identifier keras.optimizers.TFOptimizer object at 0x7fdebff42828 hadaev8 did you import keras from tf.keras Nope it raise error then i try to import No module named tensorflow.keras.optimizers.TFOptimizer This is the code I got it work Could you compile it sucessfully Oh i get it all model s layers should be from keras but not tf.keras And for tpu keras model i need tf.keras model unlucky. had the same issue when i used tf.train.AdamOptimizer. i solved the ReduceLR issue by passing adam to model.compile model.compile optimizer adam loss mse Avimor88 hadaev8 chuong98 WillBrennan Tzeny kenfehling malikaltakrori can you try to sync up with master version or tf nightly to see if it works We published a new set of optimizers to deal with this issue you can simply use it by opt tf.keras.Adam learning rate 0.01 model.compile opt loss metrics callbacks LearningRateScheduler your own schedule model.fit x y num epochs callbacks callbacks tanzhenyu it worked using opt tf.keras.optimizers.Adam lr 0.001 with tensorflow version 1.12 Thank you i had a same issue when using EarlyStopping ReduceLROnPlateau. tensorflow version 1.12 callbacks EarlyStopping ReduceLROnPlateau it doesn t work when i use optimizer opt tf.keras.optimizers.Adam lr 0.001 Error message is must be an instance of tf.train.Optimizer not a class tensorflow.python.keras.optimizers.Adam so i used optimizer tf.train.AdamOptimizer but i get this AttributeError TFOptimizer object has no attribute lr any ideas i had a same issue when using EarlyStopping ReduceLROnPlateau. tensorflow version 1.12 callbacks EarlyStopping ReduceLROnPlateau it doesn t work when i use optimizer opt tf.keras.optimizers.Adam lr 0.001 Error message is must be an instance of tf.train.Optimizer not a class tensorflow.python.keras.optimizers.Adam so i used optimizer tf.train.AdamOptimizer but i get this AttributeError TFOptimizer object has no attribute lr any ideas Same here. This breaks all models compiled with keras to tpu and means you can t get any sort of learning rate decay working pretty big bug imo. SooDevv indrasweb is there a code snippet that you can share SooDevv indrasweb is there a code snippet that you can share I have put together a minimal example here on Colab https colab.research.google.com drive 1wuOzZIYU7Tw1bHd6tE1SO0JWslZ7Q11f Note this happens for both sequential and functional models. tanzhenyu may you advise how to upgrade tensorflow on google colab tpu I tried pip uninstall tensorflow y pip install tf nightly But get error then try to train tanzhenyu I have two issues when using tf.enable eager execution or not 1. Not using tf.enable eager execution Error logs 2. Using tf.enable eager execution Error logs and then i use tf.train.Optimizer same issues occurs. AttributeError TFOptimizer object has no attribute lr SooDevv there are two changes in order to make this work 1. use tf.keras instead of keras the new optimizers is only available through tf not keras yet we have further plans to develop that in keras repo. 2. replace tf.train.AdamOptimizer with tf.keras.optimizers.Adam SooDevv Example import tensorflow as tf import numpy as np from tensorflow.python.keras.callbacks import EarlyStopping ReduceLROnPlateau reduce lr ReduceLROnPlateau monitor val acc patience 2 verbose 1 factor 0.5 min lr 0.00001 earlystop EarlyStopping patience 5 callbacks reduce lr earlystop from tensorflow.keras import models from tensorflow.keras import layers from tensorflow.keras import optimizers model models.Sequential model.add layers.Dense 10 activation relu input dim tr x.shape 1 model.add layers.Dropout .5 model.add layers.Dense 2 activation sigmoid model.compile loss tf.keras.losses.binary crossentropy optimizer tf.keras.optimizers.Adam metrics accuracy history model.fit x tr x y tr y batch size 5 epochs 30 callbacks callbacks hadaev8 I cannot edit your colab can you try changing tf.train.AdamOptimizer to tf.keras.optimizers.Adam tanzhenyu Its not mine colab its work couz it copy keras adam parameters to tensorflow adam every step. It would be better to do it without copying every step. tanzhenyu yes it works when not tensorflow eager execution mode but it s not working when tensorflow eager execution mode. ValueError optimizer must be an instance of tf.train.Optimizer not a class tensorflow.python.keras.optimizers.Adam Another problem with keras model to tpu keras adam and ReduceLROnPlateau it doesnt save current lr to checkpoint. SooDevv Oh I see. Thanks for pointing it out. This has been fixed on 12 11 18 but it seems that it s not in the 1.12 release which is on Nov . There are two options 1 use 1.13 rc which was released a week ago 2 use tf nightly. 
20626,Dataset.concatenate agrees to concat dictionaries with different keys,Please go to Stack Overflow for help and support https stackoverflow.com questions tagged tensorflow If you open a GitHub issue here is our policy 1. It must be a bug a feature request or a significant problem with documentation for small docs fixes please send a PR instead . 2. The form below must be filled out. 3. It shouldn t be a TensorBoard issue. Those go here https github.com tensorflow tensorboard issues . Here s why we have that policy TensorFlow developers respond to issues. We want to focus on work that benefits the whole community e.g. fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem rather than being redirected to Stack Overflow. System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac OS 10.13.5 TensorFlow installed from source or binary binary TensorFlow version use command below 1.8.0 Python version 3.6.5 Bazel version if compiling from source NA GCC Compiler version if compiling from source NA CUDA cuDNN version NA GPU model and memory NA Exact command to reproduce NA Describe the problem It seems like Dataset.concatenate will concatenate datasets of dictionaries with different keys values from the second key will be concatenated to the first one . a small demo is attached I ve looked at python data util nest.py and in recursive assert same structure it seems like yield value only returns values for dictionaries. Is that intended I would expect it to either fail or put None in the missing fields. Source code logs The following code Results in ,ping mrry Nagging Assignee shivaniag It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Nagging Assignee shivaniag It has been 29 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Good catch thanks for finding this bug. Fixed this This would now fail closing this issue. I still face this issue tensorflow 2.2. image https user images.githubusercontent.com 7234284 88757030 f602fd00 d119 11ea 8d83 0f80fd49f293.png From the snippet this failed. This is expected behavior the issue was that earlier it agreed to concat dictionaries with different keys. Please let us know if this behavior is not what you expect you have a use case for the behavior different than failing trying to concatenate dataset with different keys. 
20698,tf.keras multi input models don t work when using tf.data.Dataset,Please go to Stack Overflow for help and support https stackoverflow.com questions tagged tensorflow If you open a GitHub issue here is our policy 1. It must be a bug a feature request or a significant problem with documentation for small docs fixes please send a PR instead . 2. The form below must be filled out. 3. It shouldn t be a TensorBoard issue. Those go here https github.com tensorflow tensorboard issues . Here s why we have that policy TensorFlow developers respond to issues. We want to focus on work that benefits the whole community e.g. fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem rather than being redirected to Stack Overflow. System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 macOS 10.13.5 and Debian GNU Linux 9 stretch TensorFlow installed from source or binary binary TensorFlow version use command below v1.9.0 rc2 359 g95cfd8b3d9 1.10.0 dev20180711 also reproduces on v1.9.0 Python version 3.6.5 and 3.5.3 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version None GPU model and memory None Exact command to reproduce see below You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION Describe the problem tf.keras multi input models don t work when used together with tf.data.Dataset due to input broken validation checks. This problem reproduces both on tf 1.9.0 and the latest nightly. fchollet Do you have any ideas what s going on here or am I missing something obvious Source code logs Multi input model Consider the following toy model Using numpy data When fitting using numpy data this works as expected when passing a list or dictionary of inputs Using tf.data.Dataset.from tensor slices dictionary When trying the same with a tf.data.Dataset the following fails due to incorrect input validation Using tf.data.Dataset.from generator dictionary However using the same network together with tf.data.Dataset.from generator works. Probably because less validation is done Using tf.data.Dataset tuple Passing the multi input as a tuple to the model both datasets generated with from tensor slices and from generator fail ,I have the same problem and I have also multiple input dataset. But not sure if this problem caused by the multiple input datset. And I am using tensorflow 1.9 In order to be able to use dataset iterator in model.fit So 1 If I do the following dataset tf.data.TFRecordDataset train.tf records .map parse function .batch 20 .repeat model.fit dataset I got AttributeError RepeatDataset object has no attribute ndim 2 If I do the following dataset tf.data.TFRecordDataset train.tf records .map parse function .batch 20 .repeat .make initializable iterator model.fit dataset I got AttributeError Iterator object has no attribute ndim 3 If I do the following dataset tf.data.TFRecordDataset train.tf records .map parse function .batch 20 .repeat .make initializable iterator .get next model.fit dataset I got AttributeError tuple object has no attribute ndim Note if I run get next for the iterator it should give me data and label and other information I put it in tfrecords. So my input pair in iterator.get next 0 and labels in iterator.get next 1 . I opened 20753 to fix the issues not related to multi input models. I could reproduce the error. Thanks for taking the time and reproducing it. Did you have a chance to checked out my fix in 20753 Theres also a related PR that adds support for using tuples as multi dim inputs 20136 My situation seems similar. The iterator of dataset fed to model.fit is made from tf.data.Dataset.zip Both audio ds input and label ds output are instances of tf.data.TextLineDataset . Before fed to the model its iterator is created. And this is the error message when model.fit is called. tensorflow 1.9.0 keras 2.2.2 I think I discovered the problem fro my situation. The problem was I am using the standalone Keras . Not the one imported from tendorflow. So the new features of feeding the iterator directly to model.fit is valid only when you are using tf.Keras not the standalone Keras. was84san Wow same here and now it seems solved. Thanks lgeiger is this issue of passing multiple input to keras model vi tf.dataset api fixed Hi was84san. As you mentioned I am using tf.kera . But the problem still exists. Do you have any idea Thanks Which problem exactly feeding multiple inputs or feeding the iterator directly to model.fit. I figure out only the last one. hhwxxx I was also unable to use model.fit with a nested Dataset iterator for multi input and multi output models while using tf.keras on version 1.10. Installing tf gpu nightly my specific version is now 1.12.0 dev20180918 seemed to resolve this problem for me. gabrielibagon Could you post a snippet how you got a nested dataset iterator with multiple inputs working The final example at here https www.tensorflow.org guide datasets is interesting now how to define a model that accepts and trains correctly with that datase Is the full example available somewhere JanRuettinger ricoms Sorry for the delayed response. I drafted up a toy example using MNIST in order to train a model with two inputs and two outputs. The model is simply two identical models fused together which takes in two copies of the MNIST data two inputs and outputs a prediction for each two outputs . You can adapt this to more complex models and input pipelines. Note This is still using tf nightly gpu version 1.12.0 dev20180918 . I assume this will work in tensorflow 1.12 and above. Update As jashshopin mentions below the dataset object can be passed directly to model.fit if you have no need for an iterator. Is it necessary to use dataset.make one shot iterator jashshopin Thanks for pointing that out apparently you can pass the zipped dataset directly into model.fit . The example should still work for those who might want to use a one shot iterator or initializable iterator as well. thanks gabrielibagon. I have something like that here https github.com ricoms deep memorability blob master deep memorability trainer2 experiment.py . although I used the keras generator format because I could not deal with a video input pipeline using tensorflow methods. I might refactor it to tf.Dataset someday but it s working for now. Nagging Assignee fchollet It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. This isn t an issue on tensorflow 1.12 and above anymore. Thanks for the help everybody. Igeiger I tried to pass multiple inputs as a list of tf.dataset api to model fit directly like this model.fit dataset1 iterator dataset2 iterator ..... then I got this error And this is with tensorflow 1.12 so how you can pass multiple input using tf.dataset api with model fit not with model.fit generator Igeiger I tried to pass multiple inputs as a list of tf.dataset api to model fit directly like this model.fit dataset1 iterator dataset2 iterator ..... Returning a list of tensors in a single dataset and then passing it to model.fit should work. Checkout this example https colab.research.google.com drive 1h3FUGBhVsXnj6oEE3JDnC0WRFF Zu c scrollTo cjvaKWOqAQ3e lgeiger what about using dictionaries as targets https github.com tensorflow tensorflow issues 25299 issue 404556539 I can confirm this works in tensorflow 2.0.0 rc0. Multiple input and output even without all the zipping This still seems broken to me in tensorflow 2.0.0 rc0 . See this snippet which gives drasmuss workaround could be to convert list to dictionary in Dataset ds tf.data.Dataset.from tensors list input def to dict lst return a lst 0 b lst 1 ds ds.map to dict print model.predict ds I think I discovered the problem fro my situation. The problem was I am using the standalone Keras . Not the one imported from tendorflow. So the new features of feeding the iterator directly to model.fit is valid only when you are using tf.Keras not the standalone Keras. Thx my problem solved Just have changed import keras import tensorflow as tf to import tensorflow as tf from tensorflow import keras I am not finding documentation for feeding models with multiple inputs with different dimensions with tf.data. The above exchange leaves me still struggling for an understanding on feeding such models. May I asked for clarification I can use the generator directly but my goal is to move the generator to a full tf.data pipleline but I am missing something fundamental to get started. This works but does not use tf.data toy model.fit x toy generator list steps per epoch 3 epochs 2 The following as close to a solution I have gotten to but it fails Generates error ValueError The two structures don t have the same sequence length. Input structure has length 2 while the shallow structure has length 3. I know that my request smells like a request for help it is but please interpret it as a request for improved documentation. Stack overflow does not have anything on multiple inputs of different shapes. btw The real model input is an image 255 255 3 and a document type 20 with output 1 hot 40 60 into a ctc. The ideal tf.data chain would cache the preliminary processing then augment this cache version for delivery to model.fit where the model is fit across a network of servers. johngrabner The problem in your code is that your output type and output shape definitions differ. Changing the output type to tensorflow.float32 tensorflow.float32 tensorflow.float32 should to the trick. For the sake of completeness here is a minimal example of a dataset that expects two inputs shapes 1 32 and 1 128 hey guys. I have been in trouble the error below was thrown when the model with double inputs predicted. the state is a list of two nd arrays there I would really appreciate it if anyone is willing to give some tips 
20750,RunOptions.FULL TRACE produce wrong timestamps, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04 TensorFlow installed from source or binary binary TensorFlow version use command below v1.9.0 0 g25c197e023 1.9.0 Python version 3.6 Bazel version if compiling from source n a GCC Compiler version if compiling from source n a CUDA cuDNN version 9.0.176 7.1.1 GPU model and memory P100 16G can reproduce on other GPU model as well Exact command to reproduce see below The code above trains a CNN on two GPUs with FULL TRACE enabled. The returned profiling information contains correct timestamps but sometimes contains timestamps that are many years in the future. It prints the following output The issue was originally reported at https github.com tensorpack tensorpack issues 819. The issue is more likely to happen after running about 50 steps. The issue seems to disappear when training on one GPU or training a small model., zheng xq Can you have a look at this Seems possibly GPU related. zheng xq tensorflowbutler ppwwyyxx It seems like a problem in calling the cupti func tensorflow core platform default gpu cupti wrapper.cc CuptiWrapper ActivityGetNextRecord cuptiActivityGetNextRecord As the cupti api reference mentioned the reason is lack of device memory is there anyway to resolve this problem Or am I miss anything Look forward to your reply. It will return 0 timestamp on some CUPTI ACTIVITY KIND CONCURRENT KERNEL kernels in my testing. LOG 2018 08 21 11 37 10.099749 W tensorflow core platform default device tracer.cc 583 ActivityCallback kernel start 0 kernel end 0 kernel deviceId 0 kernel streamId 13 kernel correlationId 8928781 2018 08 21 11 37 10.099759 W tensorflow core platform default device tracer.cc 583 ActivityCallback kernel start 0 kernel end 0 kernel deviceId 0 kernel streamId 13 kernel correlationId 8928789 Cupti func reference https www.cs.rit.edu ark cuda doc html cupti group CUPTI ACTIVITY API.html Inputs the previous record returned by cuptiActivityGetNextRecord and returns the next activity record from the buffer. If input value is NULL returns the first activity record in the buffer. Records of kind CUPTI ACTIVITY KIND CONCURRENT KERNEL may contain invalid 0 timestamps indicating that no timing information could be collected for lack of device memory. Nagging Assignee ymodak It has been 45 days with no activity and this issue has an assignee. Please update the label and or status accordingly. CC nluehr This looks like an issue with CUPTI. Nathan could you help us triage this issue Thanks in advance I can reproduce this on CUDA 9.0 but it appears to be fixed in CUDA 10.0. Thanks Nathan for the quick update. Closing this issue. ppwwyyxx feel free to re open if the issue persists with CUDA 10 as well. 
20908,tf.nn.conv2d inconsistent dilation rate at runtime, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 LTS TensorFlow installed from source or binary binary pip install TensorFlow version use command below v1.7.0 3 g024aecf414 1.7.0 Python version 2.7.12 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version 9.0 7.0.5 GPU model and memory GTX1080 8GB Exact command to reproduce shown below Describe the problem Dilated convolution via tf.nn.conv2d with data format NHWC gets corrupted to NCHW during sess.run . Since the data format alone is corrupted and the dilation rate is unchanged the code fails with an error message indicating that it does not support dilation along the depth dimension dilation rate of 1 2 2 1 is valid for NHWC format but not for NCHW format . It seems that this is a CUDA problem since if I disable the GPU using os.environ CUDA VISIBLE DEVICES line the code does not error out. Weirdly enough if I don t do anything to the output of tf.nn.conv2d the code does not error out either corresponds to setting use reduce mean False in the below example . Also if the dilation rate is set as 1 1 2 2 the code does not error out although this goes against the documentation https www.tensorflow.org versions r1.7 api docs python tf nn conv2d which says that The dimension order is determined by the value of data format Source code logs source code to reproduce the bug error message , zheng xq this looks like a real bug can you take a look or reassign to someone else who can Looks like it s been fixed in 1.10 . Commit that fix this https github.com tensorflow tensorflow commit 032f804a2feca8995185a5fbb9dbc62d5d8df48e Nagging Assignee zheng xq It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Sorry for the late response but stegben I think you are right I updated the tensorflow version to 1.10 and the above code runs without any error thanks I am using 1.7 and facing the same issue. Is there any fix other than updating tensorflow version 
20983, Bug tf.py func Bad tensor np.array conversion in py func return with dtype int64 is returned as dtype float64 , Issue template Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes minimal example attached inline OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac OSX 10.12.6 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device Probably TensorFlow installed from source or binary Binary TensorFlow version use command below v1.5.0 0 g37aa430d84 1.5.0 Python version 2.7 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Exact command to reproduce python py func failure.py Describe the problem This bug occurs when a tf.py func returns an empty python list which is intended to be a tensor of type tf.int64 as defined in the Tout of the py func . In the ops.script ops.py the convert static method is unable to tell the numpy dtype of the incoming tensor when it is an empty list. This causes the convert method to execute a np.asarray dtype None order C which gives us a array dtype float64 instead of a array dtype int64 which then does not agree with the output tensor description in the py func. Source code logs See gist here for the example that reproduces this issue https gist.githubusercontent.com sabhiram 3f5eaf7e566ef9aefb3ae6e5b8d2edb0 raw 182ab6bc72b5b7b13fd55964c72030cbc53f7cb3 py func failure.py Inlined here ,For what its worth I think this would be easy enough to fix in the core.ops.script ops.py file by keeping track of the tf output types when the py func gets registered similar to how EagerFunc s are stored . Then during graph exec time after we have results in an array or scalar we can query the Tout list and convert to ndarrays correctly. I was able to crudely implement this just to unblock my use case. One other w a from the client side is to be explicit with the list in the py func like so Here is a branch that attempts to address this shortcoming https github.com recogni tensorflow tree dev recogni bug 20983 Nagging Assignee shivaniag It has been 13 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Marking this as contribution welcome could you submit a PR for this. Thanks. shivaniag PR submitted and attached to this about 16 days ago See this PR that is awaiting merge once CI is good to go https github.com tensorflow tensorflow pull 21038 Thanks 
21543,ConditionalBijectors not Chainable due to Bijector Mapping not supporting deep dicts, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 OSX High Sierra 10.13.1 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device n a TensorFlow installed from source or binary binary TensorFlow version use command below v1.9.0 0 g25c197e023 1.9.0 Python version Python 3.6.5 Anaconda Inc. Bazel version if compiling from source n a GCC Compiler version if compiling from source n a CUDA cuDNN version n a GPU model and memory n a Exact command to reproduce n a Describe the problem I would like to use the bijectors.Chain https github.com tensorflow probability blob master tensorflow probability python bijectors chain.py to create a flow of bijector that extend bijectors.ConditionalBijector https github.com tensorflow probability blob master tensorflow probability python bijectors conditional bijector.py L28 . It looks like Chain already supports conditioning under the hood https github.com tensorflow probability blob master tensorflow probability python bijectors chain.py L263 but the other parts of Bijector are not fully compatible with it. Suppose we have the following test function Calling test chained condition bijector with bijector.Chain we get the following error This error can be overcome by creating a ConditionalChain class which extends ConditionalBijector Now if we run the same code again this time with ConditionalChain we get the following This happens because the deep tuple https github.com tensorflow tensorflow blob r1.10 tensorflow python ops distributions bijector impl.py L126 which is used to cache Bijector inputs outputs doesn t support nested dicts. I would expect the code to run normally with ConditionalChain . Not sure if this is a feature or a bug. If it s a bug then this is a bug report and if it s a feature then this is a feature request to change the behavior. It s also possible that I m misusing the ConditionalBijectors in which case I should move this to stackoverflow. Here s the full code snippet to test the behavior ,I agree this is an API bug. I ll try to address it ASAP but can t give an ETA at the moment. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 21543 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 21543 No a 
21598,Dilation of keras.Conv2DTranspose does not do anything, Describe the problem Well using tf.keras.Conv2DTranspose I noticed the dilation function did not dilate. No matter what size of dilation you choose the result is the same. I did found the problem in keras as well but I did not now if I should report it here too https github.com keras team keras issues 8159 System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes see below OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 x64 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary Source TensorFlow version use command below 1.8.0 Python version 3.6.4 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version v8.0 GPU model and memory GeForce 1050 Exact command to reproduce See below Source code logs result You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION ,Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case or leave them as N A Thanks. Have I written custom code Bazel version Exact command to reproduce Mobile device Good bot Everything is updated wagenrace robieta I have sent 22265 to fix this would you mind to take a look Thanks. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 21598 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 21598 No a 
21614,Keras to estimator errors for RNNs,It seems like a bug that the following model results in the following assert The model simplified from seq2seq The assert we are hitting in TF 1.10 Here is the result of the environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh cat etc issue Linux 97ddc7134519 4.9.93 linuxkit aufs 1 SMP Wed Jun 6 16 55 56 UTC 2018 x86 64 x86 64 x86 64 GNU Linux VERSION 16.04.5 LTS Xenial Xerus VERSION ID 16.04 VERSION CODENAME xenial are we in docker Yes compiler c Ubuntu 5.4.0 6ubuntu1 16.04.10 5.4.0 20160609 Copyright C 2015 Free Software Foundation Inc. This is free software see the source for copying conditions. There is NO warranty not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. uname a Linux 97ddc7134519 4.9.93 linuxkit aufs 1 SMP Wed Jun 6 16 55 56 UTC 2018 x86 64 x86 64 x86 64 GNU Linux check pips numpy 1.14.5 protobuf 3.6.0 tensorflow 1.10.0 check for virtualenv False tensorflow import tf.VERSION 1.10.0 tf.GIT VERSION v1.10.0 0 g656e7a2b34 tf.COMPILER VERSION v1.10.0 0 g656e7a2b34 Sanity check array 1 dtype int32 ,Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case or leave them as N A Thanks. Have I written custom code OS Platform and Distribution TensorFlow installed from TensorFlow version Bazel version CUDA cuDNN version GPU model and memory Exact command to reproduce Mobile device I m a colleague of jmchen g and can update the issue while he is away System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 Docker on Mac Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary docker image tensorflow tensorflow 1.10.0 py3 TensorFlow version use command below Python version python 3 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Exact command to reproduce See Below Running this yields the following trace Here is some more system diag info from running tf env collect.sh Thank you for providing such a clear minimal case. cc tanzhenyu pavithrasv fchollet could you take a look Also qlzh727 for recurrent cells. And k w w because this is happening in clone and build Probably is a bug in the model to estimator. Btw the input of the model is not constructed correctly namely the input shape. The input should be in shape batch input length which should be None seq len in your case. The line tf.keras.layers.Input shape 50 will cause the emb to return None 50 32 and the expected input to LSTM should be None 32 50 batch timestep vocab . k w w and tanzhenyu I did some debug and seems that when the lstm2 layer is loaded the initial state is showing up both in input as well as initial state. Since the LSTM layer only accept one input tensor batch timestep vocab that s why it throws exception. Do u have any insight about why the initial state is picked up both as part of the input and also the kwarg initial state Hi k w w and tanzhenyu Any update on this please Thanks. Adding fchollet as the error also appears when cloning the model using models.clone model. I ve run into the same issue and it seem like clone functional model ends up passing the initial state as both inputs and as a keyword argument to the call standardize args method of the Recurrent layer. 1 A nasty workaround for us was to comment out the code extracting the initial state from the inputs as follows since the shape of the state in the input list was not complete but it was in the kwarg def standardize args ... if isinstance inputs list assert initial state is None and constants is None if num constants is not None constants inputs num constants inputs inputs num constants if len inputs 1 initial state inputs 1 inputs inputs 0 ... 1 in the call to standardize args when converting to an estimator I get this inputs tf.Tensor time distributed 7 transpose 1 0 shape 1 320 dtype float32 tf.Tensor input state h 2 input state h Identity 0 shape dtype float32 tf.Tensor input state c 2 input state c Identity 0 shape dtype float32 initial state kwarg tf.Tensor input state h 0 shape 1 100 dtype float32 tf.Tensor input state c 0 shape 1 100 dtype float32 I ve not had time to debug where this happens but I hope it helps someone. Can anyone help with this Can you call the model with some input and see it works before converting to the estimator I suspect this is because initial state is not passed to the first LSTM. This is fixed with latest tf nightly version 1.15.0 dev20190726 . Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 21614 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 21614 No a 
21697,ProfilerHook generates wrong timeline traces, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow N A OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary dockerhub 1.9.0 devel gpu TensorFlow version use command below 1.9.0 Python version 2.7.12 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version 9.0 7.1 GPU model and memory Tesla V100 16GB Exact command to reproduce model https github.com tensorflow models tree master research deep speech command python deep speech.py model dir . deepSpeech train data dir . librispeech data train clean 100 LibriSpeech train clean 100.csv eval data dir . librispeech data dev clean LibriSpeech dev clean.csv num gpus 1 wer threshold 0.23 seed 1 hooks profilerhook Describe the problem The time of the timeline trace is not correct which lasts hundreds of years.It s correct at the beginning and becoming abnormal from the third trace the 21th step for save steps 10 . Source code logs image https user images.githubusercontent.com 4105051 44328172 71ba9080 a493 11e8 8926 92249de87f76.png ,Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case or leave them as N A Thanks. Have I written custom code Bazel version Mobile device I am having a similar problem. If someone can look into this that will be great tensorflowbutler Yes Nagging Assignee tatianashp It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. same here petermattson Can you take a look at this bug Hi Tatiana I m not sure who owns timelines but unfortunately that person is not me. On Fri Nov 16 2018 at 5 37 PM Tatiana Shpeisman notifications github.com wrote petermattson https github.com petermattson Can you take a look at this bug You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 21697 issuecomment 439576428 or mute the thread https github.com notifications unsubscribe auth AhFaHY S R1h2Om mCJG7ihlLrugNXcZks5uv2hGgaJpZM4WCd6r . I ve run into this problem also. Haven t had a chance to dig in yet but a few more notes about the issue 1 In my application the shift appears to happen on every session.run but the ops that get shifted are not always consistent. 2 The amount of the shift appears to be 2 64 1000 us or 2 64 ns. This suggests that the timing values might be stored as 64 bit integers representing timing in nanoseconds. They might be overflowing or the overflow is not being handled correctly. Is this still an issue with the latest version of TF Automatically closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 21697 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 21697 No a 
21762, not all arguments converted during string formatting in rnn cell impl,I believe there s a mistake in the method will be called with inputs shape being a tuple and formatting will interpret this tuple as multiple formatting arguments. Can we change it to ,Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case or leave them as N A Thanks. Have I written custom code OS Platform and Distribution TensorFlow installed from TensorFlow version Bazel version CUDA cuDNN version GPU model and memory Exact command to reproduce Mobile device melkonyan Would you mind to create a PR for the proposed fix melkonyan Your contribution would be very welcome. Ok I ll create a PR 
21837,Incorrect gradients when different Python variables are assigned the same tf.constant value.,When different Python variables are assigned the same tf.constant value the computed gradients are incorrect. System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Only the code above. OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac OS X 10.13.6 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary Binary TensorFlow version 1.10.0 Python version 2.7.15 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Exact command to reproduce Running the code above. ,Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case or leave them as N A Thanks. Have I written custom code OS Platform and Distribution TensorFlow installed from TensorFlow version Bazel version CUDA cuDNN version GPU model and memory Exact command to reproduce Mobile device rmlarsen Will Tensorflow cache tf.constant 3.0 and reuse it for different object in the underlying implementation tensorflowbutler yes this is still an issue. Nagging Assignee tatianashp It has been 30 days with no activity and this issue has an assignee. Please update the label and or status accordingly. facaiy elliotwaite Sorry this fell through the cracks. Is this still an issue It was last time I tested it on September 8th. I was using the latest stable release at the time. However I haven t tested it since. elliotwaite tatianashp facaiy I run the test script on the tf nightly 1.12.0.dev20181005 . The outputs are correct. So we can close this issue feihugis josh11b Sounds good. 
22013,tf.scatter nd update Segmentation fault core dumped , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary source TensorFlow version use command below TF checkpoint I have built Bazel version CUDNN version GPU GEFORCE GTX 1080Ti 11GB GIGABYTE AORUS Exact command to reproduce Describe the problem I am trying to create a function that would modify a Tensor within a pipeline of Dataset API. The scoping may seem weird but that is minimal example that shows the problem that I created from my project. After adding tf.scatter nd update y 0 0.22 I started to get segmentation fault. A minimal example with tf.add instead of tf.scatter nd update worked see https github.com tensorflow tensorflow issues 22009 I tried disabling GPU with config tf.ConfigProto device count GPU 0 and CUDA VISIBLE DEVICES but the result was the same. I will recompile TF overnight from the current master with copt g and try to provide a stacktrace Source code logs log of run on CPU ,Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case or leave them as N A Thanks. CUDA cuDNN version GPU model and memory When executing line by line everything works fine until it starts the session. I ve tried running it on different environment on kaggle.com and the it also fails. I tried it on one more environment. Where I have just installed TF from https files.pythonhosted.org packages 04 7e a484776c73b1431f2b077e13801531e966113492552194fe721e6ef88d5d tensorflow 1.10.1 cp36 cp36m manylinux1 x86 64.whl I had to modify a bit lambda initializer in get variable and specify the shape but otherwise I got the same segmentation fault. This env does not have GPU so it was run on a CPU. I tried running the script in gdb but it says no stack . Any hints how to get it work I ve managed to get gdb working looks like the issue is with inferring shape in scatter nd update Full log I ve got the code running by explicitly specifying shape in tf.get variable and then making sure that the corresponding tensors in assign ops had the same shape so not assigning 1.0 but 1.0 . Still I could not make it work with dynamic shapes and segmentation fault should not take place but throw some error message. Here is the working code and the output rohan100jain I have identified the reason for the segmentation fault. Please have a look at my comments above. Nagging Assignee rohan100jain It has been 30 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Looking at the documentation for scatter nd update https www.tensorflow.org api docs python tf scatter nd update seems like indexing into a zero rank tensor is sort of invalid Adding Eugene for more info. Thanks for reporting. I found the cause of the problem and will send a fix. 
22019, Bug ImportError cannot import name checkpoint management tf1.10,In tf1.10 when I run freeze graph it comes error with msg above. I cloned source file from github and tried it different way and still have same issues. Thanks for your help File freeze graph.py line 20 in module from tensorflow.python.training import checkpoint management ImportError cannot import name checkpoint management System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 MAC Ubuntu TensorFlow installed from source or binary source TensorFlow version use command below tf1.10 Python version 3.6 Exact command to reproduce python tensorflow tensorflow python tools freeze graph.py ,same problem Same problem. I get the following error in both tensor flow 1.10.1 and 1.8.0 File freeze graph.py line 58 in module from tensorflow.python.training import checkpoint management ImportError cannot import name checkpoint management Same problem. Same problem commant from tensorflow.python.training import checkpoint management add import tensorflow as tf change line 119 not checkpoint management.checkpoint exists input checkpoint to not tf.train.checkpoint exists input checkpoint same problem same problem EDIT JunhyukHyun suggestion fixed it for me thanks JunhyukHyun Thank you for finding the fix. Would you mind submitting a PR with this code change JunhyukHyun I edited the freeze graph.py however it introduced a new error File Users cvsanbuenaventura tensorflow tensorflow python tools freeze graph.py line 365 in freeze graph checkpoint version checkpoint version File Users cvsanbuenaventura tensorflow tensorflow python tools freeze graph.py line 141 in freeze graph with def protos for node in input meta graph def.graph def.node AttributeError int object has no attribute graph def aselle Can you help here Any update here I am hitting the same issue trying to follow https github.com movidius ncappzoo tree master tensorflow mobilenets Fixed this issue with the following. tensorflow python training checkpoint management function checkpoint exists is exported as train.checkpoint exists so I made the following changes to tensorflow python tools freeze graph.py I ran tensorflow tools ci build ci build.sh CPU bazel test tensorflow python tools freeze graph test and it checked out. If it works for those of you having issues I ll put it into a PR. If None of the above works for you copy this code into freeze graph.py for a Short Term Fix Copyright 2015 The TensorFlow Authors. All Rights Reserved. Licensed under the Apache License Version 2.0 the License you may not use this file except in compliance with the License. You may obtain a copy of the License at http www.apache.org licenses LICENSE 2.0 Unless required by applicable law or agreed to in writing software distributed under the License is distributed on an AS IS BASIS WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND either express or implied. See the License for the specific language governing permissions and limitations under the License. Converts checkpoint variables into Const ops in a standalone GraphDef file. This script is designed to take a GraphDef proto a SaverDef proto and a set of variable values stored in a checkpoint file and output a GraphDef with all of the variable ops converted into const ops containing the values of the variables. It s useful to do this when we need to load a single file in C especially in environments like mobile or embedded where we may not have access to the RestoreTensor ops and file loading calls that they rely on. An example of command line usage is bazel build tensorflow python tools freeze graph bazel bin tensorflow python tools freeze graph input graph some graph def.pb input checkpoint model.ckpt 8361242 output graph tmp frozen graph.pb output node names softmax You can also look at freeze graph test.py for an example of how to use it. from future import absolute import from future import division from future import print function import argparse import sys from google.protobuf import text format from tensorflow.core.framework import graph pb2 from tensorflow.core.protobuf import saver pb2 from tensorflow.python import pywrap tensorflow from tensorflow.python.client import session from tensorflow.python.framework import graph util from tensorflow.python.framework import importer from tensorflow.python.platform import app from tensorflow.python.platform import gfile from tensorflow.python.training import saver as saver lib FLAGS None def freeze graph input graph input saver input binary input checkpoint output node names restore op name filename tensor name output graph clear devices initializer nodes variable names blacklist Converts all variables in a graph and checkpoint into constants. del restore op name filename tensor name Unused by updated loading code. if not gfile.Exists input graph print Input graph file input graph does not exist return 1 if input saver and not gfile.Exists input saver print Input saver file input saver does not exist return 1 input checkpoint may be a prefix if we re using Saver V2 format if not saver lib.checkpoint exists input checkpoint print Input checkpoint input checkpoint doesn t exist return 1 if not output node names print You need to supply the name of a node to output node names. return 1 input graph def graph pb2.GraphDef mode rb if input binary else r with gfile.FastGFile input graph mode as f if input binary input graph def.ParseFromString f.read else text format.Merge f.read input graph def Remove all the explicit device specifications for this node. This helps to make the graph more portable. if clear devices for node in input graph def.node node.device importer.import graph def input graph def name with session.Session as sess if input saver with gfile.FastGFile input saver mode as f saver def saver pb2.SaverDef if input binary saver def.ParseFromString f.read else text format.Merge f.read saver def saver saver lib.Saver saver def saver def saver.restore sess input checkpoint else var list reader pywrap tensorflow.NewCheckpointReader input checkpoint var to shape map reader.get variable to shape map for key in var to shape map try tensor sess.graph.get tensor by name key 0 except KeyError This tensor doesn t exist in the graph for example it s global step or a similar housekeeping element so skip it. continue var list key tensor saver saver lib.Saver var list var list saver.restore sess input checkpoint if initializer nodes sess.run initializer nodes variable names blacklist variable names blacklist.split if variable names blacklist else None output graph def graph util.convert variables to constants sess input graph def output node names.split variable names blacklist variable names blacklist with gfile.GFile output graph wb as f f.write output graph def.SerializeToString print d ops in the final graph. len output graph def.node def main unused args freeze graph FLAGS.input graph FLAGS.input saver FLAGS.input binary FLAGS.input checkpoint FLAGS.output node names FLAGS.restore op name FLAGS.filename tensor name FLAGS.output graph FLAGS.clear devices FLAGS.initializer nodes FLAGS.variable names blacklist if name main parser argparse.ArgumentParser parser.register type bool lambda v v.lower true parser.add argument input graph type str default help TensorFlow GraphDef file to load. parser.add argument input saver type str default help TensorFlow saver file to load. parser.add argument input checkpoint type str default help TensorFlow variables file to load. parser.add argument output graph type str default help Output GraphDef file name. parser.add argument input binary nargs const True type bool default False help Whether the input files are in binary format. parser.add argument output node names type str default help The name of the output nodes comma separated. parser.add argument restore op name type str default save restore all help The name of the master restore operator. parser.add argument filename tensor name type str default save Const 0 help The name of the tensor holding the save path. parser.add argument clear devices nargs const True type bool default True help Whether to remove device specifications. parser.add argument initializer nodes type str default help comma separated list of initializer nodes to run before freezing. parser.add argument variable names blacklist type str default help comma separated list of variables to skip converting to constants FLAGS unparsed parser.parse known args app.run main main argv sys.argv 0 unparsed Tensorflow Are you guys seriously ignore this issues and nobody cares about it aiscientist Sorry it fell through the crack. gargn Could you please take a look at this bug aiscientist did IMBurbank s suggestion work for you Let me know if the proposed solution is useful to those of you struggling with this issue. I would be happy to put it into a PR. aiscientist did IMBurbank s suggestion work for you Yes it works for me. But please update for this on the default version. Let me know if the proposed solution is useful to those of you struggling with this issue. I would be happy to put it into a PR. It works thanks for your help glad if this work on the default version so the others won t suffer from this issue. This is a bit of a stream of consciousness response. I slowly typed it as I dug back into this issue. First I decided to go back and confirm your error in 1.10 and if it still existed in the current 1.11. I cloned the tensorflow repo entered it and built a current dockerfile I entered the directory and ran freeze graph.py And received the expected message I added a TF PACKAGE build arg the nvidia.Dockerfile to install tensorflow 1.10.1 and rebuilt. This time running gave the same error originally reported. The changes I proposed above fixed the issue. Next I started digging through the diff between 1.10 and 1.11 branches... I was hoping to find the specific fix that was applied between 1.10 and 1.11 in case it could be used to patch 1.10. It was bugging me that I hadn t tracked down how it was fixed in 1.11. While looking through the freeze graph.py commit history it slowly dawned on me that the checkpoint management system changed significantly between 1.10 and 1.11. So much so that checkpoint managment.py didn t exist when 1.10 was released. I decided that the problem might be an issue of calling freeze graph.py in a local uncompiled branch that was newer than the 1.10 binary in the python lib. I switched the the git r1.10 branch. Then started back up the container with tf 1.10.1 installed. And ran free graph.py using tensorflow 1.10.1 on the r1.10 branch it worked The fix I originally proposed is only papering over this compatibility issue by pointing to the tf export ed checkpoint exist module made available in the python API. To be honest I was excited about putting in another PR but I don t think it will help here. I should have noticed this last time I looked at the issue. If you want to run files in the uncompiled tensorflow project with a legacy tensorflow version installed locally make sure to checkout the branch that matches the local version. The issue can also be avoided by updating to the current release locally or doing development using the tensorflow dockerfiles to ensure a supported environment. If I missed something please let me know. adopt JunhyukHyun methon it is ok adopt Isaac Burbank proposed solution it raises question python tensorflow python tools freeze graph.py Traceback most recent call last File tensorflow python tools freeze graph.py line 60 in module from tensorflow.train import checkpoint management ImportError No module named tensorflow.train zkl99999 What version of Tensorflow do you have installed and which branch are you using to run freeze graph.py Reference the solution provided by IMBurbank https github.com tensorflow tensorflow issues 22019 issuecomment 425378974 in order to freeze a graph in TensorFlow 1.10. Closing this bug since there is a workaround provided. zkl99999 What version of Tensorflow do you have installed and which branch are you using to run freeze graph.py Fixed this issue with the following. tensorflow python training checkpoint management function checkpoint exists is exported as train.checkpoint exists so I made the following changes to tensorflow python tools freeze graph.py I ran tensorflow tools ci build ci build.sh CPU bazel test tensorflow python tools freeze graph test and it checked out. If it works for those of you having issues I ll put it into a PR. Hi it still not work using this solution and it just report Traceback most recent call last File home stella tensorflow tensorflow python tools freeze graph.py line 58 in module from tensorflow.train import checkpoint exists ImportError No module named tensorflow.train I use tf 1.4.0. Can you help me thx I encountered the same problem. Stellaxx24 tensorflow 1.11.0 Outlook for iOS https aka.ms o0ukef ZHANG ZHAOXIANG notifications github.com Sunday May 5 2019 7 45 01 PM tensorflow tensorflow Stellaxx24 Mention Re tensorflow tensorflow Bug ImportError cannot import name checkpoint management tf1.10 22019 I encountered the same problem. Stellaxx24 https github.com Stellaxx24 You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 22019 issuecomment 489409184 or mute the thread https github.com notifications unsubscribe auth AKQJ7Y7XZXWEFR3CZPOHSETPT2UBZANCNFSM4FS4F36Q . 
22025, Bug max pool with argmax has different behaviour on CPU and GPU,The function max pool with argmax has different behaviour concerning the returned indices depending on the GPU or CPU backend. On GPU returned indices do not take into account batch dimension. It returns indices such that b y x c y width x channels c On CPU returned indices take into account batch dimension. It returns indices such that b y x c b height y width x channels c Isn t it a problem if one wants to run models with the same graph including this op both on CPU and GPU Here is an example to show the problem Which results in something like this on CPU And something like this on GPU This was obtained with 1.9 CPU GPU tensorflow versions. I think it would be more convenient to have the same behaviour on CPU and GPU isn t it ,Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case or leave them as N A Thanks. Have I written custom code OS Platform and Distribution TensorFlow installed from Bazel version CUDA cuDNN version GPU model and memory Exact command to reproduce Mobile device Have I written custom code yes OS Platform Ubuntu 16.04 LTS for CPU install and centOS 7 for GPU install But I don t think these informations are necessary The problem is really that the behaviour of the function is different for tensorflow 1.9 CPU and tensorflow 1.9 GPU. It should be reproducible no matter the CPU GPU installations I guess. qlzh727 Could you take a look at this Is it expected behavior Not sure why I am cc ed here if I was showing up from the change history I guess that was because I was on sync rotation for PRs. Probably should find someone who is the real owner of maxpooling op.cc OK looks like this code is kind of old with no real maintainer. rmlarsen do you know what s expected here I don t see any notice in the documentation that CPU GPU behavior should differ. Any news about this I do not know if this can help but in fact the documentation of the function is correct for the CPU behaviour. Plus the function has been available first on GPU and on a later TF version for CPU. I guess that there was a wrong doc when the function was available only for GPU and that when it has been later implemented for CPU it has been inspired from the doc leading to two different behaviours. Nagging Assignee poxvoculi It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. I m looking into the issue and I ll try to fix it later. Thank mpaillassa for your detailed feedback mpaillassa Hi would you mind taking a test on nightly version pip install tf nightly gpu facaiy Hi I tried the code of my first post on CPU with pip install tf nightly and on GPU with pip install tf nightly gpu . Both on Ubuntu 16.04 and with CUDA 9.0 and cuDNN 7.3.1 for GPU. But I still get the same behaviours CPU takes into account the batch dimension in the returned indices while GPU does not. Did I do something wrong Thank mpaillassa for your report I can reproduce the bug in my experimental PR 23370. Unfortunately there is something wrong with my working station environment and I cannot find an alternative recently. So I d like to mark the issue as Contributions Welcome and feel free to take over it and go head if anyone is interested . cc rmlarsen I create the PR 23993 to fix the issue please take a look thanks Hello I am new to the tensorflow community. Is this a good issue to get familiar with the tensorflow open source Welcome bono1567 As you see PR 25241 and 23993 have been in the process to fix the issue. I ll update here when they are merged. 
22071,MatMul flop incorrect for 3D inputs, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 N A Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below v1.10.0 0 g656e7a2b34 1.10.0 Python version 3.7 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Exact command to reproduce see below The outputs are correct 2M flops if a and b have shape 100 100 . But with 3D inputs it outputs no flops for matmul. The reason is that for 3D inputs it uses the BatchMatMul op which does not have flop statistics implemented., tatatodd Hi Todd could you please take a look at this Tensorflow version b v1.13.1 6 gd32c49d4e3 1.13.1 The following code which attempts to profile the flops and params of ResNet50 reports FLOPS 51 351 635 PARAMS 25 636 712 Params seems ok but flops is way off should be 4GFLOPS . Potentially could be the same issue Hi rmlarsen since this issue hasn t got addressed yet I created a commit that may solve the BatchMatMul op flops problem could you help take a look Thanks. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 22071 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 22071 No a 
22187,keras.layers.Conv3DTranspose error when reused,Version 1.10 file tensorflow python keras layers convolution.py line 1000 problem input spec is modified every time the module is called input spec uses dynamic shape causes error in shape checker when the layer is reused ,solution delete line 1000 input spec should only be modified in constructor or build function kor01 Thank you for your suggestion. Would you be interested in submitting a PR cc fchollet kor01 Thank you for your suggestion. Would you be interested in submitting a PR cc fchollet OK I ll submit a PR. Great. Thank you Please post here once you do. kor01 Hi any update on the PR harshini gadige This issue seems to be fixed. Can you update the status Version r1.10 https github.com tensorflow tensorflow blob 4dcfddc5d12018a5a0fdca652b9221ed95e9eb23 tensorflow python keras layers convolutional.py L997 Version master https github.com tensorflow tensorflow blob 7bcbcc1392516a2b2d7a7abae2ccce7091c8dae3 tensorflow python keras layers convolutional.py L1070 harshini gadige This issue seems to be fixed. Can you update the status Version r1.10 tensorflow tensorflow python keras layers convolutional.py https github.com tensorflow tensorflow blob 4dcfddc5d12018a5a0fdca652b9221ed95e9eb23 tensorflow python keras layers convolutional.py L997 Line 997 in 4dcfddc tensorflow tensorflow commit 4dcfddc5d12018a5a0fdca652b9221ed95e9eb23 self.input spec InputSpec ndim 5 axes c axis inputs shape c axis Version master tensorflow tensorflow python keras layers convolutional.py https github.com tensorflow tensorflow blob 7bcbcc1392516a2b2d7a7abae2ccce7091c8dae3 tensorflow python keras layers convolutional.py L1070 Line 1070 in 7bcbcc1 tensorflow tensorflow commit 7bcbcc1392516a2b2d7a7abae2ccce7091c8dae3 def call self inputs Sure. Closing the issue. 
22217,Session run options for configuring the timeout of a run is not working as expected, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device no TensorFlow installed from source or binary binary TensorFlow version use command below 1.9.0 Python version 3.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 9.2 GPU model and memory V100 and 16GB Exact command to reproduce cat etc issue Linux ip 172 31 35 59 4.4.0 1062 aws 71 Ubuntu SMP Fri Jun 15 10 07 39 UTC 2018 x86 64 x86 64 x86 64 GNU Linux VERSION 16.04.4 LTS Xenial Xerus VERSION ID 16.04 VERSION CODENAME xenial are we in docker No compiler c Ubuntu 5.4.0 6ubuntu1 16.04.10 5.4.0 20160609 Copyright C 2015 Free Software Foundation Inc. This is free software see the source for copying conditions. There is NO warranty not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. uname a Linux ip 172 31 35 59 4.4.0 1062 aws 71 Ubuntu SMP Fri Jun 15 10 07 39 UTC 2018 x86 64 x86 64 x86 64 GNU Linux check pips numpy 1.14.5 protobuf 3.6.0 tensorflow 1.9.0 check for virtualenv False tensorflow import tf.VERSION 1.9.0 tf.GIT VERSION v1.9.0 0 g25c197e023 tf.COMPILER VERSION v1.9.0 0 g25c197e023 Sanity check array 1 dtype int32 home ubuntu anaconda3 envs tensorflow p36 lib python3.6 site packages h5py init .py 36 FutureWarning Conversion of the second argument of issubdtype from float to np.floating is deprecated. In future it will be treated as np.float64 np.dtype float .type . from . conv import register converters as register converters env LD LIBRARY PATH usr local cuda 9.0 lib64 usr local cuda 9.0 extras CUPTI lib64 lib nccl cuda 9.0 lib usr lib64 openmpi lib usr local lib usr lib usr local mpi lib lib usr lib64 openmpi lib usr local lib usr lib usr local mpi lib lib usr lib64 openmpi lib usr local lib usr lib usr local mpi lib lib DYLD LIBRARY PATH is unset nvidia smi Tue Sep 11 19 14 44 2018 NVIDIA SMI 396.37 Driver Version 396.37 GPU Name Persistence M Bus Id Disp.A Volatile Uncorr. ECC Fan Temp Perf Pwr Usage Cap Memory Usage GPU Util Compute M. 0 Tesla V100 SXM2... On 00000000 00 1E.0 Off 0 N A 39C P0 48W 300W 40MiB 16160MiB 0 Default Processes GPU Memory GPU PID Type Process name Usage 0 7354 C nvidia cuda mps server 30MiB cuda libs usr local cuda 9.1 doc man man7 libcudart.7 usr local cuda 9.1 doc man man7 libcudart.so.7 usr local cuda 9.1 lib64 libcudart.so.9.1.85 usr local cuda 9.1 lib64 libcudart static.a usr local cuda 9.2 doc man man7 libcudart.7 usr local cuda 9.2 doc man man7 libcudart.so.7 usr local cuda 9.2 lib64 libcudart.so.9.2.88 usr local cuda 9.2 lib64 libcudart static.a usr local cuda 8.0 doc man man7 libcudart.7 usr local cuda 8.0 doc man man7 libcudart.so.7 usr local cuda 8.0 lib64 libcudart static.a usr local cuda 8.0 lib64 libcudart.so.8.0.61 usr local cuda 9.0 doc man man7 libcudart.7 usr local cuda 9.0 doc man man7 libcudart.so.7 usr local cuda 9.0 lib64 libcudart.so.9.0.176 usr local cuda 9.0 lib64 libcudart static.a Describe the problem The timeout in ms for session run options seems to be not working as expected. Source code logs The above code goes into an infinite loop and the timeout in ms is not taking effect.,Can you please help here bhagatindia I can reproduce this issue. It seems to be caused by the lack of CancellationManager in the while loop related ops. I m working on the fix. Will keep you updated. cc skye mrry Is my understanding right and any suggestions PR 23811 is submitted to fix this issue. Nagging Assignee skye It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. 
22221,Threading data out of one while loop into another interferes with gradients, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac OS 10.13.6 TensorFlow installed from source or binary source TensorFlow version use command below b v1.9.0 rc2 3217 g8e5c118ce8 1.10.0 Python version 3.6.2 Bazel version if compiling from source 0.15.2 homebrew GCC Compiler version if compiling from source Apple LLVM version 9.1.0 clang 902.0.39.2 CUDA cuDNN version n a GPU model and memory n a Exact command to reproduce Run included script Mobile device n a Describe the problem I have some code which 1. Generates some data using an inference only tf.while loop . 2. Uses a second while loop to run several minibatches of Adam using the data from the first loop. Both while loops have back prop False . The second loop computes gradients but these are used only inside the second loop which includes running the train op . I use tf.stop gradients to prevent gradients from flowing backwards from the second loop to the first...but to no avail Caveat line numbers for the above stacktrace may be slightly wrong since my TF has some debug print statements. I ve traced the problem to ops gradients impl.py PendingCount or the surrounding code. PendingCount seems to trace right through tf.stop gradients . I m not sure where the right fix is though so could use some control flow expert help. Source code logs Here s a minimized test case ,Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case or leave them as N A Thanks. Mobile device Fixed. I also tried using the stop gradients argument to tf.gradients but the problem persists it still thinks gradients are flowing out of the while loop. alextp recommended tf.GradientTape as a workaround and that seems to work great as long as variables are created with use resource True . So this is still a bug but it s no longer blocking me. girving Hi Can this be closed or any fix is being implemented alextp This is still a bug and expected to be fixed only when we transition to while v2 as described in https github.com tensorflow community pull 13 . I ll close this for now. Please file a separate issue for your problem with instructions to reproduce. On Tue Mar 10 2020 at 3 40 PM Stanley Gan notifications github.com wrote I am also experiencing this error on TF1.15 when trying to compute Laplacian with 1st gradient of some operations overwritten using decorator tf.custom gradient. I computed the Laplacian using tf.while loop by creating each row of Hessian and summing relevant parts. With this code implemented without overwriting the 1st gradient my code runs fine. However when I introduce my code in overwriting 1st gradient which involves another tf.while loop solely for tensor manipulation and multiplication it spits out this error. I changed to using tf.GradientTape but apparently the gradient computed is None. I am wondering how exactly did you solve this or any suggestions on how I should I approach this issue You are receiving this because you modified the open close state. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 22221 email source notifications email token AAABHROW4OHDKAKAQBJBN3DRG26URA5CNFSM4FURUWJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEONOVIQ issuecomment 597355170 or unsubscribe https github.com notifications unsubscribe auth AAABHRIDNITTXVAN7NDOTO3RG26URANCNFSM4FURUWJA . Alex alextp. Sorry I just deleted my comment as it is pretty vague and not a good way to explain my error. I am thinking if this should be a separate issue but I ran the same example as described by girving in this issue on TF1.15 the error still persists. And what about nightly On Tue Mar 10 2020 at 5 25 PM Stanley Gan notifications github.com wrote alextp https github.com alextp . Sorry I just deleted my comment as it is pretty vague and not a good way to explain my error. I am thinking if this should be a separate issue but I ran the same example as described by girving https github.com girving on TF1.15 the error still persists. You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 22221 email source notifications email token AAABHRP6COBEJV73LZHXMDTRG3LAFA5CNFSM4FURUWJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEONVVWA issuecomment 597383896 or unsubscribe https github.com notifications unsubscribe auth AAABHRKX2PD3GTX4JGWYNCLRG3LAFANCNFSM4FURUWJA . Alex There are no more nightly builds for version 1.15 on pypi. I mean the 2.x branch the 1.15 branch will get no further fixes. On Tue Mar 10 2020 at 5 55 PM Stanley Gan notifications github.com wrote There are no more nightly builds for version 1.15 on pypi. You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 22221 email source notifications email token AAABHRNOKRQ34D2PSYLOFOLRG3OPXA5CNFSM4FURUWJKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEONXIOI issuecomment 597390393 or unsubscribe https github.com notifications unsubscribe auth AAABHRMVVKAD5A7RXGL5TUTRG3OPXANCNFSM4FURUWJA . Alex alextp Thanks for your reply For specific reason package dependencies the code which I wrote is in 1.15 hence even if running on TF2 works I still have to figure out a way as my code is written in 1.15. Unless you are suggesting that some other fixes which I am not aware of Though based on the community link which you posted https github.com tensorflow community pull 13 I checked the commits to fix this were merged into 1.15 branch unless I am missing something . So I need an example to reproduce this against nightly which is 2.x so I can debug it. On Wed Mar 11 2020 at 8 44 AM Stanley Gan notifications github.com wrote alextp https github.com alextp Thanks for your reply For specific reason package dependencies the code which I wrote is in 1.15 hence even if running on TF2 works I still have to figure out a way as my code is written in 1.15. Unless you are suggesting that some other fixes which I am not aware of Though based on the community link which you posted tensorflow community 13 https github.com tensorflow community pull 13 I checked the commits to fix this were merged into 1.15 branch unless I am missing something . You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 22221 issuecomment 597708551 or unsubscribe https github.com notifications unsubscribe auth AAABHRIA5M4VM7WVFLI7F6LRG6WULANCNFSM4FURUWJA . Alex Using girving s example ran using tf nightly 2.2.0 dev20200311 I got a different error as shown below This is an unrelated bug in the example code where it s using tf.while loop incorrectly and returning variables where tf expects tensors I think. On Wed Mar 11 2020 at 10 21 AM Stanley Gan notifications github.com wrote Using girving s example ran using tf nightly 2.2.0 dev20200311 import tensorflow as tf First while loop rollouts tf.while loop cond lambda True body lambda tf.compat.v1.get variable b loop vars tf.zeros maximum iterations 1 back prop False rollouts tf.stop gradient rollouts def body i loss tf.stop gradient rollouts train tf.train.AdamOptimizer learning rate 1e 2 .minimize loss with tf.control dependencies train return i 1 Second while loop. Crashes since it thinks the gradients depend on something from the previous while loop. tf.while loop cond lambda True body body maximum iterations 1 parallel iterations 1 loop vars tf.zeros dtype tf.int32 back prop False I got a different error as shown below 2020 03 11 09 05 04.518937 I tensorflow core platform cpu feature guard.cc 143 Your CPU supports instructions that this TensorFlow binary was not compiled to use AVX2 FMA 2020 03 11 09 05 04.531475 I tensorflow compiler xla service service.cc 168 XLA service 0x7ff78acc51e0 initialized for platform Host this does not guarantee that XLA will be used . Devices 2020 03 11 09 05 04.531492 I tensorflow compiler xla service service.cc 176 StreamExecutor device 0 Host Default Version WARNING tensorflow From dummy example.py 9 calling while loop v2 from tensorflow.python.ops.control flow ops with back prop False is deprecated and will be removed in a future version. Instructions for updating back prop False is deprecated. Consider using tf.stop gradient instead. Instead of results tf.while loop c b vars back prop False Use results tf.nest.map structure tf.stop gradient tf.while loop c b vars Traceback most recent call last File usr local lib python3.6 site packages tensorflow python util nest.py line 378 in assert same structure expand composites ValueError The two structures don t have the same nested structure. First structure type list str TensorSpec shape dtype tf.int32 name None TensorSpec shape dtype tf.float32 name None Second structure type list str 1 tf.Variable b 0 shape dtype float32 numpy 0.98595977 More specifically Substructure type list str TensorSpec shape dtype tf.float32 name None is a sequence while substructure type ResourceVariable str tf.Variable b 0 shape dtype float32 numpy 0.98595977 is not During handling of the above exception another exception occurred Traceback most recent call last File dummy example.py line 9 in module back prop False File usr local lib python3.6 site packages tensorflow python util deprecation.py line 574 in new func return func args kwargs File usr local lib python3.6 site packages tensorflow python ops control flow ops.py line 2491 in while loop v2 return same structure True File usr local lib python3.6 site packages tensorflow python ops control flow ops.py line 2731 in while loop nest.assert same structure loop var structure list loop vars File usr local lib python3.6 site packages tensorflow python util nest.py line 385 in assert same structure str e str1 str2 ValueError The two structures don t have the same nested structure. First structure type list str TensorSpec shape dtype tf.int32 name None TensorSpec shape dtype tf.float32 name None Second structure type list str 1 tf.Variable b 0 shape dtype float32 numpy 0.98595977 More specifically Substructure type list str TensorSpec shape dtype tf.float32 name None is a sequence while substructure type ResourceVariable str tf.Variable b 0 shape dtype float32 numpy 0.98595977 is not Entire first structure . . Entire second structure . . You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 22221 issuecomment 597761869 or unsubscribe https github.com notifications unsubscribe auth AAABHRICXL4P664CMAMHHTLRG7CCRANCNFSM4FURUWJA . Alex Let me try to fix it. Should I run it with tf.compat.v1.disable v2 behavior Because v2 is eagerly executed. 
22347,Bug in tf.import graph def for Graph Editor, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow see example below OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary source TensorFlow version use command below 1.8 Python version 3.6 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version 9.0 GPU model and memory Titan X Exact command to reproduce see example below Describe the problem I m trying to export and then later import a subset of my graph after using the graph editor for network pruning such that I get only the pruned network left at the end . I ve written two toy examples to illustrate the problem as simply as possible example 1 below works I can define two subgraphs that share only their input grab the graphdef for one clear the graph and restore only the subgraph I want example 2 below tries to do the same thing but uses the graph editor to replace the weights as in the pruning case and grab the new subgraph. Source code logs Example 1 this prints as expected class tensorflow.core.framework.graph pb2.GraphDef tf.Tensor import w 1 0 shape 1 dtype float32 ref tf.Tensor import w 1 read 0 shape 1 dtype float32 tf.Tensor import x 1 0 shape dtype float32 tf.Tensor import y 1 0 shape 1 dtype float32 Example 2 Edit fixed typo in example 2 error is now ValueError Node w read 1 expects to be colocated with unknown node w The notable difference in the subgraph graphdefs is the line s loc w under node w read 1 which indicates the root cause is likely in the graph editor considering that s the only difference between the two examples . Tagging purpledog for input on the graph editor setting the attribute of that node.,Thanks for reporting this bug. As you identified the most likely cause is that the graph editor does not copy the attribute of the graph correctly. This package is maintained by the open source community and contribution are welcome. I am happy to review commits. Fixed typo in the original post. The bug is only in the graph editor not tf.import graph def and is indeed in how it copies the operations carrying over collocation attributes of the node def. I m working on a fix now. Pull request submitted. The issue was the deepcopy of the node def. I added a line that checks for the colocation attribute and clears it whenever it is present. Closing this for now. Feel free to reopen. I updated from 1.8 to 1.10 and found there s more bugs in the graph editor. In fact the graph editor s graph replace function doesn t work at all now nevermind trying to re use the graph def later it now fails right away ValueError cannot add an op with id 7443 as it already exists in the graph I think this should be re opened. It seems that a new operation is automatically added to the graph now when it s created so the line info.graph . add op op in copy op handler is redundant and causing the above error. 
22399,MirroredStrategy AssertionError in 1.11.0 rc0, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 Not on mobile device TensorFlow installed from source or binary source models s commit 17fa52864bfc7a7444a8b921d8a8eb1669e14ebd and tensorflow s commit 1e438195399650604fb3aa3a53c67339f1167882 TensorFlow version use command below git checkout b v1.11.0 rc0 v1.11.0 rc0 Python version Python 3.5.2 default Nov 23 2017 16 37 01 Bazel version if compiling from source Build label 0.17.1 GCC Compiler version if compiling from source gcc Ubuntu 5.4.0 6ubuntu1 16.04.10 5.4.0 20160609 CUDA cuDNN version GPU model and memory GeForce GTX 1070 Ti 8G 4 Exact command to reproduce python object detection model main.py pipeline config path object detection samples configs ssd mobilenet v1 0.75 depth 300x300 coco14 sync.config logtostderr model dir data checkpoint del2 num gpus 4 and meet AssertionError File home soft lib python2.7 site packages tensorflow contrib distribute python values.py line 414 in tensor conversion mirrored assert not as ref AssertionError You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh Describe the problem Similar issue https github.com tensorflow tensorflow issues 21968. Reply Actually this issue is likely fixed in release 1.11 could you try with that and see if that fixes the issue But I meet the issue in 1.11.0 rc0. Source code logs 1 git diff object detection model main.py config tf.estimator.RunConfig model dir FLAGS.model dir distribution tf.contrib.distribute.MirroredStrategy config tf.estimator.RunConfig model dir FLAGS.model dir train distribute distribution eval distribute distribution 2 cd models research python object detection model main.py pipeline config path object detection samples configs ssd mobilenet v1 0.75 depth 300x300 coco14 sync.config logtostderr model dir data sisi.wu checkpoint del2 num gpus 4 3 multi GPU.log https github.com tensorflow tensorflow files 2399199 multi GPU.log Thanks.,sorry for missed info CUDA cuDNN version Cuda compilation tools release 9.0 V9.0.176 cuDNN7.0 Update env.txt tf env.txt https github.com tensorflow tensorflow files 2399219 tf env.txt Thanks. cococyx Thanks for reporting this. Could you check whether this issue is seen in TensorFlow 1.11.0 rc1 Thanks for you reply. This issue is seen in TensorFlow 1.11.0 rc1 and I ll try 1.11.0 rc2. tf. version 1.11.0 rc1 multi GPU 1.11.0 rc1.log https github.com tensorflow tensorflow files 2413482 multi GPU 1.11.0 rc1.log tf. version 1.11.0 rc2 multi GPU 1.11.0 rc2.log https github.com tensorflow tensorflow files 2413626 multi GPU 1.11.0 rc2.log cococyx Thanks for catching and reporting this. It appears there is a sync issue we will certainly note it down. Will close this as the issue is fixed in TensorFlow 1.11.0 and 1.12.0 I still have this issue for Resnet50 and TensorFlow 1.12.0... Seems to be related with tf.slim s convolution2d but not with batch norm as I ve seen elsewhere add weight instead. On another note I had problems with batch norm namely using sonnet but disabling the corresponding layers work so this makes me think that not all layers are supported with MirroredStrategy yet Same error here. Issue for rfcn resnet101 and tensorflow 1.12.0 rc2. Related with tf.slim s convolution2d but not with batch norm. Have you solve this AVCarreiro Not yet and it seems more people have this issue. I had similar error with mirroredstrategy and I am using TensorFlow 1.12.0. I found that if I delete the regularizers.l2 regularizer weight decay in conv2d set the weight regularizer to None inside the function resnet arg scope in resnet utils to None then MirroredStrategy works fine. Is there any update on this issue I had similar error with mirroredstrategy and I am using TensorFlow 1.12.0. I found that if I delete the regularizers.l2 regularizer weight decay in conv2d set the weight regularizer to None inside the function resnet arg scope in resnet utils to None then MirroredStrategy works fine. It works for me is there any update same error TF 1.11 has passed it s end of life. Please upgrade to TF 1.14 or TF 2.0 beta. any update.. I am getting the same error while training based on multi gpu using MirroredStrategy Are you using TF2.0 or TF 1.15 Are you using TF2.0 or TF 1.15 I am using TF2.0 Please open a new issue filling the entire template. I am going to close this one so there won t be confusion w.r.t. versions used. 
22516,TF Lite QuantizationUtilTest failure bug, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 MIPS32 Linux Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary source TensorFlow version use command below SHA 8c2159a10e53e5301ae26c739a3d09fa53d3352e Python version N A Bazel version if compiling from source 0.16.1 GCC Compiler version if compiling from source mips mti linux gnu g Codescape GNU Tools 2017.10 05 for MIPS MTI Linux 6.3.0 CUDA cuDNN version N A GPU model and memory N A Exact command to reproduce Too complicated Describe the problem TensorFlow Lite test quantization util test failure Running 17 tests from 1 test case. Global test environment set up. 17 tests from QuantizationUtilTest RUN QuantizationUtilTest.SafeCast OK QuantizationUtilTest.SafeCast 12 ms RUN QuantizationUtilTest.ChooseQuantizationParams OK QuantizationUtilTest.ChooseQuantizationParams 1 ms RUN QuantizationUtilTest.ChooseQuantizationParamsZeroPointOnMinBoundary OK QuantizationUtilTest.ChooseQuantizationParamsZeroPointOnMinBoundary 0 ms RUN QuantizationUtilTest.ChooseQuantizationParamsZeroNotInRange OK QuantizationUtilTest.ChooseQuantizationParamsZeroNotInRange 179 ms RUN QuantizationUtilTest.ChooseQuantizationParamsEmptyRangePositive OK QuantizationUtilTest.ChooseQuantizationParamsEmptyRangePositive 112 ms RUN QuantizationUtilTest.ChooseQuantizationParamsEmptyRangeZero OK QuantizationUtilTest.ChooseQuantizationParamsEmptyRangeZero 0 ms RUN QuantizationUtilTest.ChooseQuantizationParamsZeroPointOnMaxBoundary OK QuantizationUtilTest.ChooseQuantizationParamsZeroPointOnMaxBoundary 0 ms RUN QuantizationUtilTest.IntegerFrExp OK QuantizationUtilTest.IntegerFrExp 1 ms RUN QuantizationUtilTest.IntegerFrExpVersusDouble tensorflow contrib lite kernels internal quantization util test.cc 262 Failure The difference between result and 0.964453 1L 31 is 4142294361.764544 which exceeds 1000 where result evaluates to 2071147315 0.964453 1L 31 evaluates to 2071147046.764544 and 1000 evaluates to 1000. FAILED QuantizationUtilTest.IntegerFrExpVersusDouble 4 ms RUN QuantizationUtilTest.DoubleFromFractionAndShift OK QuantizationUtilTest.DoubleFromFractionAndShift 1 ms RUN QuantizationUtilTest.IntegerDoubleMultiply OK QuantizationUtilTest.IntegerDoubleMultiply 0 ms RUN QuantizationUtilTest.IntegerDoubleCompare OK QuantizationUtilTest.IntegerDoubleCompare 1 ms RUN QuantizationUtilTest.ChooseQuantizationParamsInvalidRange OK QuantizationUtilTest.ChooseQuantizationParamsInvalidRange 110 ms RUN QuantizationUtilTest.QuantizeMultiplierSmallerThanOneExp OK QuantizationUtilTest.QuantizeMultiplierSmallerThanOneExp 581 ms RUN QuantizationUtilTest.QuantizeMultiplierGreaterThanOne OK QuantizationUtilTest.QuantizeMultiplierGreaterThanOne 103 ms RUN QuantizationUtilTest.PreprocessSoftmaxScaling OK QuantizationUtilTest.PreprocessSoftmaxScaling 1 ms RUN QuantizationUtilTest.CalculateInputRadius OK QuantizationUtilTest.CalculateInputRadius 0 ms 17 tests from QuantizationUtilTest 1106 ms total Global test environment tear down 17 tests from 1 test case ran. 1109 ms total PASSED 16 tests. FAILED 1 test listed below FAILED QuantizationUtilTest.IntegerFrExpVersusDouble 1 FAILED TEST Test quantization util test failed Looks like there s undefined behavior in the test. The test expects a 64 bit long whereas it s 32 bit on MIPS32. Changing 1L to 1LL fixes the failure. Source code logs See above. ,Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case or leave them as N A Thanks. Have I written custom code CUDA cuDNN version GPU model and memory Mobile device tensorflowbutler Updated though the update is mostly irrelevant. alexfru Thanks for reporting and fixing this. We will certainly note it down. Thanks for reporting this Alexey. I m making the change with your noted fix now. Addressed in this change https github.com tensorflow tensorflow commit 100714d9e5eb723525eb54142769f9bd8eec5edd . Thanks again for reporting this 
22793,tf.nn.softmax can give a result when input shape is 2 3 and axis 2 Is it a bug , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 windows 10 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below 1.10.0 Python version 3.5.2 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version no CUDA GPU model and memory no GPU Exact command to reproduce Describe the problem for tf.nn.softmax the axis 2 but the input tensor shape is 2 3 . And the result is I think the axis should 0 and 1 if the axis 2 it should be error. Source code logs ,Added a PR 22849 for the fix. Nagging Assignees drpngx harshini gadige It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. 
22837,model.save returning a NotImplementedError ,Please go to Stack Overflow for help and support https stackoverflow.com questions tagged tensorflow If you open a GitHub issue here is our policy 1. It must be a bug a feature request or a significant problem with documentation for small docs fixes please send a PR instead . 2. The form below must be filled out. 3. It shouldn t be a TensorBoard issue. Those go here https github.com tensorflow tensorboard issues . Here s why we have that policy TensorFlow developers respond to issues. We want to focus on work that benefits the whole community e.g. fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem rather than being redirected to Stack Overflow. System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 1803 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary From pip TensorFlow version use command below 1.11.0 Python version 3.6.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Exact command to reproduce You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION Describe the problem I m receiving a NotImplementedError when trying to save a new model. The code above written above creates the error. This code is from a tutorial i ve been following here Python Programming Sentdex https pythonprogramming.net introduction deep learning python tensorflow keras . It s on a fresh machine with a fresh install of python and tensorflow. Talking with some others on a Discord chat i m not the first person who s encountered this recently. Source code logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem. ,Please take a look at tf.train.Saver method https www.tensorflow.org guide saved model save variables and also an example to save a model https github.com lazyprogrammer machine learning examples blob master ann class2 tf with save.py . Working on a fix for this in the meantime a workaround is to provide input shape to the first Layer or your Model Just a heads up. I was able to execute the jam3sn s code successfully in TF 1.10 but it failed in TF 1.11 It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue Yep this is still happening with TF 1.11 I could run the code successfully in tf nightly. Great yes sorry meant to update this the fix for this is in This is still broken in 1.12. it shows not implemented error Currently save requires model to be a graph network. Consider using save weights in order to save the weights of the model. in 1.12.0 version I am also having this problem on version 1.12.0 I am also getting the same problem in version 1.12.0 TypeError Traceback most recent call last ipython input 18 b9d746b1e27b in module 1 tf.keras.models.save model epic num reader.model TypeError save model missing 1 required positional argument filepath muhammadmw I found that my problem was caused by subclassing Keras.Model. A model created by subclassing Keras.Model cannot be serialized by using the save model function. I switched over to the Keras functional Api and I no longer have the problem. this is happening with tf nightly 2.0 preview 2.0.0.dev20190225 I am experiencing this in 2.2.4 tf So this issue is closed without solution Add save weights only True for ModelCheckpoint or use model.save weights muhammadmw I found that my problem was caused by subclassing Keras.Model. A model created by subclassing Keras.Model cannot be serialized by using the save model function. I switched over to the Keras functional Api and I no longer have the problem. Hey So can you please tell me how you fixed the problem as I am trying to save the weights and when I try to load the weights it gives error. Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first then load the weights. 
22850,Tensorflow 1.11 breaks simple keras estimator global step does not increment, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 running on colab.sandbox.google.com TensorFlow installed from source or binary binary TensorFlow version use command below 1.11 Python version 3.6 Describe the problem The below code illustrates a minimal keras model that is then converted to an estimator using tf.keras.estimator.model to estimator and and run using tf.estimator.train and evaluate . The code runs properly up until Tensorflow 1.10 however upon upgrading to Tensorflow 1.11 the global step fails to increment resulting in training never ending. You can reproduce by the running the below code on colab.sandbox.google.com observe it doesn t increment. Then downgrade to 1.10 and observe it works as expected. Source code logs ,The same error occurred on examples running within docker image tensorflow tensorflow 1.11.0 gpu py3 cc lenlen I thought this was fixed k w w is this the issue with cloning iterator that you fixed When did the fix go out This should be fixed in 1.11.0 rc1 colab runs at 1.11.0. karmel how long will it take for the tensorflow version to be updated Presumably it will happen with the 1.12 release which is coming out soon. I m building TensorFlow from source from r1.12 branch and can confirm this issue is fixed in 1.12.0 rc0. Nagging Assignees karmel ispirmustafa It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. Closing as this has been fixed. 
22865,ValueError Invalid tensors num detections detection classes detection scores detection boxes were found.,Please go to Stack Overflow for help and support https stackoverflow.com questions tagged tensorflow If you open a GitHub issue here is our policy 1. It must be a bug a feature request or a significant problem with documentation for small docs fixes please send a PR instead . 2. The form below must be filled out. 3. It shouldn t be a TensorBoard issue. Those go here https github.com tensorflow tensorboard issues . Here s why we have that policy TensorFlow developers respond to issues. We want to focus on work that benefits the whole community e.g. fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem rather than being redirected to Stack Overflow. System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary TensorFlow version use command below Python version Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Exact command to reproduce You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION Describe the problem Describe the problem clearly here. Be sure to convey here why it s a bug in TensorFlow or a feature request. Source code logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem. ,when i use the post training quantize return the error ValueError Invalid tensors num detections detection classes detection scores detection boxes were found here is the code import tensorflow as tf import pathlib archive dir pathlib.Path print str archive dir graph def file pathlib.Path archive dir .pb input arrays image tensor output arrays num detections detection classes detection scores detection boxes converter tf.contrib.lite.TocoConverter.from frozen graph str graph def file input arrays output arrays input shapes input 1 1 1 3 converter.post training quantize True mobilenet tflite file graph def file.parent .tflite mobilenet tflite file.write bytes converter.convert and i use the tensorboard display the graph of the model here is the link http dell sea 6006 graphs run . url Please use this bazel run tensorflow tools graph transforms summarize graph in graph YOUR GRAPH NAME.pb to get the names of your input and output nodes and accordingly move ahead. thanks so much i solved it . but i got the input shapes 3 . here are the result when i run the commond . bazel bin tensorflow tools graph transforms summarize graph in graph home caojinrong ssd mobilenet v2 coco 2018 03 29 ssd mobilenet v2 coco 2018 03 29 frozen inference graph.pb i got the return Found 1 possible inputs name image tensor type uint8 4 shape 3 No variables spotted. Found 4 possible outputs name num detections op Identity name detection classes op Identity name detection scores op Identity name detection boxes op Identity Found 16878731 16.88M const parameters 0 0 variable parameters and 1548 control edges Op types used 2572 Const 549 Gather 465 Identity 452 Minimum 371 Reshape 360 Maximum 344 Mul 267 Sub 261 Add 211 Cast 186 Greater 180 Where 180 Split 165 Slice 144 ConcatV2 127 StridedSlice 121 Pack 116 Shape 94 Unpack 92 ZerosLike 92 Squeeze 90 NonMaxSuppressionV2 64 Rsqrt 55 Conv2D 47 Relu6 45 ExpandDims 40 Fill 37 Tile 33 RealDiv 30 Range 29 Switch 26 Enter 21 DepthwiseConv2dNative 14 Merge 12 BiasAdd 11 TensorArrayV3 8 NextIteration 6 Exit 6 TensorArrayWriteV3 6 TensorArraySizeV3 6 TensorArrayGatherV3 6 Sqrt 5 TensorArrayReadV3 5 TensorArrayScatterV3 3 Equal 3 Transpose 3 Assert 3 Rank 2 Exp 2 Less 2 LoopCond 1 All 1 TopKV2 1 Size 1 Sigmoid 1 ResizeBilinear 1 Placeholder To use with tensorflow tools benchmark benchmark model try these arguments bazel run tensorflow tools benchmark benchmark model graph home caojinrong ssd mobilenet v2 coco 2018 03 29 ssd mobilenet v2 coco 2018 03 29 frozen inference graph.pb show flops input layer image tensor input layer type uint8 input layer shape 1 1 1 3 output layer num detections detection classes detection scores detection boxes so the input shape 3 but when use the post training quantize if the input shape write as the return the relevent code is converter tf.contrib.lite.TocoConverter.from frozen graph str graph def file input arrays output arrays input shapes image tensor 1 1 1 3 converter.post training quantize True it got the error Traceback most recent call last File quant.py line 9 in module str graph def file input arrays output arrays input shapes image tensor 1 1 1 3 File usr local lib python3.5 dist packages tensorflow contrib lite python lite.py line 274 in from frozen graph set tensor shapes input tensors input shapes File usr local lib python3.5 dist packages tensorflow contrib lite python convert saved model.py line 205 in set tensor shapes tensor.set shape shape File usr local lib python3.5 dist packages tensorflow python framework ops.py line 540 in set shape shape tensor shape.TensorShape shape File usr local lib python3.5 dist packages tensorflow python framework tensor shape.py line 542 in init self. dims as dimension d for d in dims iter File usr local lib python3.5 dist packages tensorflow python framework tensor shape.py line 542 in listcomp self. dims as dimension d for d in dims iter File usr local lib python3.5 dist packages tensorflow python framework tensor shape.py line 482 in as dimension return Dimension value File usr local lib python3.5 dist packages tensorflow python framework tensor shape.py line 42 in init raise ValueError Dimension d must be 0 self. value ValueError Dimension 1 must be 0 it got confused. aejaex For input shape you must use what is the shape of the array in your input images if you have used input image size 299 x 299 for example with 3 channels RGB then input shape is 1 299 299 3 . I don t know why you used 1 1 1 3 .. Check your config file for the training model what did you use for image size there That will be your input shape. cjr0106 aejaex is correct. You should use the shape of your input images. Let us know if this is still an issue. yeah i know that is the input image size i got the training model from https github.com tensorflow models blob master research object detection g3doc detection model zoo.md url then i use the bazel it got the image size as 1 1 1 3 as you say it s the config of the training model s problem There are numerous ways to get input image size which should be prior knowledge to model training. Nevertheless there is a potential bug in bazel. 
22936,Invalid loop strucure,I had converted a h5 file to pb file then while loading the pb file I get the following error. Traceback most recent call last File import model.py line 244 in module detections sess.run detectionsT feed dict img ph molded images img meta ph image metas File usr local lib python2.7 dist packages tensorflow python client session.py line 877 in run run metadata ptr File usr local lib python2.7 dist packages tensorflow python client session.py line 1100 in run feed dict tensor options run metadata File usr local lib python2.7 dist packages tensorflow python client session.py line 1272 in do run run metadata File usr local lib python2.7 dist packages tensorflow python client session.py line 1291 in do call raise type e node def op message tensorflow.python.framework.errors impl.InvalidArgumentError Invalid loop structure Loop mrcnn detection map while while context has more than one LoopCond node mrcnn detection map while LoopCond 1 and mrcnn detection map while LoopCond . This is an internal bug please file a bug report with instructions on how to reproduce the error. Have I written custom code No forked from https github.com GustavZ Mobile Mask RCNN OS Platform and Distribution linux ubuntu 16.04 TensorFlow installed from using pip Bazel version N A CUDA cuDNN version N A GPU model and memory N A Exact command to reproduce convert the h5 file to pb and then load the pb file Mobile device N A,Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case or leave them as N A Thanks. Have I written custom code OS Platform and Distribution TensorFlow installed from Bazel version CUDA cuDNN version GPU model and memory Exact command to reproduce Mobile device ankitesh97 Hi request you to provide a code snippet to reproduce the error and share the following information. Have I written custom code OS Platform and Distribution TensorFlow installed from Bazel version CUDA cuDNN version GPU model and memory Exact command to reproduce Mobile device Hey I have updated the details please check skye PTAL ankitesh97 can you provide the exact commands you run in the Mobile Mask RCNN repo to get this error ankitesh97 can you provide the exact commands you run in the Mobile Mask RCNN repo to get this error ankitesh97 Could you please provide the information asked above. It would help us to look into the issue. Thanks skye Just run this https github.com GustavZ Mobile Mask RCNN blob master notebooks export model.ipynb python notebook. Hi sorry for the delay. This looks like a control flow bug which we re currently reimplementing see https github.com tensorflow community blob master rfcs 20180507 cond v2.md and https github.com tensorflow community blob master rfcs 20180821 differentiable functional while.md for details . Would you mind running your code with the following environment variables set TF ENABLE WHILE V2 1 TF ENABLE COND V2 1 Please report any errors you get with these env variables set. Thanks ankitesh97 Have you tried with the above setting of environment variables Please keep us posted and we will reopen. 
23044,Failed to allocate tensors., Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 CentOS 7 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device iPhone 6s TensorFlow installed from source or binary source TensorFlow version use command below master Python version 2.7 Bazel version if compiling from source 0.15.0 GCC Compiler version if compiling from source None CUDA cuDNN version 7.0 GPU model and memory None Exact command to reproduce None I get this error tensorflow contrib lite kernels conv.cc 201 filter type data type 3 1 Node 1 failed to prepare. in this part of code For float model everything work correctly but for quant I get this issue. How could I Fix it , GlebGer Can you attach the model and the conversion command. GlebGer Can you attach the model and the conversion command. Thanks for your answer image https user images.githubusercontent.com 26429942 47206922 90091380 d392 11e8 9219 5ca28c2e8e63.png model.zip https github.com tensorflow tensorflow files 2495010 model.zip GlebGer I was able to run model without problems can you also paste the code before allocating tensors. Also trying running on latest TFLite if possible. Also perhaps related 22995. GlebGer I was able to run model without problems can you also paste the code before allocating tensors. Also trying running on latest TFLite if possible. Also perhaps related 22995. Thanks a lot for your help Did you run it in camera example Did you run it on iOS Can you please share me your code GlebGer Is it still happening can you checkout latest or new release branch and retry. Since 22995 had a fix submitted. Thanks Automatically closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks 
23145,Could not initialize a memory descriptor when using softmax layer,I have both CPU and GPU version installed by Miniconda each with a unique environment. While GPU version works fine the CPU version seems to throw an error when I try to add a softmax layer after a convolution layer. System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Manjaro 4.14.74 TensorFlow installed from source or binary binary from Miniconda TensorFlow version use command below 1.11.0 Python version Python 3.6.6 Anaconda Inc. CUDA cuDNN version CPU version no CUDA cuDNN Bazel version N A GPU model and memory N A Mobile device N A Exact command to reproduce python code.py Describe the current behavior Run the test code the program throws AbortedError info is Describe the expected behavior The program should finish with no error. Code to reproduce the issue Other info logs I set up the environment by Traceback is GPU version works fine. If i set axis to 0 1 or 2 the program finishes with no error but with it set to 1 or 3 the error occurs. If the softmax layer is added after a dense layer it also works fine. I ve also tested on another server with CentOS 7 and a Quadro P2000 the problem still occurs. GPU version works fine while CPU version not This code still not work ,Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case or leave them as N A Thanks. Bazel version GPU model and memory Exact command to reproduce Mobile device Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case or leave them as N A Thanks. Bazel version GPU model and memory Exact command to reproduce Mobile device Bazel version N A GPU model and memory N A Mobile device N A Exact command to reproduce python code.py Thanks. kawaiiQ Please refer to 17494. kawaiiQ Hi please feel free to close this issue if this no longer exists. If the problem still persists request you to submit all the information asked by the tensorflowbutler. Thank you I swiched to system python3 and it seems the problem has disappeared. However the problem still exists on Anaconda. And I think I have submitted all the required information. I can reproduce this on my system with Tensorflow 1.11.0 and Python 3.6.7 Anaconda Inc. However code.py can successfully run with Python 3.5.6 Anaconda Inc. kawaiiQ ikarth The conda binaries are not built by the TensorFlow maintainers but by the Anaconda folks. According to their recent blog post https www.anaconda.com blog developer blog tensorflow in anaconda those builds require Intel s MKL libraries. The error message suggests that there is something amiss with the MKL initialization. wei v wang tatianashp Any quick ideas here Thanks kawaiiQ for reporting and the reproducer and for trying TF w MKL DNN. Thanks asimshankar for including me. I could reproduce the error with build from src with TF v1.11 w MKL DNN so we take the responsibility to fix the bug. Please stay tuned. Thank you Eigen will exit silently i.e. no error . TF w MKL DNN errors out with the above error. As this issue has invited community support please remove the assignee. Otherwise remove the community support label. Thank you. wei v wang kawaiiQ were you able to solve this I m using python 3.6.5 and tensorflow 1.9 appyfizzA this bug seems to be fixed with latest TF. Can you please try TensorFlow MKL please help confirm double check on this. Thank you wei v wang appyfizzA I ve just tried it using python 3.6.8 and tensorflow 1.12.0 on CentOS Windows10 and WSL based on Debian all installed using the lastest Miniconda. It seems that Windows version works fine but Linux CentOS and Debian version still not work. Yes it s definitely broken from 1.6 to 1.12 we gave up building with MKL here BTW I uploaded on one of the MKL related bug a snippet producing the issue if I recall correctly eLvErDe sorry for the issues. Indeed 1.12 the issue is there. The good news is I have tested e.g. this commit id 07d5d08579bbbff910653a59163b4f8f180d16ac master branch tagged with v1.13.0 rc2 v1.13.0 rc1 v1.13.0 rc0 that the issue is gone. Can you please try one last time with this commit id with MKL Below is what I got 2019 02 25 00 20 09.309341 I tensorflow core platform cpu feature guard.cc 141 Your CPU supports instructions that this TensorFlow binary was not compiled to use AVX512F 2019 02 25 00 20 09.335924 I tensorflow core common runtime process util.cc 71 Creating new thread pool with default inter op setting 2. Tune using inter op parallelism threads for best performance. WARNING tensorflow From TF Public 07d5d08579bbbff910653a59163b4f8f180d16ac tensorflow python framework op def library.py 263 colocate with from tensorflow.python.framework.ops is deprecated and will be removed in a future version. Instructions for updating Colocations handled automatically by placer. i.e. No errors were thrown. eLvErDe sorry for the issues. Indeed 1.12 the issue is there. The good news is I have tested e.g. this commit id 07d5d08 https github.com tensorflow tensorflow commit 07d5d08579bbbff910653a59163b4f8f180d16ac master branch tagged with v1.13.0 rc2 v1.13.0 rc1 v1.13.0 rc0 that the issue is gone. Can you please try one last time with this commit id with MKL Below is what I got 2019 02 25 00 20 09.309341 I tensorflow core platform cpu feature guard.cc 141 Your CPU supports instructions that this TensorFlow binary was not compiled to use AVX512F 2019 02 25 00 20 09.335924 I tensorflow core common runtime process util.cc 71 Creating new thread pool with default inter op setting 2. Tune using inter op parallelism threads for best performance. WARNING tensorflow From TF Public 07d5d08579bbbff910653a59163b4f8f180d16ac tensorflow python framework op def library.py 263 colocate with from tensorflow.python.framework.ops is deprecated and will be removed in a future version. Instructions for updating Colocations handled automatically by placer. i.e. No errors were thrown. Sorry for my late reply. I ve tried the latest version and the problem is fixed. Thanks. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 23145 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 23145 No a Yes it s fixed with 1.13 but again I d like to get commit id fixing the issue it would make sense for us to backport the fix to 1.11.... Hi Adam eLvErDe sorry for the delay. But this PR fixed the issue https github.com tensorflow tensorflow pull 24057 The exact commit id is https github.com tensorflow tensorflow commit 15deae5c7957861f912b12cce47956c979f2c11c Please confirm if the above resolves your issue completely. Thanks 
23196,sess.run returns inconsistent results, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device No TensorFlow installed from source or binary source TensorFlow version use command below b v1.10.1 0 g4dcfddc 1.10.1 Python version Python 3.6.6 Anaconda Inc. Bazel version if compiling from source Build label 0.16.1 GCC Compiler version if compiling from source gcc Ubuntu 5.4.0 6ubuntu1 16.04.10 5.4.0 20160609 CUDA cuDNN version CUDA 8.0 CuDNN 7 GPU model and memory GeForce GTX 1080 Ti x 4 Describe the current behavior Consider following two source codes The only difference between them is stddev stddev and stddev stddev 0 But the first code with stddev stddev outputs and the second with stddev stddev 0 outputs It seems like that the correct output is stddev 0.8401555 But in the first code 1. stddev 1. 0.8401555 1.1902559 was calculated Here is the screenshot image https user images.githubusercontent.com 5780122 47388961 15e7de80 d746 11e8 8959 f37c46e66c8b.png Describe the expected behavior The outputs of two codes should be correct consistent at least Code to reproduce the issue Other info logs ,The bug can be reproduced with a shorter snippet The bug can be reproduced with a shorter snippet Thank you for simplifying my code to reproduce the same problem. According to my experiments the shorter bug reproducing code leads to correct 121.88354 11.040088 0.22156183 on tensorflow gpu v1.8.0 and v1.9.0 but prints incorrect 121.88354 0.09057899 0.22156183 on tensorflow gpu v1.10.0. ppwwyyxx shyoshyo csy530216 The issue has been solved in the latest version tb nightly 1.13.0a20181104 . Nagging Assignee harshini gadige It has been 15 days with no activity and this issue has an assignee. Please update the label and or status accordingly. 
23220,verbs local protection error when doing rdma send, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 14.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary source TensorFlow version use command below 1.9.0 Python version 2.7 Bazel version if compiling from source 0.16.1 GCC Compiler version if compiling from source 6.3 CUDA cuDNN version 9.0 GPU model and memory TITAN Xp. 12 GB You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION Describe the current behavior When verbs support is enabled getting Describe the expected behavior Should not produce this error. Code to reproduce the issue Need substantial infrastructure RoCE and code to reproduce. Other info logs The wr s verbs work request that trigger this error has lkey set to 0 and non zero buffer length indicating that it s trying to send unregistered memory region.,Currently a PR is in progress targeting this issue. guoshimin could you confirm if 24250 fix your issue If so could you close it I m not in a position to test it since we are still using 1.9. I ll close my PR for now. On Mon Jan 7 2019 5 24 AM Bairen Yi notifications github.com wrote guoshimin https github.com guoshimin could you confirm if 24250 https github.com tensorflow tensorflow pull 24250 fix your issue If so could you close it You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 23220 issuecomment 451932338 or mute the thread https github.com notifications unsubscribe auth ALPcHdW9iqH sEEOea5LZ95qHhtTFOzmks5vA0qkgaJpZM4X4nbz . 
23298,Student t distribution beta parameter in gamma,I think the beta parameter in gamma sampling for student t distribution should be 2 instead of 0.5 https github.com tensorflow tensorflow blob r1.11 tensorflow python ops distributions student t.py url gamma sample random ops.random gamma n 0.5 df beta 0.5 dtype self.dtype seed distribution util.gen new seed seed salt student t ,Hi Please check pull request 23301 missmagnum Hi a PR has been raised for this and please wait until it s merged. Nagging Assignee harshini gadige It has been 14 days with no activity and this issue has an assignee. Please update the label and or status accordingly. As commented by nickyungyung in the PR closing this issue. 
23299, tf.keras.layers.Bidirectional does not work under eager execution mode, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow NO OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac OS X 10.12.6 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device NO TensorFlow installed from source or binary BINARY TensorFlow version use command below v1.12.0 rc0 17 g7b08198113 1.12.0 rc1 Python version 3.6.5 Bazel version if compiling from source NA GCC Compiler version if compiling from source NA CUDA cuDNN version NA GPU model and memory NA Describe the current behavior A very simple model with Bidirectional layer will stop work after we enable the eager execution mode. Describe the expected behavior Code should work with eager execution mode. Code to reproduce the issue The following code will be able to reprorduce this problem However if we comment out the tf.enable eager execution then it will work as expected Other info logs The error message with eager execution enabled is After we disable the eager execution then the above code works as expected. , Update It s related with the following code https github.com tensorflow tensorflow blob 47c84f6053faebf0c9e3b5fb54da315e89ad37ec tensorflow python keras layers wrappers.py L443 L445 When in eager execution mode the child input shape is a list already. zixia alextp I submitted a PR https github.com tensorflow tensorflow pull 23441 to fix this issue. Could you help review it feihugis awesome it seems should work. LGTM and thank you very much Hi zixia I think the issue has been recently fixed on tensorflow side and I cannot reproduce the error with latest nightly build. Can u try pip install U tf nightly or pip install U tf nightly gpu to verify that the error has been fixed qlzh727 Thank you for pointing out that I can confirm that this issue had been fixed with the tensorflow tensorflow nightly devel gpu py3 docker image. I think we have another repository inside Google and the latest code is not synced with GitHub in time am I right If so I d like to suggest that we should keep them automatic in sync so that we can see the latest code. Have a nice day Hi huan the repository within google is fully synced with github. The issue here is that the fix is currently only available in the latest nightly and has not reach the release yet. We usually have monthly release cycle so the fix should reach stable release within a month. qlzh727 It s great that we had fully synced. Thanks for the clarification 
23334,Bug issue initializing iterator, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary Binary TensorFlow version use command below Version v1.11.0 0 gc19e29306c 1.11.0 Python version 3.5.2 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version N A GPU model and memory N A Exact command to reproduce See source code below Describe the problem There seems to be an issue initializing an iterator created with tf.data.Iterator.form structure a second time in the same session where the dataset contains Function instances. Source code logs To produce problem Executing the above leads to the following error ,It seems fine when I run your code on an environment that replicates yours. Can you try again and provide more information See updated version. The previous example worked for me when I used e.g. version 1.10. The updated example fails for me for both version 1.10 and 1.11. If it still works for you let me know what other information you think could be helpful. This does appear to be a bug in the latest version. One workaround is to ensure that all calls to make initializer have been made before creating the tf.Session . For example the following program should work as intended Update we have a fix for this case and it s currently pending review. Ok thanks 
23376,ParameterServerStrategy throws an exception when training locally with multiple GPUS, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 centos release 7 4.1708.el7.centos.x86 64 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device No TensorFlow installed from source or binary compiled form source with flags TensorFlow version use command below b v1.11.0 0 gc19e293 1.11.0 Python version Python 3.6.6 Bazel version if compiling from source 0.18.0 GCC Compiler version if compiling from source 4.8.5 20150623 CUDA cuDNN version 9.2 7 GPU model and memory Tesla V100 PCIE 16GB and Tesla V100 PCIE 16GB Describe the current behavior I m trying to run an official resnet model on cifar 10 dataset from https github.com tensorflow models tree master official resnet where I replaced MirroredStrategy by ParameterServerStrategy in https github.com tensorflow models blob master official utils misc distribution utils.py L48 as following It throws an exception Describe the expected behavior Replacing MirroredStrategy by ParameterServerStrategy launches a training with CPU acting as PS and 2 GPU workers. Code to reproduce the issue Run command ,Looks like there s an open PR that fix the problem https github.com tensorflow tensorflow pull 22713 Does https github.com tensorflow tensorflow commit 66dd1e21e7ab6e2aed8413880a7f2dd7f0a20e50 fix your issue Yes thank you. 
23397,parallel for No converter defined for MaxPoolGradGrad,I have written a fwd gradients based implementation of batch jacobian comparable to the tf.gradients based tensorflow.python.ops.parallel for.batch jacobian . To efficiently implement batch jacobian TensorFlow added support parallel for unfortunately pfor does not support the MaxPoolGradGrad op requiring a fallback to a slow while loop. For performance reasons it would be great to add a converter for MaxPoolGradGrad to parallel for . Code to reproduce the issue The following code is a minimal example to reproduce the issue and can be run as is. Describe the current behavior Describe the expected behavior produced by running the above script with op conversion fallback to while loop P.S. If you are interested I d be happy to contribute my fwd gradients based batch jacobian to TensorFlow once this performance issue with MaxPool ops is fixed and I ve cleaned up my code., agarwal ashish can you take a look A converter for MaxPoolGradGrad has been added and should make its way to a nightly build soon. However I suspect there might be a bunch of other ops that would turn up as not having converters given that you are relying on second order gradients. Thanks agarwal ashish for fixing this so fast Works 
23449,Bug in tensorflow lite java wrapper,In file https github.com tensorflow tensorflow blob master tensorflow lite java src main java org tensorflow lite NativeInterpreterWrapper.java line 286 Gets the number of output tensors. int getOutputTensorCount return inputTensors.length In this function output tensors.length should be used instead of inputTensors.length,Wow good catch that s a nasty bug. Will have a fix up shortly thanks for the feedback. 
23508,AttributeError Estimator object has no attribute distribution , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution Ubuntu 18.04 TensorFlow installed from source or binary binary TensorFlow version use command below 1.11 Python version 3.6 CUDA cuDNN version 9.2 7.2.1 GPU model and memory TitanXp 12Gb When I use an Estimator with a MirroredDistributionStrategy I encounter the error AttributeError Estimator object has no attribute distribution . This seems to be tied to this line https github.com tensorflow tensorflow blob a6d8ffae097d0132989ae4688d224121ec6d8f35 tensorflow python estimator estimator.py L1330 which has self. distribution.unwrap per device hook 0 . I m pretty sure this should be self. train distribution.unwrap per device hook 0 . The estimator is clearly never assigned a distribution attribute this is the only line on which its referenced so I assume this is just a typo. Let me know if I m missing something.,Same problem on a fresh install from source from the master branch. I made that change and it works perfectly. rmrao This should not be a problem with TF version 1.12. Have you tried with 1.12 yuefengz Any inputs on this harshini gadige This was broken when I did it on the master branch a few days ago which is v1.12. I haven t tried with 1.12 although the reference to distribution still exists in the source of 1.12 so not sure that would do anything. I can test later today This bug has been fixed in the master. 1.12 does have this bug. rmrao This issue is fixed and please wait for the next release. Thanks Are you satisfied with the resolution of your issue Yes https goo.gl forms Oe0tEvODFRoI2gJF3 No https goo.gl forms fUjzOfrtkFbrOT8d2 
23619,tf.contrib.distributions.percentile didn t support eager execution, em Please make sure that this is a feature request. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag feature template em System information TensorFlow version you are using tensorflow py2 cpu API r1.1 Are you willing to contribute it Yes No Yes Describe the feature and the current behavior state. When I using the eager execution. I try this will report an error TypeErrorTraceback most recent call last ipython input 1 b7b170c83880 in module 4 5 x tf.constant 1 2 3 4 dtype float64 6 y tf.contrib.distributions.percentile x q 30 usr local lib python2.7 dist packages tensorflow contrib distributions python ops sample stats.pyc in percentile x q axis interpolation keep dims validate args name 301 allowed interpolations interpolation 302 303 with ops.name scope name x q 304 x ops.convert to tensor x name x 305 Double is needed here and below else we get the wrong index if the array usr local lib python2.7 dist packages tensorflow python framework ops.pyc in enter self 5743 else 5744 cache key self. name self. old name self. default name 5745 if cache key in name scope cache 5746 self. ctx.scope name name scope cache cache key 5747 return self. ctx.scope name TypeError unhashable type list but when I use tf.Session to run the same code block It worked and report that I wanted 2.0 Will this change the current api How Who will benefit with this feature Any Other info. I m using tensorflow to make some calculate. Eager is very help but this problem stop me to use it. Thanks for anyone will fix it.,Added PR 23637 for the fix. The issue has been fixed in https github.com tensorflow tensorflow pull 23637 issuecomment 449498024 Will close the issue now. 
23702,Weird Bug in Tf.keras.Model.Predict x tf.Dataset iterator , em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary pip TensorFlow version use command below 1.9.0 and 1.12.0 I am using 1.9.0 but the bug is present in 1.12.0 also Python version 3.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory M60 16 GB two 8GB GPUs Describe the current behavior when using tf.data.Dataset.Iterator in tf.keras.Model.predict x tf.data.Dataset.Iterator steps . I am getting a weird value error Please provide data as a list or tuple of 2 elements input and target pair. Received Tensor IteratorGetNext 0 dtype int64 The above error is misleading. why does it need X Y for prediction My Testing tf.Dataset iterator obviously does not give a X Y tuple. It gives only X in batches. When I give a numpy array of X as input it works as intended. If I use the dataset iterator with eager execution enabled I get this error my batch size is 2 Please provide data as a list or tuple of 2 elements input and target pair. Received tf.Tensor 68 5 521 ... 0 0 0 6705 1235757 2411 ... 2804 147 13 shape 2 5000 dtype int64 . We do not use the target value here. Which makes it clear that when eager execution is enabled Y is not used. Moreover why does tf.Dataset iterator need to output a tuple of X Y when using tf.keras.Model.predict Is this the expected behaviour NOTE My model is a single input model not a multi input model EDIT I worked around the error by providing X Y But the keras progress bar doesn t seem to work with it., wt huang were you able to reproduce this issue Or am I missing something Abhijit 2592 You should be able to feed inputs to model.predict method without getting any errors. Make sure your model is instantiated properly and invoke model.fit to feed inputs and outputs. You can find more details on Keras Documentation https keras.io . You can also post your code snippet here. Ok I m lost about all of this I been reading a lot of this and to no end I still don t know what or who it s for I hut like to type but now I want to know what I need to do cab I make money dy doing this On Fri Nov 16 2018 9 04 PM wt huang notifications github.com wrote Abhijit 2592 https github.com Abhijit 2592 You should be able to feed inputs to model.predict method without getting any errors. Make sure your model is instantiated properly and invoke model.fit to feed inputs and outputs. You can find more details on Keras Documentation https keras.io . You can also post your code snippet here. You are receiving this because you are subscribed to this thread. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 23702 issuecomment 439582666 or mute the thread https github.com notifications unsubscribe auth ArBbKtbA0bZaTW50ILg9ZGiG3 x60m6ks5uv3zZgaJpZM4Ya 7j . wt huang I am able to use model.fit there isn t any issue there. But the problem is model.predict requires X Y When I am passing a tf.dataset.iterator But if I pass a numpy it takes only X and works as intended. wt huang I am able to use model.fit there isn t any issue there. But the problem is model.predict requires X Y When I am passing a tf.dataset.iterator But if I pass a numpy it takes only X and works as intended. What if numpy X is too large to fit in memory when I call mode.predict I am trying to use tf.dataset to batch it. BUt it requires X Y ... XYudong exactly the same issue Abhijit 2592 and XYudong Is this still an issue Could you try loading newer TF version and check it Please let me know the progress. Thanks jvishnuvardhan I checked on Tensorflow 1.12.0 pip released on 6th Nov 2018 and the bug still exists. You want me to check on the 1.13.0rc1 version which was released on 8th Feb 2019 Abhijit 2592 Yes. It would be great if you can check with the 1.13.0rc1 version. Thanks Abhijit 2592 If this still fails with 1.13.0rc1 please share a code snippet that reproduces the problem so that we can investigate further. mrry and jvishnuvardhan. Thanks This appears to be fixed in tf nightly 1.13.0 dev20190213 The following code snippet threw the error I mentioned when running with 1.12.0 stable version This runs as intended in nightly version for both normal and eager mode . One more thing which I really liked was It doesn t look like I need to create a one shot iterator or initializable iterator . I can directly pass the dataset object to model.predict which feels more natural 1 . Before while using tf.data with Embedding layers I had to write a few boilerplate code to initialize lookup tables My question is do I need to run this boilerplate code in upcoming versions of tensorflow It will be really great if it is handled internally. i guess i am encountering this error in tensorflow 1.14.0 the model.fit is finished successfully with tf.data.Dataset input but the model.predict keep giving error when i construct the dataset with X only looks it is expecting Y also I am having the same issue with tensorflow version 1.12.0. Any solution lnshi samra irshad I don t see any issues with TF1.14.0 which is also a stable version or you could try with tf nightly also. With recent versions you need to pass dataset and not dataset iterator . Please check the code below. Please let us know whether it was resolved for you. Thanks lnshi samra irshad I don t see any issues with TF1.14.0 which is also a stable version or you could try with tf nightly also. With recent versions you need to pass dataset and not dataset iterator . Please check the code below. Please let us know whether it was resolved for you. Thanks jvishnuvardhan I was able to resolve this issue. I have tensorflow version 1.12.0. The predict function demands target labels in tensorflow version 1.12.0 where logically it should not. Anyhow I provided the targets since it does not use them so I guess it should not matter whether I provide it the labels or not samra irshad Got it. There were lot of modifications and improvements between TF1.12.0 and current version. I would suggest you to upgrade to TF1.14.0 which is also a stable version. If you are familiar with TF2.0 then it is better to upgrade to TF2.0. Thanks. Automatically closing this out since I understand it to be resolved in TF1.14.0 but please let me know if I m mistaken.Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 23702 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 23702 No a 
23748,tensorflow.keras Dense layers complain if the input is a sparse Input layer., System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes. OS Platform and Distribution e.g. Linux Ubuntu 16.04 OSX Mojave 10.14.1 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device na TensorFlow installed from source or binary binary TensorFlow version use command below 1.10.0 v1.10.0 rc1 19 g656e7a2b34 Python version 3.5 Bazel version if compiling from source NA GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION Describe the current behavior See below which I ran on a OSX Mojave Macbook Pro Early 2015 ipython running python 3.5 tensorflow 1.10.0 Describe the expected behavior If I were using normal Keras I d expect no errors trying to do the above and for the model to compile subsequently without issue. Code to reproduce the issue See the code in my snippet above. Other info logs , omalleyt12 I recall you worked on something similar recently can you comment on what is expected here Note that in 1.12 the above gives a different error karmel omalleyt12 I looked into this issue last week. There are several things that need to be fixed. One is that sparse.placeholder could not recognize None 4 . It will always converts to None None so that the error of I created a PR 24048 to fix the above shape inforamtion issue first. There might be some other places that need to be fixed to make JamesGlooTeam example work though. Note as pointed out by omalleyt12 the error is based in 1.12 and is different from the original error posted by JamesGlooTeam This is because for SparseTensors the shape is actually a Tensor. Tensors can t have None values. You can get around this on the Input layer by defining your batch size However Dense layers and most layers in general it seems don t support sparse inputs so you would need to subclass Layer in order to call tf.sparse.sparse dense matmul on your inputs or create a Lambda layer to convert your sparse inputs to dense. Ideally the shape of a SparseTensor would probably be a TensorShape but I think that would be a pretty substantial rewrite of a lot of the sparse ops in order to allow None values to flow through omalleyt12 The PR 24048 propose to convert None 4 to 1 4 for the shape tensor then the information could be preserved without making drastic changes. Hi I got the same issue if the input is sparse then even when I apply the Dense layer I got an error. That s too bad because to train a neural network specifying sparse True in the input layer makes the learning phase 5 times faster ... Anyone has a solution for that Thanks a lot. omalleyt12 I recall you worked on something similar recently can you comment on what is expected here Note that in 1.12 the above gives a different error i got the same error in tf1.13. Anyones has a quick local fix solution Thanks a lot SparseTensors aren t supported in Dense layer currently although this is something we are considering. You can implement a custom layer that calls tf.sparse.sparse dense matmul on your SparseTensor omalleyt12 Is there some progress on this issue I d like to build a large sparse logistic regression model with Keras and having a dense layer supporting sparse input in Keras would be quite cool. Should I wait for such a feature landing in Keras or should I implement my own layer Is there already an existing snippet with such a layer somewhere Maybe the medium article I wrote on ingesting sparse inputs in Tensorflow Keras can help you out https medium.com dailymotion how to design deep learning models with sparse inputs in tensorflow keras fd5e754abec1 SharoneDayan Maybe the medium article I wrote on ingesting sparse inputs in Tensorflow Keras can help you out https medium.com dailymotion how to design deep learning models with sparse inputs in tensorflow keras fd5e754abec1 On TensorFlow 2.0.0 rc0 I get ValueError The two structures don t have the same nested structure. trying your DenseLayerForSparse layer. Using sparse inputs as to regular Dense gives the ValueError The last dimension of the inputs to Dense should be defined. Found None . There doesn t seem to be any simple fix to this that doesn t involve in depth understanding of TF2 internals.. Fixing the batch size to all inputs as omalleyt12 suggests gives the same two structures don t have the same nested structure error and it seems that SparseTensorSpec gets the training dataset size as its first axis whereas the SparseTensor structure gets the batch size as its first axis. Same when trying the DenseLayerSparse workaround. Sparse inputs have worked out of the box for Keras so far as in every other DL backend. These are a common use case with document and graph data so it s strange that this feature is not working so close to TF2 full release especially since SparseTensors are one of the advertised new features. anttttti Possibly related https github.com tensorflow tensorflow issues 31999 Can confirm that updating to 1.14 breaks most code dealing with sparse data which is especially relevant for graphs and text as anttttti mentioned. This does not seem to be an issue that can be easily worked around and the only stable solution is to use Keras 2.2.5 not 2.3 because that s broken too since it was made to be similar to tf.keras . This seems like a huge step back and effectively makes it impossible to use sparse tensors in a Keras model. For tensorflow 2.0 not rc x Input shape 32 sparse True y Dense 1 activation sigmoid x model Model x y I am still getting ValueError The last dimension of the inputs to Dense should be defined. Found None . Anyone know how to resolve Hi kechan did you figure out the solution I m running into the same issue too. quangkevin No. I haven t looked into it again. I thought i just use Masking for my particular model and move on. But try install tf.nightly and see if the issue is still there. it appears this bug has been opened for a long while. This is because for SparseTensors the shape is actually a Tensor. Tensors can t have None values. You can get around this on the Input layer by defining your batch size However Dense layers and most layers in general it seems don t support sparse inputs so you would need to subclass Layer in order to call tf.sparse.sparse dense matmul on your inputs or create a Lambda layer to convert your sparse inputs to dense. Hi omalleyt12 I have tried using the tf.sparse.sparse dense matmul function to convert my sparse tensor into a dense one. But it still cannot fit into a dense layer for p dim in zip adjacency powers dim per power net p adj times x sparse adjacency x p with tf.variable scope r i l i p s replica layer id str p layer tf.layers.Dense dim kernel regularizer kernel regularizer activation None use bias False net p layer.apply net p def adj times x adj x adj pow 1 Multiplies adj adj pow x. for i in range adj pow x tf.sparse tensor dense matmul adj x return x and the error is still ValueError The last dimension of the inputs to Dense should be defined. Found None . Any suggestions regarding this Thanks This is not fixed in 2.1.0 This is a major blocker for the Spektral https danielegrattarola.github.io spektral library. Can we get some attention on this issue cgarciae can you file a new bug with a minimal repro in 2.x This bug is very old and it s hard to tell exactly what condition set doesn t work here. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 23748 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 23748 No a Hi karmel I just submitted the fix for this. It should be synced out shortly. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 23748 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 23748 No a 
23767,Tensorflow calculates incorrect loss for tf.keras models when using Weights., em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code No OS Platform and Distribution e.g. MacOS Google Cloud VMs TensorFlow installed from source TensorFlow version 1.10 and 1.12 Python version 3.5 Describe the current behavior The loss calculation is not correct when working with tf.keras. . After building the model tf.keras.fit generator https www.tensorflow.org api docs python tf keras models Model should accept inputs targets sample weights as inputs. However if I multiply the sample weights by 10000 the loss doesn t change. The bug seems to appear from 1.10 version of Tensorflow onwards e.g. 1.11 1.12 Describe the expected behavior If I increase the weighting by a certain factor the overall loss of the model should increase by the same factor. Given the model is doing random guessing. Code to reproduce the issue In the above code you simply need to change the WEIGHT VARIABLE 1 From 1 to 100000 and rerun the file. Other info logs v1.10 WEIGHT VARIABLE 1 10 10 1s 128ms step loss 0.7407 binary crossentropy 0.7407 binary accuracy 0.5031 Epoch 2 5 10 10 0s 4ms step loss 0.7043 binary crossentropy 0.7043 binary accuracy 0.5125 Epoch 3 5 10 10 0s 4ms step loss 0.7055 binary crossentropy 0.7055 binary accuracy 0.5219 Epoch 4 5 10 10 0s 4ms step loss 0.7002 binary crossentropy 0.7002 binary accuracy 0.5250 Epoch 5 5 10 10 0s 4ms step loss 0.6944 binary crossentropy 0.6944 binary accuracy 0.5375 WEIGHT VARIABLE 10000 10 10 1s 131ms step loss 7235.5976 binary crossentropy 0.7236 binary accuracy 0.4562 Epoch 2 5 10 10 0s 4ms step loss 7271.9184 binary crossentropy 0.7272 binary accuracy 0.4844 Epoch 3 5 10 10 0s 4ms step loss 7276.9147 binary crossentropy 0.7277 binary accuracy 0.4500 Epoch 4 5 10 10 0s 4ms step loss 7052.0121 binary crossentropy 0.7052 binary accuracy 0.4625 Epoch 5 5 10 10 0s 4ms step loss 7187.0285 binary crossentropy 0.7187 binary accuracy 0.4969 v1.12 WEIGHT VARIABLE 1 10 10 1s 68ms step loss 0.7188 binary crossentropy 0.7188 binary accuracy 0.5312 Epoch 2 5 10 10 0s 4ms step loss 0.7044 binary crossentropy 0.7044 binary accuracy 0.4969 Epoch 3 5 10 10 0s 4ms step loss 0.7086 binary crossentropy 0.7086 binary accuracy 0.4844 Epoch 4 5 10 10 0s 4ms step loss 0.7075 binary crossentropy 0.7075 binary accuracy 0.4500 Epoch 5 5 10 10 0s 4ms step loss 0.6950 binary crossentropy 0.6950 binary accuracy 0.5187 WEIGHT VARIABLE 10000 10 10 1s 74ms step loss 0.9084 binary crossentropy 0.9084 binary accuracy 0.4719 Epoch 2 5 10 10 0s 4ms step loss 0.7120 binary crossentropy 0.7120 binary accuracy 0.5062 Epoch 3 5 10 10 0s 4ms step loss 0.7024 binary crossentropy 0.7024 binary accuracy 0.5344 Epoch 4 5 10 10 0s 4ms step loss 0.7257 binary crossentropy 0.7257 binary accuracy 0.4500 Epoch 5 5 10 10 0s 4ms step loss 0.7013 binary crossentropy 0.7013 binary accuracy 0.4844,I can also confirm this issue looks like it was introduced in Tensorflow 1.11.0 The same issue effects class weights too Confirmed that we ran into the same issue. Same issue here Can we get an acknowledgement of this issue I know fchollet is very busy maybe pavithrasv could give it a quick look I do not think it has been fixed. I can still see it happens in 1.14 and even tf2.0 beta. I think it was fixed in 1.13 but re introduced afterwards. This was resolved already. I cannot reproduce the issue when I use tf nightly. Please check the gist with TF1.14.0 https colab.sandbox.google.com gist jvishnuvardhan 2d385f53d15b4e024d5d56a02892cab6 tf 23767 keras weights.ipynb and gist with tf nightly https colab.sandbox.google.com gist jvishnuvardhan 75d60f494371d04184a98ab217c07f69 tfnightly 22207 keras datasets.ipynb . Thanks output with WEIGHT VARIABLE 1 output with WEIGHT VARIABLE 1000 I am closing this issue as it was resolved. Please feel free to open it if the issue persists again. Thanks 
23789,Dataset.map with random shuffle and num parallel calls 1 has non deterministic result, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Fedora 22 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary pip TensorFlow version use command below 1.12 Python version 3.6.5 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version N A GPU model and memory N A Describe the current behavior tf.data.Dataset.range 100 .batch 2 .map lambda x tf.random shuffle x num parallel calls 1 is non deterministic. Different runs of the test program produce different output. It s as if the random shuffle is ignoring the random seed. Although adding a seed argument to random shuffle makes the problem go away random shuffle should still use the graph level random seed when the seed argument is unspecified. Describe the expected behavior Two different runs should always have the same output. Code to reproduce the issue Other info logs ,It seems that both I and the OP are experiencing unexpected nondeterminism from tf.data.Dataset.map although my usage scenario is slightly different. I m using tf.data.Dataset.map to sample from tf.random.uniform . According to 13932 random ops that are nested in map should be deterministic as long as a seed has been set and num parallel calls 1 . However this is not the case for me. Output My details System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device NA TensorFlow installed from source or binary source TensorFlow version use command below 1.12 Python version 3.6 Bazel version if compiling from source 0.19.1 GCC Compiler version if compiling from source 7.3.0 CUDA cuDNN version 10 7 GPU model and memory Tesla K80 12GB jchia I suggest the title be modified to Dataset.map with random ops are non deterministic with num parallel calls 1 and user defined seed shivaniag Assigning this to you because I think the fix here will depend on switching to the new Python function implementation and making sure that it respects seeds from the outer context. We should definitely add a test to confirm this. This will mostly get fixed with switching to the new python function implementation which we are expecting to be in by end of this month. Thank you for the issue. feihugis I will keep this PR on hold for now since the issue will get fixed with the changes I mentioned above. Thank you for the PR though. shivaniag Thanks for your updates. Would you mind to share what is the new python function implementation about So that I could avoid the contribution conflicts. shivaniag Is the new python function implementation in yet Will it make it into 1.13 jchia The new python function implementation is in and this issue has been fixed with that. It will not make it into 1.13 but it is available with nightly build. shivaniag do you know which nightly build it was fixed in having trouble running the most recent nightly build so would like to try an exact one for this fix 
23874,warning using ptxas 10.0.145 which is older than 9.2.88 , WARNING You are using ptxas 10.0.145 which is older than 9.2.88 https storage.googleapis.com tf performance tf binary tensorflow 1.12.0.a6d8ffa.AVX2.CUDA10 cp27 cp27mu linux x86 64.whl https github.com tensorflow tensorflow blob f827bad24efc949e657e141464ea1010c18d812c tensorflow compiler xla service gpu nvptx compiler.cc L427 ,It looks like you haven t used a template to create this issue. Please resubmit your issue using a template from here https github.com tensorflow tensorflow issues new choose . We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation. This is a bug and has been fixed in https github.com tensorflow tensorflow commit 83ff640fa5026b8bd3cb9c2ceff9e99e8e03823a diff 888940c777f83519bd1e27349427d51e . Closing since it has been resolved. Feel free to reopen the issue if the problem still persists. Thanks This is a bug and has been fixed in 83ff640 diff 888940c777f83519bd1e27349427d51e https github.com tensorflow tensorflow commit 83ff640fa5026b8bd3cb9c2ceff9e99e8e03823a diff 888940c777f83519bd1e27349427d51e . 
23878,Bug instantiating dynamic rnn with tf.int32 in input and state raises TypeError, System information OS Platform OS X 10.13.3 Custom code tensorflow version 1.12.0 python version 3.6.5 Describe the current behavior Tensorflow raises a TypeError when creating a dynamic rnn with tf.int32 type in its input and state. When changing the type to tf.float32 the error is not raised. Describe the expected behavior Ideally a dynamic rnn should support tf.in32 types. If there s any reason why instantiating a dynamic rnn with tf.int32 type in its input and state should not be allowed a custom error should be raised. Code to reproduce the issue The code below reproduces the error The code below doesn t Note the change in dtype. Other info logs TRACEBACK TypeError Traceback most recent call last ipython input 3 1d83a30f7748 in module 2 X tf.placeholder tf.int32 None 10 1 3 cell tf.nn.rnn cell.LSTMCell 1 dtype tf.int32 4 output state tf.nn.dynamic rnn cell cell inputs X dtype tf.int32 initial state state 5 jonassucks3 lib python3.6 site packages tensorflow python ops rnn.py in dynamic rnn cell inputs sequence length initial state dtype parallel iterations swap memory time major scope 662 swap memory swap memory 663 sequence length sequence length 664 dtype dtype 665 666 Outputs of dynamic rnn loop are always shaped time batch depth . jonassucks3 lib python3.6 site packages tensorflow python ops rnn.py in dynamic rnn loop cell inputs initial state parallel iterations swap memory sequence length dtype 870 parallel iterations parallel iterations 871 maximum iterations time steps 872 swap memory swap memory 873 874 Unpack final output if not using output tuples. jonassucks3 lib python3.6 site packages tensorflow python ops control flow ops.py in while loop cond body loop vars shape invariants parallel iterations back prop swap memory name maximum iterations return same structure 3289 ops.add to collection ops.GraphKeys.WHILE CONTEXT loop context 3290 result loop context.BuildLoop cond body loop vars shape invariants 3291 return same structure 3292 if maximum iterations is not None 3293 return result 1 jonassucks3 lib python3.6 site packages tensorflow python ops control flow ops.py in BuildLoop self pred body loop vars shape invariants return same structure 3002 with ops.get default graph . mutation lock pylint disable protected access 3003 original body result exit vars self. BuildLoop 3004 pred body original loop vars loop vars shape invariants 3005 finally 3006 self.Exit jonassucks3 lib python3.6 site packages tensorflow python ops control flow ops.py in BuildLoop self pred body original loop vars loop vars shape invariants 2937 flat sequence vars for body with tensor arrays 2938 pre summaries ops.get collection ops.GraphKeys. SUMMARY COLLECTION pylint disable protected access 2939 body result body packed vars for body 2940 post summaries ops.get collection ops.GraphKeys. SUMMARY COLLECTION pylint disable protected access 2941 if not nest.is sequence body result jonassucks3 lib python3.6 site packages tensorflow python ops control flow ops.py in lambda i lv 3258 cond lambda i lv pylint disable g long lambda 3259 math ops.logical and i maximum iterations orig cond lv 3260 body lambda i lv i 1 orig body lv 3261 3262 if context.executing eagerly jonassucks3 lib python3.6 site packages tensorflow python ops rnn.py in time step time output ta t state 838 skip conditionals True 839 else 840 output new state call cell 841 842 Keras cells always wrap state as list even if it s a single tensor. jonassucks3 lib python3.6 site packages tensorflow python ops rnn.py in lambda 824 if is keras rnn cell and not nest.is sequence state 825 state state 826 call cell lambda cell input t state 827 828 if sequence length is not None jonassucks3 lib python3.6 site packages tensorflow python ops rnn cell impl.py in call self inputs state scope args kwargs 368 method. See the class docstring for more details. 369 return base layer.Layer. call self inputs state scope scope 370 args kwargs 371 372 jonassucks3 lib python3.6 site packages tensorflow python layers base.py in call self inputs args kwargs 372 373 Actually call layer 374 outputs super Layer self . call inputs args kwargs 375 376 if not context.executing eagerly jonassucks3 lib python3.6 site packages tensorflow python keras engine base layer.py in call self inputs args kwargs 755 if not in deferred mode 756 self. in call True 757 outputs self.call inputs args kwargs 758 self. in call False 759 if outputs is None jonassucks3 lib python3.6 site packages tensorflow python ops rnn cell impl.py in call self inputs state 1003 sigmoid i self. w i diag c prev self. activation j 1004 else 1005 c sigmoid f self. forget bias c prev sigmoid i 1006 self. activation j 1007 TypeError unsupported operand type s for Tensor and float ,Thanks for the clear repro example. Probably this is a duplicate of 14729 https github.com tensorflow tensorflow issues 14729 Thanks for the report I will take a look today or tomorrow. Sorry for the very late reply. After some digging the root cause is because of the default forget gate bias being initialized as float. However even casting it to be int32 will still causing the code to fail down the road since the activation function for LSTM tanh and sigmoid only support floating numbers. So the conclusion is that LSTM will only support floating numbers as the dtype for input and states. I will update the code to have a clear error message when the input dtype is not float number. 
23933,38c9b12 stucks gdr in ResNet50 v1.5 official benchmark,With 38c9b12464b23aac132fbc8005cb74de86eee241 grpc gdr server protocol gets stuck in ResNet50 v1.5 official benchmark. This issue is a marker to other users of GDR. Hopefully I could find some time to fix it next week.,
23946, BUG accumulate n fatal in InteractiveSession , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary Anaconda TensorFlow version use command below 1.12.0 Python version 3.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 9.2 7.2.1 GPU model and memory GTX 1050 mobile 4GB Describe the current behavior Failure when using accumulate n add n works with tensors also with tf.constant array here with an InteractiveSession works for normal session . Describe the expected behavior Same result as add n and no error Code to reproduce the issue Other info logs Error message 2018 11 24 11 11 27.877871 F tensorflow compiler jit deadness analysis.cc 639 Check failed it predicate map .end AccumulateNV2 Internal 3 ,Hello josh11b user points out that accumulate n op does not work in interactive session mode but add n op works. Can you please advise. Thanks. I m not familiar with the specifics of that code. Interactive session should not be significantly different from regular session. alextp likely knows more. sanjoy do you know why this jit pass is failing here Usually this means the graph has an illegal cycle. I ve since changes this piece of code to print a friendlier error message https github.com tensorflow tensorflow blob 1ee193a2563d51ee45b401f2cff91f6e480e21db tensorflow compiler jit deadness analysis.cc L639 Maybe you could try running on HEAD to see if that helps I ve run it again using the most recent master branch CPU only default compilation flags XLA on others off . The problem persists the message changed though. This is the error message For completeness the stacktrace Should be fixed by https github.com tensorflow tensorflow commit 0b3c3c55e177b35d38ba33170ebe2baa3f5badff Please re open if you this is still broken. Thank you sanjoy. 
23979,Save Load problem with keras.layers.ReLU,Running this example returns trackback 232 233 return np.asarray x dtype FLOATX 234 235 data virtualenv venv2 lib python2.7 site packages numpy core numeric.pyc in asarray a dtype order 499 500 501 return array a dtype copy False order order 502 503 TypeError float argument must be a string or a number def get config self config max value self.max value negative slope self.negative slope threshold self.threshold base config super ReLU self .get config return dict list base config.items list config.items def get config self config max value self.max value negative slope float self.negative slope threshold float self.threshold base config super ReLU self .get config return dict list base config.items list config.items However the problem might be somewhere in deserialize layer as for a some reason negative slope is a dictionary not a numpy.ndarray , cataclysmus I am able to reproduce with tf 1.12.0 but not with tf nightly. Can you try with tf nightly and see if the issue still exist Thanks yongtang for taking a look. Previously I failed to check with nightly version. However I checked with TF nightly version 1.13.0 dev20181108 now and the issue seems to be fixed. Will wait for cataclysmus confirmation. yongtang ymodak Checked. No bug at the build 1.13.0 dev20181128 Are you satisfied with the resolution of your issue Yes https goo.gl forms Oe0tEvODFRoI2gJF3 No https goo.gl forms fUjzOfrtkFbrOT8d2 
23987,LARSOptimizer does not initialize learning rate tensor and momentum tensor, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution Linux Ubuntu 16.04 TensorFlow installed from binary TensorFlow version tags 1.12 Python version 2.7 cpu mode Describe the current behavior LARSOptimizer object has no attribute learning rate tensor Code to reproduce the issue tensorflow tensorflow contrib opt python training lars optimizer test.py ,class LARSOptimizer should implement prepare method. MR https github.com tensorflow tensorflow pull 23989 commits 6d3d0de39f3eaecdbf7fbdb52a7a02b2539efeeb 
24465,Unexpected result for tf.sqrt under certain conditions, System information Have I written custom code Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 macOS Mojave 10.14.1 TensorFlow installed from source or binary Binary TensorFlow version use command below 1.12.0 Python version 3.6 Describe the current behavior The output a sqrt 1 of tf.sqrt https www.tensorflow.org api docs python tf math sqrt in the provided snippet is incorrect. Concretely when a is a tf.Variable https www.tensorflow.org api docs python tf Variable and a sqrt is run together with a scaled the output a sqrt 1 is a sqrt 2 1 . Describe the expected behavior The expected result is a sqrt 2 . Code to reproduce the issue , ezhulenev this looks like another case of fetches not being preserved by grappler. Can you take a look Tested with nightly tensorflow the code produces the correct output. Similar to 23196. 
24517,BUG code throws exceptions when using COND V2, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 archlinux Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device n a TensorFlow installed from source or binary binary TensorFlow version use command below v1.12.0 rc2 0 g748435b8ef 1.12.0 rc2 Python version 3.6 Bazel version if compiling from source n a GCC Compiler version if compiling from source n a CUDA cuDNN version n a GPU model and memory n a Code to reproduce the issue Other info logs It fails with the following message. However the code works well without cond v2. cc skye who seems to work on cond v2 in https github.com tensorflow tensorflow issues 15874 issuecomment 436833266.,I was able to run your code snippet successfully using TF 1.12. Can you try using https colab.sandbox.google.com notebooks welcome.ipynb to execute your code and confirm 1226 11 07 10 https user images.githubusercontent.com 1381301 50454757 6cb49480 08fe 11e9 96f2 6c34d0542594.png You may be executing my code in the wrong way e.g. import tensorflow before setting envvar . Yes you are right. Hi thanks for reporting this Just to let you know I m working on some other control flow bugs at the moment so I haven t had a chance to take a look at this yet but this is on my radar. I am unable to repro this at head so it s possible this has been fixed since. Please retry and reopen if this is still an issue. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 24517 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 24517 No a 
24524,AttributeError module tensorflow.compat has no attribute v1 ,I ran cnn mnist.py on my machine but got AttributeError module tensorflow.compat has no attribute v1 . can anyone solve this problem ,You can run it by changing train input fn tf.compat.v1.estimator.inputs.numpy input fn to train input fn tf.estimator.inputs.numpy input fn and eval input fn tf.compat.v1.estimator.inputs.numpy input fn to eval input fn tf.estimator.inputs.numpy input fn basically the reverse than what was done in the last commit https github.com tensorflow tensorflow commit 95e808ba44075dfe0b7db57bb49d2e64a1977a95 Closing this issue since workaround has been provided and PR https github.com tensorflow tensorflow pull 24552 is in process. Thanks I tried to upgrade tf to 1.13 and problems solved. I met a similar bug that is AttributeError module tensorflow. api.v1.compat has no attribute v1 when I run inference.py of FPN tensorflow https github.com DetectionTeamUCAS FPN Tensorflow the source code is summary image v1 tf.compat.v1.summary.image . Then I change the code to summary image v1 tf.summary.image the program can run correctly. because the tensorflow plot version is higher. so pip install tensorflow plot 0.3.0 will ok Hello I run into the same Problem. I have successfully installed tensorflow gpu 1.12 on Ubuntu 18.04 with CUDA 9.0 and cudnn 7.1.2 in an anaconda environment I followed this tutorial https medium.com redowan no bullshit guide on installing tensorflow gpu ubuntu 18 04 18 10 238924cc4a6a . Then i followed this tutorial for installing object detection models https medium.com teyou21 setup tensorflow for object detection on ubuntu 16 04 e2485b52e32a But when i try to python object detection builders model builder test.py i get following error Traceback most recent call last File object detection builders model builder test.py line 23 in module from object detection.builders import model builder File home schaupp models research object detection builders model builder.py line 35 in module from object detection.models import faster rcnn inception resnet v2 feature extractor as frcnn inc res File home schaupp models research object detection models faster rcnn inception resnet v2 feature extractor.py line 30 in module from nets import inception resnet v2 File home schaupp models research slim nets inception resnet v2.py line 375 in module batch norm updates collections tf.compat.v1.GraphKeys.UPDATE OPS AttributeError module tensorflow. api.v1.compat has no attribute v1 I installed exact the same ah few month ago on the same machine but this time it won t work. I hope someone can help me i struggled many hours with this problem. I met a similar bug that is AttributeError module tensorflow. api.v1.compat has no attribute v1 when I run inference.py of FPN tensorflow https github.com DetectionTeamUCAS FPN Tensorflow the source code is summary image v1 tf.compat.v1.summary.image . Then I change the code to summary image v1 tf.summary.image the program can run correctly. This worked for me . Thanks I tried to upgrade tf to 1.13 and problems solved. Many thanks. It also worked for me. 
24555,Error loading Keras model with custom layer in JSON format, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 OSX Mojave Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below 1.12.0 Python version 2.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version N A GPU model and memory N A You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION Describe the current behavior I was following the Keras tutorial on TF website and I was trying out my custom layer which was basically the same as in the tutorial. However I got the following error when I tried to run it and I could not find any related issues online. Describe the expected behavior If the tutorial was correct the code should run without any bugs and the evaluation results should be the same after loading. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached., larry0123du Can you provide the minimal full code sample that could reproduce the error The code you pasted is not complete with get dataset and callbacks missing. Sure my get dataset is as follows and the callback is Let me know if you need anything else Hi larry0123du this issue should be fixed in the latest nightly I m able to run the code successfully 
24566,leaky relu buggy when alpha 1 or alpha 0,tf.nn.leaky relu is implemented as math ops.maximum alpha features features name name . This won t work if alpha 1 or alpha 0. Although this is not expected there is nothing that prevents users from setting alpha parameter as such. A better implementation could be nn.relu features alpha nn.relu features which won t suffer from the above issue., jvishnuvardhan Hi I want to know when this problem will be solved. Thank you Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 24566 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 24566 No a 
24573,Calling a Dense layer fails when it is created with kernel initializer tf.keras.initializers.Zeros , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac OS X 10.13.6 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below VERSION 1.13.0 dev20181225 GIT VERSION b v1.12.0 5131 gc6f3c5dc48 Python version 3.6.6 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior I get a TypeError exception when I call a Dense layer which was created with kernel initializer tf.keras.initializers.Zeros . Describe the expected behavior I expect no error and the kernel should be initialized to zeros just like when I set kernel initializer zeros . Code to reproduce the issue If you do not set the kernel initializer or if you set it to zeros everything works fine. Other info logs Here is the stacktrace , ageron I tried with ubuntu but could not reproduce it Wondering if the issue is similar to 24577 as well Hi yongtang I m not sure we are using the same version. I installed the 2.0 preview but it says the version is 1.13.0 dev20181225. ageron I used the following command to install tf nightly on Ubuntu 18.04 May I know the installation method you used ageron Think I found out where the issue is. The initializer could be v1 or v2 depending on the version 1.x vs. 2.0 the source code is built. For 2.0 initializer the signature is For 1.x initializer the signature is However the check of the initializer always assumes the initializer is 1.x https github.com tensorflow tensorflow blob a2f7f39d982682fa8de050e001522581570c510f tensorflow python keras engine base layer utils.py L124 L128 ageron Added a PR 24699 for the fix. Thanks once more yongtang Closing this issue since its resolved. Thanks Hi all. I have the same problem but have no idea how this solution can be implemented. 
24578,tf.version.GIT VERSION looks like repr GIT VERSION , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac OS X 10.13.6 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below VERSION 1.13.0 dev20181225 GIT VERSION b v1.12.0 5131 gc6f3c5dc48 Python version 3.6.6 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior The current GIT VERSION is b v1.12.0 5131 gc6f3c5dc48 . It starts with b and ends with this seems wrong. It may break scripts that rely on the GIT VERSION . Note that the GIT VERSION is fine in TF 1.12 v1.12.0 rc2 3 ga6d8ffae09 . Describe the expected behavior I expect v1.12.0 5131 gc6f3c5dc48 rather than b v1.12.0 5131 gc6f3c5dc48 Code to reproduce the issue Other info logs I checked the other versions in tf.version they look fine. ,Added PR 24583 for a fix. 
24585,tf.dynamic stitch does not work when one of the partitions has zero elements., em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information OS Platform and Distribution Windows 10 TensorFlow installed from source or binary Binary TensorFlow version use command below 1.12.0 Python version 3.6.6 CUDA cuDNN version V8.0.60 GPU model and memory Geforce GTX 1070 8GB Describe the current behavior I am building a model which partitions a given minibatch into different parts processes each part with different computation units in parallel and then stitch them back together. To implement this I decided to use tf.dynamic partition and tf.dynamic stitch methods. I have the following code for testing purposes So I divide the dataTensor into 3 parts with tf.dynamic partition and each part goes through a convolutional layer. After that they are stitched back together into a single minibatch again with tf.dynamic stitch using individual samples original location information from condition indices . This works without any visible problems when all three partitions have assigned at least one sample. But when at least one partition does have zero samples like in code above which is controlled with indices arr Tensorflow crashes at the tf.dynamic stitch line with the following error InvalidArgumentError see above for traceback data 0 .shape 0 14 14 32 does not start with indices 0 .shape 249 It seems like tf.dynamic partition and tf.dynamic stitch stops working when one of the partitions is empty receiving no samples and all partitions receive zero samples as the result . The code above is reproducible with simple copy and paste. Am I missing something or doing something wrong or is this a bug in tf.dynamic stitch implementation Doesn t it support empty partitions which can occur in practice if you determine partitions as the result of an algorithm , ufukcbicici I tried your code example on Ubuntu with the tf same version but could not reproduce the error you mentioned. Wondering if the issue only exists on Windows yongtang Interestingly again on Windows but using a much older Tensorflow 1.7.0 I cannot reproduce the error as well. The newest Tensorflow seems to be affected. I think this does not reproduce on nightly so closing it. Please reopen if I m wrong. 
24591,list index out of range in freeze graph.py, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow NO OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04.5 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device No TensorFlow installed from source or binary binary TensorFlow version use command below 1.12.0 Python version 3.5.2 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 9.0 7.0 GPU model and memory GTX 1080 11GB Describe the current behavior I try to freeze graph generated by tf.esitimator. esitimator will generate .pbtxt and ckpt file automatically so I use following command python freeze graph.py input graph path to .pbtxt input checkpoint path to model.ckpt 0 output graph path to saved .pb output node name OutPutOp Other info logs Traceback most recent call last File model freeze.py line 494 in module run main File model freeze.py line 490 in run main app.run main my main argv sys.argv 0 unparsed File usr local lib python3.5 dist packages tensorflow python platform app.py line 125 in run sys.exit main argv File model freeze.py line 489 in lambda my main lambda unused args main unused args flags File model freeze.py line 382 in main flags.saved model tags checkpoint version File model freeze.py line 364 in freeze graph checkpoint version checkpoint version File model freeze.py line 191 in freeze graph with def protos var list var list write version checkpoint version File usr local lib python3.5 dist packages tensorflow python training saver.py line 1102 in init self.build File usr local lib python3.5 dist packages tensorflow python training saver.py line 1114 in build self. build self. filename build save True build restore True File usr local lib python3.5 dist packages tensorflow python training saver.py line 1151 in build build save build save build restore build restore File usr local lib python3.5 dist packages tensorflow python training saver.py line 773 in build internal saveables self. ValidateAndSliceInputs names to saveables File usr local lib python3.5 dist packages tensorflow python training saver.py line 680 in ValidateAndSliceInputs for converted saveable object in self.SaveableObjectsForOp op name File usr local lib python3.5 dist packages tensorflow python training saver.py line 654 in SaveableObjectsForOp variable name File usr local lib python3.5 dist packages tensorflow python training saver.py line 128 in init self.handle op var.op.inputs 0 File usr local lib python3.5 dist packages tensorflow python framework ops.py line 2128 in getitem return self. inputs i IndexError list index out of range , burui11087 Could you provide a code to reproduce the issue to find root cause of the issue Thank you jvishnuvardhan Thank you for checking on this. It links to couple other issues 22029 5387 4363 Here I provides a minimal code using tf hub Then I encountered the IndexError as I run python m tensorflow.python.tools.freeze graph input graph resnet model.pb input checkpoint resnet model output graph resnet frozen graph.pb output node names module resnet v2 50 predictions Reshape 1 input binary At the same time i have successfully frozen a plain vanilla single layer CNN. Could you point some directions to check Thanks I have the same problem when using keras advanced layers. same here. Encountered the same error while freezing transformer from tensorflow official models I m getting the same error on a keras implementation of yolo. The error occurs in init of a ResourceVariableSaveable when trying to access var.op.inputs 0 on a tensor op is a VarHandleOp and var.op.inputs is a tensorflow.python.framework.ops.Operation. InputList object. The input list in this case is empty. Does the resource variable have to have an input What should handle op be in this case Any suggestions for how to proceed stevehawley have you tried with the tf.keras rather than the native keras jiayiliu yes I am using the internal tensorflow.keras . My workaround has been to use tf.saved model.simple save instead of a Saver and have freeze graph use input saved model dir. It gets me past this problem but I don t consider the issue resolved. If freeze graph is not going to support working on normal graphs it should deprecate that api and update the docs or at least put a note that for keras models you have to used saved model. Thanks for sharing the workaround stevehawley . I agree with you that the problem is not solved especially the simple save is marked as DEPRECATED in the documentation. I switched to keras.applications rather than using the tf.hub. Unclear why this was assigned to me. TensorFlow Hub was used to produce a failing case but IIUC the issue exists even without using hub. I am hitting this same problem too. Is there any updates Is freeze graph.py still officially supported Is there an alternative way of generating a protobuf out of a tf.graph I also meet this porblem when using mnasnet and efficientnet to freeze where I download models from tensorflow tpu models https github.com tensorflow tpu tree master models official. I use tensorflow 1.11.0 and I have checked my input output node names which are right. File usr local lib python3.5 dist packages tensorflow python tools freeze graph.py line 363 in freeze graph checkpoint version checkpoint version File usr local lib python3.5 dist packages tensorflow python tools freeze graph.py line 190 in freeze graph with def protos var list var list write version checkpoint version File usr local lib python3.5 dist packages tensorflow python training saver.py line 1094 in init self.build File usr local lib python3.5 dist packages tensorflow python training saver.py line 1106 in build self. build self. filename build save True build restore True File usr local lib python3.5 dist packages tensorflow python training saver.py line 1143 in build build save build save build restore build restore File usr local lib python3.5 dist packages tensorflow python training saver.py line 765 in build internal saveables self. ValidateAndSliceInputs names to saveables File usr local lib python3.5 dist packages tensorflow python training saver.py line 672 in ValidateAndSliceInputs for converted saveable object in self.SaveableObjectsForOp op name File usr local lib python3.5 dist packages tensorflow python training saver.py line 646 in SaveableObjectsForOp variable name File usr local lib python3.5 dist packages tensorflow python training saver.py line 128 in init self.handle op var.op.inputs 0 File usr local lib python3.5 dist packages tensorflow python framework ops.py line 2126 in getitem return self. inputs i IndexError list index out of range Can any one help me Thx I got same issue. Here is the actual script https gist.github.com thepulkitagarwal 36bc45aa43ae43f83baa5111a89be73e file freezekerasmodel py. help me to freeze the model. Note that you need to modify the output node name manually. And I successfully loaded the frozen model onto android. I get the same issue when I try to freeze the bidirectional LSTM network from tensor2tensor https github.com tensorflow tensor2tensor blob v.1.12.0 tensor2tensor models lstm.py . I managed to freeze my tensorflow model with tf.keras layers in it by using tf.SavedModel in conjuntion with freeze graph.freeze graph with def protos . Below is a generic snippet to convert a tf.keras based tensorflow checkpoint into a frozen checkpoint. I am working in tensorflow 1.13.1 I was able to freeze a SSD model with Keras mobilenet V2 feature extractor from tensorflow models https github.com tensorflow models blob master research object detection models ssd mobilenet v2 keras feature extractor.py ymodak Should we expect any further updates to graph freezing freeze graph.py is still in master. Tensorflow has moved to Keras and the incompatibility should probably be either documented or fixed. Please test with the latest version of tensorflow for using freeze graph tool. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 24591 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 24591 No a I believe this is still not working with the latest version of tensorflow 2.1.0. The error becomes self.handle op var.op.inputs 0 IndexError tuple index out of range ymodak Any updates on this The full stack error is I believe this is still not working with the latest version of tensorflow 2.1.0. The error becomes self.handle op var.op.inputs 0 IndexError tuple index out of range ymodak Any updates on this The full stack error is Do you have any progress I have the same problem. Same here. Tensorflow 2.1.0 and keras 2.3.1. cmehrshad Maybe because of TF2.x use resource variables by default. My job is make export inference graph.py working in TF models slim with TF2.x. Then I try print var list.value before the code line saver saver lib.Saver var list write version checkpoint version the tensor like shape dtype resource . when I use tf.compat.v1.disable resource variables error occurs NameError gobal name distribute strategy is not defined . so I haven t any solution now. I guess the main reason is because of the use of keras i change tf.keras.layers.BatchNormalization to tf.layers.batch normalization and the freeze graph is work. Any updates on this I have the exact same callstack as qipengh. Any updates on the issue ymodak Same here. Tensorflow 2.2.0 Is it fixed Same here Tensorflow 2.2 root cause maybe list index out of range in freeze graph.py is caused by a bug in saveable object util.py stack is https github.com tensorflow tensorflow blob v1.14.0 tensorflow python tools freeze graph.py L189 https github.com tensorflow tensorflow blob v1.14.0 tensorflow python training saver.py L481 https github.com tensorflow tensorflow blob v1.14.0 tensorflow python training saving saveable object util.py L82 image https user images.githubusercontent.com 33881637 127797539 b4032bab d383 4176 80c3 48be6107e77e.png solution use tf.graph util.convert variables to constants sess sess.graph.as graph def add shapes True FLAGS.output names.split instead of freeze graph.py. 
24593,Keras model evaluate progress bar randomly stops before 100 , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac OS X 10.13.6 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below VERSION 1.13.0 dev20181225 note this is the 2.0 preview GIT VERSION b v1.12.0 5131 gc6f3c5dc48 Python version 3.6.6 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior When evaluating a Keras model the progress bar randomly stops before 100 however the loss and metrics returned by the function are correct . Also it does not end with a newline. Describe the expected behavior I expect the progress bar to go up to 100 and display a newline. Code to reproduce the issue Other info logs Here is the output of this program Notice that the evaluation progress bar last line does not go up to 100 it stops at 9792 10000 . Moreover there is no newline at the end so the function s returned values 33.969303797870886 0.8358 are printed on the same line. Moreover when I run the same code again I get a different output only the last line differs . This time the progress bar stopped at 9088 10000 but notice that the function s results are the same as above ,I think the issue was that on batch end was relying on on epoch end for the last step processing https github.com tensorflow tensorflow blob 8f60a381d210478f21762a6cf14f547a05e98878 tensorflow python keras callbacks.py L724 L727 but on epoch end was only called for train mode https github.com tensorflow tensorflow blob 8f60a381d210478f21762a6cf14f547a05e98878 tensorflow python keras engine training arrays.py L375 L378 Created a PR 24633 for the fix. Once again thanks yongtang and happy New Year Happy New Year This issue can be closed because some commit already fixed the problem according to 24633. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 24593 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 24593 No a 
24601,possible bug in examples speech commands test streaming accuracy.cc,in tensorflow tensorflow examples speech commands test streaming accuracy.cc line 247 the following code is not correct const int64 audio data end sample count clip duration ms it should be corrected as follows const int64 audio data end sample count clip duration samples it is wrong because the dimensions do not match. sample count is in units sample but clip duration ms is in ms units. Additionally it gives the following error Segmentation fault core dumped but if audio data end is set correctly as i mentioned above this error would not occur. , Alireza89 I think you are correct do you want to create a PR for that yongtang Well I don t have a plan to create a PR for now but if it is actually a bug I hope it gets corrected in the master branch in the future. Alireza89 I have created the PR 24613 for the fix. Thanks for your contribution Thanks for the fix The PR has been reviewed so closing this now. 
24679,Out of bounds error while iterating on the rows of a RaggedTensor, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac OS X 10.13.6 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below VERSION 1.13.0 dev20181226 this is the TF 2.0 preview GIT VERSION b v1.12.0 5133 gc343196842 Python version 3.6.6 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior Iterating over the elements of a RaggedTensor results in an tf.errors.InvalidArgumentError after the last element. Describe the expected behavior I expect to be able to iterate over the rows of a RaggedTensor without any error. Code to reproduce the issue Other info logs Here is the output of the program above ,Here is a simple workaround However it moves all the data to Python. To preserve Tensors and remain within TensorFlow The issue was that the getitem in RaggedTensor should throw out IndexError vs. InvalidArgumentError in order to allow python to process iteration correctly translated to StopIteration in next next . Created a PR 24723 for the fix. The fix will only work with eager mode as non eager mode will not be able to find out the out of bound errors before session run. 
24741,When passing tf.data.Dataset instance to model.fit method which is instantiated by tf.keras.Sequential tf.keras.Model subclassing tf.keras.Model passing metrics argument to accuracy in model.compile method provokes TypeError , em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 macOS Mojave 10.14.2 TensorFlow installed from source or binary binary TensorFlow version use command below 1.12.0 Python version 3.6.6 You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION Describe the current behavior When passing tf.data.Dataset instance to model.fit method which is instantiated by tf.keras.Sequential tf.keras.Model subclassing tf.keras.Model passing metrics argument to accuracy in model.compile method provokes TypeError . But passing np.array to model.fit method doesn t provoke TypeError . Code to reproduce the issue tf.data.Dataset provkes TypeError . because I think tf.keras.metrics.sparse categorical accuracy output np.array doesn t provoke TypeError output Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. , aisolab I tried with tf nightly and it works fine. I think the issue may have been fixed. Can you try with tf nightly and see if the issue persist yongtang Thanks. I confirmed that it works fine at tf nightly version Closing this issue since its resolved. Thanks everybody Thankyou It is working for me. 
24938,tf.keras.backend.zeros implementation ends up tracking tensors as well in graph mode , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 OSX Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device No TensorFlow installed from source or binary binary TensorFlow version use command below 1.12 Python version 3.6.6 Describe the current behavior Here is my custom layer Please note the commented lines i.e. K.zeros vs tf.zeros In graph mode if I use K.zeros even though the graph gets built later on I get an exception with long stack trace probably because this layer gets used many times in my network that Tensor object does not have is initialized property K.zeros works in eager mode. Usage of tf.zeros work fine in both graph and eager mode. After debugging the tensorflow code I figured that towards the very end when keras tries to initialize the variables it sees some entries that are of type Tensor Those entries are the ones generated by K.zeros. I then looked at the implementation of K.zeros https github.com tensorflow tensorflow blob a6d8ffae097d0132989ae4688d224121ec6d8f35 tensorflow python keras backend.py L1010 In that it clear says that it could return either a variable or tensor based on the input i.e. shape . This is correct however it seems like that irrespective of the return value being a variable or tensor it still ends up tracking it via track variable in graph mode. During debugging I can see that since the tensor is part of the collection how invoking is initialized on it would result in error. Based on the code flow I would think that if K.zeros is going to return a tensor then it should not track it i.e add it to the variables collection . Part of the stack trace ,Same error is there with K.ones also. Similar behavior is seen when using tf.zeros like too. Pavithra looks like you added the tracking. Should it just be made conditional Yes the fix for this is in and should be available in the next nightly. Thank you Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 24938 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 24938 No a 
24980,MKL backed tensorflow throws errors on softmax on empty tensors GPU and Eigen versions do not, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary from anaconda TensorFlow version use command below 1.12.0 Python version 3.6.8 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION Describe the current behavior Calling tf.nn.softmax on a tensor that has a dimension 0 throws an exception at runtime when Tensorflow is backed by MKL. The same op returns an empty array of suitable size i.e. the same size as the input tensor under GPU and Eigen. Note that this happens no matter which axis the softmax is taken across the 0 length dim or a nonzero length one . Stacktrace Describe the expected behavior tf.nn.softmax on an empty tensor returns an empty tensor on all configurations. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs This may be related to https github.com tensorflow tensorflow issues 23145 which has a similar error message but seems to be caused by a different situation. , azaks2 who should look at this I have the same issue with Tensorflow softmax. I use macOS 10.14 python 3.6 with Conda installed Tensorflow. File Users lvhw anaconda3 lib python3.6 site packages tensorflow python client session.py line 1348 in do call raise type e node def op message AbortedError Operation received an exception Status 3 message could not initialize a memory descriptor in file tensorflow core kernels mkl softmax op.cc 164 node Softmax defined at Users lvhw Desktop CRF image segmentation master CRFCNNImageSegmentation.py 171 MklSoftmax T DT FLOAT kernel MklOp device job localhost replica 0 task 0 device CPU 0 conv2d transpose conv2d transpose 1 Caused by op Softmax defined at File Users lvhw anaconda3 lib python3.6 runpy.py line 193 in run module as main main mod spec File Users lvhw anaconda3 lib python3.6 runpy.py line 85 in run code exec code run globals File Users lvhw anaconda3 lib python3.6 site packages spyder kernels console main .py line 11 in module start.main agramesh1 Can you take a look at this tatianashp thanks. Pinging TensorFlow MKL Would you please upgrade your tensorflow to the latest version ver 1.13.1 It seems to be fixed in this release. pip install upgrade intel tensorflow python c import tensorflow as tf print tf.GIT VERSION tf.VERSION b v1.13.1 0 g6612da8 1.13.1 python test.py 2019 03 19 16 15 55.541077 I tensorflow core platform cpu feature guard.cc 141 Your CPU supports instructions that this TensorFlow binary was not compiled to use AVX2 AVX512F FMA 2019 03 19 16 15 55.574388 I tensorflow core platform profile utils cpu utils.cc 94 CPU Frequency 2500000000 Hz 2019 03 19 16 15 55.585260 I tensorflow compiler xla service service.cc 150 XLA service 0x2f945c0 executing computations on platform Host. Devices 2019 03 19 16 15 55.585295 I tensorflow compiler xla service service.cc 158 StreamExecutor device 0 undefined undefined 2019 03 19 16 15 55.595114 I tensorflow core common runtime process util.cc 71 Creating new thread pool with default inter op setting 2. Tune using inter op parallelism threads for best performance. 2019 03 19 16 15 55.630654 E tensorflow core common runtime bfc allocator.cc 373 tried to deallocate nullptr 2019 03 19 16 15 55.630703 E tensorflow core common runtime bfc allocator.cc 373 tried to deallocate nullptr test.py is the reproducible test case. Please let me know if this does not work on your side. please close this issue if you have this solution working workaround suggested. Closing this issue. Please reopen this issue if the solution provided doesn t solve the issue Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 24980 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 24980 No a 
25067,Latest commit in deprecation.py breaks R TensorFlow client , em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Fedora 28 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device NA TensorFlow installed from source or binary binary TensorFlow version use command below nightly r1.13 Python version 3.6 Bazel version if compiling from source NA GCC Compiler version if compiling from source NA CUDA cuDNN version NA GPU model and memory NA Hi the commit https github.com tensorflow tensorflow commit b97727bc3c7a9216670f361b639a60ed516917e0 diff fa5fb3b8d9f512ad269f7eb67903ec2b breaks the R TensorFlow client which uses embedded Python from R see https github.com rstudio tfdatasets issues 17 Specifically the line triggers the error on our side. For us in one specific example instead of a list with at least 4 items stack contains a list of 2 tuples each of which are of length 6. Unfortunately this will error and stop execution every time a new deprecation warning is issued. Current workaround on our side is https github.com rstudio tensorflow pull 287 however this comes at the cost of totally disabling deprecation warnings. Is there anything you could do to make this change compatible Thanks Sigrid , jtkeeling could you look at this I think it s simply a matter of adding bounds checking before accessing the higher levels of the stack. I ll take a look now I ve sent a fix to Martin to review so hopefully it should be submitted soon. Great thanks so much for reacting this fast Is there any chance this could still make it into 1.13 aselle would be the decider on that. Great thanks a lot for the quick fix I just tested with the nightly and it works aselle it would be great if you could answer reg. 1.13 we do have a temporary workaround but in case that s not needed we d remove it Thanks 
25069,Floating point exception when trying to build and compile my keras model with xla support. ,SYSTEM INFO UBUNTU 16.04 TENSORFLOW 1.12.0 compiled from source with XLA support KERAS 2.2.4 GPU Nvidia Geforce RTX 2080 Ti I am trying to build and compile a simple CNN with XLA but I get a floating point exception. Here s the block of code I am getting the following Output for this 2019 01 21 16 33 21.512092 I tensorflow stream executor cuda cuda gpu executor.cc 964 successful NUMA node read from SysFS had negative value 1 but there must be at least one NUMA node so returning NUMA node zero 2019 01 21 16 33 21.512662 I tensorflow core common runtime gpu gpu device.cc 1432 Found device 0 with properties name GeForce RTX 2080 Ti major 7 minor 5 memoryClockRate GHz 1.65 pciBusID 0000 02 00.0 totalMemory 10.73GiB freeMemory 10.33GiB 2019 01 21 16 33 21.512678 I tensorflow core common runtime gpu gpu device.cc 1511 Adding visible gpu devices 0 2019 01 21 16 33 24.538253 I tensorflow core common runtime gpu gpu device.cc 982 Device interconnect StreamExecutor with strength 1 edge matrix 2019 01 21 16 33 24.538298 I tensorflow core common runtime gpu gpu device.cc 988 0 2019 01 21 16 33 24.538312 I tensorflow core common runtime gpu gpu device.cc 1001 0 N 2019 01 21 16 33 24.538695 I tensorflow core common runtime gpu gpu device.cc 1115 Created TensorFlow device job localhost replica 0 task 0 device GPU 0 with 9966 MB memory physical GPU device 0 name GeForce RTX 2080 Ti pci bus id 0000 02 00.0 compute capability 7.5 Layer type Output Shape Param conv2d 1 Conv2D None 128 128 32 320 activation 1 Activation None 128 128 32 0 conv2d 2 Conv2D None 126 126 32 9248 activation 2 Activation None 126 126 32 0 max pooling2d 1 MaxPooling2 None 63 63 32 0 dropout 1 Dropout None 63 63 32 0 conv2d 3 Conv2D None 61 61 64 18496 activation 3 Activation None 61 61 64 0 max pooling2d 2 MaxPooling2 None 30 30 64 0 dropout 2 Dropout None 30 30 64 0 flatten 1 Flatten None 57600 0 dense 1 Dense None 256 14745856 activation 4 Activation None 256 0 dropout 3 Dropout None 256 0 dense 2 Dense None 34 8738 activation 5 Activation None 34 0 Total params 14 782 658 Trainable params 14 782 658 Non trainable params 0 2019 01 21 16 33 25.223019 I tensorflow compiler xla service service.cc 149 XLA service 0x7f6f20001130 executing computations on platform CUDA. Devices 2019 01 21 16 33 25.223063 I tensorflow compiler xla service service.cc 157 StreamExecutor device 0 GeForce RTX 2080 Ti Compute Capability 7.5 Floating point exception core dumped when I remove this part import keras.backend.tensorflow backend as bck config bck.tf.ConfigProto config.graph options.optimizer options.global jit level bck.tf.OptimizerOptions.ON 1 bck.set session bck.tf.Session config config everything works fine. ,I run in the same issue when using XLA with the GeForce RTX 2080 TI. It is reproducible using example code from tensorflow. More concretely I tried running the follwing script tensorflow examples tutorials mnist mnist softmax xla.py Here is the console output when running with the 2080 Here is the console output when running with a 1080 jlebar Could you please take a look at this issue Thanks Judging from the warning output it seems you are running with a old version of tensorflow. There was a bug in the detection logic if you are using an old ptxas which was fixed in https github.com tensorflow tensorflow commit 83ff640fa5026b8bd3cb9c2ceff9e99e8e03823a This is how I know you are using an old version. Can you try to run this with tensorflow at head or a nightly version I also have this exact problem using the newest release of TensorFlow. Without XLA the network runs at similar speed on an RTX 2080 Ti as it does with XLA on a GTX 1080 Ti. The newest release means TF 1.12.0 like mentioned above It is very much possible that those bugs have been fixed in the meantime. So could you please try with a version compiled from head Yes TF 1.12. I ve just finished compiling TF 1.13rc2 on branch r1.13 and with it I can run tensorflow examples tutorials mnist mnist softmax xla.py without crashing. The incorrect warning because of ptxas is also gone. But I get a new warning The result of this is faster than without XLA and it appears to work But it is not as fast as expected maybe compiling for sm 30 is not optimal. This line is probably missing from 1.13rc2 https github.com tensorflow tensorflow blob master tensorflow compiler xla service gpu llvm gpu backend nvptx backend lib.cc L126 I ll try master next. Thanks for trying it out and pointing out the new warning. We should try to get this line cherry picked into the next release if it is not too late for that. On master the warning is not present as expected but its performance on our internal graph of the result is slower than with 1.13rc2. Still faster than without XLA though I have no idea why yet or if this is a more general problem or just an artefact of my testing. tfboyd re the cherry pick of 83ff640. The new warning about sm75 should also be fixed in master. mnist is such a small thing that we haven t been bothered by its performance in XLA one way or another. If you have real models where XLA is slower we d definitely be interested in seeing those. I think otherwise this bug is closed This bug isn t originally mine so I can t say if it is fixed but at least for me 1.13rc2 works. On the model I m working on at the moment XLA itself in master is still faster than not using it but it is slower than XLA in 1.13rc2 was. But the model is private and I m not allowed to disclose any details so I ll try to debug why this is the case on my own and open a separate ticket should I find out anything. So the only solution for this is to install TF 1.13 Yes. akuegel Alright then I am closing this issue. Also is there any wheel file availabe for TF 1.13 with XLA. Compiling 1.12 from source with XLA was a pain and I don t want to go through the same steps again. If anybody has a wheel file plz share I thought that TF version 1.12 already had XLA support linked in by default https github.com tensorflow tensorflow releases tag v1.12.0 under major features and improvements The same should be true for every TF release after that. akuegel Oh okay I didn t knew that. actually I was working with an older version on a Jetson tx2 and then cross compiled from source for xla support. and then I wanted to use a newer version on my PC so I compiled 1.12 from source without even knowing that it had already xla support by default. 
25096,TF2.0 preview cannot use layer as activation function anymore, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac OS X 10.13.6 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below VERSION 1.13.0 dev20190117 this is the TF 2.0 preview GIT VERSION b v1.12.0 6228 g69b9e5358b Python version 3.6.8 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior When I try to use a layer as an activation function in a Dense layer I get an AttributeError Tensor object has no attribute numpy . This did not happen a few days ago in TF 2.0 preview. Describe the expected behavior I expect no error. Code to reproduce the issue There is no problem when the my softplus layer is used as a separate layer but the Keras API specifies that layers can be used like any function so I expect to be able to use them as activation functions and it was possible before . Other info logs Here is the stacktrace ,I just ran into the same AttributeError when trying to create a custom layer The exception disappears if I replace X 1. with X or if I add tf.function before def call ... . Thanks for raising this issue this should be fixed in the next nightly of the preview. It may take a day or two to appear 
25175,Dropout layer is broken, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow NO OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.10 TensorFlow installed from source or binary conda default channel TensorFlow version use command below 1.12 conda version mkl py36h69b6ba0 0 Python version 3.6.8 Describe the current behavior The Dropout layer always acts as if it is in testing mode. Describe the expected behavior The Dropout layer switches between training and testing. Code to reproduce the issue The tutorial code available from the hompage https www.tensorflow.org tutorials import tensorflow as tf mnist tf.keras.datasets.mnist x train y train x test y test mnist.load data x train x test x train 255.0 x test 255.0 model tf.keras.models.Sequential tf.keras.layers.Flatten tf.keras.layers.Dense 512 activation tf.nn.relu tf.keras.layers.Dropout 0.2 tf.keras.layers.Dense 10 activation tf.nn.softmax model.compile optimizer adam loss sparse categorical crossentropy metrics accuracy model.fit x train y train epochs 5 model.evaluate x test y test Setting the droprate to 0.999 yields the same performance as 0.2 well above 95 acc . This is not possible it means that the training training switch inside the Dropout layer is broken it is always returning the inputs even during the training phase. With tensorflow 1.10 the tutorial script gives the correct result which is 11 accuracy with 99.9 droprate. Other info logs It seems that either tensorflow.keras.backend.learning phase is at the root of the problem or model.fit doesn t correctly sets the training flag. , randolf scholz I test it on tf nightly 1.13.0.dev20190124 . The results are in below and look correct. When drop rate 0.99 When drop rate 0.2 1. The results are correct with tf nightly. Please test against tf nightly. Feel free to reopen if still running into problems. Thanks aselle I don t know when this was fixed but this may be important enough to cherry pick into 1.13 if it wasn t fixed before the branch cut. Also run the tests on tensorflow 1.13.0rc0 . The results look right. Thanks feihugis martinwicke tested dropout with 1.12 from PyPI works fine. We have unit tests for this always had. Hmm... strange thing to be broken by a conda rebuild. But then... who knows. fchollet martinwicke The problems seems to appear in tf.keras but not in the standalone keras package. The issue also seems affect the keras.backend.in train phase function. I tested it with the following files https pastebin.com SvTwrSC8 https pastebin.com SvTwrSC8 https pastebin.com VtBwL5mz https pastebin.com VtBwL5mz martinwicke tested dropout with 1.12 from PyPI and in colab with the above code. Does not work fine. Lets leave the issue open while we figure out more details and provide updates here. randolf scholz thank you for taking the time to report this. To be fair I have not done a full investigation and I have not looked at what was wrong with the script posted or at which commit its behavior might have started changing. Here is the test script I have used. I have only tested it with 1.13 and 1.12 on CPU. I have tested it in eager and graph mode. The script rigorously test that the Dropout layer injects ones in the forward pass activations of the Dense layer below it. In addition to this we have correctness tests for dropout in our test suite multiple in fact because we typically use a dropout layer as a way to test whether the learning phase was correctly set which is something we have to test for a range of model types and situations . To be the best I can tell tf.keras.layers.Dropout is working normally in TF 1.12 in training and inference modes with fit train on batch test on batch evaluate . I have now done a full investigation of the issue with the script posted. I confirm that incorrect behavior is present in 1.11 and 1.12. This behavior is entirely unrelated to either the Dropout layer or to the in train phase backend utility. The Dropout layer works completely fine. The bug is an issue that occurs when using a Sequential model in deferred mode . Deferred mode is a recently introduce way to use Sequential without passing an input shape argument as first layer. It s looking like the learning phase value was incorrectly set in this case. I think this would not have affected many users since deferred mode for Sequential is new and not widely used because hardly any Keras example use it none on keras.io . It is unfortunate that some of our tutorials on tensorflow.org have started using it. I m contacting devrel to make sure that the code examples are updated to add input shape arguments in our Sequential models which is better practice anyway since it allows for static layer compatibility checks . You can fix the issue in the script by simply passing input shape 28 28 to the first flatten layer. This bug has been fixed some time ago on the TF side. 
25178,Inconsistent behavior of sample weight for Keras,TF home compiled master from a few days ago Keras 2.2.4 pypi Hi Consider the following snippet This seems a bit all over the place. Keras behavior is cringeworthy as the loss is discontinuous in w . For w 1 1e 6 one gets 0.13333... but at least its consistent. I don t know what should be done but I m sure others have wasted half a day of work because of this ,Possible 23767 Hi 0x0L thanks for the issue It looks like the inconsistency b t mse and tf.keras.losses.mean squared error is fixed in the latest nightly I m seeing 0.13 for both. I think the external Keras logic should be updated to reflect the tf.keras logic as 0 values are being considered as null in external Keras and thus increasing the effective sample weight for other samples Made a tracking bug in external Keras https github.com keras team keras issues 12176 omalleyt12 Hi thanks for testing this. I wasn t able to try tf nightly since I m using python 3.7. I guess we should close this issue EDIT I just recompiled master and can confirm tf gives 0.13 for both losses 
25262,Usage of tf stack.extract stack in registry.py breaks TensorFlow R client, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow no OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Fedora 29 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device NA TensorFlow installed from source or binary binary TensorFlow version use command below nightly Python version 3.6 Bazel version if compiling from source NA GCC Compiler version if compiling from source NA CUDA cuDNN version NA GPU model and memory NA Hi this usage of tf stack in registry.py breaks the TensorFlow for R client because at that point when called from R stack is of length 2 with both elements being of length 6. This is analogous to the recently fixed https github.com tensorflow tensorflow issues 25067 thank you jtkeeling It would be awesome if this could still be fixed for the 1.13 release as I m aware of no workaround and we have users that want to register a custom gradient. Many thanks ,Yes this is entirely analogous to 25067. I ve authored a fix and sent for review. Great thank you for the quick fix This is fixed right cf other issue martinwicke This is another piece of code that uses tf stack. I ve sent you another change that fixes this one. Thanks We ll wait for that to close. 
25327,Error when restoring model using an Embedding layer, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device NA TensorFlow installed from source or binary binary TensorFlow version use command below 1.12.0 Python version 3.6 Bazel version if compiling from source NA GCC Compiler version if compiling from source NA CUDA cuDNN version 10.0 GPU model and memory RTX 2070 8GB Describe the current behavior Attempts to restore a frozen Keras model including an Embedding layer fail. The same problem appears when tf.graph util.convert variables to constants and tf.import graph def are used. The error is I tried Saving the model with tensorflow.saved model.simple save then running the freeze graph script to generate a frozen model then loading the frozen model again in C Python. The same error. convert variables to constants and tf.import graph def the graph same error. Code to reproduce the issue Minimal model Steps when save model is used OR steps when convert variables to constants is used Other info logs Seems to be related to 21889 the error message is different ,Can you try using the tf.keras.experimental.export https www.tensorflow.org versions r2.0 api docs python tf keras experimental export This handles Keras models more gracefully. Hi thanks for the suggestion I tried tf.keras.experimental.export and the export itself works fine. However the same error remains when I try to load the graph after freezing. Here is my code Any more ideas werner rammer I am getting the same error did you find a solution for this disha3 unfortunately not Bad thing is that in the not too distant future I ll really need this ... cebce4a should fix this issue. Please reopen if it s still an issue. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 25327 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 25327 No a Thanks When I use the changed version of graph util impl.py by copying the file to my local pyhton lib it works Looks as if the mentioned commit above fixes the issue. Thanks again Can you please merge this to master it s been since April 18 the merge is tagged with 1.12.1 but it s not even in master so we can t verify or even use graph freezing if it works I don t know if it does . What happened to this PR Why is it only in this testkevinbonz repo This is a breaking bug that makes it impossible to freeze saved models that contain embeddings so much for the keras by default also the bug breaks BERT for me Thanks When I use the changed version of graph util impl.py by copying the file to my local pyhton lib it works Looks as if the mentioned commit above fixes the issue. Thanks again changed version of graph util impl.py which vesion i have same problem need your help Hi well I simply copied from the github repo https github.com tensorflow tensorflow blob cebce4a2b5e33a06a1c92772008082895568f10a tensorflow python tools freeze graph.py and updated the file locally in the python lib maybe just search for freeze graph.py on your machine . problem solved replace local graph util impl.py with https github.com tensorflow tensorflow blob cebce4a2b5e33a06a1c92772008082895568f10a tensorflow python framework graph util impl.py 
25414,Tensorflow 2.0 Preview tf.function and tensorflow dataset incompatibility, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Arch Linux TensorFlow installed from source or binary pip TensorFlow version use command below tf nightly 2.0 preview Python version 3.6 CUDA cuDNN version 10 GPU model and memory GTX 1080Ti Describe the current behavior Creating a dataset using tensorflow dataset and passing it to a function decorated with tf.function gives the error Describe the expected behavior Not encountering the error. I think the problem is related to the magic behind tf.function. The issue is that there isn t a complete guide on how to use correctly tf.function. Code to reproduce the issue Other info logs If I remove the tf.function annotation the code works as expected. If I create the dataset inside the annotate function the code works as expected. Basically in the code below I create a td.data.Dataset using the new package tensorflow dataset. Then I pass the created dataset to a function annotated with tf.function that should perform the training loop. The errors I get are not informative. Unfortunately the tensorflow docs do not explain well how to use tf.function and the admitted operations. , jsimsa I think this is the same function registration issue we ve discussed before right Correct. It is not a TF 2.0 issue. The same behavior would happen in TF 1.x nightly. The use program will need to pass a dataset factory into train method so that the dataset is created in the same graph in which it is iterated over. EmanueleGhelfi Can you try the workaround jsimsa suggested by creating the dataset inside the tf.function Alternatively you can create the iterator outside. We re still working on a fix for this so I d like to leave this open. alextp jsimsa If I create the dataset inside the tf.function the code works as expected. I think this should be at least documented. For me it is not clear how to use correctly tf.function the things that are allowed and the things not allowed. If I need to use tf.function in every function I call I think it would be nicer to use a global setting. The reason why this is not documented is that this is a bug we re still working on a fix for and documenting broken behavior leads to workarounds persisting for long after the issue has been fixed . On Sat Feb 2 2019 at 1 54 AM Emanuele Ghelfi notifications github.com wrote alextp https github.com alextp jsimsa https github.com jsimsa If I create the dataset inside the tf.function the code works as expected. I think this should be at least documented. For me it is not clear how to use correctly tf.function the things that are allowed and the things not allowed. If I need to use tf.function in every function I call I think it would be nicer to use a global setting. You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 25414 issuecomment 459951747 or mute the thread https github.com notifications unsubscribe auth AAATxSMmfdXlXCR3Em58TmVEJGbk2R0Jks5vJWBNgaJpZM4ad2jI . Alex Ok thank you alextp for your support. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 25414 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 25414 No a 
25426,Segmentation Fault with tf.io.decode csv numpy record defaults and tensor input , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04.4 LTS on Windows Linux SubSystem Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device n a TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 preview b v1.12.0 6503 g7cfe43a11d Python version Python 3.6.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version No GPU model and memory No You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION this doesn t work you mean tf.version.x Describe the current behavior Segmentation fault core dumped Describe the expected behavior prints result Code to reproduce the issue import numpy as np import tensorflow as tf record defaults np.zeros 5 parsed fields tf.io.decode csv tf.constant 1 2 3 4 5 record defaults Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. It s fine with either tf.constants for record defaults or plain string to decode eg parsed fields tf.io.decode csv 1 2 3 4 5 record defaults Numpy version numpy 1.16.0 py36 blas openblash1522bff 1000 blas openblas conda forge ,I can confirm that I could reproduce this issue on MacOSX 10.13.6 python 3.6.8 TF version 2.0.0 dev20190126 git version b v1.12.0 6726 g5522d670af installed using pip3 install U tf nightly 2.0 preview . The decode csv function expects the record defaults to be an array of tensors so replacing np.zeros 5 with tf.constant 0. 5 or even with 0. 5 solves the problem but still segmentation faults should never happen. I can reproduce this on a recent nightly in eager mode. In graph mode a shape inference check raises a safe failure exceptio. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 25426 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 25426 No a 
25443,Bug in embedding ops.py Leads to Crash when importing Frozen Wide and Deep model graph, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 CentOS Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary Source TensorFlow version use command below 1.12 1.13 and master Python version 2.7 Bazel version if compiling from source 0.19.2 GCC Compiler version if compiling from source GCC 6.3 CUDA cuDNN version N A GPU model and memory N A Describe the current behavior After successfully training and exporting the trained Wide and Deep model from here https github.com tensorflow models tree master official wide deep Tried to freeze the exported model using freeze graph.py. The frozen graph got generated without errors. However when tried to load the frozen graph using the call tf.import graph def graph def got the following error File .. python2.7 site packages tensorflow python framework importer.py line 430 in import graph def raise ValueError str e ValueError Input 0 of node import linear linear model linear model linear model age bucketized weighted sum embedding lookup sparse embedding lookup was passed float from import linear linear model age bucketized weights part 0 0 incompatible with expected resource. After inspecting the frozen graph we found that ResourceGather Op is receiving float form Const node which used to be VarHandleOP before freezing but ResourceGather is expecting resource data type. The issue was resolved after changing this line https github.com tensorflow tensorflow blob master tensorflow python ops embedding ops.py L693 by removing the if statement and calling convert to tensor unconditionally. Describe the expected behavior The graph is expected to load and run successfully. Code to reproduce the issue https github.com tensorflow models tree master official wide deep , zhenlinluo I believe this is a bug in freeze graph or possibly our loading code as freeze graph doesn t seem to properly handle ResourceVariables. The fix in 25501 presents a regression when dealing with ResourceVariables. petewarden aselle is my hunch about freeze graph correct gargn for freeze resource variables work actually there are several issues reporting similar problem 25327 25721 https github.com tensorflow hub issues 208 https github.com tensorflow tfjs issues 947 21889 alextp gargn suharshs https github.com tensorflow tensorflow commit cebce4a2b5e33a06a1c92772008082895568f10a should fix this issue. Please reopen if it s still an issue. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 25443 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 25443 No a When can we expect this in nightly or master edit Tested this in nightly and it didn t work in my scenario Hi I met the same problem for the wide and deep learning model. The redirect of the line 693 may have been changed but according to the description of your I ve do the following changes from embedding weights w if isinstance w resource variable ops.ResourceVariable and dtype in None w.dtype else ops.convert to tensor w dtype dtype for w in embedding weights to embedding weights ops.convert to tensor w dtype dtype for w in embedding weights where the commented part is the original code. However it still did not work. Did I change the wrong part Or the bug still remains update Sry that I should have retrain the model as the code changed influences the model def phrase. But after I had frozen the graph I found there was any input which should be a TextLineDataset operation other than IteratorV2 or IteratorGetNext How should I organize the input dataset single data veqtor CharlesKung Can you both provide reproducible examples with the error that you are getting gargn I just changed the code in https github.com tensorflow tensorflow blob master tensorflow python ops embedding ops.py L693 from embedding weights w if isinstance w resource variable ops.ResourceVariable and dtype in None w.dtype else ops.convert to tensor w dtype dtype for w in embedding weights to embedding weights ops.convert to tensor w dtype dtype for w in embedding weights then freeze the model and reload it. I was confused by the input of the frozen graph as the nodes left for processing were IteratorV2 and IteratorGetNext . I thought it was removed by the freezing parts because the previous nodes are doing data processing. Is there any reference for the input of the frozen graph Like in the saved model methods the input should be wrapped as tf.Example and build a serving input receiver fn function. Thanks. CharlesKung A few questions comments 1. Can you attach the input files that are being used in your code snippet so that I can run it and reproduce it locally Additionally can you update your code snippet to work as a stand alone. I noticed output nodes is not defined. 2. Why are you changing the logic in the embedding op In general freeze graph works by pattern matching. If you are changing the patterns then it will no longer work. 3. Iterator ops should not be removed unless you are specifying the output nodes as nodes before the Iterator ops. However by default nothing happens to them in freeze graph. 4. The input of your frozen graph should be the same as the original graph that you created. The only thing that changes in freeze graph is that Variable ops are converted to Const ops and other similar transformations relating to resources . The structure of the graph and the op names stay the same. I would suggest loading the frozen graph using TensorBoard to examine what the inputs should be if you are uncertain. Automatically closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 25443 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 25443 No a 
25460,usage of Dataset.window causes TypeError when creating iterator, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution Linux Ubuntu 16.04 TensorFlow installed from source or binary binary TensorFlow version use command below 1.12.0 Python version 3.5 Describe the current behavior Following code is a snippet from Dataset.window documentation https www.tensorflow.org api docs python tf data Dataset window Calling iterator.get next throws following error Describe the expected behavior should create iterator object. Code to reproduce the issue See above. , tomzo I could reproduce the issue with tensorflow 1.12.0 but not with tf nightly . Looks like the issue has been fixed in the master branch. Could you try with tf nightly Tested in docker image tensorflow tensorflow nightly Does not throw errors anymore. Thanks. 
25463,TensorFlow GCS access does not work from colab , System information Using colab.research.google.com Describe the current behavior Hangs. Describe the expected behavior Should not hang.,Linked tensorflow datasets 36 Yes this one s a known bug the problem is that it s trying an authenticated request with some number of timeouts IIRC it ll eventually complete after 10m. Quick workaround first do from google.colab import auth auth.authenticate user and it ll work. This was intended to be fixed upstream but it looks like that didn t work at least it still hangs for me . I ll take a look. Thanks craigcitro. Why is it trying to make an authenticated request In the particular case I m running into it s a publicly accessible GCS bucket so no auth necessary. Where is the authenticated request logic happening I m guessing it s a TF thing not a colab thing right Should we update it to first check if credentials exist if so make authenticated request if not don t In my particular case TFDS is storing some files on a public GCS bucket and is trying to load them. The user shouldn t know anything about the GCS bucket. Calling auth.authenticate user forces the user to go to an oauth link copy a verification code and paste it back in. This isn t something we want to force users to do. In the short term TFDS will probably use the GCS HTTP API instead of tf.io.gfile. I ve added a NO GCE CHECK environment variable that allows a user to completely sidestep the GCE metadata checks anyone who s hitting the original issue slow timeouts retries attempting to fetch a public GCS resource should be able to use this to get unblocked. For Colab in particular I m going to add this patch and enable it in our runtimes where GCE metadata is never available . Thanks On Wed Feb 6 2019 at 10 24 PM Craig Citro notifications github.com wrote I ve added a NO GCE CHECK environment variable that allows a user to completely sidestep the GCE metadata checks anyone who s hitting the original issue slow timeouts retries attempting to fetch a public GCS resource should be able to use this to get unblocked. For Colab in particular I m going to add this patch and enable it in our runtimes where GCE metadata is never available . You are receiving this because you authored the thread. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 25463 issuecomment 461301381 or mute the thread https github.com notifications unsubscribe auth ABEGWxqzc1Y2eueoxJ6Cux6txS1tKe5Dks5vK8aIgaJpZM4af5x9 . 
25472,tf2.0 tf.keras.optimizers.Optimizer.get updates does not work, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux 4.14.79 x86 64 with Ubuntu 18.04 bionic Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device No TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 dev20190203 Python version 3.6 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION Describe the current behavior When keras optimizer s get updates function is called it raises error RuntimeError tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead. Describe the expected behavior I expect it returns correct operators that can be used to build keras function that changes weights. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. Traceback ,This is fixed TF 2.0 nightly 2.0.0 dev20190718 Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 25472 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 25472 No a 
25641,tf.rnn wrappers are incompatible with tf.keras.layers cells, System information Have I written custom code Yes OS Platform and Distribution Ubuntu 16.04 TensorFlow installed binary TensorFlow version 2.0.0 dev20190206 Python version 3.6.6 Describe the current behavior In the TensorFlow 2.0 preview the tf.rnn.DropoutWrapper and tf.rnn.ResidualWrapper wrappers are incompatible with cells from tf.keras.layers . It raises an error for missing a zero state method. Describe the expected behavior The classes in tf.rnn should be compatible with cells defined in tf.keras.layers . Code to reproduce the issue Other info logs If this API is not ready yet please ignore this issue. ,Added a PR 25646 for the fix. This should be fixed by https github.com tensorflow tensorflow commit 76a22b340814425b6a8b6e0c0bb81bcd162c458b and https github.com tensorflow tensorflow commit 42a1de008f301fb2d4c575ef5f817b5513156328 Btw the tf.rnn. namespace has been moved to tf.nn.RNNCell to consolidate the namespace. How about the progress I also meet the same problems. lycanthropes As mentioned by qlzh727 it was resolved in TF2.0.0 alpha0 by I have the same issues with the dropoutwrapper in 1.13. it states that a lstm cell has no attribute zero state. Btw the tf.nn.RNNCell Wrapper has different implementation between v1 and v2 TF binary. You will probably still hit the same issue if you are using TF 1.x binary. 
25652,export saved model raises TypeError Failed to convert object of type class dict values to Tensor., TPUEstimator.export saved model raises TypeError when used on CPU . The same code with TPUEstimator replaced with Estimator works correctly. The error is The relevant code Current behaviour export saved model fails with TypeError . Expected behaviour export saved model should export the model successfully. System information MacOS Python 3.6 Tensorflow 1.12.0 from PyPI. Full code Since exporting a model requires a trained model it wasn t easy to fully isolate the failing code so I m posting a full model file with as much irrelevant stuff removed as possible . With TPUEstimator failing https gist.github.com mluszczyk d60ed4205060eb7ef53c309c490fbe48 With Estimator working https gist.github.com mluszczyk bbd4f9136fc4788251079ac5b2176a01 Traceback Output of the variant with Estimator Flags Flag values to the attached files , mluszczyk When I run it in google colab with TPU i see different kinds of error for both the codes. Are you sure it is related to Bug performance If it is a support kind of question please ask it in Stackoverflow https stackoverflow.com questions tagged tensorflow . There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks Thanks for looking into it jvishnuvardhan. I created two public colabs reproducing the behaviour. I believe this is a bug. The error can be seen here https colab.research.google.com drive 1FEvd47JRZtAJZcwB10gqKDhSCWx cRCW Working version is here https colab.research.google.com drive 1bzpxlevnmDriBUrg3RihxXGOI4pmtYQQ I ve found out that the error occurs with tensorflow 1.12.0 and not with 1.13.0rc0 which is installed on Colab by default. So the working version in Colab uses TPUEstimator with Tensorflow 1.13.0rc0 rather than non TPU Estimator with Tensorflow 1.12.0 as in the original issue description which should be as good I can create the colab with Estimator if you d like too . So it looks like the error is fixed in 1.13. However TPUs with TF 1.13 are not available in Google Cloud so I still cannot use my model. I think a backport of the fix or a workaround would be very useful. When I tried to use TPUEstimator.export saved model I saw the same error Is there another way to export SavedModel from TPUEstimator I found we can avoid the error as below We except Googlers need not to serve our model on TPU so I guess the above idea might be helpful. Marking this as fixed as it has been fixed in 1.13 and using export to tpu False resolves the issue for earlier versions. 
25707,summary v2 create file writer behavior leads to incorrect resource deletion,This is a tracking bug for the known issue that the V2 summary API s create file writer logic behaves unexpectedly in regards to the underlying resource lifecycle management. This typically manifests as users seeing errors like tensorflow.python.framework.errors impl.NotFoundError Resource localhost logdir . log N10tensorflow22SummaryWriterInterfaceE does not exist. Op WriteScalarSummary name epoch loss Users can encounter this in 1.x when using tf.contrib.summary.create file writer in eager mode or in 2.0 as tf.summary.create file writer or via wrapper APIs like the Keras TensorBoard callback. Example issues 24632 25524 The root cause is that create file writer logdir returns a SummaryWriter python object backed by a C SummaryWriterInterface resource but the mapping is not 1 1 which violates other assumptions about resource management under eager mode. In particular two SummaryWriter instances created for the same logdir by default will both attempt to reference the same named resource. Sharing by logdir was intended to be a mechanism for using one eventfile per logdir akin to tf.summary.FileWriterCache in 1.x which makes TensorBoard better behaved. This is a problem when one of the SummaryWriters is deleted aka loses its last remaining reference because it will unconditionally attempt to delete the named resource it was created with. If a resource with that same name is referenced by any other SummaryWriters those ones will now emit the above NotFoundError when attempting to use them. Note that this happens even if the deleted SummaryWriter was properly close d before the new one was created because the logdir name sharing means that it new writer s resource has the same name as the old one. The correct fix is changing the resource names to be unique so that the mapping from SummaryWriter object SummaryWriterInterface resource is 1 1 as designed. This will likely require other workarounds for the one eventfile per logdir problem. Minimal repro ,Is there a workaround while the bug is being resolved UPDATE opened a new bug 25976 as I had a issue with the same error but different reason Reopening to track propagating this fix into the TF 2.0 Keras TensorBoard callback which will fix the user issues reported in the original description above. Closed by 059ea3ba68db861e40d750eba688281011d2735f. Seeing a similar error in scalars demo Googlers see b 239742608. 
25731,AttributeError Tensor object has no attribute numpy in image captioning with attention.ipynb, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow NO OS Platform and Distribution e.g. Linux Ubuntu 16.04 Colab Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary pip TensorFlow version use command below 2.0 Python version 3.x Describe the current behavior When running the code in Caching the features extracted from InceptionV3 AttributeError Tensor object has no attribute numpy Describe the expected behavior Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. for img path in tqdm image dataset batch features image features extract model img batch features tf.reshape batch features batch features.shape 0 1 batch features.shape 3 for bf p in zip batch features path path of feature p.numpy .decode utf 8 np.save path of feature bf.numpy Other info logs AttributeError Traceback most recent call last ipython input 15 9b3dc8e2ecd4 in module 15 for bf p in zip batch features path 16 path of feature p.numpy .decode utf 8 17 np.save path of feature bf.numpy AttributeError Tensor object has no attribute numpy ,In order to expedite the trouble shooting process please provide a code snippet weblink of the tutorial to reproduce the issue reported here. Thanks https colab.research.google.com github tensorflow docs blob master site en r2 tutorials generative image captioning.ipynb scrollTo Dx fvbVgRPGQ Thanks for the link and creating this issue. Thanks for dealing with this On Thu Feb 14 2019 5 50 PM ymodak notifications github.com wrote Thanks for the link and creating this issue. You are receiving this because you authored the thread. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 25731 issuecomment 463833648 or mute the thread https github.com notifications unsubscribe auth AEZlcMANlqeX7KDZBKrNtFveT4EbYpYmks5vNeg7gaJpZM4a6egl . This is resolved in the alpha closing I have installed the 2.0 alpha version but still get the Tensor object has no attribute numpy error during runtime. Why It works fines in ipython terminal. I also tried calling import tensorflow.compat.v1 as tf tf.enable eager execution at the beginning of the code but still the error. Traceback most recent call last File cnn.py line 192 in module tf.app.run File home bowang .local lib python3.5 site packages tensorflow python platform app.py line 40 in run run main main argv argv flags parser parse flags tolerate undef File home aaa .local lib python3.5 site packages absl app.py line 300 in run run main main args File home aaa .local lib python3.5 site packages absl app.py line 251 in run main sys.exit main argv File cnn.py line 161 in main tf.estimator.train and evaluate model train spec eval spec File home aaa .local lib python3.5 site packages tensorflow estimator python estimator training.py line 473 in train and evaluate return executor.run File home aaa .local lib python3.5 site packages tensorflow estimator python estimator training.py line 613 in run return self.run local File home aaa .local lib python3.5 site packages tensorflow estimator python estimator training.py line 714 in run local saving listeners saving listeners File home bowang .local lib python3.5 site packages tensorflow estimator python estimator estimator.py line 359 in train loss self. train model input fn hooks saving listeners File home aaa .local lib python3.5 site packages tensorflow estimator python estimator estimator.py line 1139 in train model return self. train model default input fn hooks saving listeners File home aaa .local lib python3.5 site packages tensorflow estimator python estimator estimator.py line 1169 in train model default features labels ModeKeys.TRAIN self.config File home aaa .local lib python3.5 site packages tensorflow estimator python estimator estimator.py line 1127 in call model fn model fn results self. model fn features features kwargs File data home BASELINES tf v2 network.py line 140 in model fn2 logits train conv net features params reuse False is training True File data home BASELINES tf v2 network.py line 40 in conv net lens np.array len xi for xi in x.numpy AttributeError Tensor object has no attribute numpy This seems like the code is running with estimators. Can you share the code yashk2810 Yes I am. So I just call model fn conv net function https stackoverflow.com questions 55147097 tensor conversion function numpy doesnt work within tf estimator model functi 55147364 55147364 Ok so Calling methods of Estimator will work while eager execution is enabled. However the model fn and input fn is not executed eagerly 
25754,Model.fit leaves training arg and learning phase unset TF2 preview , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac OS X 10.13.6 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below VERSION 2.0.0 dev20190213 GIT VERSION v1.12.0 7959 gabeb4d0acd Python version 3.6.8 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior When training a Keras model containing a custom layer whose behavior depends on the training phase training is not set by the fit method and K.learning phase is 0 . I can work around this issue by calling K.set learning phase 1 before creating the model but oddly this does not work if I call K.set learning phase 1 after creating and compiling the model and just before calling the fit method. It looks like the learning phase is hard coded into the call method s graph when the model is built Describe the expected behavior I expected the training argument to be set to True or 1 automatically when I called the fit method. Alternatively I expected that at least K.learning phase would return True or 1 but it s always 0 . Code to reproduce the issue If you re in a hurry just look at the failing tests C H and I. ,Ah I think I got it I m using tf.function so the call method gets traced and at that point K.learning phase gets called and it turns out that it returns 0 as an integer instead of a symbolic tensor so this value gets burnt into the graph edit actually it does return a symbolic tensor but it always evaluates to 0 . This would explain all 3 failing tests above. Shouldn t K.learning phase return a symbolic tensor when running in graph mode edit it does but apparently it is always 0 ageron Is this resolved If yes please close it. Thanks Hi jvishnuvardhan No it s not resolved I think I got it just meant I thought I understood the issue not that it was fixed I ll be clearer next time. I think what s happening is that when K.learning phase is run in the context of a tf.function it returns a symbolic tensor that is not affected by the fit method. FYI fchollet wrote I think in this case tf.function is extracting the layer s call in a form where it is no longer dependent on the learning phase placeholder. The fit method sets the value of the placeholder but by that time the tf.function is completely independent from it and relies on the default value of 0 . And I have a fix for this issue that should be reflected in the nightly build within a few days. Hi fchollet test C which is the most important now seems to work but unfortunately not test H and test I. I m using version 2.0.0 dev20190226 git version v1.12.0 8969 g0ae3728b74 . Side note it would be nice if keras.set learning phase None could reset the learning phase to the default mode so we wouldn t have to clear the whole session for that. Wdyt Oops I think the fix created a new issue I can create a new issue if you prefer but I think it s all about the same thing Last line raise this exception I suppose this is because the first call to fit creates a FuncGraph with 7 arguments but the learning phase scope adds an extra argument so it fails. Hi fchollet unfortunately right now the training argument seems broken always None using Because of the learning phase issues I am trying to use the training argument instead but it is not set by the fit method as shown in the following code This code displays This is true even if I decorate the call method with tf.function . Any updates on this ageron gijswobben are you still seeing this issue There was a commit https github.com tensorflow tensorflow commit cf31e9001c5ab89fc2e92eb98443d48cd37a726b diff 8bfbe586f2ba61d9aa9e39676c7f69ee which may have fixed this issue. Can you try on the latest nightly release Hi pavithrasv I just tested again unfortunately there are still some issues The good news is that the training argument now works as expected. But the learning phase seems to be always equal to 0. I think TF2 should get rid of global scopes anyway and this includes K.learning phase so I would recommend just deprecating learning phase and learning phase scope altogether. However it s part of the Keras API. Here s a little test that highlights what works and what doesn t It outputs this When is this issue going to be fixed Sorry about the delay. The issue here is because of the tf.function decoration on call. It causes learning phase value to be cached. Decorating small code snippets using tf.function will lead to lot of performance overhead and is not recommended. Also if you are training using fit this is not required. If you are training using custom training loops we recommend decorating the training step function with tf.function . Thank you Closing this as the main questions have been addressed I think.Thank you Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 25754 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 25754 No a 
25787,Warning or error about incompatible device when I use tf.nn.rnn cell.DropoutWrapper on multiple GPUs, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 TensorFlow installed from source or binary source TensorFlow version use command below 1.11.0 and 1.12.0 Python version 3.6.5 Bazel version if compiling from source 0.15.0 GCC Compiler version if compiling from source 4.8 CUDA cuDNN version 9.0 7.3.0 GPU model and memory nvidia 1080ti 11GB Describe the current behavior I write a very simple 2 layers LSTM model working on 2 GPUs. The model has no specific meaning and I just want to test LSTM network. I randomly generalize some data as inputs and take reduced sum of logits as loss. Then I compute the gradients and loss. I would get some warnings like below. And the same situation happens when I run TensorFlow Neural Machine Translation Tutorial https github.com tensorflow nmt tree tf 1.4 which is the reason why I tested this simple code. If I don t set allow soft placement True I would get error. Describe the expected behavior get no warning or make sure that this situation would affect result or performance. Code to reproduce the issue The warning only emerges when I use tf.nn.rnn cell.DropoutWrapper . Other info logs Full info with warning Full info with error no allow soft placement True . ,Thank you so much for the detailed report From a quick look it seems that the check was dropped in the following commit. https github.com tensorflow tensorflow commit f8f17cebcc89921b5f1a204dab1f9fdb47b120c1 The commit is part of TensorFlow 1.13 release. Could you give it a try with 1.13 to see if that resolves the issue Closing this issue but let us know if this is still an issue with TensorFlow 1.13 or later release. Closing this issue but let us know if this is still an issue with TensorFlow 1.13 or later release. Thank you so much for the detailed report From a quick look it seems that the check was dropped in the following commit. f8f17ce https github.com tensorflow tensorflow commit f8f17cebcc89921b5f1a204dab1f9fdb47b120c1 The commit is part of TensorFlow 1.13 release. Could you give it a try with 1.13 to see if that resolves the issue Thank you. Similar warnings don t exist with TF 1.13.1. 
25835,keras model s saving format is inconsistent tf and h5 ,hi I found the behavior is inconsistent when the keras model s saving format changed. the keras model have two saving format h5 and tf Describe the current behavior the reproduce code as below the photo below is the behavior in my computer. image https user images.githubusercontent.com 13925796 52948264 f2f7cf00 33b3 11e9 9ad1 fff9716911c8.png Describe the expected behavior when we change save format from tf to h5 the expection OSError should not happen. ,Created a PR 25851 to address the different behavior between tf and h5. 
25882,tf.image.random jpeg quality only products images of single jpeg quality, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Debian 9 TensorFlow installed from source or binary pip install TensorFlow version use command below v1.12.0 0 ga6d8ffae09 Python version 2.7.13 CUDA cuDNN version 9.0 7.0.3 GPU model and memory GeForce GTX 1080 Ti 10405 MB Describe the current behavior tf.image.random jpeg quality generates random jpeg quality on graph creation which is then fixed. Describe the expected behavior tf.image.random jpeg quality generates random jpeg quality for each image batch of images passed through it. Code to reproduce the issue The code causing this is located at https github.com tensorflow tensorflow blob a6d8ffae097d0132989ae4688d224121ec6d8f35 tensorflow python ops image ops impl.py L1634 , drpngx Can you answer the issue or reassign I think this is more of a type bug than type feature The root cause of this problem is that the EncodeJpeg op s quality parameter is a fixed Attr not a variable Input . Fixing random jpeg quality will necessitate a new version of EncodeJpeg with a different signature. drpngx I d be happy to put in a PR for this issue if that s ok. Sounds like a good idea. frreiss are you familiar with the op replacement process You have to 1. Create a new op call it v2. 2. Check it in with tests etc and wait for two weeks. 3. Replace the call from v1 to v2. The v2 version should be a strict superset of v1. Ideally it should behave exactly like v1 with the default options. drpngx yes I ve done an op replacement in the past with the accumulate n op. I ll break this work down into a few smaller PRs to make the diff size more manageable 1. Add a new C op EncodeJpegV2 along with C regression tests. 2. Modify the random jpeg quality Python API to use EncodeJpegV2 and also fix issue 25882. 3. Change the other Python code in image ops impl.py that uses gen image ops.encode jpeg to use gen image ops.encode jpeg v2 . Regarding the first step I see that EncodeJpeg has a number of other arguments that are currently static attributes Do you have any preference about which arguments I should turn into 0 D tensors in the new op I will let the API reviewer comment. My general intuition is that numbers can be changed and everything else in the worst case put in the graph. So quality x y density would be input tensors. For API owners This might be more appropriate for the TensorFlow Addons https github.com tensorflow addons repository which has a number of special use ops like this. In either case in the op registration code above the Attrs should be Inputs. Inputs are more flexible. I was about to start work on this but it looks like an anonymous Googler has already committed a patch. Someone should close this issue. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 25882 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 25882 No a 
25895,keras ConvLSTM2d not working with eager execution, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device NA TensorFlow installed from source or binary binary TensorFlow version use command below 1.13.0 dev20190130 Python version 3.6 Bazel version if compiling from source NA GCC Compiler version if compiling from source NA CUDA cuDNN version 10 7.4.2.24 GPU model and memory GeForce GTX 1050 Ti 4096 MB Describe the current behavior While using Eager Execution keras ConvLSTM2D seems like not initialized. After that we tried implement our model using gradient tape and it seemed fine but after loading checkpoint results was different than expected. The second issue probably has something in common with the first so i m posting code to reproduce first one and logs. Describe the expected behavior ConvLSTM2D works with Eager Execution Code to reproduce the issue Other info logs ,I encountered the same preblem in tensorflow gpu 1.9 I think the issue should be the fixed by the recent commit https github.com tensorflow tensorflow commit 391bee73641f1c23ef90cc40a9e8893f343c30fc. Can u try the same snippet with tf nightly and see if it works I think the issue should be the fixed by the recent commit 391bee7 https github.com tensorflow tensorflow commit 391bee73641f1c23ef90cc40a9e8893f343c30fc . Can u try the same snippet with tf nightly and see if it works I install tf nightly 1.14.1 dev20190304 I re run the code to find it started wording. thank you I install tf nightly 1.14.1 dev20190304 I re run the code to find it started wording. thank you SG glad its fixed. 
25938,keras.models.load model fails when the model uses a keras.losses.Loss subclass, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac OS X 10.13.6 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below tf.version.VERSION 2.0.0 dev20190220 tf.version.GIT VERSION v1.12.0 8385 gaaef4e8e43 Python version 3.6.8 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior keras.models.load model raises a ValueError when the model to be loaded uses a keras.losses.Loss subclass such as keras.losses.Huber . Describe the expected behavior Should load normally and I should be able to continue training where it left off using the loss. Code to reproduce the issue Other info logs Here is the stacktrace I did some debugging and I think I found the origin of the problem. In hdf5 format.py around line 233 the following lines use convert custom objects but they should be using losses.deserialize and metrics.deserialize . I ll send a PR. ,but it raise NameError name losses is not defined Hi zhaoyingjun Please check out my PR 25956 ageron hi i check out your PR but i load the model and train the accuracy is 0.0000e 00 no any update zhaoyingjun I m not sure the problem you are having is related to this issue you might want to share some code in a gist https gist.github.com if it s big . i find the reason in my code loss object tf.losses.SparseCategoricalCrossentropy model.complie loss loss object optimizer sgd it raise the error. the i change my code to model.complie loss sparse categorical crossentropy optimizer sgd it is ok Can confirm changing the loss function to a string removes this error. This is fixed TF 2.0 nightly 2.0.0 dev20190718 Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 25938 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 25938 No a This is fixed TF 2.0 nightly 2.0.0 dev20190718 Thanks Can confirm changing the loss function to a string removes this error. yeah It solved for me. ymodak can you please refer to the commit PR that fixes this Hi there I am still seeing the same issue. Below is my model code. After successfully train and save the model I come across with Unknown entries in loss dictionary error. Full error message as follows Anyone can help I am using tf. version 2.0.0 on Ubuntu 16.6. Can confirm changing the loss function to a string removes this error. How about my above case that we cannot assign a string in customer loss Same issue on the stable 2.0.0 release I have a similar problem. I trained and saved the model with TripletLoss which is a subclass of tf.keras.losses.Loss. When I load model as follows I got error message I had implemented in get config in TripletLoss but still get this error Does anyone can help me i find the reason in my code loss object tf.losses.SparseCategoricalCrossentropy model.complie loss loss object optimizer sgd it raise the error. the i change my code to model.complie loss sparse categorical crossentropy optimizer sgd it is ok Hi I change to use model.compile optimizer adam loss sparse categorical crossentropy metrics accuracy but the accuracy keeps to be 0.58 and not change. When I use loss tf.losses.SparseCategoricalCrossentropy the accuracy rised and looked normal. Do you have this problem 
25970,tf.keras computes incorrect loss values with 3 D data, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes. For a minimal example run and observe that loss and poisson values are different and loss values vary OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 TensorFlow installed from source or binary pip install tensorflow TensorFlow version use command below v1.13.0 rc1 19 gc865ec5621 1.13.0 rc2 Python version 3.7.2 x64 CUDA cuDNN version n a GPU model and memory n a Describe the current behavior When fitting a model with loss poisson I would expect reported loss and poisson values to be identical. Describe the expected behavior loss values are incorrect. They vary from epoch to epoch. Code to reproduce the issue See above. Other info logs More code examples and investigations at https stackoverflow.com q 54802328 880783 , bersbersbers are you still seeing this issue I was not able to repro this on the latest nightly. pavithrasv you are right tf nightly 1.13.0.dev20190227 does not have this issue. I can still repro it in 1.13.0rc2 as well as in 1.13.1 which has been released in the meantime. Since the issue reproduces in a stable release I would be very interested what the underlying cause is and in particular how long it has existed. Here s my pip freeze in my current installation that does repro the issue I tried to pin down when this issue was introduced and fixed So introduced some time in Aug Sep 2018 due to missing tf nightly packages on PyPi from that time I cannot get any closer. Fixed some time between Jan 25 and 29. That makes about 600 commits https github.com tensorflow tensorflow search q committer date 3A2019 01 24..2019 01 30 unscoped q committer date 3A2019 01 24..2019 01 30 type Commits I am closing this issue as it has been fixed. Thank you for digging into the release details This bug is still in the most current version 1.13.1 . Is there any release scheduled to be released soon to fix this If not I would be glad to use a workaround but so far I have not found any. By the way this is a reduced example where the batch size does divide the number of samples It outputs Note how loss and mean squared error are different. How can I get them to be identical Have you tried https github.com tensorflow tensorflow releases tag v2.0.0 alpha0 We will also have a TF 1.14 release very soon. Have you tried https github.com tensorflow tensorflow releases tag v2.0.0 alpha0 Yes I have see https github.com tensorflow tensorflow issues 25970 issuecomment 470210090. TF2 does not have that issue but I don t want my research to rely on Alpha software currently We will also have a TF 1.14 release very soon. That is good news thank you I can confirm that this bug has been fixed in 1.14.0rc0 Output Thank you I am using tf 2.1.0 and experience the same problem can you suggest anything Thank you I am also getting a delta between mse loss and mse metric values but only when applying regularization l2 or dropout . image https user images.githubusercontent.com 10877157 93761631 ae43b200 fc40 11ea 9913 c1636f45d86b.png 1 I have the same problem with tensorflow 2.3.0 when using l1 l2 regularization. I m having the same issue with TensorFlow GPU 2.1.0 and no regularization . However this happens only on the validation step. oO0oO0oO0o0o00 I have the same strange problem. same problem When using weight regularization it seems to me that it is normal that the two functions don t ouptut the same results. That is because regularization adds the squared values of all weights to the loss function. In the other hand the MSE metric computes the MSE between the true output and the predicted one. I have a similar problem. I have written my own test step self data during the validation of my model in the fit call I get these outputs for the validation I wonder why val loss is not the same as val student loss since they are supposed to be the same. When I call model.evaluate after training val loss and val student loss are the same. I noticed that when the batch size for my generator is equal to the size of the dataset the correct results are calculated. The generator is called in a tf.keras.utils.Sequence Object which is used for the validation data. I assume that the output is calculated based on the last batch only. 
25985,reset states failure in a stateful network with initial states set and training in batch TypeError NoneType object is not subscriptable, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow N OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 dev20190217 Python version 3.5.2 Bazel version if compiling from source n a GCC Compiler version if compiling from source n a CUDA cuDNN version 10 GPU model and memory GTX 1080 8G Describe the current behavior As manojrege said from https github.com keras team keras issues 11148 when we use initial states with RNN in some case we will get an exception There s another issue talking about this problem at 25852 Describe the expected behavior Should not throw exception. Code to reproduce the issue Other info logs I can confirm that the Pull Request https github.com keras team keras pull 11149 files can fix this problem. ,This should be now fixed by https github.com tensorflow tensorflow commit 83df61b4d4ad11f3b8cf05ee98d29e6fb5e25506. Thanks that s awesome. 
26029,super does not work within a tf.function, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac OS X 10.13.6 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below tf.version.VERSION 2.0.0 dev20190222 tf.version.GIT VERSION v1.12.0 8615 g74016a0d51 Python version 3.6.8 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior When I call a method decorated by tf.function I get an error if it uses super RuntimeError super class cell not found . Describe the expected behavior I expect no error tf.function should ensure that super works normally. Code to reproduce the issue Other info logs I can work around this issue in multiple ways The easiest is to replace super with super B self . But it s 2019 who still uses Python 2 style Or else I can work around the issue by using autograph False . This shows that the issue is linked to autograph not recognizing super only super B self I can also work around this issue by calling super outside of the method e.g. in the constructor I tried to work around it using a tf.init scope but I could not get it to work not sure why. Here is the full stacktrace for the first example code ,Oh this is tricky. mdanatg I think we can monkey patch the locals ag uses to define class and fix this Interesting. I m confident we can handle it correctly we just need to make sure we understand the mechanics of the new style super. I ve been looking at the pep https www.python.org dev peps pep 3135 and I think it amounts to setting the class cell. On Wed Feb 27 2019 at 10 06 AM Dan Moldovan notifications github.com wrote Interesting. I m confident we can handle it correctly we just need to make sure we understand the mechanics of the new style super. You are receiving this because you were assigned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 26029 issuecomment 467966267 or mute the thread https github.com notifications unsubscribe auth AAATxYM0sOzHmCKDtnYAgaGVASVlQ146ks5vRskCgaJpZM4bN1IF . Alex Fixed by https github.com tensorflow tensorflow commit 612ceb6c488e228fa5246d2452799cf2691ef5f1 Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 26029 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 26029 No a 
26061,Cannot load a Keras model with a custom initializer regularizer constraint function, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac OS X 10.13.6 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below tf.version.VERSION 2.0.0 dev20190222 tf.version.GIT VERSION v1.12.0 8615 g74016a0d51 Python version 3.6.8 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior I cannot load a model containing a custom initializer or a custom regularizer or a custom constraint if they are defined as regular functions rather than by subclassing the appropriate classes . Describe the expected behavior I expect it to work since the model otherwise works fine and is saved correctly. Code to reproduce the issue The following model uses a custom initializer and a custom regularizer and a custom constraint it works fine saves fine but cannot be loaded. You can try using only one at a time they all fail. Other info logs Here s the stacktrace , omalleyt12 can you comment on the proper format for serializing deserializing of initializers karmel Sure ageron Thanks for the issue looks like all custom objects are being treated as classes rather than functions. I ll work on the fix a quick workaround would be to turn your functions into classes with the current function as the class s call method A fix should be available in the latest nightly Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 26061 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 26061 No a 
26095,Warnings raised for deprecated collections.abc usage., System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No. OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary pip TensorFlow version use command below b v1.13.0 rc1 0 g63c13ff 1.13.0 rc1 Python version 3.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory irrelevant Describe the current behavior TensorFlows raises warnings like Describe the expected behavior These warnings should not be raised. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs I can do this if the patch will be accepted., NeilGirdhar Could you try the solution provided here https stackoverflow.com questions 35911252 disable tensorflow debugging information to disable warnings errors etc. If that was not solving please provide a code to reproduce the issue. Thanks jvishnuvardhan The tensorflow log level is not related to the warnings I m talking about which are raised by Python. The problem is that tensorflow needs to fix its imports. Right now these are just warnings but Python 3.8 will be released in October. If we want tensorflow to keep working we will need to fix this anyway. For 1.13 the branch is closed unless we have security issues. But I am happy to accept this patch for master. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 26095 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 26095 No a 
26099,tf.one hot crashes when indices is tf.uint8, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes. OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04 Windows 7 TensorFlow installed from source or binary Official pip source tensorflow gpu TensorFlow version use command below 1.12.0 Python version 3.6 CUDA cuDNN version 9.0 7.5 GPU model and memory 1080Ti 12GB Describe the current behavior tf.one hot crashes when the indices tensor has dtype tf.uint8 The error message shows Check failed new num elements NumElements Code to reproduce the issue https gist.github.com elmirador 4fc5148e5044478d668237209d265eac Other info logs I ve also tested under TF 1.4.1 and TF 1.10.0 both on GPU on different machines both have the same problem.,Hello I am new to the tensorflow community and which going thorugh the code I couldn t find gen array ops. If possible I would like to contribute to the issue. Hello I am new to the tensorflow community and which going thorugh the code I couldn t find gen array ops. If possible I would like to contribute to the issue. gen array ops.py is located under tensorflow python ops . You can t find it on github because gen array ops is generated during building. Basically it s a wrapper around the underlying C code of tensorflow. If you re interested the C code of one hot ops is located at tensorflow core kernels one hot op.cc . bono1567 Don t know if you had a chance to look into it and thank you for your interest Since you haven t posted an update for a while and I just looked into it I will submit a fix now. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 26099 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 26099 No a 
26143, TF2.0 Error Logging for GradientTape,Hello everyone I was wondering if there is an option for error logging or could we have tf output error message for gradient calculation. In the example below the below will output None values in the current setting but will output correct gradients when the tf.Variable s are a float type. My question is could we please add an error message stating something like gradient calculation supports only float types Best Regards Seung jae Bang System information Linux TensorFlow installed from pip install U tf nightly 2.0 preview 2.0.0 dev20190226 Python version 3.6 ccing random forests ,I d love to approve a PR adding this test. Feel like giving it a shot I would be happy to give it a shot I haven t contributed to TF before would you have any pointers Also would tensorflow python eager backprop.py be the right place to make this change cc random forests Yes the change would be to add some logging to that particular file. If you need any more pointers happy to help. On Mon Mar 4 2019 at 5 58 PM lazysjb notifications github.com wrote I would be happy to give it a shot I haven t contributed to TF before would you have any pointers Also would tensorflow python eager backprop.py be the right place to make this change cc random forests https github.com random forests You are receiving this because you were assigned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 26143 issuecomment 469502963 or mute the thread https github.com notifications unsubscribe auth AAATxdlUmDjQehdKpLmQuotUBd4yku7iks5vTc9VgaJpZM4bTIcM . Alex i would also like to contribute can you please help me where i can add logging in backprop.py You probably want to add the warning in tape.watch here https github.com tensorflow tensorflow blob d280d3d9b951063bf9e02ac79e248d729d33a29c tensorflow python eager backprop.py L804 and tape.gradient here https github.com tensorflow tensorflow blob d280d3d9b951063bf9e02ac79e248d729d33a29c tensorflow python eager backprop.py L890 Hi I wish to start contributing to TensorFlow and hence would like to know if someone is already working on this issue. achalagarwal I think shashvatshahi1998 is already working on this. shashvatshahi1998 can you confirm Ya I am working on that but anyone else who is interested can start working. Hey This would be my first contribution to tf and i would like to know whether this issue is open and if anybody is contributing to this. Closing this out since I understand it to be resolved by the PR. I have checked the code which output warning as expected. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 26143 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 26143 No a 
26174,TFLite Mfcc op has inconsistent requirements with standard Mfcc op, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes here s the code to reproduce https gist.github.com reuben 57bee91669a5bd2717c32cf406ca951d OS Platform and Distribution e.g. Linux Ubuntu 16.04 macOS 10.14 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below v1.13.0 rc2 5 g6612da8951 1.13.1 Python version 3.6.6 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Exact command to reproduce curl https gist.githubusercontent.com reuben 57bee91669a5bd2717c32cf406ca951d raw 6b81d28d00ff3ec73ab1bcc6a698366bbe3dcb51 test tflite mfcc.py python Describe the problem The Mfcc op in tensorflow.contrib.framework.python.ops.audio ops a.k.a. contrib audio enforces the shape of the sample rate parameter to be rank 0. The TFLite Mfcc op enforces the sample rate parameter to be rank 1. Converting a model with an Mfcc op in it results in a TFLite model that fails in the preparation step. Source code logs If you try to pass a sample rate of rank 1 to the contrib audio op If you pass a sample rate of rank 0 and then try to convert and use the TFLite model And here s the source of the testing script just in case , andrewharp pinging since you wrote the TFLite Mfcc op. Is there another way of converting the op to TFLite that I m missing As far as I can tell because of the different behavior w.r.t. the sample rate parameter there s no way to convert a TF graph to a TFLite model with an Mfcc op in it. FWIW I ve changed the test here to check if there s 0 dimensions instead of 1 and things have been working fine https github.com tensorflow tensorflow blob e6d074140d851e0ef52dbe4d382903b9100ba0bb tensorflow lite kernels mfcc.cc L75 FWIW I ve changed the test here to check if there s 0 dimensions instead of 1 and things have been working fine tensorflow tensorflow lite kernels mfcc.cc https github.com tensorflow tensorflow blob e6d074140d851e0ef52dbe4d382903b9100ba0bb tensorflow lite kernels mfcc.cc L75 Line 75 in e6d0741 tensorflow tensorflow commit e6d074140d851e0ef52dbe4d382903b9100ba0bb TF LITE ENSURE EQ context NumDimensions inputRate 1 When the code can be merged to the master branch rryan ping. Can you let me know if this patch makes sense I ll can make a PR. We tried to convert the model to tflite from the speech recognition sample with this script https github.com aselle tensorflow blob 7e71aa528111cd73cabc9fefdbb68422e35c16ce tensorflow examples speech commands conv only.py By the way it would be nice to have the python code of the converter used for the android tflite version It works perfectly for audio sample of 1 second but it fails for 2 seconds with the same kind of error as the initial post NumDimensions inputRate 1 0 1 Node number 1 Mfcc failed to prepare. Is this fix the solution we ve changed the shape of the input from 16000 1 to 32000 1 and 32000 2 but we get the error above reuben I can confirm your patch fixed the issue. I was able to rebuild and convert audio ops.mfcc to TFLite I am still looking into how to correctly use sample rate input. Currently it is hard coded in the audio ops.mfcc call. I suppose we can declare it as a placeholder and feed it to sess.run but I haven t had success with that while converting it to TFLite yet. Will be glad to hear if you have tried anything there. I am also curious to see where is tensorflow headed with MFCCs. There s some discussion on this thread https github.com tensorflow tensorflow issues 11339 issuecomment 345741527 about retiring audio ops and making tf.signal compatible to convert to TFLite. Nevertheless this patch definitely seems worth merging. Or it will be nice if someone can shed some light on why is the expected dimensions of inputRate 1 so we can feed it the right shape of inputs. FWIW I ve changed the test here to check if there s 0 dimensions instead of 1 and things have been working fine https github.com tensorflow tensorflow blob e6d074140d851e0ef52dbe4d382903b9100ba0bb tensorflow lite kernels mfcc.cc L75 Hello Did you find a solution to this problem My app crashes giving the same error. It isn t able to load the model. I tried the same solution but the error remained. Yes that patch is the solution We ve been using it in our packages ever since I ran into this issue without problems. I tried that patch. Replaced the 1 with 0. It didn t seem to work. Am I missing something I tried that patch. Replaced the 1 with 0. It didn t seem to work. Am I missing something Tensorflow Lite Interpreter is compiled code. Hope you are rebuilding Tensorflow from source https www.tensorflow.org install source after making the code change. I guess that s what I was missing. So I have to start from Build the pip package onwards I tried that patch. Replaced the 1 with 0. It didn t seem to work. Am I missing something Tensorflow Lite Interpreter is compiled code. Hope you are rebuilding Tensorflow from source https www.tensorflow.org install source after making the code change. I changed 1 to 0 and rebuilt it. It didn t work. Where do I make the following change a tensorflow lite kernels mfcc.cc b tensorflow lite kernels mfcc.cc I don t find this line in the code. Maybe this is the reason why the error still exists This patch does not affect training it only changes the TFLite interpreter so whatever you re using to load a trained .tflite file that s what you need to modify and recompile. If you re using the Python interpreter API then yeah you need to rebuild and reinstall the pip package. I m trying to use an pretrained tflite just to see how the app works well. I did the rebuilding but to no effect. The model doesn t load but keeps giving the same error May I know what command did you execute for compilation after making that change. I don t seem to be getting the solution. jdduke Looks like d5cc8288d939b522982738370facd456a0a643ba fixed this Ah yes thanks for flagging should be fixed by d5cc828 . Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 26174 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 26174 No a 
26222,Estimator training hangs in multiple gpu if dataset doesn t have enough element to feed both gpus last batches, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow YES OS Platform and Distribution e.g. Linux Ubuntu 16.04 Distributed training one node Multiple GPUs TensorFlow installed from source or binary PIP TensorFlow version use command below TF 1.12 Python version 3.6.8 CUDA cuDNN version 9.0 GPU model and memory 2 GTX1080 8Go Describe the current behavior Basically if the dataset doesn t have enough elements to feed both gpus last batches the training hangs. If you doesn t have enough to feed the first gpu last batch and don t want to drop the last batch then the training hangs. If you doesn t have enough to feed the first gpu last batch and want to drop the last batch then you re fine. If you have enough to feed the first gpu last batch but not the second gpu last batch and don t want to drop the last batch then the training hangs If you have enough to feed the first gpu last batch but not the second gpu last batch and want to drop the last batch then the training hangs Describe the expected behavior If you doesn t have enough to feed the first gpu last batch and don t want to drop the last batch then run the first gpu partial batch and do nothing with the second gpu If you doesn t have enough to feed the first gpu last batch and want to drop the last batch then drop the last batch for both gpus. If you have enough to feed the first gpu last batch but not the second gpu last batch and don t want to drop the last batch then run the first gpu entire batch and run the second gpu partial batch. If you have enough to feed the first gpu last batch but not the second gpu last batch and want to drop the last batch then run the first gpu entire batch and do nothing with the second gpu Code to reproduce the issue Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. , Silb78dg Could you check whether the bug persists with newer TF versions Thanks I encounter the same problem using tf1.12 and MirroredStratety. This can be solved by use batch after shuffle not after repeat. However I think this bug should be solved because batch should be after repeat for data efficiency. Successful code Failed code Hey jsimsa that s the issue we were discussing during tf dev summit. Thanks for your help. Denis. jsimsa will you look into this issue I believe that rxsang is actively working on fixing this issue. rxsang mentioned that this is fixed in TF 2 so closing the issue. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 26222 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 26222 No a 
26260,Creating an unused Mean metric in a custom model s constructor breaks the model, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOSX 10.13.6 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device No TensorFlow installed from source or binary Binary TensorFlow version use command below tf.version.VERSION 2.0.0 dev20190301 tf.version.GIT VERSION v1.12.0 9345 g4eeb2714f4 Python version 3.6.8 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior When creating a custom model simply creating a Mean metric in the constructor and setting it as one of the attributes leads to an exception when fitting the model. Moreover if I use the metric it seems to burn the batch size into the model so it behaves like a stateful model. Describe the expected behavior Creating a Mean instance should be harmless especially if it is unused. Code to reproduce the issue Other info logs Here is the stacktrace ,This is fixed TF 2.0 nightly 2.0.0 dev20190718 Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 26260 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 26260 No a 
26263,Error when using tf.constant to provide input to Keras model.predict, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Google Colab Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device NA TensorFlow installed from source or binary Binary TensorFlow version use command below 1.13.1 Python version 2.7 3.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 10 GPU model and memory K80 You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION Describe the current behavior This is a followup to the issue https github.com aamini introtodeeplearning labs issues 22 issue 416226096 raised in MIT s TensorFlow labs. When I use a TensorFlow tensor created using tf.constant as input to Keras model.predict it gives an error. However when I directly feed in the same tensor to the Keras model it gives output as expected. Describe the expected behavior According to the documentation provided model.predict should also be able to take a TensorFlow tensor as input. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Now when I do prediction using I get following error InvalidArgumentError In 0 is not a matrix. Instead it has shape 2 node MatMul 3 Op StatefulPartitionedCall When I use directly feed the tensor I get the expected behaviour tf.Tensor 0.31025296 0.48313126 0.7821198 shape 1 3 dtype float32 aamini also had the same behavior. Is this expected in TensorFlow 2.0 or a bug . If it is expected please update the documentation.,Thanks for the bug report sibyjackgrove it looks like this is fixed in the latest tf nightly as I m unable to reproduce Thanks for update omalleyt12 will try it out. 
26274,Custom model s build method is not called automatically, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOSX 10.13.6 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device No TensorFlow installed from source or binary Binary TensorFlow version use command below tf.version.VERSION 2.0.0 dev20190301 tf.version.GIT VERSION v1.12.0 9345 g4eeb2714f4 Python version 3.6.8 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior When creating a custom model with a build method e.g. if one of the model s layers has a size that depends on the input shape such as a reconstruction layer the model cannot be trained unless I explicitly call build with a tf.TensorShape . Moreover I cannot specify an input shape . Describe the expected behavior I expect the custom model to be built automatically the first time it is called e.g. by the fit method . Code to reproduce the issue Other info logs Here is the stacktrace ,Thanks for the bug report I was able to reproduce this issue looking into it now This should be fixed in the latest nightly build Has it been fixed I still get an error in a specific case when using add loss on a model. For example as illustrated in this question https stackoverflow.com questions 60106829 cannot build custom keras model with custom loss 60986815 60986815 . 
26294, TF 2.0 Conversion script fails to parse IPython functions.,The tf upgrade v2 https github.com tensorflow docs blob master site en r2 guide upgrade.md tool fails to parse commands with common IPython functions for example pip install tf nightly and matplotlib inline . ,Hi I would like to work on this issue. Any starting pointers would this work https github.com tensorflow tensorflow pull 26332 files Is anyone currently working on this Issue Hi dynamicwebpaige kyscg NikolaMandic shreyashub I am doing a research about how the labels such as good first issue help newcomers to contribute but I found many PR were rejected because of duplication or newcomers feel puzzled whether this issue are under processing especially for popular projects. How do you think of this problem I wonder whether mechanisms to avoid this problem are needed for example in addition to open and closed add a new status of issue ongoing . you like read what people wrote then think what was last written that is the status here for example one puts a pull request then people look at that one dynamicwebpaige this one should be been fixed for quite a while. Would you mind to close the issue or point us out if you still have any open issues with current nightly. Thanks Closing and thanks to lc0 for the contribution Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 26294 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 26294 No a 
26319,Training parameter in Keras models passed as None in 1.13, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary pip TensorFlow version use command below v1.13.1 0 g6612da8951 1.13.1 Python version 3.5.2 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version CUDA 10 cuDNN 7.5 GPU model and memory GTX 1050ti 4GB Describe the current behavior As described in the third example in the documentation for Keras models https www.tensorflow.org api docs python tf keras models Model class model a boolean training parameter can be used in the call method of subclassed models. However the parameter is passed as None when it should be True . Describe the expected behavior The training parameter should be passed as True when the model is training. Code to reproduce the issue Other info logs This only happens in TF 1.13 and not in 1.12. I also tried 2.0 alpha and the bug is still present,Hi pavithrasv has there been any progress in regard to this janhartman I was able to reproduce this issue. Will look into the fix. janhartman I think this was resolved in the latest TF versions TF1.14.0 and tf nightly . I cannot reproduce the issue. Please check the gist https colab.sandbox.google.com gist jvishnuvardhan 4c95c00604552800c6201c651dd758c8 untitled428.ipynb with TF1.14.0 . Automatically closing this out since I understand it to be resolved but please let me know if I m mistaken. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 26319 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 26319 No a jvishnuvardhan It s not completely solved. The parameter is now a bool tensor instead of a plain bool but it s better than before. Thanks 
26363,No gen bigquery reader ops, System information OS Platform and Distribution Linux Ubuntu 16.04 TensorFlow installed from binary TensorFlow version 1.12.0 Python version 3.5 Installed using pip CUDA cuDNN version 9.0 GPU model and memory Jetson TX2 I can t run uff conversion. The traceback is There s no such file gen bigquery reader ops.py at this location., yarons Could you provide more details on the problem and its context If possible please provide a code to reproduce the issue that makes it faster to find rootcause of the issue. Thanks jvishnuvardhan https github.com tensorflow tensorflow blob afab5b322f780c235c61038b49593e34b523d400 tensorflow contrib cloud python ops bigquery reader ops.py L21 This line is referencing this library but I couldn t find it anywhere. yarons This is fixed in Tensorflow 1.14.0. Would you like to upgrade your Tensorflow version and try. Thanks I will try that again thank you Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 26363 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 26363 No a 
26454, Bug report wrong container setting in OpsTestBase AddResourceInput easily fix,File tensorflow core kernels ops testutil.h Class OpsTestBase Function AddResourceInput https github.com wendy2003888 tensorflow commit 18b867eabd1bb8f3540ee56a8f47f96d9f3bc20d resource container set to empty when using default container. Due to CLA problem I can t contribute rn. Please review pull reques 26428 https github.com tensorflow tensorflow pull 26428 from ppwwyyxx. Thank you em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary python3 pip TensorFlow version use command below 1.12.0 Python version 3.5.2 Bazel version if compiling from source 0.22.0 GCC Compiler version if compiling from source 5.4.0 CUDA cuDNN version 10.0 GPU model and memory You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION Describe the current behavior Describe the expected behavior Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,
26549, Lite Assertion failure if shape of dynamic output tensor changes between invoke s, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device n a TensorFlow installed from source or binary pip TensorFlow version use command below 1.13.1 cpu Python version 3.6.7 Bazel version if compiling from source n a GCC Compiler version if compiling from source n a CUDA cuDNN version n a GPU model and memory n a Moved from 26248. If an op depends on a 1 dynamic tensor and 2 a normal intermediate tensor the computation of 2 is performed before 1 dims of 1 change between Interpreter invoke s Then assertion failure ERROR tensorflow lite simple memory arena.cc 100 erased allocs count 1 0 1 is triggered. An unit test is attached in 26248. On my box it is also reproducible by the Python snippet below which generate a model dynamically. However my reviewer can only reproduce the issue by unit test but not by script so the script may miss something. To help debugging a TFLite model file which I can reproduce the issue with is uploaded failed.tar.gz https github.com tensorflow tensorflow files 2949840 failed.tar.gz ,Hi Scott Sorry for late reply Can you still reproduce this issue I tested it on tf nightly but no assertion errors show up again. The unit test added in 26248 still failed with latest commit on master b0432d52 . As for the python script it is not always reproducible. scottcjt The issue is resolved in Tensorflow 1.14. Would you please check and confirm. If still you are facing issue please let me know Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 26549 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 26549 No a ANSHUMAN87 can you point me to the specific location where this is fixed in 1.14 I am facing similar issue with 1.14 umangmehta12 Are you able to reproduce the issue in 1.14 using the scripts provided by the author of this issue There is big difference most of the time between similar and same issue. If not able to reproduce the issue with the script then try to raise a new issue with your reproducible script and issue details. I can help you. You can TAG me if you are raising a new one. 
26581,Wrapped tf.nn.RNNCell layers are incompatible with tf.keras.layers.Bidirectional, System information Have I written custom code Yes OS Platform and Distribution Ubuntu 16.04 TensorFlow installed binary TensorFlow version 2.0.0.dev20190311 Python version 3.6.6 Describe the current behavior In the TensorFlow 2.0 preview the tf.nn.RNNCellDropoutWrapper and tf.nn.RNNCellResidualWrapper wrappers are incompatible with tf.keras.layers.Bidirectional . It raises an Unknown layer exception. Describe the expected behavior The RNN classes in tf.nn should be compatible with all Keras RNN layers. Code to reproduce the issue Other info logs qlzh727 ,Thanks for reporting the issue I will send some fix within this week. This is weird. Why tf.nn.rnn cell. is moved to tf.keras but ResidualWrapper and DeviceWrapper are not and they are inheriting new base classes such like ResidualWrapperV2 Sorry for the very late reply. The issue was caused by the serial deserialization for a non keras object and the nn.RNNCell wrappers are not keras object. The commit https github.com tensorflow tensorflow commit 0667725ba4e010c06fcf5e2a388eb8c2f3f6efe1 is trying to fix the custom object serialization issue which allows you to do something like https gist.github.com fchollet 9e361d1fe9046b512a2f3e0d31382a96. We are still discussing the issue internally and see how we will fix this issue. Sorry for the long wait this should now by fixed by e62dc433fcce833313b4174e20fc24c418593d27 Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 26581 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 26581 No a 
26590, tf.keras.layers.LSTM Initializer fails with input length parameter, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 TensorFlow installed from source or binary conda as binary. TensorFlow version use command below 1.10 1.12 and 1.13 confirmed Python version Python 3.6.6 Anaconda Inc. default Oct 9 2018 12 34 16 The following are irrelevant since I m not even running with a session or in eager mode CUDA cuDNN version CUDA 9.0 GPU model and memory GeForce GTX TITAN X MWE Current behavior Here s python error message Expected behavior LSTM class inherits from RNN which has input length as a parameter as described here https www.tensorflow.org versions r1.13 api docs python tf keras layers RNN . Therefore the constructor of RNN should use this parameter but it does not as you can see here https github.com tensorflow tensorflow blob master tensorflow python keras layers recurrent.py L391 L399 in the code., qlzh727 Qianli are you a good person to look at this Yes I will take a look within this week. Ah seems that it is an error in the documentation the input length is no longer needed and is inferred from the input tensor shape. I will fix the documentation soon. This should now be fixed. 
26608,Subsequent gather nd Operation leads to error Tensor.graph is meaningless when eager execution is enabled., System information OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04 TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 alpha0 Python version python 3.5 CUDA cuDNN version 10. GPU model and memory TITAN Describe the current behavior Gradient computation fails with the message Tensor.graph is meaningless when eager execution is enabled. The gradient becomes an IndexedSlices through the second gather nd operation. On the IndexedSlices the check in https github.com tensorflow tensorflow blob 8cb1aa18882be9142a5e4d3977de0a076ce2c791 tensorflow python framework ops.py L5956 L5957 evaluates to True and in line 5965 the .graph attribute is called which in EagerExecution causes the error mentioned above. https github.com tensorflow tensorflow blob 8cb1aa18882be9142a5e4d3977de0a076ce2c791 tensorflow python framework ops.py L5965 A workaround fix for the problem can be achieved by adding another check to the if clause making sure that op input is not an IndexedSlices and not isinstance op input IndexedSlices Describe the expected behavior Gradients to be computed correctly Code to reproduce the issue , josh11b You are mentioned in a TODO above the problematic if statement. Can you check whether this is a valid fix goldiegadde This is still present in 2.0.0 dev20190319 It seems plausible but I m not certain I understand the implications of this change. We should probably drop this validating that everything is in the same graph bit. It ll likely be false with functions anyway. meyerjo do you feel like sending a pull request which adds a unit test based on your code and deletes this validation block in ops.py Do you feel like this whole validation block is not necessary I will try to create a pull request beginning of April. Till then I am rather occupied. Yes I feel like it s wrong more often than not now. On Wed Mar 27 2019 at 12 25 AM Johannes Meyer notifications github.com wrote Do you feel like this whole validation block is not necessary I will try to create a pull request beginning of April. Till then I am rather occupied. You are receiving this because you were assigned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 26608 issuecomment 477012346 or mute the thread https github.com notifications unsubscribe auth AAATxYRqeL06Lw4JLnIFuEJaY1dRhkaqks5vaxzygaJpZM4bq6HM . Alex meyerjo did you ever send a PR to fix this meyerjo did you had a chance to send a PR to fix this Thanks For now the only fix I found is to multiply the tensor by 1. In between both the operations But not sure if that is the best way to get rid of this error Unfortunately I did not find time yet to work on the pull request. However the quick fix mentioned in the initial issue is still working in TF2.0 . Hopefully by mid of this week I will have some more time to take a deeper look at it. meyerjo I was able to reprocude the error with TF v2.0. However the issue seems to be fixed with TF v2.1 https colab.research.google.com gist amahendrakar 55ed1dfb50678c98c263f9a0c5d2e098 2 1 template.ipynb . Please find the attached gist. Thanks meyerjo Any updates regarding this issue Thanks Automatically closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 26608 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 26608 No a 
26625,tf.sparse tensor to dense does not have a gradient,This issue has never been solved why so eager to close it see 6391 22543,On a first read those two issues look different to me. Can you put here an example of the code you think should work and sorry for the unintentional closing of non fixed issues import tensorflow as tf indices tf.placeholder tf.int64 None 2 values tf.placeholder tf.float32 None sparse tensor tf.SparseTensor indices values 5 7 dense tensor1 tf.sparse tensor to dense sparse tensor dense tensor2 tf.sparse to dense indices 5 7 values dense tensor3 tf.sparse add tf.zeros 5 7 sparse tensor dense tensor4 tf.sparse tensor dense matmul sparse tensor tf.zeros 7 5 sum1 tf.reduce sum dense tensor1 sum2 tf.reduce sum dense tensor2 sum3 tf.reduce sum dense tensor3 sum4 tf.reduce sum dense tensor4 print tf.gradients sum1 values print tf.gradients sum2 values print tf.gradients sum3 values print tf.gradients sum4 values Thanks Indeed in all of these cases the correct gradient is not None while now I get On Fri Mar 15 2019 at 8 33 AM chwang notifications github.com wrote import tensorflow as tf indices tf.placeholder tf.int64 None 2 values tf.placeholder tf.float32 None sparse tensor tf.SparseTensor indices values 5 7 dense tensor1 tf.sparse tensor to dense sparse tensor dense tensor2 tf.sparse to dense indices 5 7 values dense tensor3 tf.sparse add tf.zeros 5 7 sparse tensor dense tensor4 tf.sparse tensor dense matmul sparse tensor tf.zeros 7 5 sum1 tf.reduce sum dense tensor1 sum2 tf.reduce sum dense tensor2 sum3 tf.reduce sum dense tensor3 sum4 tf.reduce sum dense tensor4 print tf.gradients sum1 values print tf.gradients sum2 values print tf.gradients sum3 values print tf.gradients sum4 values You are receiving this because you were assigned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 26625 issuecomment 473330342 or mute the thread https github.com notifications unsubscribe auth AAATxbHnalqlZvHhOWZxpq5QznPJbozrks5vW70ugaJpZM4brnr6 . Alex I confirm that this has been fixed in tf nightly. Thanks. 
26665,tensorflow 2.0 variable scope TypeError call got an unexpected keyword argument partition info ,I have convert a CNN model from tf1.x to tf2.0 by using tf upgrade v2 but when i used this converted model i got an error File home hsw virtual env tf2.0 lib python3.6 site packages tensorflow python ops variable scope.py line 2492 in default variable creator import scope import scope distribute strategy distribute strategy File home hsw virtual env tf2.0 lib python3.6 site packages tensorflow python ops variables.py line 216 in call return super VariableMetaclass cls . call args kwargs File home hsw virtual env tf2.0 lib python3.6 site packages tensorflow python ops resource variable ops.py line 422 in init constraint constraint File home hsw virtual env tf2.0 lib python3.6 site packages tensorflow python ops resource variable ops.py line 545 in init from args initial value if init from fn else initial value File home hsw virtual env tf2.0 lib python3.6 site packages tensorflow python ops variable scope.py line 886 in lambda shape.as list dtype dtype partition info partition info TypeError call got an unexpected keyword argument partition info it seems like something wrong in variables.py and the converted model such as like this with tf.compat.v1.variable scope backbone reuse tf.compat.v1.AUTO REUSE net tf.compat.v1.layers.separable conv2d inputs 16 3 1 same activation tf.nn.elu depthwise initializer tf.keras.initializers.glorot normal pointwise initializer tf.keras.initializers.glorot normal name conv1 net tf.compat.v1.layers.max pooling2d net 2 2 padding same net tf.compat.v1.layers.separable conv2d net 32 3 1 same activation tf.nn.elu depthwise initializer tf.keras.initializers.glorot normal pointwise initializer tf.keras.initializers.glorot normal name conv2 how should do to solve this problem ,Can I see the full stack trace Selection 012 https user images.githubusercontent.com 18358653 54325147 ac586600 463b 11e9 9f22 7060a5acb656.png This is the full stack trace. Thanks This seems to be an issue with the line which calls the initializer. For some reason this is calling the v2 tf.keras initializers instead of the compat.v1 initializers. I think this is a bug in the rename script. pavithrasv can you revert the change in the rename script to rename tf v1 initializers to tf v2 initializers since they re not compatible A workaround here is to replace the calls to tf.keras.initializers in the translated code with tf.compat.v1 initializers take what was there originally and replace tf with tf.compat.v1 . Thank you Alex. We do not rename v1 initializers to v2. The issue is because we do not rename v1 initializers to compat.V1 because of this users starts seeing the V2 version. https github.com tensorflow tensorflow blob 75eca85ab68d984ee905978483fa4d871fd92660 tensorflow tools compatibility tf upgrade v2.py L781 I will add this renaming for all the initializers. Thanks all fixed this by using tf.compat.v1.keras replace tf.keras. murdockhou Have you resolved this problem how to do it I ve the same problem. alextp I have similar issue. I attached my problem below. I replace tf.compat.v1.keras with tf.keras but can t solve my problem. In tf2 util.py def conv2d . . . . . . . . . . skip with tf.compat.v1.variable scope scope as sc . . . . . . . kernel variable with weight decay weights shape kernel shape use xavier use xavier stddev stddev wd weight decay def variable with weight decay name shape stddev wd use xavier True . . . . . . . . skip if use xavier initializer tf.keras.initializers.VarianceScaling scale 1.0 mode fan avg distribution uniform else initializer tf.compat.v1.truncated normal initializer stddev stddev def variable on cpu name shape initializer use fp16 False . . . . . . . . skip with tf.device cpu 0 dtype tf.float16 if use fp16 else tf.float32 var tf.compat.v1.get variable name shape initializer initializer dtype dtype return var I guess have error above code tf.compat.v1.get variable on def variable on cpu Detail error explaination is below Downloads 3D pointnet2 tf2 train.py 94 train one epoch train one step train data train label model optimizer anaconda3 envs tf 2 lib python3.6 site packages tensorflow python eager def function.py 416 call self. initialize args kwds add initializers to initializer map Downloads 3D pointnet2 tf2 train.py 67 train one step pred model.get model data True None Downloads 3D pointnet2 models from pointnet pointnet cls.py 27 get model transform input transform net point cloud is training bn decay K 3 Downloads 3D pointnet2 models from pointnet transform nets.py 19 input transform net net tf util.conv2d input image 64 1 3 Downloads 3D pointnet2 utils from pointnet tf2 util.py 164 conv2d kernel variable with weight decay weights Downloads 3D pointnet2 utils from pointnet tf2 util.py 45 variable with weight decay var variable on cpu name shape initializer Downloads 3D pointnet2 utils from pointnet tf2 util.py 21 variable on cpu var tf.compat.v1.get variable name shape initializer initializer dtype dtype anaconda3 envs tf 2 lib python3.6 site packages tensorflow python ops variable scope.py 1503 get variable aggregation aggregation anaconda3 envs tf 2 lib python3.6 site packages tensorflow python ops variable scope.py 1246 get variable aggregation aggregation anaconda3 envs tf 2 lib python3.6 site packages tensorflow python ops variable scope.py 569 get variable aggregation aggregation anaconda3 envs tf 2 lib python3.6 site packages tensorflow python ops variable scope.py 521 true getter aggregation aggregation anaconda3 envs tf 2 lib python3.6 site packages tensorflow python ops variable scope.py 936 get single variable aggregation aggregation anaconda3 envs tf 2 lib python3.6 site packages tensorflow python ops variables.py 260 call return cls. variable v1 call args kwargs anaconda3 envs tf 2 lib python3.6 site packages tensorflow python ops variables.py 221 variable v1 call shape shape anaconda3 envs tf 2 lib python3.6 site packages tensorflow python ops variables.py 60 getter return captured getter captured previous kwargs anaconda3 envs tf 2 lib python3.6 site packages tensorflow python eager def function.py 347 variable capturing scope lifted initializer graph lifted initializer graph kwds anaconda3 envs tf 2 lib python3.6 site packages tensorflow python ops variables.py 264 call return super VariableMetaclass cls . call args kwargs anaconda3 envs tf 2 lib python3.6 site packages tensorflow python eager def function.py 139 init initial value if init from fn else initial value anaconda3 envs tf 2 lib python3.6 site packages tensorflow python ops variable scope.py 908 lambda partition info partition info TypeError call got an unexpected keyword argument partition info HOW CAN I SOLVE IT THANKS IN ADVANCE. Can you file a separate issue Downloads 3D pointnet2 tf2 train.py 94 train one epoch train one step train data train label model optimizer anaconda3 envs tf 2 lib python3.6 site packages tensorflow python eager def function.py 416 call self. initialize args kwds add initializers to initializer map Downloads 3D pointnet2 tf2 train.py 67 train one step pred model.get model data True None Downloads 3D pointnet2 models from pointnet pointnet cls.py 27 get model transform input transform net point cloud is training bn decay K 3 Downloads 3D pointnet2 models from pointnet transform nets.py 19 input transform net net tf util.conv2d input image 64 1 3 Downloads 3D pointnet2 utils from pointnet tf2 util.py 164 conv2d kernel variable with weight decay weights Downloads 3D pointnet2 utils from pointnet tf2 util.py 45 variable with weight decay var variable on cpu name shape initializer Downloads 3D pointnet2 utils from pointnet tf2 util.py 21 variable on cpu var tf.compat.v1.get variable name shape initializer initializer dtype dtype anaconda3 envs tf 2 lib python3.6 site packages tensorflow python ops variable scope.py 1503 get variable aggregation aggregation anaconda3 envs tf 2 lib python3.6 site packages tensorflow python ops variable scope.py 1246 get variable aggregation aggregation anaconda3 envs tf 2 lib python3.6 site packages tensorflow python ops variable scope.py 569 get variable aggregation aggregation anaconda3 envs tf 2 lib python3.6 site packages tensorflow python ops variable scope.py 521 true getter aggregation aggregation anaconda3 envs tf 2 lib python3.6 site packages tensorflow python ops variable scope.py 936 get single variable aggregation aggregation anaconda3 envs tf 2 lib python3.6 site packages tensorflow python ops variables.py 260 call return cls. variable v1 call args kwargs anaconda3 envs tf 2 lib python3.6 site packages tensorflow python ops variables.py 221 variable v1 call shape shape anaconda3 envs tf 2 lib python3.6 site packages tensorflow python ops variables.py 60 getter return captured getter captured previous kwargs anaconda3 envs tf 2 lib python3.6 site packages tensorflow python eager def function.py 347 variable capturing scope lifted initializer graph lifted initializer graph kwds anaconda3 envs tf 2 lib python3.6 site packages tensorflow python ops variables.py 264 call return super VariableMetaclass cls .call args kwargs anaconda3 envs tf 2 lib python3.6 site packages tensorflow python eager def function.py 139 init initial value if init from fn else initial value anaconda3 envs tf 2 lib python3.6 site packages tensorflow python ops variable scope.py 908 partition info partition info TypeError call got an unexpected keyword argument partition info Above Error is the main issue. I Guess it came from below this lines. def variable on cpu name shape initializer use fp16 False . . . . . . . . skip with tf.device cpu 0 dtype tf.float16 if use fp16 else tf.float32 var tf.compat.v1.get variable name shape initializer initializer dtype dtype return var Thanks you alextp I meant a separate issue with full instructions to reproduce. This looks like it s coming from mixing tf v1 layers with tf v2 initializers. On Thu Jun 27 2019 at 5 23 PM tolry418 notifications github.com wrote Downloads 3D pointnet2 tf2 train.py 94 train one epoch train one step train data train label model optimizer anaconda3 envs tf 2 lib python3.6 site packages tensorflow python eager def function.py 416 call self. initialize args kwds add initializers to initializer map Downloads 3D pointnet2 tf2 train.py 67 train one step pred model.get model data True None Downloads 3D pointnet2 models from pointnet pointnet cls.py 27 get model transform input transform net point cloud is training bn decay K 3 Downloads 3D pointnet2 models from pointnet transform nets.py 19 input transform net net tf util.conv2d input image 64 1 3 Downloads 3D pointnet2 utils from pointnet tf2 util.py 164 conv2d kernel variable with weight decay weights Downloads 3D pointnet2 utils from pointnet tf2 util.py 45 variable with weight decay var variable on cpu name shape initializer Downloads 3D pointnet2 utils from pointnet tf2 util.py 21 variable on cpu var tf.compat.v1.get variable name shape initializer initializer dtype dtype anaconda3 envs tf 2 lib python3.6 site packages tensorflow python ops variable scope.py 1503 get variable aggregation aggregation anaconda3 envs tf 2 lib python3.6 site packages tensorflow python ops variable scope.py 1246 get variable aggregation aggregation anaconda3 envs tf 2 lib python3.6 site packages tensorflow python ops variable scope.py 569 get variable aggregation aggregation anaconda3 envs tf 2 lib python3.6 site packages tensorflow python ops variable scope.py 521 true getter aggregation aggregation anaconda3 envs tf 2 lib python3.6 site packages tensorflow python ops variable scope.py 936 get single variable aggregation aggregation anaconda3 envs tf 2 lib python3.6 site packages tensorflow python ops variables.py 260 call return cls. variable v1 call args kwargs anaconda3 envs tf 2 lib python3.6 site packages tensorflow python ops variables.py 221 variable v1 call shape shape anaconda3 envs tf 2 lib python3.6 site packages tensorflow python ops variables.py 60 getter return captured getter captured previous kwargs anaconda3 envs tf 2 lib python3.6 site packages tensorflow python eager def function.py 347 variable capturing scope lifted initializer graph lifted initializer graph kwds anaconda3 envs tf 2 lib python3.6 site packages tensorflow python ops variables.py 264 call return super VariableMetaclass cls .call args kwargs anaconda3 envs tf 2 lib python3.6 site packages tensorflow python eager def function.py 139 init initial value if init from fn else initial value anaconda3 envs tf 2 lib python3.6 site packages tensorflow python ops variable scope.py 908 partition info partition info TypeError call got an unexpected keyword argument partition info Above Error is the main issue. I Guess it came from below this lines. def variable on cpu name shape initializer use fp16 False . . . . . . . . skip with tf.device cpu 0 dtype tf.float16 if use fp16 else tf.float32 var tf.compat.v1.get variable name shape initializer initializer dtype dtype return var Thanks you alextp https github.com alextp You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 26665 email source notifications email token AAABHRJXDK7Y6SOKZWLZCTDP4VKWRA5CNFSM4G5UOJCKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODYYWV5I issuecomment 506555125 or mute the thread https github.com notifications unsubscribe auth AAABHRPL2UGAC5CO6RKBJLLP4VKWRANCNFSM4G5UOJCA . Alex Now tf 2.0.0beta0 has solved this problem... Thank pavithrasv 
26680,boolean mask with all zero mask produces allocator error, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 archlinux Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device n a TensorFlow installed from source or binary binary TensorFlow version use command below b v1.13.1 0 g6612da8 1.13.1 Python version 3.7 Bazel version if compiling from source n a GCC Compiler version if compiling from source n a CUDA cuDNN version 10.0 7.5.0 GPU model and memory GTX1080ti This code runs on GPU and prints There is an error in the end. It is not fatal the code returns correct results. But I assume it is still a bug that is worth fixing. If x is not full of zeros it does not produce such errors. UPDATE this issue does not exist in 1.12, azaks2 do you know who should debug this allocator issue I found the issue disappeared in nightly tf nightly gpu 1.14.1.dev20190412 Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 26680 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 26680 No a 
26687,Iterate on dataset iterator when using tf.function, System information OS Platform and Distribution e.g. Linux Ubuntu 16.04 Archlinux TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 dev20190312 Python version 3.6 CUDA cuDNN version 10 GPU model and memory 1080 Ti Describe the current behavior Can t convert the dataset iterator to its graph representation I m forced to loop in eager mode but I want to have a graph representation of the loop too since I need to export a SavedModel that contains the loop itself . Describe the expected behavior The iterator the dataset should be converted in its graph representation. Code to reproduce the issue A code that loops using Tensorflow primitives like for i in tf.range 10 can be converted into its graph representation without any problem while a code that creates a python iterator from a dataset object using iter dataset can t. Moreover maybe because we are in the early stage of the development I m unable to convert a loop that loops over a dataset to its graph representation. The itertest call fails because RuntimeError dataset. iter is only supported when eager execution is enabled. While the loopiter call fails because tensorflow.python.framework.errors impl.NotFoundError Function inference Dataset flat map flat map fn 22 is not defined. node ReduceDataset Op inference loopiter 35 ,I am closing this issue as this should now be supported as for TF 1.14 and TF 2.0 beta. Please verify and if you encounter a problem re open this issue with up to date information. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 26687 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 26687 No a 
26714,Error when using the class weight parameter in the fit function in tensorflow.python.keras, OS Platform and Distribution Linux Ubuntu 16.04 TensorFlow version 1.13.1 Python version 3.6 I wanted to test my network on a small set of records with two imbalanced classes 0 and 1 . I was going to use the class weight parameter in the fit function to improve the balance but unfortunately there were problems probably associated with tensor support. Without the class weight parameter the fit function works correctly. I also tried to use tensorflow.python.keras.layers.Input as an input to the Xception model but with the same effect. , jvishnuvardhan any updates imarkiew This looks like due to the fact that code in training utils.py of tf.keras is still wrongly assuming input data to be numpy arrays. Try enabling eager execution. It goes away for me if I do that but then it also uncovers other bugs. dd1923 Honestly I changed my approach some time ago and completely switched to Kerasa so I don t use TFRecords and Datset API . But I also think that there is something broken underneath between the Tensors and numpy arrays. imarkiew dd1923 There were lots of improvements over the last two releases. Can you please check with TF2.0 or tf nightly and let us know whether the issue persists in the newer TF versions. Please provide a standalone code reproduce the issue. Thanks imarkiew Is this still an issue If it not please close the issue. Thanks jvishnuvardhan I tested this for 2.0.0b1 version and it still seems that the problem exists but with another error . Unfortunately due to some problems I wasn t able to test tf nightly . imarkiew Can you please try TF2.0 or create a simple standalone code to reproduce the issue. Thanks jvishnuvardhan It seems that the change form 2.0.0b1 to 2.0.0 has helped. I don t see this issue anymore. imarkiew Thanks for the confirmation. We will close this issue as it was resolved. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 26714 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 26714 No a 
26725,BaseCollectiveExecutor StartAbort Invalid argument Upper bound check fail is reported with CollectiveAllReduceStrategy, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 CentOS Linux release 7.4.1708 Core Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device No TensorFlow installed from source or binary binary TensorFlow version use command below v1.13.1 0 g6612da8951 1.13.1 Python version 2.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 10.0 GPU model and memory Tesla P100 16G You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION Describe the current behavior When running transformer within the tensor2tensor library with CollectiveAllReduceStrategy via little customized change for distributed training in t2t only it reported below error INFO tensorflow Graph was finalized. WARNING tensorflow From home xh tf 1.13.1 lib python2.7 site packages tensorflow python training saver.py 1266 checkpoint exists from tensorflow.python.training.checkpoint management is deprecated and will be removed in a future version. Instructions for updating Use standard file APIs to check for files with this prefix. INFO tensorflow Restoring parameters from home xh datasets t2t train enzh multi gpu ok translate enzh wmt32k transformer transformer base model.ckpt 179000 2019 03 14 15 30 36.585213 I tensorflow core distributed runtime master session.cc 1192 Start master session 4c2e055e28f41c2f with config device filters job worker task 0 device filters job worker task 0 gpu options per process gpu memory fraction 0.95 allow soft placement true graph options optimizer options global jit level OFF rewrite options scoped allocator optimization ON scoped allocator opts enable op CollectiveReduce enable op CollectiveReduce enable op CollectiveReduce enable op CollectiveReduce experimental collective group leader job worker replica 0 task 0 WARNING tensorflow From home xh tf 1.13.1 lib python2.7 site packages tensorflow python training saver.py 1070 get checkpoint mtimes from tensorflow.python.training.checkpoint management is deprecated and will be removed in a future version. Instructions for updating Use standard file utilities to get mtimes. INFO tensorflow Running local init op. INFO tensorflow Done running local init op. INFO tensorflow Initialize strategy INFO tensorflow Saving checkpoints for 179000 into home xh datasets t2t train enzh multi gpu ok translate enzh wmt32k transformer transformer base model.ckpt. 2019 03 14 15 32 18.009875 I tensorflow stream executor dso loader.cc 152 successfully opened CUDA library libcublas.so.10.0 locally 2019 03 14 15 32 28.447121 I tensorflow core kernels data shuffle dataset op.cc 101 Filling up shuffle buffer this may take a while 387 of 512 2019 03 14 15 32 31.537519 I tensorflow core kernels data shuffle dataset op.cc 140 Shuffle buffer filled. 2019 03 14 15 32 31.695275 W tensorflow core common runtime base collective executor.cc 203 BaseCollectiveExecutor StartAbort Invalid argument Upper bound check fail for input 1 from node training gradients transformer parallel 0 5 transformer transformer symbol modality 32610 512 1 softmax concat grad Slice 7 to node scoped allocator concat 232 input bounds 0x7fb060a84f00 0x7fb060e7ff00 backing tensor bounds 0x7faf8da62700 0x7faf8fe35700 node scoped allocator concat 232 2019 03 14 15 32 31.708208 W tensorflow core common runtime base collective executor.cc 203 BaseCollectiveExecutor StartAbort Invalid argument Upper bound check fail for input 1 from node training gradients transformer parallel 0 5 transformer transformer symbol modality 32610 512 1 softmax concat grad Slice 7 to node scoped allocator concat 232 input bounds 0x7fb060a84f00 0x7fb060e7ff00 backing tensor bounds 0x7faf8da62700 0x7faf8fe35700 node scoped allocator concat 232 node send add 1 0 InvalidArgumentError InvalidA...ntError Describe the expected behavior What does above error log mean and is there any way to fix it thanks. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,Also seeing this bug when using tf.contrib.distribute.CollectiveAllReduceStrategy any information on what it means Any update on this issue Even I am facing the exact same error. isaacnoble jvishnuvardhan Ping poxvoculi looks like a ScopeAllocator issue. poxvoculi by any chance did you get time to look into this issue byronyi any workaround you can suggest for this Pinging dubey too Even I m facing the same issue. Do update us as soon as you find a solution. Even I m facing the same with tensorflow version 1.11 Python 3.6 Thanks for filing the issue. 589fb0dfdb https github.com tensorflow tensorflow commit 589fb0dfdb694d6318784e1523672fc2833b736d fixed a similar issue in ScopedAllocator . Can you try running with a nightly build and let us know if you still see this bug If yes please also post instructions on how to reproduce. Thanks for filing the issue. 589fb0dfdb https github.com tensorflow tensorflow commit 589fb0dfdb694d6318784e1523672fc2833b736d fixed a similar issue in ScopedAllocator . Can you try running with a nightly build and let us know if you still see this bug If yes please also post instructions on how to reproduce. Hi dubey I have a constraint where I need to work on conda tensorflow v1.11 Following are the steps to reproduce the error with tensorflow v1.11 1. Run the worker1.ipynb on node 1 worker1.zip https github.com tensorflow tensorflow files 3062532 worker1.zip 2. Run the worker2.ipynb on node 2 worker2.zip https github.com tensorflow tensorflow files 3062533 worker2.zip Note Do modify the TF Config variable according to your cluster specs in both the jupyter files You will get errors https gist.github.com vishald527 5c464addba66188625ea0088853db970 url and https gist.github.com vishald527 4c06f908651cae57b8a9f72996ea834d url on node 1 and 2 respectively. After making the changes as suggested in https github.com tensorflow tensorflow commit f10b00558de87020554c9c0512537dab96dba918 url re run the jupyter files. Now you will get this https gist.github.com vishald527 616c1dc32f20d8226d712f4552b025fa url error on node 1 and this https gist.github.com vishald527 d47696e61c3f31f4d3063ff48dffaea7 url log on node 2. Post making the changes suggested here https github.com tensorflow tensorflow commit e692dda4c8b199555e2fa32132a7784e0893c870 url re run the jupyter files. Now you will get this https gist.github.com vishald527 b1a0d7cb436f70902594dbd742701794 url error on node 1 and this https gist.github.com vishald527 6dc8b8719b4a67e0ee6dabd3a61ad779 url on node 2. We even tried building tensorflow v1.11 after making the changes you had suggested here https github.com tensorflow tensorflow commit 589fb0dfdb694d6318784e1523672fc2833b736d url but we are again getting the same Upper Lower bound check fail error on both nodes. Request you to please suggest what fixes are to be done in tensorflow v1.11 to get rid of this error and get CollectiveAllReduceStrategy up and running. vishald527 I am not a member of the TF team but I believe they do not accept bugfix PR on release branches for experimental features a.k.a. those in the contrib folder. CollectiveAllReduceStrategy was an experimental feature in r1.11 so you probably need to cherry pick the bugfix patches and backport them to r1.11. I am not sure how hard it is though. Please correct me if I am wrong. byronyi got your point but how to get a patch for this particular error i am getting this error in v1.11 and the person who raised this issue got the same error in v1.13. The patch for this error which dubey had mentioned above didn t work. Also do we say that currently tensorflow does not support a stable multi node gpu based synchronous distributed training strategy at all byronyi got your point but how to get a patch for this particular error i am getting this error in v1.11 and the person who raised this issue got the same error in v1.13. The patch for this error which dubey had mentioned above didn t work. I do not know. I would suggest you to upgrade your TF version. Also do we say that currently tensorflow does not support a stable multi node gpu based synchronous distributed training strategy at all It depends on your definition to stable . It was an experimental feature in r1.11 it is still an experimental feature right now. Use it at your own risk. I do not know. I would suggest you to upgrade your TF version. TF v1.13 already has the same error and TF v2.0 does not have this strategy..so not sure which TF version you are suggesting to upgrade. It depends on your definition to stable . It was an experimental feature in r1.11 it is still an experimental feature right now. Use it at your own risk. By stable I meant a strategy which would at least start synchronous distributed training on a multiple node gpu based system. Currently we are in a situation where we don t have a single strategy which even starts distributed training on multiple nodes synchronously. Please do correct me if I am wrong. Also is there any method you are aware of by which we can run distributed training on gpu based multiple node system TF v1.13 already has the same error and TF v2.0 does not have this strategy..so not sure which TF version you are suggesting to upgrade. CollectiveAllReduce is exposed as tf.distribute.experimental.MultiWorkerMirroredStrategy in TF 2.0. See here https github.com tensorflow tensorflow blob master tensorflow python distribute collective all reduce strategy.py L48 for details. Also is there any method you are aware of by which we can run distributed training on gpu based multiple node system Apart from MultiWorkerMirroredStrategy you could try ParameterServerStrategy as well. They are both in the tf.distribute.experimental subpackage. CollectiveAllReduce is exposed as tf.distribute.experimental.MultiWorkerMirroredStrategy in TF 2.0. See here for details. TF 2.0 i guess requires CUDA 10 and I have CUDA 9.2 installed. Upgrading CUDA is not an option for me right now and that is why I need help on fixing this reported issue for TF 1.11 1.13. Any pointers on who might be able to help resolving this issue Apart from MultiWorkerMirroredStrategy you could try ParameterServerStrategy as well. They are both in the tf.distribute.experimental subpackage. ParameterServerStrategy is an asynchronous technique. I am looking for a synchronous one. TF 2.0 i guess requires CUDA 10 and I have CUDA 9.2 installed. Upgrading CUDA is not an option for me right now and that is why I need help on fixing this reported issue for TF 1.11 1.13. Any pointers on who might be able to help resolving this issue You could recompile your TF using CUDA 9.2. You could recompile your TF using CUDA 9.2. Will try that too. dubey any luck on finding the cause and fix for this issue Apologies for the delay this was a bit tricky to debug and fix. I just submitted a3d2ee6 https github.com tensorflow tensorflow commit a3d2ee6ada53fdfde96a76d635c721629acb9582 which should fix the bounds check failure. Let us know if it works for you. dubey do I need to make both the code changes you had given and then try or only the latest one I d recommend patching both changes. dubey I used a recent built 6 days ago nightly gpu docker image this error still exists royxue thanks for the update. Can you post the error message you get with the recent image as well as instructions to reproduce the error I will try run again with the most recent docker image and get back to you the result dubey Here is the logs. I slightly modified the TF object detection. And run the collective allreduce strategy with 4 workers 1 gpu per worker on k8s. Can you provide a bare minimum test case that reproduces this error For some context the scoped allocator is a static optimization that converts multiple collective all reduce ops in to a single all reduce. In this optimization we pre allocate a buffer that is large enough to hold all the inputs to the all reduce. We add an attribute to each OpKernel whose output takes part in this all reduce indicating that the output should be assigned from a slice of this pre allocated buffer. The bounds check failure is a runtime error that indicates that the OpKernel somehow circumvented this process and used a different buffer. a3d2ee6 https github.com tensorflow tensorflow commit a3d2ee6ada53fdfde96a76d635c721629acb9582 fixed the case when an OpKernel calls set output instead of allocate output . In this example it looks like the input to the all reduce comes from an AddN op. I tried a small test case which fuses 2 ops using the scoped allocator whose inputs are both AddN but my test case passed. dubey I was checking the solution yesterdays. If I started training from a checkpoint this error happened. But is I started training from scratch this error will not happen. However the training process will hang up after the graph is initialized. Im thinking this is may caused by the conflicts between slim.argscope object detection api used and distributed strategy . As mentioned the strategy do not support deprecated slim api. Is this resolved or is it still an issue It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue Closing due to lack of recent activity. Please update the issue when new information becomes available and we will open a new issue. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 26725 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 26725 No a 
26738,TF 2.0 Keras model utilizing another model with metrics cannot fit evaluate in graph mode mistakenly using the metrics of the inner model , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow YES OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Debian Stable Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below CPU both TF 2.0.0a0 and tf nightly 2.0 preview 2.0.0.dev20190315 Python version 3.5.3 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior When running the script below it fails during execution with error Describe the expected behavior The code should not crash. Code to reproduce the issue Other info logs outer model.predict works when outer model.run eagerly True then outer model. evaluate fit works when the inner model has no metrics metrics in inner model.compile then outer model. evaluate fit works I have trace the problem to an incorrect FuncGraph generated it mistakenly uses the metrics from the inner graph i.e. the metrics calculations utilize the original placeholders of the inner graph . ,The name conflict is further supported by the fact that naming the metric in the outer model differently as in also makes outer model. evaluate fit work. Is there a workaround for this Preferably without using Sequential https github.com tensorflow tensorflow commit 659c981a3556c6424237eacd0bf4cdc86f228f16 should fix this issue. Please give it a try in the next nightly and let me know if it works as expected. Thank you I can confirm that tf nightly 1.14.1.dev190623 fixes the issue and that tf nightly 1.14.1.dev190620 was still broken. Great thank you for verifying Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 26738 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 26738 No a 
26763, TF2.0 Embedding batch input shape not aware of distribute.MirroredStrategy , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes modified an example from Seedbank https colab.research.google.com github tensorflow docs blob master site en tutorials sequences text generation.ipynb scrollTo MtCrdfzEI2N0 to use with TF2.0 OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary pip binary TensorFlow version use command below 2.0.0 alpha0 git v1.12.0 9492 g2c319fb415 Python version 3.6 Bazel version if compiling from source NIL GCC Compiler version if compiling from source NIL CUDA cuDNN version 10.0 GPU model and memory V100 16GB Describe the current behavior 1. when batch input shape is not specified in tf.keras.layers.Embedding With or without distribute.MirroredStrategy model works perfectly fine 2. when batch input shape is specified in tf.keras.layers.Embedding Without distribute.MirroredStrategy model works perfectly fine But with distribute.MirroredStrategy tf.data.Dataset splits the inputs to the model correctly but the model s expected input is not correct The model replicas each expect batchsize per replica replica the un split output from Dataset instead of batchsize per replica split output from Dataset . If the model s batch input shape or Dataset output is adjusted to match the above expectation the keras Model immediately errors out as it expects batchsize per replica replica split to batchsize per replica as an input to each model replica. Illustrated Example It seems like the problem is everywhere else the batch size etc. is calculated correctly except when batch input shape is specified in tf.keras.layers.Embedding . If anyone is wondering why we need to specify this this is for stateful RNNs to work. Describe the expected behavior When the scope is distribute.MirroredStrategy tf.keras.layers.Embedding specified batch input shape should also be divided by the number of replicas. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. The notebook below contains code that will reproduce the error on a multi GPU system. On Colab there is only one GPU hence it runs fine since distribute.MirroredStrategy only creates one replica. However with two or more replicas on a multi GPU system the error is observed. Notebook presented on Google Colab https colab.research.google.com drive 1R3h2Jf9rKCsi952KLcg7b8PtPyyXgx6b There is a header section that shows the model training with and without distribute.MirroredStrategy . Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. Following the Colab notebook above when training is run on a 4 GPU system overall batch size 128 batchsize per replica 32 the error is Notebook that shows the entire run resulting in the above error can be seen here https nbviewer.jupyter.org github tlkh arxiv lm blob master tf distributed embedding bugreport.ipynb If you modify batch input shape to match for batchsize per replica 32 In all cases model.summary gives the same result The exact same code will run fine on single GPU system even with distribute.MirroredStrategy as the scope you can view Colab demo https colab.research.google.com drive 1R3h2Jf9rKCsi952KLcg7b8PtPyyXgx6b , 26245 shows a similar error but somewhat different in nature 1. That one is with Keras multi gpu model I believe this is not recommended any more with TF2.0 2. That one gives an error even when batch input shape is not specified whereas in my case it works fine tlkh why is multi gpu model not recommended with TF 2.0 any longer ghego With TF 2.0 tf.Keras works out of the box with tf.distribute.MirroredStrategy . My own experience is that it is much nicer to use. Check out https www.tensorflow.org alpha tutorials distribute keras yep I ve seen that. Thanks Assigning to Priya since this seems to be related with distribution strategy. Please reassign to someone who owns the embedding if the fix should be on that side. It should be fixed after https github.com tensorflow tensorflow commit a3f9c59153203020dcd008527db5247deefad95a. Could you please try and re open if that doesn t help Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 26763 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 26763 No a 
26788, TF 2.0 Keras in graph mode crashes when fitting data with L2 regularizer set to 0., System information Have I written custom code as opposed to using a stock example script provided in TensorFlow YES OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Debian Stable Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below CPU both TF 2.0.0a0 and tf nightly 2.0 preview 2.0.0.dev20190315 Python version 3.5.3 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior The below code using zero L2 regularization fails with Describe the expected behavior The code runs without a failure. Code to reproduce the issue Other info logs Running with model.run eagerly works. Running with non zero L2 regularization works. The problem is in https github.com tensorflow tensorflow blob b57c7d71eff5914a503d15130cb90a240b3bcf40 tensorflow python keras engine network.py L651 where a ops.get default graph is called. The problem is probably connected to the fact that L1L2 regularizer returns an explicit 0 K.constant 0. when no l1 l2 is set while otherwise summing weights and applying l1 l2 .,Thanks for the bug I was able to reproduce looking into now I could not reproduce the issue with pip install tf nightly gpu 2.0 preview 2.0.0.dev20190808 . Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 5b56a644fd9e48c4cdde180460950b66 tf26788.ipynb . Automatically closing this out since I understand it to be resolved but please let me know if I m mistaken.Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 26788 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 26788 No a 
26793, tf.function doesn t compile functions specified as parameters to other functions, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No. OS Platform and Distribution e.g. Linux Ubuntu 16.04 Colab. Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary Binary TensorFlow version use command below 2.0.0 alpha0 Python version 3.6.7 default Oct 22 2018 11 32 17 n GCC 8.2.0 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version GPU model and memory Describe the current behavior Code annotated with tf.function behaves differently depending on whether functions are called and captured as variables or called and passed directly to other functions. In particular it seems the autograph magic doesn t get applied to functions that are only called within the parameter list of other functions. I may be misinterpreting what exactly is going wrong but certainly the behavior shown in the colab below is incorrect. Describe the expected behavior Capturing via an intermediate variable should never change code behavior. Code to reproduce the issue https colab.research.google.com drive 1CcWfHnGkFehUN8LYsbODf fNSsQncE0G ,Thank you for the nice repro Yes that is definitely a bug. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 26793 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 26793 No a Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 26793 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 26793 No a 
26800,saved model cli tensorrt convert bug saved model main op collection and it s related operation was mistakenly pruned.,Hi guys Tensorflow serving has released 1.13.0 recently which adds a support for TF TRT and I m trying to introduce it into our production enviroment. There is a good introduction for this feature https medium.com tensorflow optimizing tensorflow serving performance with nvidia tensorrt 6d8a2347869a According to the post I have to convert the SavedModel into a TRT optimized one first with the help of saved model cli and then serve it in Tensorflow Serving. It all goes well with the example the post provides but it failed in my case. After I converted my own model and served it in Tensorflow Serving the server threw Failed precondition Table not initialized. error. I searched the related source code and finally figured out what happened. There is an index to string subgraph in my model which is mainly composed of a HashTableV2 Operation and a InitializeTableV2 Operation. There s also a collection named saved model main op which finally points to the InitializeTableV2 Operation. When TensorFlow Serving loads the SavedModel it tries to initialize the HashTable via executing the operations in saved model main op collection. But after saved model cli converted the graph into a TensorRT Optimized one The saved model main op collection and its related Operation has been pruned. As a result tf serving failed to initialize the table. This is the partial graph before conversion https raw.githubusercontent.com monklof assets master original graph tb.png graph before conversion After conversion https raw.githubusercontent.com monklof assets master table not initialized bug.png graph after conversion System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 CentOS 7.5 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary source TensorFlow version use command below 1.13.1 Python version 2.7 Bazel version if compiling from source 0.19.2 GCC Compiler version if compiling from source 4.8.5 CUDA cuDNN version Cuda 9.0 cuDNN 7.3 GPU model and memory Tesla V100 16G Describe the current behavior as mentioned above. Describe the expected behavior The saved model main op collection and it s related op should be preserved after conversion. Code to reproduce the issue This is the code for exporting SavedModel the conversion command Other info logs tensorflow model server port 8413 rest api port 8414 model name resnet model base path workdir tmp trttest pb6 2019 03 14 23 14 17.626533 I tensorflow serving model servers server.cc 82 Building single TensorFlow model file config model name resnet model base path workdir tmp trttest pb6 2019 03 14 23 14 17.626858 I tensorflow serving model servers server core.cc 461 Adding updating models. 2019 03 14 23 14 17.626876 I tensorflow serving model servers server core.cc 558 Re adding model resnet 2019 03 14 23 14 17.727170 I tensorflow serving core basic manager.cc 739 Successfully reserved resources to load servable name resnet version 1 2019 03 14 23 14 17.727195 I tensorflow serving core loader harness.cc 66 Approving load for servable version name resnet version 1 2019 03 14 23 14 17.727208 I tensorflow serving core loader harness.cc 74 Loading servable version name resnet version 1 2019 03 14 23 14 17.727228 I external org tensorflow tensorflow contrib session bundle bundle shim.cc 363 Attempting to load native SavedModelBundle in bundle shim from workdir tmp trttest pb6 1 2019 03 14 23 14 17.727242 I external org tensorflow tensorflow cc saved model reader.cc 31 Reading SavedModel from workdir tmp trttest pb6 1 2019 03 14 23 14 17.878579 I external org tensorflow tensorflow cc saved model reader.cc 54 Reading meta graph with tags serve 2019 03 14 23 14 18.977544 I external org tensorflow tensorflow core common runtime gpu gpu device.cc 1433 Found device 0 with properties name Tesla V100 PCIE 16GB major 7 minor 0 memoryClockRate GHz 1.38 pciBusID 0000 05 00.0 totalMemory 15.78GiB freeMemory 15.36GiB 2019 03 14 23 14 18.977591 I external org tensorflow tensorflow core common runtime gpu gpu device.cc 1512 Adding visible gpu devices 0 2019 03 14 23 14 19.762198 I external org tensorflow tensorflow core common runtime gpu gpu device.cc 984 Device interconnect StreamExecutor with strength 1 edge matrix 2019 03 14 23 14 19.762240 I external org tensorflow tensorflow core common runtime gpu gpu device.cc 990 0 2019 03 14 23 14 19.762248 I external org tensorflow tensorflow core common runtime gpu gpu device.cc 1003 0 N 2019 03 14 23 14 19.762739 I external org tensorflow tensorflow core common runtime gpu gpu device.cc 1115 Created TensorFlow device job localhost replica 0 task 0 device GPU 0 with 14843 MB memory physical GPU device 0 name Tesla V100 PCIE 16GB pci bus id 0000 05 00.0 compute capability 7.0 2019 03 14 23 14 20.050500 I external org tensorflow tensorflow cc saved model loader.cc 182 Restoring SavedModel bundle. 2019 03 14 23 14 20.050589 I external org tensorflow tensorflow cc saved model loader.cc 192 The specified SavedModel has no variables no checkpoints were restored. File does not exist workdir tmp trttest pb6 1 variables variables.index 2019 03 14 23 14 20.050607 I external org tensorflow tensorflow cc saved model loader.cc 285 SavedModel load for tags serve Status success. Took 2323360 microseconds. 2019 03 14 23 14 20.050644 I tensorflow serving servables tensorflow saved model warmup.cc 101 No warmup data file found at workdir tmp trttest pb6 1 assets.extra tf serving warmup requests 2019 03 14 23 14 20.050759 I tensorflow serving core loader harness.cc 86 Successfully loaded servable version name resnet version 1 2019 03 14 23 14 20.081590 I tensorflow serving model servers server.cc 313 Running gRPC ModelServer at 0.0.0.0 8413 ... warn getaddrinfo address family for nodename not supported 2019 03 14 23 14 20.090390 I tensorflow serving model servers server.cc 333 Exporting HTTP REST API at localhost 8414 ... evhttp server.cc 237 RAW Entering the event loop ... 2019 03 14 23 14 29.297322 I external org tensorflow tensorflow contrib tensorrt kernels trt engine op.cc 496 Building a new TensorRT engine for map while TRTEngineOp 1 with batch size 224 2019 03 14 23 14 29.541322 W external org tensorflow tensorflow contrib tensorrt log trt logger.cc 34 DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output. 2019 03 14 23 14 29.541399 W external org tensorflow tensorflow contrib tensorrt log trt logger.cc 34 DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output. 2019 03 14 23 14 29.541421 W external org tensorflow tensorflow contrib tensorrt log trt logger.cc 34 DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output. 2019 03 14 23 14 31.924073 I external org tensorflow tensorflow contrib tensorrt kernels trt engine op.cc 496 Building a new TensorRT engine for TRTEngineOp 0 with batch size 224 2019 03 14 23 14 47.935236 W external org tensorflow tensorflow core framework op kernel.cc 1401 OP REQUIRES failed at lookup table op.cc 809 Failed precondition Table not initialized. My Solution I have fixed this bug in my way and it works in my case but I m not sure if it is the correct way to do so. Also I d like to contribute to tensorflow but there are so many versions and branches of tensorflow which one should I send a Pull Request to The Patch https github.com monklof tensorflow pull 1 files Thanks for checking this issue I m looking forward to hearing from you soon. ,Hi smit hinsu would you please help to take a look Thanks Thanks for the detailed report and also proposing a solution Pull requests should be sent to the master branch. You can refer to guidelines for contributing code to TensorFlow at https www.tensorflow.org community contribute code. Let us know if you have any other questions smit hinsu thanks for the review I ll update the Pull request later. smit hinsu Hi I ve updated the pull request. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 26800 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 26800 No a 
26809, TF 2.0 Cannot load Keras Sequential model witn InputLayer from h5., System information Have I written custom code as opposed to using a stock example script provided in TensorFlow YES OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Debian Stable Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below CPU both TF 2.0.0a0 and tf nightly 2.0 preview 2.0.0.dev20190315 Python version 3.5.3 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior When a Sequential Keras model contains InputLayer and it is saved it cannot be loaded and fails with a message Describe the expected behavior The model can be loaded correctly. Code to reproduce the issue Other info logs The problem is in the fact that Sequential.layers does not return the InputLayer . To quote comments from the method The problem is that if InputLayer was added manually with a specified input shape then it is an error not to serialize it because then the following layers do not know what the input shape is. Workarounds if instead of InputLayer you add input shape to the Dense layer it works. However input dtype cannot be specified in this way and when you pass tf.int32 on input using an InputLayer is required in Functional API the tf.keras.layers.Input is serialized in the model and funnily as a InputLayer . Solutions when serializing a Sequential model all layers need to be serialized and no filtering of InputLayer performed .,I ran into the same issue in tensorflow 1.13.1 as well. I don t understand why the InputLayer is being filtered out while serializing the model. A work around which I found online is to add the batch input shape parameter to the first layer in the model file. This is fixed in latest TF 2.0 nightly build 2.0.0 dev20190718 Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 26809 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 26809 No a 
26835, TF2.0 KerasLayer cannot be loaded from .h5, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Run on Colab example from copied from tensorflow hub https colab.research.google.com drive 1ymmzlWHieCuXR7UjXezULnYsNsDeHc9m Version 2.0.0 alpha0 Eager mode True Hub version 0.3.0 GPU is available The issue When I try to run Colab example from tensorflow hub for image retraining and then save load model according to the documentation https www.tensorflow.org alpha guide keras saving and serializing there are some issues with it. After retraining when we try to save the model This error occurs already described here 26811 But actually path to my model.h5 file is created. Then when I try to load it via I get an error Entire stacktrace Entire colab to review https colab.research.google.com drive 1ymmzlWHieCuXR7UjXezULnYsNsDeHc9m,same problem here. Same for Unknown layer DenseFeatures same for Unknown layer DenseFeatures Same issue for LSTM layers. When using ModelCheckpoint callback. File h5py objects.pyx line 54 in h5py. objects.with phil.wrapper File h5py objects.pyx line 55 in h5py. objects.with phil.wrapper File h5py h5o.pyx line 202 in h5py.h5o.link RuntimeError Unable to create link name already exists Same here. Is there any update on this I m wondering if it s a good idea to use tf.keras.applications.MobileNetV2 model instead of the module loaded from TF Hub. It seems working after quick checks tested model saving in this example https www.tensorflow.org alpha tutorials images transfer learning Not perfect but there is a workaround SImply create the model from scratch every time instead of loading from JSON YAML and then load the weights. When using the DenseFeatures layer the model will not be built after calling compile so you will not be able to load the weights right away. You must either manually call the build method or run a single batch through the model and then load the weights. I think I found the solution or another workaround . Instead of Call Instead of Call Works like a charm. Colaboratory with the code above https colab.research.google.com drive 17YerjXGUwqp48mB3 WSTipv4itizuOCP I use follow this code it work https colab.research.google.com drive 1JLkVWpXhDwImuF930i5PWstJvu4ENDoc The crash with hub.KerasLayer likely needs to be fixed in tensorflow hub not tensorflow. I ve filed issue https github.com tensorflow hub issues 287 for that. Can this issue here be closed then Same problem. The crash with hub.KerasLayer and an image module from tfhub.dev should be fixed by updating to their latest versions https tfhub.dev google tf2 preview inception v3 feature vector 3 https tfhub.dev google tf2 preview mobilenet v2 feature vector 3 For details see issue https github.com tensorflow hub issues 287 Can this issue be closed now or is anyone using it to track the DenseLayer and LSTM problems with similar error messages but probably different root causes Any updates on ValueError Unknown layer DenseFeatures Have the same issue when do model.save model.h5 under tensorflow 1.13.1 Have the same issue when do model.save model.h5 under tensorflow 1.13.1 Recommended to install tf nightly or tf nightly gpu also and test follow https github.com tensorflow tensorflow issues 26835 issuecomment 488972247 resolved by updating tf version to 2.0 iPhone 2019 5 20 17 47 OooO notifications github.com mailto notifications github.com Have the same issue when do model.save model.h5 under tensorflow 1.13.1 Recommended to install tf nightly or tf nightly gpu also and test follow 26835 comment https nam02.safelinks.protection.outlook.com url https 3A 2F 2Fgithub.com 2Ftensorflow 2Ftensorflow 2Fissues 2F26835 23issuecomment 488972247 data 02 7C01 7C 7C7e2e94c948854a0bf13508d6dd08346b 7C84df9e7fe9f640afb435aaaaaaaaaaaa 7C1 7C0 7C636939424643466257 sdata M5tDagIFSyWvX 2Bh 2BX1eWmEPBeT8Xeaqrb7HyaTHTWHI 3D reserved 0 You are receiving this because you commented. Reply to this email directly view it on GitHub https nam02.safelinks.protection.outlook.com url https 3A 2F 2Fgithub.com 2Ftensorflow 2Ftensorflow 2Fissues 2F26835 3Femail source 3Dnotifications 26email token 3DAGN6QFXOJJJ5TODYI7LYTWLPWJXT5A5CNFSM4G7HUWZKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVYIHMY 23issuecomment 493913011 data 02 7C01 7C 7C7e2e94c948854a0bf13508d6dd08346b 7C84df9e7fe9f640afb435aaaaaaaaaaaa 7C1 7C0 7C636939424643476268 sdata LbB7PLQ68j1qcnPjTiwa67qxvU80XsE 2B9jCNdXeEPL8 3D reserved 0 or mute the thread https nam02.safelinks.protection.outlook.com url https 3A 2F 2Fgithub.com 2Fnotifications 2Funsubscribe auth 2FAGN6QFRPEUTENZOMTXRUTH3PWJXT5ANCNFSM4G7HUWZA data 02 7C01 7C 7C7e2e94c948854a0bf13508d6dd08346b 7C84df9e7fe9f640afb435aaaaaaaaaaaa 7C1 7C0 7C636939424643486273 sdata 8UOTn 2B 2Bu6IBnXmjw0omM6PKA1gLIqASEi 2BexYO5LsmQ 3D reserved 0 . Same for Unknown layer DenseFeatures I find a solution https github.com tensorflow tensorflow issues 26496 issuecomment 477246896 https github.com tensorflow tensorflow issues 26496 issuecomment 477246896 Has anyone found a solution for how I can save a model with a feature layer. Right now I saved the weights of my model and then I rebuild the model and fit the model with epochs 0 seems to do the trick for now. The results are repeatable but this seems like it is not the most professional way to do it. This is fixed in latest tf nightly build 2.0.0 dev20190802 . Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 26835 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 26835 No a I think I found the solution or another workaround . Instead of Call Instead of Call Works like a charm. Colaboratory with the code above https colab.research.google.com drive 17YerjXGUwqp48mB3 WSTipv4itizuOCP Thank you It is worked for me in this way My error was ValueError Unknown layer BatchNorm . As I have used KL.BatchNormalization class to compose this custom layer I solved the error as follows Not perfect but there is a workaround SImply create the model from scratch every time instead of loading from JSON YAML and then load the weights. When using the DenseFeatures layer the model will not be built after calling compile so you will not be able to load the weights right away. You must either manually call the build method or run a single batch through the model and then load the weights. This is what I do every time but is there a faster way same unknown layer kerasLayer I use follow this code it work https colab.research.google.com drive 1JLkVWpXhDwImuF930i5PWstJvu4ENDoc don t miss after custom objects Oh thanks for pointing out. I was also wondering why my code didn t run last time. On Wednesday 27 May 2020 05 20 00 pm GMT 8 AlAlaa Tashkandi notifications github.com wrote I use follow this code it work model.save . model saved.h5 new model tf.keras.models.load model . model saved.h5 custom objects KerasLayer hub.KerasLayer new model.summary https colab.research.google.com drive 1JLkVWpXhDwImuF930i5PWstJvu4ENDoc don t miss after custom objects You are receiving this because you commented. Reply to this email directly view it on GitHub or unsubscribe. Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template System information Run on Colab example from copied from tensorflow hub https colab.research.google.com drive 1ymmzlWHieCuXR7UjXezULnYsNsDeHc9m Version 2.0.0 alpha0 Eager mode True Hub version 0.3.0 GPU is available The issue When I try to run Colab example from tensorflow hub for image retraining and then save load model according to the documentation https www.tensorflow.org alpha guide keras saving and serializing there are some issues with it. After retraining when we try to save the model This error occurs already described here 26811 But actually path to my model.h5 file is created. Then when I try to load it via I get an error Entire stacktrace Entire colab to review https colab.research.google.com drive 1ymmzlWHieCuXR7UjXezULnYsNsDeHc9m use this tf.keras.models.load model model.h5 custom objects KerasLayer tfhub.KerasLayer Use this code instead tf.keras.models.load model DogVisionModel.h5 custom objects KerasLayer tfhub.KerasLayer Thanks I will update the code according to your post. I really appreciate your effort. On Sunday 31 May 2020 04 52 21 pm GMT 8 Philip Purwoko notifications github.com wrote Use this code instead tf.keras.models.load model DogVisionModel.h5 custom objects KerasLayer tfhub.KerasLayer You are receiving this because you commented. Reply to this email directly view it on GitHub or unsubscribe. I use follow this code it work https colab.research.google.com drive 1JLkVWpXhDwImuF930i5PWstJvu4ENDoc Adding the KeralLayer inside the model from json command also did the trick for me. 
26902,tf upgrade v2 does not preserve file attributes and symbolic links, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow n a OS Platform and Distribution e.g. Linux Ubuntu 16.04 archlinux Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device n a TensorFlow installed from source or binary binary TensorFlow version use command below tf2 preview nightly yesterday Python version 3.7 Bazel version if compiling from source n a GCC Compiler version if compiling from source n a CUDA cuDNN version n a GPU model and memory n a 1. tf upgrade v2 changes executable files to non executable files. I expect executable files are still executable after the upgrade. 2. tf upgrade v2 always changes symbolic links to regular files. However I expect 1 For in place upgrade modify the file the link points to but the symbolic link should be the same. 2 For non in place upgrade if intree and outtree is used symbolic links which point to files within the tree should become symbolic links pointing to the new file in the outtree. Symbolic links which point to external files should become a regular file. 3 For non in place single file upgrade the output should be a regular file.,Hi ppwwyyxx Do you have a simple example where tf upgrade v2 changes a file to be non executable I just tried the following After these commands are run test2 bar.sh does have executable permissions. Could it be instead that file ownership changed Sorry for delay looking at this issue. I submitted a change to fix symbolic link behavior. However the behavior is slightly different than you proposed. Specifically I keep symlink unchanged if it points to external file as opposed to converting it to regular file . I was thinking that someone who has code in a different directory might want to update it separately while keeping the symlink. I will keep this issue open for the executable permission. Per my previous comment I need some example to reproduce it. ppwwyyxx Can you check whether the issue resolved in the TF2.0 or tf nightly If this was resolved please close the issue. Thanks It has been 44 days with no activity and the awaiting response label was assigned. Is this still an issue Closing due to no other updates Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 26902 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 26902 No a 
26980, TF2.0a0 Distribute Strategies and Summary Writer don t work together, System information OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04 TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 alpha0 Python version python 3.6 CUDA cuDNN version 10.1 GPU model and memory TITAN X Describe the current behavior I am trying to use tf.summary at the same time as using distribute strategies. However the call to strategy.experimental run seems to end the scope of the summary writer . I have initialized it properly but it is not available to me. I have tried passing the SummaryWriter object directly as another parameter of the train step but it does not work and fails with the error message. Somehow the scope seems to be lost through the experimental run call. For the most part I followed this guide on how to work with DuplicateStrategies https www.tensorflow.org alpha tutorials distribute training loops Describe the expected behavior Being able to write to write to the summary from within the training loop. Code to reproduce the issue ,Thanks for reporting we will take a look at it. Issue identified fix is on the way should be submitted today Hi Johannes Could you please test if https github.com tensorflow tensorflow commit 7bdb838313d322773908cf8368c03ec98117ad55 solves your issue Thanks I guess it is already included in the most recent nightly isn t it It seems to be working now. Yeah nightly is less than one day old. Good to know it works for you Closing this issue. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 26980 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 26980 No a 
27043,In Eager mode batch norm doesn t support Second order derivative, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 CentOS Linux release 7.4.1708 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below 1.12.0 and 1.10.1 Python version 3.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version CUDA 9.0 cuDNN 7 GPU model and memory TITAN Xp Describe the current behavior I wanted to implement an eager mode version of the large margin code from google research code https github.com google research google research tree master large margin . In other words I computed a second order derivative. If my model network didn t contain enable the batch norm layers it was fine. But if I enabled the batch norm layer it said that tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead. however I didn t use tf.gradients and other layers didn t raise such an error. The static graph code work fine on both cases with or without batch normalization Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs ,This appears to be working as of the latest tf nightly Great I installed the tf nightly and this bug has been solved. Perfect Thank you Glad to hear it Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27043 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27043 No a 
27049,Resuming training from saved checkpoint produces different result than uninterrupted training, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes Google Collab https colab.research.google.com drive 1ZRhX4VBEWo4fh4zbXL p HDCUGTRelnH OS Platform and Distribution e.g. Linux Ubuntu 16.04 Google Collab TensorFlow installed from source or binary Google Collab TensorFlow version use command below 1.13.1 Python version Python 3 Describe the current behavior Loading a model with tf.keras.models.load model produced with tf.keras.callbacks.ModelCheckpoint and resuming training produces different results from running the training without save model restore model in the middle. Describe the expected behavior Saving and restoring the model should allow to resume training as if there was no interruption in the first place. Code to reproduce the issue Google Collab https colab.research.google.com drive 1ZRhX4VBEWo4fh4zbXL p HDCUGTRelnH Other info logs No interruption Epoch 49 100 0s loss 3.5190 val loss 3.3597 Epoch 50 100 0s loss 3.4090 val loss 3.2668 Epoch 51 100 0s loss 3.2637 val loss 3.1623 Epoch 52 100 0s loss 3.0962 val loss 2.9975 With interruption Epoch 49 50 0s loss 3.5190 val loss 3.3597 Epoch 50 50 Epoch 00050 saving model to weights.50.ckpt 0s loss 3.4090 val loss 3.2668 ... load model weights.50.ckpt ... Epoch 51 100 0s loss 3.2637 val loss 3.3816 Epoch 52 100 0s loss 3.3175 val loss 3.1457 The model does not have any random elements so it looks like optimizer state is lost. ,Model.fit is taking random slices of the data and batching them together. If you control for that e.g. make all of the examples identical can you still reproduce allenlavoie the model will converge instantly if there s just 1 training sample. I added shuffle False to .fit ... calls. The issue persisted. notebook updated . Also this should not have changed anything at all since Has no effect when steps per epoch is not None. which was 1 in my original sample anyway. Indeed the losses after this change are the same. Hrm. It looks like it s passing include optimizer to Model.save. tanzhenyu any ideas lostmsu The previous keras optimizer is problematic. We have provided a new set of optimizers under exactly the same name and fully backward compatible for users. However as unfortunate as this is the new ones did not make to tf 1.13.1. If you do pip install tf nightly gpu 2.0 preview then you should be able to see identical behavior. That said you need to change one line from tf.set random seed seed to tf.random.set seed seed tanzhenyu confirming seems to be fixed in 2.0 preview Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27049 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27049 No a 
27086,Tensorflow 2.0 Adding tf.function decorator with categorical feature column raises FailedPreconditionError Table already initialized, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac OS X 10.14 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 alpha0 Python version Python 3.6.7 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior Just want to preface this by saying while strictly speaking I am using custom code all I did was piece together code from the classify structured data and the introduction for experts . I am trying to add tf.function for performance enhancement reasons to my custom training code in Tensorflow 2.0. However running the code raises a FailedPreconditionError Table already initialized. when using categorical feature columns. What I suspect is happening is that the lookup tables for the categorical encoding are being initialized more than once in the model definition and in tf.function . How should I go about using feature columns and tf.function Code to reproduce the issue Actual Output Expected Output , alextp how to combine tf.function with feature columns rohan100jain I thought you had already fixed this issue Thank you for responding Btw interesting enough I just added a counter and the model managed to run for one epoch before crashing ... EDIT Just updated the code in the issue to show the behavior with the counter added... Ok the code below solves the problem adding a dummy counter for both training and testing steps ... but I am not sure why thoughts rohan100jain Not incrementing the counter i.e. passing a fixed value causes the error to be rasied again seems to me like counter is acting like an incrementing global step which tells the model not to initialize the lookup tables again ... rohan100jain did you have time to take a look https github.com tensorflow tensorflow commit f6e0ec468a3dee7c2ae1fa3575bb46d884b4a319 should hopefully fix this issue Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27086 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27086 No a 
27102,tf.py function InternalError in distributed mode, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04.6 LTS TensorFlow installed from source or binary pip TensorFlow version use command below 1.13.1 Python version 3.6.7 Describe the current behavior I m testing tensorflow distributed on the same machine. I opened two separate shells and created a cluster using on shell 0 and on shell 1. On shell 1 I ran which returns the error InternalError see above for traceback expected the py func to return a Tensor backed by memory in job local replica 0 task 1 device CPU 0 but is actually in job localhost replica 0 task 0 device CPU 0. This is a bug. while this does not happen if I replace tf.py function in the code above with tf.py func. Describe the expected behavior Code should run successfully like when using tf.py func and should print the arrays array 1. 1. 1. 1. ... 1. 1. 1. 1. array 1. 1. 1. 1. ... 1. 1. 1. 1. array 2. 2. 2. 2. ... 2. 2. 2. 2. ,I believe this has been fixed in nightly. Can you try Hi alextp I confirm this issue has been fixed on 1.14.1 dev20190325. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27102 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27102 No a 
27190,Losses with Reduction.NONE do not keep the input shape the result is averaged along the last axis , System information OS Platform and Distribution Linux Manjaro TensorFlow installed from binary TensorFlow version use command below 2.0.0 dev20190326 nightly build Python version 3.7.2 Describe the current behavior Currently when a loss object is created with reduction tf.losses.Reduction.NONE the result is averaged along the last axis. This happens with all the losses in the tf.losses module. Describe the expected behavior When a loss object is created with reduction tf.losses.Reduction.NONE the output shape should match the input shape y true and y pred shape . This is also reported in the doc tf.losses.Reduction https www.tensorflow.org versions r2.0 api docs python tf losses Reduction Moreover when a sample weight parameter BinaryCrossentropy https www.tensorflow.org versions r2.0 api docs python tf losses BinaryCrossentropy is provided with the same shape of y pred it throws an exception. Code to reproduce the issue Loss function with Reduction.NONE CURRENT RESULT 0.34657347 EXPECTED RESULT 0. 0. 0.69314694 0.69314694 Loss function with Reduction.NONE and sample weight CURRENT RESULT Exception EXPECTED RESULT 0. 0. 1.3862939 0.69314694 Other info logs Currently to achieve the expected result an additional fake dimension must be added to y true and y pred as in the following example. , amitsrivastava78 Can you please take a look at this issue Thanks matgad actually BinaryCrossentropy uses the function nn.sigmoid cross entropy with logits which takes into account the reduction and applies applying the below formula max x 0 x z log 1 exp abs x where in your case y pred x and y true z. So based on this the computed size is same as that of input but there is a twist in this before the actual result is passed it is done a reduce mean with axis as 1 which means it will add the axis with input dimension and do the mean on tat axes which s the result you are getting 0.34657347 Hope this clears things. Regards Amit Hi amitsrivastava78 thanks for the answer. The behavior that you described is the default one. But when you define a loss with the optional parameter reduction tf.losses.Reduction.NONE the last reduce mean or any other reduce should be skipped. This issue only applies with None reduction. BinaryCrossentropy reduction tf.losses.Reduction.NONE is just an example the same behavior should be extended for any loss function. matgad i see your point i think this is a BUG irrespective of the reduction the tensorflow is applying the reduce mean to everything in the coming days i will try to raise one PR to fix this issue. Regards Amit Great many thanks. Please also take a look on the exception that is thrown when sample weight has the same shape of the input see the example in my first post for details I think the two issues are related. matgad i have raised the PR and included your cases as unit test cases as well Lets wait for this PR to merge in the mean time you can use this PR in case you are blocked. 27784 Regards Amit Wonderful thank you. matgad amitsrivastava78 The loss functions actually do not perform any reduction by default. The expectation is that the input passed to the functions will be atleast 2D. These functions return one loss value per sample as output. Eg if y true and y pred have the shape 3 3 return value will be of shape 3 . we get one loss value per sample if y true and y pred have the shape 3 2 4 return value will be of shape 3 2 we get one loss value per sample per timestep etc. Sorry that the functions do not have any documentation about this currently. We are working on adding that please feel free to contribute to the documentation if you are interested. I just replied on the PR with this as well. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27190 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27190 No a Why would issue be closed before updating the documentation I just ran into the problem because documentation specifically said reduction Reduction.NONE would give me same shape as I send in. pat coady sure we can leave this issue open until we have documentation updated. pavithrasv If it is helpful I can update docstrings in appropriate places and submit the PR. I haven t contributed to the project before but am reviewing Contributing Guidelines now. pat coady Oh yes absolutely. You are welcome to make contributions anytime As mentioned in my comment above we will need to add documentation for all standalone functions in losses.py https github.com tensorflow tensorflow blob master tensorflow python keras losses.py L752 indicating that the input needs to be at least 2D. Submitted PR https github.com tensorflow tensorflow pull 30463 Great thank you Automatically closing this out since I understand it to be resolved by the PR 30463 merged already but please let me know if I m mistaken.Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27190 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27190 No a jvishnuvardhan This still is NOT resolved even with TF2.2. reduction none needs to mean reduction equals none...not sure why this would still not be the case. pGit1 This is expected and this is what pavithrasv described in detail here https github.com tensorflow tensorflow issues 27190 issuecomment 487233022 . Please check the usage examples and description here https www.tensorflow.org api docs python tf keras losses BinaryCrossentropy version nightly . Please let me know if there is anything I am missing. Thanks jvishnuvardhan I see that but it still doesn t make sense to me. I discovered the tf.nn.sigmoid cross entropy by accident and it behaves exactly as one would expect when there is no reduction i.e. element wise loss scalars . pavithrasv explanation doesn t makes sense to me and the documentation doesn t either. It s is weird to me that no reduction actually leads to a loss in tensor dimension...I just cannot wrap my head around on what sample means in pavithrasv explanation or the official documentation. If I am comparing two 3x3 tensors with no reduction I have no idea why I am not getting a 3x3 loss tensor out. I really may be just misunderstanding what sample means here. It almost seems like by sample you mean row in y pred and y true tensors. Its as if you actually do compute an element wise loss and once this is done you take the mean along the 1 dimension. But the fact that a mean or sum is taken means there is a reduction by definition. I really dislike this inflexibility. It would be nice if reduction none actually meant no reduction in tensor dimension. This behavior means of all the useful losses that tf.losses already provides would need to be re implemented or do the adding an extra dimension workaround that matgad pointed out. Again reduction equals none should not mean the software takes an average for me. I ve updated and confirmed my example above No reduction should mean no reduction. Even if the documentation points out the behavior it is still incredibly inflexible and frustrating . pGit1 Sorry about that. I understand this can be confusing. This is by design and has been since the beginning of Keras so any changes to this will be a big breaking change. If your data is 1D we request that this be converted to 2D as losses expect 2D data. You will get 1D data back ie. one loss value per example when reduction is None. pavithrasv I guess adding an extra dimension solution isnt so bad but I still think its weird. Thanks for clarifying. pavithrasv jvishnuvardhan Quick question by the way while on this topic. I wonder what tf.keras is actually doing with tensors in general because if I use a tf.keras.Reshape layer I can literally reshape the layer to anything I want and it will still work. Also I can have an output layer for a model of some fixed dimension and pass it a tensor of different dimension during training and it still works. Why is that What is going on now Im using TF 2.2 by the way. Also tf.reshape behaves as it should i.e. it breaks when I try to arbitrarily reshape a tensor with more or less elements than input tensor... Hi amitsrivastava78 thanks for the answer. The behavior that you described is the default one. But when you define a loss with the optional parameter reduction tf.losses.Reduction.NONE the last reduce mean or any other reduce should be skipped. This issue only applies with None reduction. BinaryCrossentropy reduction tf.losses.Reduction.NONE is just an example the same behavior should be extended for any loss function. That was exactly the case in my custom loss function created in class inherited from tf.keras.losses.Loss . After deletion of all reduce mean reduce sum the function works like a charm. Thank you so much for your short explanation If we set the reduction in our binary cross entropy loss to tf.losses.Reduction.NONE when we compile our model is there a way to retrieve the unreduced losses when we evaluate on a sample I m running into an issue where the loss in my model with zeroed out weights and bias code below on how i m doing this is not what I m expecting. So I d like to diagnose how my model is calculating the loss by looking at output from the 3 stages of reduction NONE SUM and SUM OVER BATCH SIZE Issue still exists on CoLab 
27255,Issues with tf.function and abstracted classes, System information Have I written custom code YES OS Platform and Distribution Ubuntu 18.04 Mobile device NO TensorFlow installed from source or binary PyPI TensorFlow version use command below 2.0.0 alpha0 Python version 3.6.4 Bazel version if compiling from source NA GCC Compiler version if compiling from source NA CUDA cuDNN version NA GPU model and memory NA Describe the current behavior When using tf.function on a function that involves abstract classes e.g. for instance layers or other NN abstractions Tensorflow fails with the error Describe the expected behaviour To work as though if it was run without tf.function e.g. in Eager Mode . Code to reproduce the issue Other info logs This seems to stem somehow from the inheritance of abstract methods. ,It seems that the error stems from the using calls of the type I can confirm that error trace with TF2.0.0 alpha0. When I ran with tf nightly the error is as follows. TypeError Traceback most recent call last ipython input 2 f79db8f201dc in module 84 85 if name main 86 main ipython input 2 f79db8f201dc in main 80 loss train one step params data 81 epoch 1 82 print Epoch loss 0.3f .format epoch loss 83 84 TypeError unsupported format string passed to Tensor. format I just ran this in a colab with version 2.0.0 dev20190405 and it runs without an error so I think this has been fixed. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27255 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27255 No a 
27275,The training value of tf.keras.Model.call becomes None when tf.keras.Model.fit . tf2.0.0 alpha0 ,I read the document https www.tensorflow.org versions r2.0 api docs python tf keras Model call and implemented a model with tf.keras.Model as a subclass but the value of training argument becomes None . Is this a bug Or is my understanding wrong This is a basic question but please let me know. Here is the simplified code. System information macOS Mojave 10.14.3 TensorFlow installed from pip TensorFlow version 2.0.0a0 Python version 3.5.6 using pyenv , 1 I noticed the same bug and I m also interested in the solution. I use Ubuntu 16.04.1 Python 3.7 in Anaconda Tensorflow installed from pip Thanks for the bug Was able to repro will update when a fix is available A fix should now be available in the latest nightly Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27275 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27275 No a 
27292,keras.layers.RNN with contants , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Arch Linux Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary No TensorFlow version use command below 1.13 and 1.14 Python version 3.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 10.1 and 10.0 GPU model and memory 1080 Ti Describe the current behavior TypeError can only concatenate list not tuple to list in RNN build if a call the RNN with a Tensor as constants. Describe the expected behavior Basically the build function of the RNNCellWithConstants should be called with the input shape 3 3 5 3 3 Code to reproduce the issue Other info logs Exception from example If I correct the error temporarily I come to another problem that the input shapes at build call are not correct any more 3 5 5 So I think the mistake lies in that distinction If I set is keras tensor to True everything will behave as expected.,I want to work on this issue please ymodak can you guide me please Should be fixed by https github.com tensorflow tensorflow commit 3e8a80bce0f7ef0ab2ee49f3528a2652f26110f0 now. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27292 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27292 No a I wanted to reopen this because it now seems that its impossible to build the cell based on the shape of any constants since the cells build method is only ever called with the timestep input shape. We can pass constants into the cells call method but without having some sort of weights to go along with it there s not much functionality 
27298,Tensorflow 2.0 tf.name scope has no effect on weights created by keras functional api, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 macOS 10.14 TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 alpha0 Python version 3.7.1 Describe the current behavior tf.name scope does not effect the name of weights created by keras.layers output in tf 2.0.0 alpha0 Describe the expected behavior tf.name scope is applied to wights created by keras.layers output in tf 1.13.1 is the expected behavior Code to reproduce the issue Other info logs I was able to reproduce the output of tf 1.13 in tf 2.0.0 by using the following code keras.layers is using the keras graph which overrides the name scope generated by tf.name scope in the default graph I think the problem is actually caused by tf.name scope incorrectly set has symbolic input in eager to False when we are building graph using the keras functional api. since the input here is keras.Input the name scope should be applied to keras graph instead of default graph ,In TF 2.0 the Eager Execution is set as Default therefore in eager execetion the TF completely ignores the name scope . I think this ops is particularly designed for Graphs. Therefore you can do all the above things when Graphs are set as Default I would disagree here in a real eager execution scenario keras layers does work with tf.name scope. output I don t think keras should have inconsistent behavior when the input type change from numerical to symbolic. tf.name scope v1 used to take a variable parameter for this exact case but it s removed in tf2. In fact an old keras tutorial https blog.keras.io keras as a simplified interface to tensorflow tutorial.html specifically mentioned tf.name scope is compatible with the functional api which is no longer true in tf2. I don t know if this is indeed the expected outcome but this definitely needs more clarification since name scope is pretty essential for building complex models. Maybe you re right. Lets see what does the Member of Tensorflow have to say about this. Please do not hesitate to add something to my Stackoverflow issue TensorFlow 2.0 how to group graph using tf.keras tf.name scope tf.variable scope not used anymore https stackoverflow.com questions 55318952 tensorflow 2 0 how to group graph using tf keras tf name scope tf variable sco 55376814 noredirect 1 comment97651032 55376814 However I also think this is actually a bug. tf.Variable works as intended The variable is placed within the scope foo bar . Eager execution as already demonstrated also works using tf.keras.layers ... ...while simply building a model with tf.keras.layers completeley ignores tf.name scope name scope is definitely not working very nicely in tf2 throwing in some of my findings here. In eager mode with symbolic input tf.name scope pushes scope into default context here https github.com tensorflow tensorflow blob 6a603d8cf6df64cb9bd7bbac33245a02821cf198 tensorflow python framework ops.py L6400 In keras when layers gets call with symbolic input it tries to build a graph and replace the current context with the keras graph here https github.com tensorflow tensorflow blob 6a603d8cf6df64cb9bd7bbac33245a02821cf198 tensorflow python keras engine base layer.py L580 However if the input is numpy call follows this code path https github.com tensorflow tensorflow blob 6a603d8cf6df64cb9bd7bbac33245a02821cf198 tensorflow python keras engine base layer.py L644 which correctly inherits the parent tf.name scope The weirdness happens because tf.name scope is not in the same graph as keras model. I think the best solution might be providing tf.name scope an extra parameter like it did before to specify the input type. But if there is a more TFonic way to merge scopes without doing it explicitly that be great. I m seeing this too ... does anyone know in what direction this is moving I could reproduce the issue in tf nightly 2.0 preview . Please take a look at the gist https colab.sandbox.google.com gist jvishnuvardhan d517caf07e65b06cb4209c55e1de0025 tf27298.ipynb . Thanks zzh8829 rchao are there any updates Any plans to fix the name scope integration I found the name scope worked correctly if I additionally wrapped the with tf.name scope ... with with tensorflow.python.keras.backend.get graph .as default . This was sort of mirroring this line https github.com tensorflow tensorflow blob 6a603d8cf6df64cb9bd7bbac33245a02821cf198 tensorflow python keras engine base layer.py L580 My guess was that the name scopes were specific to eager mode vs graph based mode and the normal Keras model construction forced the graph based mode so you want to specify the name scope in graph mode. This is a very vague guess and I don t know if this causes other issues. Using the normal tensorflow.keras.backend also didn t work I had to use tensorflow.python.keras.backend . I find similar problem and here is one solution for name scope control with Sequential rather than tf.name scope for tf 2.0 keras It works as expected inside the build phase. You re in a functional graph and name space stack there. jvishnuvardhan zzh8829 any updates rchao does not seem to be active in this issue. zzh8829 I think this was resolved in recent tf nightly . I could not reproduce with tf nightly . Here https colab.research.google.com gist jvishnuvardhan 1ece1f2942cfb0aadbd5a72e693e867d untitled56.ipynb is the colab gist for your reference. Please verify once and if this was resolved for you then close the issue. Thanks zzh8829 Can you verify once and close the issue if this was resolved for you. Thanks zzh8829 I am closing this issue as this was resolved in recent tf nightly . Please feel free to reopen if this issue was not resolved for you. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 27298 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 27298 No a jvishnuvardhan this issue not resolved in 2.2.0 including latest nightly. Please see this gist https gist.github.com Daniel451 03d173825235369be46b27e06e5e109a Daniel451 Can you please open a new issue with your standalone code. I think the second example in your code uses graph context and providing scope name works well as shown below. Please check the gist here https colab.research.google.com gist jvishnuvardhan 79327bcdc98a09d3f72afcec18d076c7 untitled109.ipynb . Output is Please open a new issue for further question on this issue. Thanks jvishnuvardhan I can t help but notice that the tf.name scope is a no op in that example. The following gives the same output What we want is for tf.name scope to work not a tutorial on how to work around it by manually calculating values for name How is this still not resolved in tf 2.3.0 tgsmith61591 I have been able to use it since tf 2.1 Have you put it under a with block Like nicolasshu this is not what this issue is about. The problem persists during graph mode when constructing the model. Your example is just another variation of jvishnuvardhan demonstration that given an actual call eager mode name scope is inferred correctly. However as phsyron already stated the issue persists for graph mode model construction your example only works because you subclass tf.keras.Model and infer name scope in call i.e. during runtime with actual inputs x given. We were talking about correctly inferring name scope during model construction and only once as well as a correct inferring during graph mode. jvishnuvardhan I actually lost track of this issue but it still persists. If opening a new issue is necessary I ll do this and supply a standalone Gist soon. Thank you for your help time. Daniel451 If you don t mind please open a new issue as there are many comments above to follow the actual problem. In the new issue you can refer this issue for further details. Thanks I was wondering how tf.keras.layers.Layer is always able to append the prefix to all the operations inside while tf.name scope doesn t work for tf.keras.Input . Looks like the key lies in here https github.com tensorflow tensorflow blob da6568a tensorflow python keras engine base layer.py L875 L878 . Reopened it as this is still an issue with tf nightly 2.9.0 dev20220222. Thanks It is still replicating in 2.9 https colab.sandbox.google.com gist mohantym 45b195bfa177a64af960f530308f8ad6 untitled56.ipynb scrollTo 659SZpY7a5eu and 2.10.0dev https colab.sandbox.google.com gist mohantym e4c752c98c44fda71e1514a91b4aa441 untitled56.ipynb scrollTo S 9S7X1kKXCH . zzh8829 I tried to replicate this issue using TF v2.11 please find the gist https colab.research.google.com gist sushreebarsa a0acbe789e5f6a4eb0a95423ed85fe2a untitled56.ipynb scrollTo 659SZpY7a5eu and refer to this official doc https www.tensorflow.org guide keras functional . This issue seems to be Keras issue. Please post this issue on keras team keras repo. https github.com keras team keras issues as Keras development is fully moving to github.com keras team keras http github.com keras team keras . All issues and PRs related to keras will be addressed in that repo. To know more see this TF forum discussion https discuss.tensorflow.org t keras project moved to new repository in https github com keras team keras 1999 https discuss.tensorflow.org t keras project moved to new repository in https github com keras team keras 1999 Thank you This issue has been automatically marked as stale because it has no recent activity. It will be closed if no further activity occurs. Thank you. Closing as stale. Please reopen if you d like to work on this further. 
27345, TF2.0 EstimatorV2 uses non existing export savedmodel method,Dear tensorflowers as per the title EstimatorV2 s exporter.py calls a method that has been removed. The fix is pretty easy but I m not sure about the implications of the change. See more below. System information Have I written custom code as opposed to using a stock example script provided in TensorFlow custom code but using estimators OS Platform and Distribution e.g. Linux Ubuntu 16.04 4.19.20 1rodete1 amd64 1 SMP Debian 4.19.20 1rodete1 TensorFlow installed from source or binary pip install upgrade tensorflow 2.0.0alpha0 TensorFlow version use command below 2.0.0 alpha0 Python version 3.7.1 Describe the problem using tf.estimator.train and evaluate estimator train spec eval spec results in AttributeError EstimatorV2 object has no attribute export savedmodel being thrown. 2 small modifications to my local tensorflow estimator python estimator exporter.py file fix the issue. However before submitting a PR i wanted to clarify that I had to 1. Rename the method call to estimator.export saved model ... and this poses no problem. 2. Remove the strip default attrs self. strip default attrs keyword argument https github.com tensorflow estimator blob d14b0dce35baea00f27e17d8a44690080abd7bce tensorflow estimator python estimator exporter.py L120 but I have no idea what this entails. I assume that since you can t specify this argument anymore it defaults to the default policy in TF 1.X of stripping the GraphDef default attributes. If that s the case is there any action required Logs Code I m trying to adapt the CMLE template https github.com GoogleCloudPlatform cloudml samples tree master cloudml template to TF 2.0 . I avoided putting the input fns and model fns since the model trains and evals correctly. The only problem is during the export. Happy to provide any additional information , toby this is the PR for estimator exporter. He need 2 clarification before. We already have export saved model is there anything else needed here Are you saving it was already fixed in a recent nightly If yes I will give a try. Thanks Yes please let me know once you try it using master version. tanzhenyu I tried with the latest nightly tf nightly 2.0 preview 2.0.0.dev20190426 and I get another crash but still related to exporter the crash seems related to the 2 points mention above by MMMarcy Remove the strip default attrs self. strip default attrs keyword argument but I have no idea what this entails. I assume that since you can t specify this argument anymore it defaults to the default policy in TF 1.X of stripping the GraphDef default attributes. If that s the case is there any action required so 1 seems to be fixed but not 2 . Thanks tanzhenyu did you managed to run tf.estimator.LatestExporter with Estimator with nightly tf nightly 2.0 preview 2.0.0.dev20190614 I have new errors Is there something to change with respect to TF 1.1.2 I am not sure now if the issue is with my code or if the issue is still on the TF side. I look at the official doc and I couldn t an a example with TF 2.0 beta tarrade I am assigning this to someone who s more familiar with this topic. Any status on this issue tanzhenyu MMMarcy ymodak I am wondering what is the status of the migration of tf.estimator to Tensorfloe 2.0. It seems few things are not working yet. Probably tf.estimator is will be the last part migrationg to Tensorflow 2.0. For example in the official documentation the example on tf.estimator is very very limited still use compat.v1 module for some ops and don t save the model https www.tensorflow.org beta tutorials distribute multi worker with estimator I did a new test with the latest nightly tf nightly 2.0 preview 2.0.0.dev20190725 and I still have issue I0726 12 01 23.221257 4583781824 estimator.py 2100 Saving checkpoint path summary for global step 10 results Models Mnist tf 1 12 estimator v3 ckpt model.ckpt 10 TypeError Traceback most recent call last anaconda release conda env env gcp dl 2 0 nightly lib python3.6 site packages tensorflow core python framework tensor util.py in make tensor proto values dtype shape verify shape allow broadcast 534 try 535 str values compat.as bytes x for x in proto values 536 except TypeError anaconda release conda env env gcp dl 2 0 nightly lib python3.6 site packages tensorflow core python framework tensor util.py in listcomp .0 534 try 535 str values compat.as bytes x for x in proto values 536 except TypeError anaconda release conda env env gcp dl 2 0 nightly lib python3.6 site packages tensorflow core python util compat.py in as bytes bytes or text encoding 64 raise TypeError Expected binary or unicode string got r 65 bytes or text 66 TypeError Expected binary or unicode string got tf.float32 During handling of the above exception another exception occurred I managed to find where is the issue now I didn t find how this should be in TF 2.0. Normally we can still use gragh and tf.Variable. I couldn t find up to know anything in the doc of tf.estimator.export.ServingInputReceiver or some official example. Any idea or expert that could know that. I have no idea if this is a bug or if I should update the problematic line. Thanks The issue in the OP with the wrong use of export savedmodel should be fixed in 1.14. tarrade That looks like a different bug can you create a new issue with some code to reproduce the errors Although from the code snippet you should be creating a placeholder and not a variable. If you re using a variable the initialization arguments are in a different order. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27345 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27345 No a Excuse me k w w but how do I create a placeholder in tensorflow 2.0 module tensorflow has no attribute placeholder I am trying to update my serving input receiver fn code where I need to create a tf.estimator.export.TensorServingInputReceiver. I don t find any example to update anywhere laetitiaoist Estimator still relies on graph based code so you would have to use tf.compat.v1.placeholder . We re recommending that users switch to tf.Keras in the future which has built in support for eager mode. It would be great to have instructions on how to update from Estimator to Keras. I thought Estimator was supported in TF2... For example how to save the top N models when evaluating I only see save best only option . Estimator is supported in TF2 but the APIs for building the graph have been removed from the V2 API to keep the new API compatible with eager. You can continue to use Estimators but you ll have to use the compat.v1 for certain APIs like placeholder. There are instructions here for updating your model to use the Keras API https www.tensorflow.org guide migrate converting models Do you mean in Keras ModelCheckpoint class Yes there re currently isn t a way to specify the keep the top N checkpoints. Feel free to create a new issue asking for a feature request. 
27353,tf.keras is ignoring specified step per epoch when keras doesn t, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow no OS Platform and Distribution e.g. Linux Ubuntu 16.04 unknown Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary colab TensorFlow version use command below 1.13 Python version python 3 Bazel version if compiling from source unknown GCC Compiler version if compiling from source unknown CUDA cuDNN version unknown GPU model and memory unknown You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION Describe the current behavior history model.fit generator train generator steps per epoch 8 epochs 15 verbose 1 If train generator is a instance of Sequence. in my case coming from flow from directory steps per epoch is overridden by len train generator in this file training generator.py https github.com tensorflow tensorflow blob r1.13 tensorflow python keras engine training generator.py L371 This doesn t happen in keras. In keras steps per epoch is kept. Describe the expected behavior Use specified steps per epoch instead of len data if specified. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. I find this when doing a coursera online course. The notebook i used can be found at link https github.com lmoroney dlaicourse blob master Course 201 20 20Part 208 20 20Lesson 202 20 20Notebook.ipynb Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,This is not Build Installation or Bug Performance issue. Please post this kind of support questions at Stackoverflow https stackoverflow.com questions tagged tensorflow . There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks jvishnuvardhan I understand what is happening. I don t need stackoverflown for clarification. The problem I m reporting is that the parameter is not working as expected in tf.keras and is behaving differently in keras and tf.keras. I heard that you are trying to keep tf.keras api in agreement with keras. This could be fixed on either ends to make them behave the same. got the same issue with the last keras and tensorflow model.fit generator just ignores steps per epoch parameter. Thanks for the issue It seems reasonable to prefer steps per epoch if provided over the Sequence length will look into this Looks like this has already been fixed in the TF 2.0 nightly Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27353 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27353 No a 
27431,Using layer classes as attribute throw an exception, System information Have I written custom code Yes OS Platform and Distribution Ubuntu 16.04 TensorFlow installed from binary TensorFlow version 2.0.0.dev20190402 Python version 2.7.12 Describe the current behavior When a layer class is used as attribute the code will throw a TypeError exception when calling self. gather children attribute . It appears that the layer class is tracked. Describe the expected behavior Only layer instances should be tracked not classes. Code to reproduce the issue Other info logs , qlzh727 this is a better project for someone working on TF Keras. I am realistically never going to get to it. It s near code I ve modified but I don t think those modifications made the situation any worse. Ack I will take it from here. Should be fixed by https github.com tensorflow tensorflow commit 9d724a8e6034d321e97cdc9972d4d6e7adb3e3ca now. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27431 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27431 No a 
27455,TF2.0 gradient problem of using tf.nn.relu in tf.keras.Model., System information OS Platform and Distribution Linux Ubuntu 18.04 TensorFlow installed from binary TensorFlow version 2.0.0 alpha0 Python version 3.6.8 Describe the current behavior I built a keras model with only a tf.nn.relu but the gradient seems to be None after being decorated by tf.function Code to reproduce the issue 1. tf.nn.relu tf.keras.Model tf.function this is the only case that produce None gradient 1.2 tf.nn.relu tf.keras.Model without tf.function 2. tf.keras.layers.ReLU tf.keras.Model tf.function 2.2 tf.keras.layers.ReLU tf.keras.Model without tf.function 3. only tf.nn.relu So I think its the problem between tf.nn.relu and tf.keras.Model Besides tf.nn.tanh has the same problem., tomerk I think something is broken with the keras graph here since the tape isn t seeing it. Can you take a look or help triage to the right person I have a fix for this that will be submitted soon. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27455 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27455 No a 
27496,Can not set dynamic property on custom layer, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Colab environment TensorFlow version use command below 2.0 alpha I am trying to write a custom layer in TF 2.0 and having trouble making it run eagerly. For example if I open a documentation page https www.tensorflow.org alpha guide eager And try to use the following piece of code from that page to create custom layer And then try to create an instance for example like this I get the following error message This error appears in Colab as well as on my machine Windows 8 . Am I doing something wrong here ,This question is better asked on StackOverflow http stackoverflow.com questions tagged tensorflow since it is not a bug or feature request. There is also a larger community that reads questions there. How is this not a bug if code from documentation does not work in the environment provided by Google Thanks for trying TF 2.0 alpha. I was able to reproduce your behavior in TF 2.0 alpha however it executed successfully using TF 1.13.1 after enabling eager mode fchollet can you triage this I think dynamic is now a property which is why this error happens but I don t remember the right way to set it. Having the same problem what is the correct way to set this property The alternative I tried is passing it as an argument to the superclass init but that raises a NotImplementedError ilia nikiforov What exactly is your problem Initially I thought that setting this property is needed to run layers eagerly but after digging some more I found out that it is not needed in my case my custom layer was not running eagerly but the source of the problem turned out to be completely different . In fact at the moment I am not sure what does setting dynamic property does at all. This is fixed with latest tf nightly 2.0 build version 2.0.0 dev20190808 Now we can expect new improved error message Changing the name of attribute helps fixing this issue. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27496 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27496 No a I m using tensorflow 2.3 and still get this error. I m using py function in my layers and would like to set dynamic True as it is documented here https www.tensorflow.org api docs python tf keras layers Layer url Update The error is ok i had to implement the compute output shape method in my custom layer. Example to reproduce the error outputs I think this means your dynamic True layer must implement compute output shape. On Tue Aug 4 2020 at 4 02 AM oholimoli notifications github.com wrote I m using tensorflow 2.3 and still get this error. I m using py function in my layers and would like to set dynamic True as it is documented here https www.tensorflow.org api docs python tf keras layers Layer http url Example to reproduce the error class Custom keras.layers.Layer def init self kwargs super . init kwargs def build self input shape self. outputshape None 1 super .build input shape def py function self inputs return inputs.numpy .sum axis 0 def call self inputs x tf.py function func self.py function inp inputs Tout tf.float32 x.set shape self. outputshape return x x input keras.layers.Input shape 16 name x input x Custom dynamic True x input custom model keras.Model x input x name custom model custom model.summary outputs NotImplementedError Traceback most recent call last ipython input 52 4ed37a9049c5 in module 1 x input keras.layers.Input shape 16 name x input 2 x Custom dynamic True x input 3 custom model keras.Model x input x name custom model 4 custom model.summary c users hofo envs py smart bin sensing lib site packages tensorflow python keras engine base layer.py in call self args kwargs 924 if in functional construction mode self inputs args kwargs input list 925 return self. functional construction call inputs args kwargs 926 input list 927 928 Maintains info about the Layer.call stack. c users hofo envs py smart bin sensing lib site packages tensorflow python keras engine base layer.py in functional construction call self inputs args kwargs input list 1130 TODO fchollet consider py func as an alternative which 1131 would enable us to run the underlying graph if needed. 1132 outputs self. symbolic call inputs 1133 1134 if outputs is None c users hofo envs py smart bin sensing lib site packages tensorflow python keras engine base layer.py in symbolic call self inputs 2660 def symbolic call self inputs 2661 input shapes nest.map structure lambda x x.shape inputs 2662 output shapes self.compute output shape input shapes 2663 Convert to TensorShape so that nest.map structure will not map into 2664 individual dim of the shape. c users hofo envs py smart bin sensing lib site packages tensorflow python keras engine base layer.py in compute output shape self input shape 739 self. class . name e 740 return nest.map structure lambda t t.shape outputs 741 raise NotImplementedError 742 743 doc controls.for subclass implementers NotImplementedError You are receiving this because you were assigned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 27496 issuecomment 668529878 or unsubscribe https github.com notifications unsubscribe auth AAABHRKXVK3Q23RREK3RRY3R67TELANCNFSM4HDQOTLQ . Alex 
27497, tf.reduce sum with multiple negative axes and tf.RaggedTensor bugged, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 ubuntu 16.04 TensorFlow installed from source or binary pip TensorFlow version use command below Python version v1.13.1 0 g6612da8951 1.13.1 GPU model and memory quadro k620 2gb Current behaviour Multiple axes passed to tf.reduce sum with first argument being a ragged tensor results in incorrect behaviour. Expected behaviour Same result as corresponding positive indices separate reductions. ,Created a PR 27699 for the fix. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27497 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27497 No a 
27507,Weird behavior of tf.Variable.assign in a tf.Dataset.map callback function, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Custom code OS Platform and Distribution e.g. Linux Ubuntu 16.04 OSX Mojave Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device NA TensorFlow installed from source or binary pip3.7 TensorFlow version use command below 1.13.1 Python version 3.7.2 Bazel version if compiling from source NA GCC Compiler version if compiling from source NA CUDA cuDNN version Running the CPU version GPU model and memory NA You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION I was trying using a Variable to track a seed number in Dataset. Because I want my model to resume training with the same training order as before break if I can checkpoint the variable. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. A minimal code is like Describe the expected behavior The expected print should be 0 9 for both x and seed . Describe the current behavior But the output of seed does not stays the same and changes over different executions. The output is like Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,I could reproduce the issue with TF1.13.1 and TF1.12. Thanks prclibo We need to initialize the seed also. Pleas check here https www.tensorflow.org api docs python tf Variable for more details about tf.Variable. Please check the code below Please let me know what you think Thanks jvishnuvardhan Thanks for the remind. Though below is not related to this question itself. But I found that seed is included in tf.global variables so is it necessary to separately initialize seed besides tf.global variables initializer This will output global variables tf.Variable ssss 0 shape dtype int64 prclibo Looks like this was an issue which was resolved in tf nightly. Could you try running it in Google colab with pip install tf nightly. I don t see any issue with tf nightly. Please let me know how it progress and close the issue if it was resolved by tf nightly. Thanks jvishnuvardhan muddham See https colab.research.google.com drive 1DzTvOJRXYUk0CSjvGa11ewmjTP7VJ1kF The issue did not happen in my very limited tests. Also could you please take a look at 28043 . It seems a similar problem but not fixed in tf nightly. prclibo I will close this issue as it was resolved in tf nightly. We will resolve the other issue there. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27507 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27507 No a 
27530,Duplicate layer name when using a b and Add c d in the same Keras model, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOSX 10.13.6 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device No TensorFlow installed from source or binary Binary TensorFlow version use command below tf.version.VERSION 2.0.0 dev20190404 tf.version.GIT VERSION v1.12.0 11729 g98c3cfbf74 Python version 3.6.8 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior When using a b then keras.layers.Add in the same Keras model I get an exception saying the name add was used twice. The problem goes away if I use keras.layers.Add first or if I use twice the same operation either twice or keras.layers.Add twice . Describe the expected behavior I don t expect any name conflicts. Code to reproduce the issue The following code raises a ValueError exception full stacktrace below . Other info logs Here is the full stacktrace ,I am able to repro the issue will look into the fix. ageron can you verify if this is fixed https github.com tensorflow tensorflow commit 0a9f68955ec7916f6f57916ccd60cd5e9c93d901 diff 8eb7e20502209f082d0cb15119a50413 Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27530 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27530 No a Yes problem fixed thank you very much 
27541, TF 2.0.0 alpha0 tf.keras.Model reinitializes set weights. , System information Have I written custom code Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 TensorFlow installed from source or binary pip install tensorflow 2.0.0 alpha0 TensorFlow version use command below 2.0.0 alpha0 Python version 3.7.1 Using only CPU Describe the current behavior When I initialize a convolution layer with some weights instead of the weights being set it will be randomly re initialized. Print output returns at each run a different weight. Code to reproduce the issue Describe the expected behavior with keras version 2.1.5 using tensorflow backend version 1.6.0 When I initialize a convolution layer with some weights the weights will be set and stick. Print output returns as expected 0.8888 0. This is an useful feature especially when one is converting from one model format to another. For example when converting darknet model.cfg model.weights to keras model.h5 . Code to reproduce Questions Is this intended Or how can this be disabled How can I help you solve this issue. Thanks for taking your time looking into this. , mauricelucy I am closing this as it was resolved in tf nightly gpu 2.0 preview 2.0.0.dev20190723 . Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 2df53e68f336fa3e02524632e7612bbf tf 27541.ipynb . Please feel free to open if the issue persisits. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27541 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27541 No a 
27543,tf2.0 failed to save model if an input is used not used in the model directly, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Win 10 x64 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device No TensorFlow installed from source or binary Binary TensorFlow version use command below 2.0a0 Python version 3.7 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior I have this use case where the model requires multiple inputs and some inputs go into the model layers and some inputs go into the loss function directly. It works fine in the past until I updated my package unit test to tf2.0 to test drive and find out tf2.0 failed test that run fine with tf1.x I noticed that its mainly due to the tensor that goes directly to the loss function in this case is not presented in self. network nodes in the line if node key not in nn.keras model. network nodes in tensorflow keras network.py. Describe the expected behavior I expect the model to be saved successfully with tf2.0 just as tf1.x Code to reproduce the issue Other info logs ,I could reproduce the issue with TF2.0.0 alpha0 in Google colab. Thanks is there a schedule for this issue to be fixed Because I really want this issue to be fixed before 2.0.0a1 so that I can see if there is any further issue in my CI tests. Same problem with 1.14.RC1 on colab now. It can t load model from an h5 file either. jvishnuvardhan and k w w Is there any ETA for a fix or any direction so I can contribute for a fix Thanks henrysky As you are interested to contribute You can raise a PR. Thanks Hi henrysky Thanks so much for implementing your fix Sorry for not updating sooner I submitted a change that addressed this issue earlier here https github.com tensorflow tensorflow commit 401bbfc33684c21325d81a03708fe123d59ce527 diff 4ee308ea180d49ae81691348531a2b6d Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27543 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27543 No a same issue in 1.14.0 same issue in 1.14.0 It is just a one line patch you can patch your 1.14.0 like me by yourself https github.com tensorflow tensorflow commit 401bbfc33684c21325d81a03708fe123d59ce527 diff 4ee308ea180d49ae81691348531a2b6d same issue in 1.14.0 It is just a one line patch you can patch your 1.14.0 like me by yourself 401bbfc diff 4ee308ea180d49ae81691348531a2b6d https github.com tensorflow tensorflow commit 401bbfc33684c21325d81a03708fe123d59ce527 diff 4ee308ea180d49ae81691348531a2b6d oh thank you I have tried it but there will be another problem. And I use tf.saved model.save to save the model and there is no error. TachikakaMin What model are you trying to save out where is b n x03Mul x12 x03Mul x1a x0fconv2d7 Sigmoid x1a x0cdepth result x07 n x01T x12 x020 x01 coming from same issue in 1.14.0 It is just a one line patch you can patch your 1.14.0 like me by yourself 401bbfc diff 4ee308ea180d49ae81691348531a2b6d https github.com tensorflow tensorflow commit 401bbfc33684c21325d81a03708fe123d59ce527 diff 4ee308ea180d49ae81691348531a2b6d by this it gives error as TypeError Not JSON Serializable b n rdense Softmax x12 x07Softmax x1a rdense BiasAdd x07 n x01T x12 x020 x01 
27565, TF 2.0.0a0 tf.function raises ValueError when computing gradients, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04 TensorFlow version use command below pip install tensorflow gpu 2.0.0a0 Python version 3.6 Describe the current behavior The code executes normally but raise ValueError when computing gradients tape.gradient if I decorate the training function with tf.function . The traceback is as follows pre ValueError Traceback most recent call last Workspaces fgenl run.py in module 80 for batch id in range num batches each epoch 81 batch data data generator.get data v2 82 loss outputs train one step batch data v2 83 loss outputs inputs sess.run opt op loss outputs batch data 84 if loss metrics is None .pyenv versions anaconda3 5.2.0 envs tf lib python3.6 site packages tensorflow python eager def function.py in call self args kwds 424 This is the first call of call so we have to initialize. 425 initializer map 426 self. initialize args kwds add initializers to initializer map 427 if self. created variables 428 try .pyenv versions anaconda3 5.2.0 envs tf lib python3.6 site packages tensorflow python eager def function.py in initialize self args kwds add initializers to 368 self. concrete stateful fn 369 self. stateful fn. get concrete function internal garbage collected pylint disable protected access 370 args kwds 371 372 def invalid creator scope unused args unused kwds .pyenv versions anaconda3 5.2.0 envs tf lib python3.6 site packages tensorflow python eager function.py in get concrete function internal garbage collected self args kwargs 1311 if self. input signature 1312 args kwargs None None 1313 graph function self. maybe define function args kwargs 1314 return graph function 1315 .pyenv versions anaconda3 5.2.0 envs tf lib python3.6 site packages tensorflow python eager function.py in maybe define function self args kwargs 1578 or call context key not in self. function cache.missed 1579 self. function cache.missed.add call context key 1580 graph function self. create graph function args kwargs 1581 self. function cache.primary cache key graph function 1582 return graph function args kwargs .pyenv versions anaconda3 5.2.0 envs tf lib python3.6 site packages tensorflow python eager function.py in create graph function self args kwargs override flat arg shapes 1510 arg names arg names 1511 override flat arg shapes override flat arg shapes 1512 capture by value self. capture by value 1513 self. function attributes 1514 .pyenv versions anaconda3 5.2.0 envs tf lib python3.6 site packages tensorflow python framework func graph.py in func graph from py func name python func args kwargs signature func graph autograph autograph options add control dependencies arg names op return value collections capture by value override flat arg shapes 692 converted func 693 694 func outputs python func func args func kwargs 695 696 invariant func outputs contains only Tensors IndexedSlices .pyenv versions anaconda3 5.2.0 envs tf lib python3.6 site packages tensorflow python eager def function.py in wrapped fn args kwds 315 wrapped allows AutoGraph to swap in a converted function. We give 316 the function a weak reference to itself to avoid a reference cycle. 317 return weak wrapped fn . wrapped args kwds 318 weak wrapped fn weakref.ref wrapped fn 319 .pyenv versions anaconda3 5.2.0 envs tf lib python3.6 site packages tensorflow python framework func graph.py in wrapper args kwargs 684 optional features autograph options 685 force conversion True 686 args kwargs 687 688 Wrapping around a decorator allows checks like tf inspect.getargspec .pyenv versions anaconda3 5.2.0 envs tf lib python3.6 site packages tensorflow python autograph impl api.py in converted call f owner options args kwargs 390 return call unconverted f args kwargs 391 392 result converted f effective args kwargs 393 394 The converted function s closure is simply inserted into the function s tmp tmpx0xgcbu3.py in tf train one step batch data 6 outputs ag .converted call model None ag .ConversionOptions recursive True verbose 0 strip decorators tf.function defun ag .convert ag .do not convert ag .converted call force conversion False optional features internal convert user code True batch data 7 loss info ag .converted call calculate loss loss object ag .ConversionOptions recursive True verbose 0 strip decorators tf.function defun 1 ag .convert ag .do not convert ag .converted call force conversion False optional features internal convert user code True outputs batch data 8 gradients ag .converted call gradient tape ag .ConversionOptions recursive True verbose 0 strip decorators tf.function defun 2 ag .convert ag .do not convert ag .converted call force conversion False optional features internal convert user code True loss model.trainable variables 9 update list grad var for grad var in ag .converted call zip None ag .ConversionOptions recursive True verbose 0 strip decorators tf.function defun 3 ag .convert ag .do not convert ag .converted call force conversion False optional features internal convert user code True gradients model.trainable variables if grad is not None 10 ag .converted call apply gradients optimizer ag .ConversionOptions recursive True verbose 0 strip decorators tf.function defun 4 ag .convert ag .do not convert ag .converted call force conversion False optional features internal convert user code True update list .pyenv versions anaconda3 5.2.0 envs tf lib python3.6 site packages tensorflow python autograph impl api.py in converted call f owner options args kwargs 265 266 if not options.force conversion and conversion.is whitelisted for graph f 267 return call unconverted f args kwargs 268 269 internal convert user code is for example turned off when issuing a dynamic .pyenv versions anaconda3 5.2.0 envs tf lib python3.6 site packages tensorflow python autograph impl api.py in call unconverted f args kwargs 186 return f. self .call args kwargs 187 188 return f args kwargs 189 190 .pyenv versions anaconda3 5.2.0 envs tf lib python3.6 site packages tensorflow python eager backprop.py in gradient self target sources output gradients unconnected gradients 954 flat sources 955 output gradients output gradients 956 unconnected gradients unconnected gradients 957 958 if not self. persistent .pyenv versions anaconda3 5.2.0 envs tf lib python3.6 site packages tensorflow python eager imperative grad.py in imperative grad tape target sources output gradients unconnected gradients 70 sources 71 output gradients 72 compat.as str unconnected gradients.value .pyenv versions anaconda3 5.2.0 envs tf lib python3.6 site packages tensorflow python eager backprop.py in aggregate grads gradients 565 indexed slices ops.IndexedSlices 566 grad 567 math ops.range grad.shape 0 568 constant op.constant grad.shape.as list 569 indexed slices list.append indexed slices .pyenv versions anaconda3 5.2.0 envs tf lib python3.6 site packages tensorflow python ops math ops.py in range start limit delta dtype name 1258 with ops.name scope name Range start limit delta as name 1259 start ops.convert to tensor start dtype dtype name start 1260 limit ops.convert to tensor limit dtype dtype name limit 1261 delta ops.convert to tensor delta dtype dtype name delta 1262 .pyenv versions anaconda3 5.2.0 envs tf lib python3.6 site packages tensorflow python framework ops.py in convert to tensor value dtype name preferred dtype dtype hint 1048 preferred dtype deprecation.deprecated argument lookup 1049 dtype hint dtype hint preferred dtype preferred dtype 1050 return convert to tensor v2 value dtype preferred dtype name 1051 1052 .pyenv versions anaconda3 5.2.0 envs tf lib python3.6 site packages tensorflow python framework ops.py in convert to tensor v2 value dtype dtype hint name 1106 name name 1107 preferred dtype dtype hint 1108 as ref False 1109 1110 .pyenv versions anaconda3 5.2.0 envs tf lib python3.6 site packages tensorflow python framework ops.py in internal convert to tensor value dtype name as ref preferred dtype ctx accept symbolic tensors 1184 1185 if ret is None 1186 ret conversion func value dtype dtype name name as ref as ref 1187 1188 if ret is NotImplemented .pyenv versions anaconda3 5.2.0 envs tf lib python3.6 site packages tensorflow python framework constant op.py in constant tensor conversion function v dtype name as ref 302 as ref False 303 as ref 304 return constant v dtype dtype name name 305 306 .pyenv versions anaconda3 5.2.0 envs tf lib python3.6 site packages tensorflow python framework constant op.py in constant value dtype shape name 243 244 return constant impl value dtype shape name verify shape False 245 allow broadcast True 246 247 .pyenv versions anaconda3 5.2.0 envs tf lib python3.6 site packages tensorflow python framework constant op.py in constant impl value dtype shape name verify shape allow broadcast 281 tensor util.make tensor proto 282 value dtype dtype shape shape verify shape verify shape 283 allow broadcast allow broadcast 284 dtype value attr value pb2.AttrValue type tensor value.tensor.dtype 285 const tensor g.create op .pyenv versions anaconda3 5.2.0 envs tf lib python3.6 site packages tensorflow python framework tensor util.py in make tensor proto values dtype shape verify shape allow broadcast 453 else 454 if values is None 455 raise ValueError None values not supported. 456 if dtype is provided forces numpy array to be the type 457 provided if possible. ValueError None values not supported. pre Describe the expected behavior The code should also execute normally when using tf.function . Code to reproduce the issue Sorry I do not have a simple snippet to reproduce this issue. But could you find something in the traceback See below please. ,I found a simple script to reproduce this issue and it seems that the op tf.sparse.sparse dense matmul causes this issue. pre coding utf 8 Author Lin Lan ryan.linlan gmail.com from future import absolute import from future import division from future import print function import numpy as np import scipy as sp import tensorflow as tf def sparse to tuple sparse mx Convert sparse matrix to tuple representation. def to tuple mx if not sp.sparse.isspmatrix coo mx mx mx.tocoo coords np.vstack mx.row mx.col .transpose values mx.data shape mx.shape return coords values shape if isinstance sparse mx list for i in range len sparse mx sparse mx i to tuple sparse mx i else sparse mx to tuple sparse mx return sparse mx def construct tf sparse tensor sp sparse matrix if not sp.sparse.issparse sp sparse matrix raise TypeError tuple format sparse to tuple sp sparse matrix tf sparse tensor tf.sparse.SparseTensor indices tuple format 0 values tuple format 1 dense shape tuple format 2 tf sparse tensor tf.sparse.reorder tf sparse tensor return tf sparse tensor weights tf.Variable tf.random.uniform 512 128 dtype tf.float32 trainable True optimizer tf.optimizers.Adam tf.function def train x with tf.GradientTape as tape embeddings tf.sparse.sparse dense matmul x weights batch embeddings tf.nn.embedding lookup embeddings 1 2 3 4 5 7 8 9 10 embeddings tf.nn.embedding lookup embeddings list range 512 logits tf.matmul batch embeddings embeddings transpose b True loss tf.reduce mean logits gradients tape.gradient loss weights optimizer.apply gradients zip gradients weights random array np.random.rand 512 512 sparse array sp.sparse.csr matrix np.asarray random array 0.5 dtype np.float32 sparse tensor construct tf sparse tensor sparse array train sparse tensor pre To let the above code compute gradients normally one way is to uncomment embeddings tf.nn.embedding lookup embeddings list range 512 . The Tape is unable to see the variables. So use tape.watch embeddings after sparse dense matmul . Sparse Tensors cannot be watched so watching x is not an option . That solves the problem. captain pool The above code works well in eager mode. It only fails when we use tf.function decoration. So the tape only cannot see the variable with AutoGraph Well that is exactly what s happening. For Autograph it works only with watch . I m still looking through the codebase to find the reason. There seems to be a slightly more helpful error in tf nightly but it looks like it s unrelated to tape.watch or autograph. The shape of embeddings seems to be partially unknown after the sparse dense matmul and this line fixed in my tests Reassigning to triage the tape error. There is a bug in the backprop code where it does math ops.range grad.shape 0 which uses the static shape of the grad tensor which might be known a number or None. To use the dynamic shape we need something like math ops.range array ops.shape grad 0 . Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27565 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27565 No a 
27581,keras model.predict on batch model.test on batch with tf.dataset dataset.make initializable iterator is not supported when eager execution is enabled., System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOS Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary yes TensorFlow version use command below 2.0.0 alpha0 Python version 3.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Describe the current behavior best model.test on batch testing dataset and best model.predict on batch training dataset crashing with the following error messages RuntimeError dataset.make initializable iterator is not supported when eager execution is enabled. but best model.evaluate testing dataset is wokring. Describe the expected behavior Exact same code working with Tensorflow 1.12 Code to reproduce the issue All the code and llogs are here https github.com tarrade proj DL models and pipelines with GCP blob master notebook TF 2.0 05 Mnist model validation and interpretation.ipynb Other info logs Here the full logs AttributeError Traceback most recent call last anaconda3 envs env gcp dl 2 0 alpha lib python3.6 site packages tensorflow python data ops dataset ops.py in make initializable iterator dataset shared name 1852 some datasets e.g. for prefetching override its behavior. 1853 return dataset. make initializable iterator shared name pylint disable protected access 1854 except AttributeError AttributeError PrefetchDataset object has no attribute make initializable iterator During handling of the above exception another exception occurred RuntimeError Traceback most recent call last ipython input 21 3d53d52be0f8 in module 1 Bug TF 2.0 2 score best model.test on batch testing dataset 3 4 print test accuracy 5 print Loss anaconda3 envs env gcp dl 2 0 alpha lib python3.6 site packages tensorflow python keras engine training.py in test on batch self x y sample weight reset metrics 1310 Validate and standardize user data. 1311 x y sample weights self. standardize user data 1312 x y sample weight sample weight extract tensors from dataset True 1313 1314 if self.run eagerly anaconda3 envs env gcp dl 2 0 alpha lib python3.6 site packages tensorflow python keras engine training.py in standardize user data self x y sample weight class weight batch size check steps steps name steps validation split shuffle extract tensors from dataset 2439 if extract tensors from dataset 2440 We do this for train on batch etc. 2441 x y sample weight training utils.extract tensors from dataset x 2442 elif isinstance x iterator ops.Iterator 2443 Graph mode iterator. We extract the symbolic tensors. anaconda3 envs env gcp dl 2 0 alpha lib python3.6 site packages tensorflow python keras engine training utils.py in extract tensors from dataset dataset 1382 Tuple of tensors x y weights . y and weights entry may be None. 1383 1384 iterator get iterator dataset 1385 inputs targets sample weight unpack iterator input iterator 1386 return inputs targets sample weight anaconda3 envs env gcp dl 2 0 alpha lib python3.6 site packages tensorflow python keras engine training utils.py in get iterator dataset 1362 def get iterator dataset 1363 Create and initialize an iterator from a dataset. 1364 iterator dataset ops.make initializable iterator dataset 1365 initialize iterator iterator 1366 return iterator anaconda3 envs env gcp dl 2 0 alpha lib python3.6 site packages tensorflow python data ops dataset ops.py in make initializable iterator dataset shared name 1853 return dataset. make initializable iterator shared name pylint disable protected access 1854 except AttributeError 1855 return DatasetV1Adapter dataset . make initializable iterator shared name pylint disable protected access 1856 1857 anaconda3 envs env gcp dl 2 0 alpha lib python3.6 site packages tensorflow python data ops dataset ops.py in make initializable iterator self shared name 1495 if context.executing eagerly 1496 raise RuntimeError 1497 dataset.make initializable iterator is not supported when eager 1498 execution is enabled. 1499 ensure same dataset graph self RuntimeError dataset.make initializable iterator is not supported when eager execution is enabled.,Did you upgrade your TF 1.X code to TensorFlow 2.0 https www.tensorflow.org alpha guide upgrade before executing it in TF 2.0 ymodak yes I migrated my python code using the script tf upgrade v2 and I use the tool tf2up.ml to help migrating the notebook. I will update the logs with the error because it was corrupted after I deleted some flags the error messgae stay the same I am not saying that I didn t do something wrong on my side but do you see why the following will work best model.evaluate testing dataset and not best model.predict on batch training dataset both use the same best model keras and the tf.dataset migrated to TF 2.0 . img width 1080 alt Screenshot 2019 04 09 at 21 15 33 src https user images.githubusercontent.com 12021701 55828465 cb4bfa00 5b0c 11e9 92a7 7358216a8322.png ymodak here some modifided code from Tensorflow 2.0 where you can easily reproduce the issue pip install tensorflow 2.0.0 alpha0 pip install tensorflow datasets the usage I am doing from predict on btach if conform to what is written in the doc https www.tensorflow.org versions r2.0 api docs python tf keras Model tarrade Apologies for the delay in response. Thanks for the minimal code snippet. I was able to reproduce your behavior. It executes successfully in TF 1.13.1 and fails at with TF 2.0 alpha. ymodak I tested today with tf nightly 2.0 preview 2.0.0.dev20190516 and the issues are fixed for both model.predict on batch training dataset and model.test on batch training dataset closing this ticket Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27581 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27581 No a 
27649,strided slice leads to unknown shape for Input layer, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow no TensorFlow installed from source or binary TensorFlow version use command below Python version 3.7.1 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version not relevent GPU model and memory not relevent Describe the current behavior Output is unknown Describe the expected behavior strided slice should calculate the shape of the output tensor correctly. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,hello i would like to work on this and make a PR for it ipod825 Thanks for trying TF 2.0 alpha. I was able to reproduce the observed behavior. Executing the same snippet in TF 2.0 cpu version returns correct results. This is fixed with latest TF 2.0 Nightly. See gist https colab.sandbox.google.com gist jvishnuvardhan c499ffa64c5c541e31a81c876c83fbd9 tf27649.ipynb scrollTo wZkzcfDM3TCh Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27649 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27649 No a 
27696,tf.keras.estimator.model to estimator crashing when using tf.distribute.MirroredStrategy with Only TensorFlow native optimizers are supported with DistributionStrategy, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac OS Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary TensorFlow version use command below 2.0.0 alpha0 Python version 3.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Describe the current behavior I am using a keras model with TF 2.0. I am converting the model to estimator tf.keras.estimator.model to estimator then it is crashing when running estimator train model.train .. when using tf.distribute.MirroredStrategy after migrating the code to TF 2.0 of course . It works fine when using None as strategy I tried to follow the instruction https www.tensorflow.org alpha guide distribute strategy Describe the expected behavior The same was working with TF 1.x Code to reproduce the issue Work in progress notebook can be found here http localhost 8888 notebooks proj DL models and pipelines with GCP notebook TF 2.0 08 Mnist keras estimator.ipynb I am using a very basic Keras model with Other info logs I0409 22 06 27.293305 4531660224 model.py 211 input dataset fn TRAIN train I0409 22 06 27.469371 123145492971520 estimator.py 1126 Calling model fn. I0409 22 06 27.475003 123145492971520 coordinator.py 219 Error reported to Coordinator Only TensorFlow native optimizers are supported with DistributionStrategy. Traceback most recent call last File Users tarrade anaconda3 envs env gcp dl 2 0 alpha lib python3.6 site packages tensorflow python training coordinator.py line 297 in stop on exception yield File Users tarrade anaconda3 envs env gcp dl 2 0 alpha lib python3.6 site packages tensorflow python distribute mirrored strategy.py line 882 in run self.main result self.main fn self.main args self.main kwargs File Users tarrade anaconda3 envs env gcp dl 2 0 alpha lib python3.6 site packages tensorflow estimator python estimator estimator.py line 1127 in call model fn model fn results self. model fn features features kwargs File Users tarrade anaconda3 envs env gcp dl 2 0 alpha lib python3.6 site packages tensorflow estimator python estimator keras.py line 278 in model fn raise ValueError Only TensorFlow native optimizers are supported with ValueError Only TensorFlow native optimizers are supported with DistributionStrategy. ValueError Traceback most recent call last timed eval in module anaconda3 envs env gcp dl 2 0 alpha lib python3.6 site packages tensorflow estimator python estimator estimator.py in train self input fn hooks steps max steps saving listeners 357 358 saving listeners check listeners type saving listeners 359 loss self. train model input fn hooks saving listeners 360 logging.info Loss for final step s. loss 361 return self anaconda3 envs env gcp dl 2 0 alpha lib python3.6 site packages tensorflow estimator python estimator estimator.py in train model self input fn hooks saving listeners 1135 def train model self input fn hooks saving listeners 1136 if self. train distribution 1137 return self. train model distributed input fn hooks saving listeners 1138 else 1139 return self. train model default input fn hooks saving listeners anaconda3 envs env gcp dl 2 0 alpha lib python3.6 site packages tensorflow estimator python estimator estimator.py in train model distributed self input fn hooks saving listeners 1198 self. config. train distribute.configure self. config.session config 1199 return self. actual train model distributed 1200 self. config. train distribute input fn hooks saving listeners 1201 pylint enable protected access 1202 anaconda3 envs env gcp dl 2 0 alpha lib python3.6 site packages tensorflow estimator python estimator estimator.py in actual train model distributed self strategy input fn hooks saving listeners 1267 labels although this will be None it seems 1268 ModeKeys.TRAIN 1269 self.config 1270 loss strategy.reduce reduce util.ReduceOp.SUM 1271 grouped estimator spec.loss anaconda3 envs env gcp dl 2 0 alpha lib python3.6 site packages tensorflow python distribute distribute lib.py in call for each replica self fn args kwargs 1076 if kwargs is None 1077 kwargs 1078 return self. call for each replica fn args kwargs 1079 1080 def call for each replica self fn args kwargs anaconda3 envs env gcp dl 2 0 alpha lib python3.6 site packages tensorflow python distribute mirrored strategy.py in call for each replica self fn args kwargs 663 def call for each replica self fn args kwargs 664 return call for each replica self. container strategy self. device map 665 fn args kwargs 666 667 def configure self anaconda3 envs env gcp dl 2 0 alpha lib python3.6 site packages tensorflow python distribute mirrored strategy.py in call for each replica distribution device map fn args kwargs 191 for t in threads 192 t.should run.set 193 coord.join threads 194 195 return values.regroup device map tuple t.main result for t in threads anaconda3 envs env gcp dl 2 0 alpha lib python3.6 site packages tensorflow python training coordinator.py in join self threads stop grace period secs ignore live threads 387 self. registered threads set 388 if self. exc info to raise 389 six.reraise self. exc info to raise 390 elif stragglers 391 if ignore live threads anaconda3 envs env gcp dl 2 0 alpha lib python3.6 site packages six.py in reraise tp value tb 691 if value. traceback is not tb 692 raise value.with traceback tb 693 raise value 694 finally 695 value None anaconda3 envs env gcp dl 2 0 alpha lib python3.6 site packages tensorflow python training coordinator.py in stop on exception self 295 296 try 297 yield 298 except pylint disable bare except 299 self.request stop ex sys.exc info anaconda3 envs env gcp dl 2 0 alpha lib python3.6 site packages tensorflow python distribute mirrored strategy.py in run self 880 self. captured var scope reuse self.replica id 0 881 variable scope.variable creator scope self.variable creator fn 882 self.main result self.main fn self.main args self.main kwargs 883 self.done True 884 finally anaconda3 envs env gcp dl 2 0 alpha lib python3.6 site packages tensorflow estimator python estimator estimator.py in call model fn self features labels mode config 1125 1126 logging.info Calling model fn. 1127 model fn results self. model fn features features kwargs 1128 logging.info Done calling model fn. 1129 anaconda3 envs env gcp dl 2 0 alpha lib python3.6 site packages tensorflow estimator python estimator keras.py in model fn features labels mode 276 not isinstance keras model.optimizer 277 tf optimizer module.Optimizer optimizers.TFOptimizer 278 raise ValueError Only TensorFlow native optimizers are supported with 279 DistributionStrategy. 280 ValueError Only TensorFlow native optimizers are supported with DistributionStrategy. ,Here a code based on examples from TF 2.0 official doc to reproduce the issue pip install tensorflow 2.0.0 alpha0 pip install tensorflow datasets I have the same problem on colab this is my code https colab.research.google.com drive 1mf PK0a20CkObnT0hCl9VPEje1szhHat scrollTo MMbPOC3f5ku3 Correct link https colab.research.google.com drive 1mf PK0a20CkObnT0hCl9VPEje1szhHat Have you tried the suggestion from the docs https www.tensorflow.org alpha guide distribute strategy using tfdistributestrategy with keras of placing the code inside the strategy scope I came here because I m seeing this same error even when I do the scoping above. mckinziebrandon thanks for the idea. From the same doc see the section estimator The usage of tf.distribute.Strategy with Estimator is slightly different than the Keras case. Instead of using strategy.scope now we pass the strategy object into the RunConfig for the Estimator. This is what I did. This appears to be a real issue. We flagged it internally and someone will hopefully get to it soon. Thanks for reporting. If you are curious about fixing it then my guess is that keras.optimizer v2.OptimizerV2 needs to be allowed in the set of classes that are allowed right where that ValueError is thrown. It should work. This should be fixed in master now. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27696 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27696 No a I still have the problem with pip install tf nightly gpu on https colab.research.google.com drive 1mf PK0a20CkObnT0hCl9VPEje1szhHat ValueError Only TensorFlow native optimizers are supported with DistributionStrategy. guptapriya same observation as AntGul I am getting the same error ValueError Only TensorFlow native optimizers are supported with DistributionStrategy. I am using tf nightly 2.0 preview 2.0.0.dev20190509 The fix is in estimator https github.com tensorflow estimator commit 818c6163ba759dc257b7c27421bc46ef8259e217 diff 93ed941ec864a6ef78bd656293c99cea Sometimes the version of estimator pip package might be stale. Would you mind updating the estimator package directly and testing then https pypi.org project tf estimator nightly guptapriya you are right. When I did my test I had I am mot using and it works. What is very weird is now I have 2 differents version of estimators I recreated everything from scratch Is it expected Should I report it It s likely that you did pip install on tf estimator nightly while tensorflow 2.0 did an extra tensorflow estimator 2.0 preview under the hood. Can you pip uninstall everything and then just pip install tf nightly 2.0 preview only tanzhenyu you are right. Now I got All good. I misinterpret what was said earlier. I am facing the same error when using tensorflow run time version 1.13 on AI platform. For anyone facing this issue you can simply change the optimizer to be from tf.train instead. This will be a compatible optimizer with tf.keras code worked for me on 1.13 . I found this post helpful on the subject https medium.com tensorflow multi gpu training with estimators tf keras and tf data ba584c3134db 
27705,Keras subclassing and explicit dtype of Input, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 Arch Linux Tensorflow Version 2.0.0 alpha0 Description When using Keras subclassing there is no apparent way of defining the dtype of the Input node of the network. In some cases it would be neccecary to use tf.float16 instead of 32 but as of now i cannot find any way to adjust this. Also trying to set the dtype using self.dtype tf.float16 is not permitted.,In order to expedite the trouble shooting process please provide a code snippet to reproduce the issue reported here. Thanks Method 1 Exception Method 2 Exception Use case The use case is when you want specifically to use custom dtypes for the whole net typically when running on TensorCores etc. Freetext I dont know if this would be the correct way of defining this but it should be possible if its not already to set the dtype of the input excplictly i assume there is a tf.cast in there somewhere anyways... Hi A workaround you can do now is to directly set the private property dtype . I ll make a change that adds dtype as one of the allowed keyword arguments super . init . Automatically closing this out since I understand it to be resolved but please let me know if I m mistaken.Thanks Reopening as I find some issues are there still with tf nightly gpu 2.0 preview 2.0.0.dev20190723 . Here is the gist https colab.sandbox.google.com gist jvishnuvardhan 4ee37c87967ef4abfa7c7cb3bed63c0c tf 27705.ipynb . Thanks For method 2 dtype is a property therefore we see error The error message is fixed with latest tf nightly 2.0 build version 2.0.0 dev20190809 jvishnuvardhan Use method 1. The above error is appearing because 1 2 0 is being converted into separate tensors. Using tf.constant to convert the entire array into a tensor try model tf.constant 1 2 0 . Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27705 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27705 No a 
27750,TF 2 Method estimator.model to estimator fails but model.fit works for tf.keras created model, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Win 10 Colab Linux TensorFlow installed from source or binary Binary TensorFlow version use command below TF 2.0.0 Alpha Python version 3.5 Describe the current behavior The below code has been taken from the Tensorflow without PHD series RNN time series prediction. While converting the Keras model to estimator version below line gives error estimator tf.keras.estimator.model to estimator keras model model fn keras Describe the expected behavior Should not give any error Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,This question is better asked on StackOverflow http stackoverflow.com questions tagged tensorflow since it is not a bug or feature request. There is also a larger community that reads questions there. If you think we ve misinterpreted a bug please comment again with a clear explanation as well as all of the information requested in the issue template https github.com tensorflow tensorflow issues new choose . Thanks ymodak I am not able to relate as to why should this not be a bug A simple operation done with a Keras exported TF model is resulting in an error which should not be happening. On the same model above if I call model.fit it runs absolutely fine so it should also be converted easily using the function tf.keras.estimator.model to estimator . As per the official TF documentation here https www.tensorflow.org versions r2.0 api docs python tf keras estimator model to estimator this should work if the Keras model is valid. I believe this is a bug. Also asked on https stackoverflow.com questions 55772880 tf 2 0 method estimator model to estimator fails but model fit works for tf Reopening this to debug further and handle appropriately. This is indeed a bug which comes from a bad interaction between tf.function and model to estimator. https colab.sandbox.google.com gist robieta d20dee003c8a21aceb251ac3c64583c1 model to est fn.ipynb The RNN just happen to be the only layers that I know of at least which use functions internally which is why this surfaced on the GRU layer. Thanks for reporting the issue. We are able to reproduce it. As robieta stated this issue was caused by the combination of tf.function in keras layer and v1 tf.session which is used by model to estimator. With LSTM GRU v2 the call body has been changed to use tf.function. In model to estimator model was first created with forward graph and then model. make train function which will try to update the graph with gradient part. This will fail if they run under same session with warning message W0613 09 53 26.989550 66929 c api.cc 338 Operation name gru StatefulPartitionedCall id 65 op device def node gru StatefulPartitionedCall StatefulPartitionedCall Tin DT FLOAT DT FLOAT DT RESOURCE DT RESOURCE DT RESOURCE Tout DT FLOAT DT FLOAT DT FLOAT DT FLOAT DT FLOAT ... DT VARIANT DT VARIANT DT VARIANT DT FLOAT DT INT32 gradient op type PartitionedCall 1192 config config proto n 007 n 003GPU 020 000 n 007 n 003CPU 020 0012 002J 0008 001 executor type f forward standard gru 2674 reshape Reshape gru zeros gru kernel gru recurrent kernel gru bias was changed by setting attribute after it was run by a session. This mutation will have no effect and will trigger an error in the future. Either don t modify nodes after running them or create a new session. The follow up K. initialize variables sess will fail since the graph is in a weird state now. Let me see if there is any workaround for this issue. This should now be fixed by https github.com tensorflow estimator commit c956dd32561bac645a1cd870d3c8cfe8e9fe969b which should be available in tf estimator nightly. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27750 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27750 No a Thanks a lot guys 
27769, TF 2.0 keras Unable save and load weights for double nested models, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 mac TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 Python version 3.7 Describe the current behavior load weights throw exception on a doubly nested model Describe the expected behavior load weights should work This problem only happens on two layers of nested model with non trainable weights. The reason is save weights and load weights handles nested model differently save weights call layer.weights for each layer load weights recursively call model.weights if layer is a nested Model Code to reproduce the issue Other info logs This bug is also reported on upstream keras https github.com keras team keras pull 11847 Here is a detailed analysis on why this is happening https github.com keras team keras pull 11847 issuecomment 482438283 Full Exception ,This only affected .h5 format tensorflow checkpoints format works fine. I guess alternatively we can tell users to not use h5 format instead of fixing it zzh8829 What is the alternative way to save a model weights I am having this proble min .hdf5 fromat too. abhigyank the alternative is save to .tf which will create tensorflow checkpoint files instead of hdf5. Any news on this issue I tried the .tf and it works. It might seem like .tf saving works but in my experience the only difference is that it doesn t throw an error. Steps to reproduce Make a model with nested models and set some layers to trainable False Train for some epochs Save weights Evaluate and save metrics Clear everything Make model Load weights Evaluate I am currently submitting a fix for H5. veqtor What problem are you seeing with using the TF format k w w I have tested your fix and it works for me Thank you a lot k w w How can I use your fix I have the same problem. 19giorgosts The fix should be in tensorflow nightly which you can install using pip install tf nightly It might seem like .tf saving works but in my experience the only difference is that it doesn t throw an error. Steps to reproduce Make a model with nested models and set some layers to trainable False Train for some epochs Save weights Evaluate and save metrics Clear everything Make model Load weights Evaluate I am new coder to keras Can you show me a demo about your description Thx Lannist Here https colab.sandbox.google.com gist jvishnuvardhan 0153524fb3f6e0b114ace9da25ac3f77 tf 27769 saveweights tfformat.ipynb is the colab gist to save load the weights in .tf format. Here is the gist https colab.sandbox.google.com gist jvishnuvardhan 48daec236b2c8a5c8cf7a482f258ee8a tf 27769 saveweights h5format.ipynb to save load the weights in .h5 format. The only difference between those two gist is in changing the extension. Thanks I am closing the issue as it was resolved in tf nightly . Please feel free to open if the issue persists again. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27769 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27769 No a is this change gonna be in tf 1 is this change gonna be in tf 1 have you found the solution I using the tensorflow 1.1.4 and meet the same error but can not find way to fix it 19giorgosts The fix should be in tensorflow nightly which you can install using pip install tf nightly how about tensorflow 1.1.4 or 1.1.5 can not install tensorflow nightly by pip I am currently submitting a fix for H5. veqtor What problem are you seeing with using the TF format That didn t work for me using that fix in tf nightly for a siamese model such as 
27776,Cannot compute gradients in graphs using RaggedTensors, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac 10.14 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device n a TensorFlow installed from source or binary pip install tensorflow 1.13.1 TensorFlow version use command below 1.13.1 Python version 3.6 Bazel version if compiling from source n a GCC Compiler version if compiling from source n a CUDA cuDNN version n a GPU model and memory n a Describe the current behavior Computing gradients for some graphs involving RaggedTensor s causes an error Describe the expected behavior The gradients should work. This looks similar to https github.com tensorflow tensorflow issues 26015 Code to reproduce the issue Other info logs Full error for grad of ragged embeddings 1 Full error for ragged embeddings 2 ,I found that this has been fixed in the nightly builds. In 1.14.1 dev20190413 ragged embeddings 1 is fixed but ragged embeddings 2 is not. That is the XlaCompile error goes away but apparently RaggedGather still has no registered gradient Yes you re right the second error still exists. I ve had to avoid using tf.concat on ragged tensors. Also been having trouble with this issue. I have written a quick patch for the RaggedGather The ragged gather op is found in tensorflow python ops gen ragged array ops.py This is working for concat s along a ragged dimension. I ve only tested with ragged rank 1 Happy to work this into a future PR. Thanks coopie Your code works perfect for input ragged ragged rank is 1 and indices is 1D . Just a typo want to remind all grads inside the function should be out grads Updated model is able to running now but compared to convert it to tensor and then gather RaggedGather does not converge. Updated I figured it out there is a bug here which should be the whole code then it could converge Fixed in f7415d1ef. Gradient is now defined for RaggedGather for any inputs including ragged rank 1 and indices.ndims 1 . Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27776 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27776 No a 
27785,How to s fully connected reader.py raises an AttributeError, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution Mac OSX Mobile device No TensorFlow installed from source or binary pip installed TensorFlow version tensorflow 1.12.0 Python version Python 2.7.16 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version no GPU model and memory no You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION v1.12.0 rc2 3 ga6d8ffae09 1.12.0 Describe the current behavior When running this how to script https github.com tensorflow tensorflow blob master tensorflow examples how tos reading data fully connected reader.py An AttributeError is raised Describe the expected behavior Code should run without error in the how to Code to reproduce the issue Call the Python file with tf installed To fix To fix this error I changed line 129 from to ,The example has been updated to support latest version of TF. is correct for 2.0 compatibility. The master branch example https github.com tensorflow tensorflow blob master tensorflow examples how tos reading data fully connected reader.py should pass in TF 1.13. You can also try TF 1.12 branch example https github.com tensorflow tensorflow blob r1.12 tensorflow examples how tos reading data fully connected reader.py for your case. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27785 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27785 No a 
27807,NNAPI doesn t support tensors with rank 0 index 3 name PadV2 constant values , em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information OS Platform and Distribution Windows 10 and Android 9.0 Mobile device Xiaomi 8 TensorFlow version org.tensorflow tensorflow lite 0.0.0 nightly using 1.11.0 has the same problem Model resnet18 convert from Pytorch logs from Android 9.0 E tflite NNAPI doesn t support tensors with rank 0 index 3 name PadV2 constant values E tflite Returning error since TFLite returned failure nnapi delegate.cc 736. Failed to build graph for NNAPI other if I use resnet50 from tensorflow.contrib.slim.python.slim.nets it also has similar problem like E tflite NNAPI doesn t support tensors with rank 0 index 7 name inference model Mean reduction indices Returning error since TFLite returned failure nnapi delegate.cc 736. Failed to build graph for NNAPI , RoyIronGrey In order to expedite the trouble shooting process please provide a code snippet to reproduce the issue reported here. Thanks RoyIronGrey In order to expedite the trouble shooting process please provide a code snippet to reproduce the issue reported here. Thanks Below is Python Code and Script to describe how to get the model RoyIronGrey In order to expedite the trouble shooting process please provide a code snippet to reproduce the issue reported here. Thanks And Thanks for your attention miaowang14 can you provide an update on scalar support I have a pending fix internally and will further test improve it. Looking forward to get it ready for the next couple of days. RoyIronGrey is the issue still reproducible on top of tree RoyIronGrey is the issue still reproducible on top of tree Thanks for your reply. Yes If I use the same edition of TF lite API. Is there anything new or any new edition for this problem RoyIronGrey is the issue still reproducible on top of tree Or I may misunderstand what you mean RoyIronGrey We just submitted a couple of fixes related to this issue. I don t think tensorflow lite 0.0.0 nightly or 1.11.0 build include them. You might be able to build TFLite binaries from source and try it out https www.tensorflow.org lite guide build arm64 The nightly should have the fixes but you might need to clear your gradle cache https stackoverflow.com questions 23025433 how to clear gradle cache . The nightly should have the fixes but you might need to clear your gradle cache https stackoverflow.com questions 23025433 how to clear gradle cache . Yes It works. But why does it cost more time and unsteady Maybe I made some mistakes Below it s the difference I guess the performance issues could be related two possible causes 1. There are still operations in the Model not supported by NNAPI so they fallback to TFLite CPU implementation. The overhead and sync cost between multiple subgraphs could be high. 2. The model itself is a floating point model while Mi8 may just have an hardware accelerator Hexagon DSP that can only accelerate quantized 8bit models. I will mark this specific issue as fixed. But feel free to keep comment on it or create new ones if you have questions about the performance and NNAPI delegate. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27807 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27807 No a I will mark this specific issue as fixed. But feel free to keep comment on it or create new ones if you have questions about the performance and NNAPI delegate. Thanks a lot I ll stay focus on the processing of this project as it really supports my research 
27829,Cannot create a stateful RNN with recurrent dropout, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOSX 10.13.6 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below tf.version.VERSION 2.0.0 dev20190413 tf.version.GIT VERSION v1.12.0 12481 gc7ce6f4cd9 Python version 3.6.8 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior I get an exception when trying to use recurrent dropout in a stateful RNN The full stacktrace is below. Describe the expected behavior No exception. Code to reproduce the issue Other info logs Complete stacktrace , ageron In order to expedite the trouble shooting process please provide a code snippet to reproduce the issue reported here. Thanks Hi muddham I did It s in the section Code to reproduce the issue . ageron I tried to reproduce the bug in TF2.0.0 alpha0 but I don t get the runtime error. I see a warning and a deprecation message as follows. Just for your info I ran your code in Google colab WARNING Logging before flag parsing goes to stderr. W0418 17 16 01.808634 139806816728960 deprecation.py 506 From usr local lib python3.6 dist packages tensorflow python keras backend.py 4081 calling dropout from tensorflow.python.ops.nn ops with keep prob is deprecated and will be removed in a future version. Instructions for updating Please use rate instead of keep prob . Rate should be set to rate 1 keep prob . Please let me know what you think. Thanks Apparently the problem is now fixed I don t get the error anymore. Thanks jvishnuvardhan . Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27829 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27829 No a ageron does it still work for you I tried using recurrent dropout with a GRU as you are and it seems to break for me. The problem seems to be with recurrent dropout cos if you switch it out everything seems to work. This problem also exists with LSTMs and not just GRUs. Apparently the bug is back. Using VERSION 2.0.0 dev20190524 and GIT VERSION v1.12.1 2720 geafe861c2b . I am also having a similar issue. Was wondering if there was an update or an older nightly where this is stable jlanday it worked when I posted my comment on April 19th so perhaps try a nightly from April 18th or 19th ageron I tried to use the nightly version from both April 18th and 19th and it looks like it still doesn t work. Does version pip install tf nightly gpu 2.0 preview 2.0.0 dev20190518 work for you ageron I don t see any error with pip install tf nightly . Gist is here https colab.sandbox.google.com gist jvishnuvardhan 4b32a507c8209a95d38335e8efc3e231 untitled203.ipynb . But I notice error is back with pip install tf nightly gpu 2.0 preview 2.0.0 dev20190518 Thanks Thanks for reporting the issue will send a fix very soon. Should now be fixed by https github.com tensorflow tensorflow commit 6a6e8c2586dfd2aeeebe0d94d60dcca4604ab481. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27829 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27829 No a def build cell self char input self.tf.keras.layers.Input shape 1 batch size 1 name char input dtype self.tf.int32 char input one hot self.tf.keras.backend.one hot char input self.vocab size previous hidden state input self.tf.keras.layers.Input shape self.num units batch size 1 name previous hidden state input previous cell state input self.tf.keras.layers.Input shape self.num units batch size 1 name previous cell state input hidden input stacked self.tf.keras.layers.concatenate self.tf.keras.backend.squeeze char input one hot axis 1 previous hidden state input axis 1 Forget gate forget gate f self.tf.keras.layers.Dense units self.num units activation sigmoid hidden input stacked Helps us to take decisions about what must be removed from previous hidden state Input gate input gate i self.tf.keras.layers.Dense units self.num units activation sigmoid hidden input stacked Decides which values to update input gate g self.tf.keras.layers.Dense units self.num units activation tanh hidden input stacked Creates a vector for new candidates to added to present cell state. Output gate output state o self.tf.keras.layers.Dense units self.num units activation sigmoid hidden input stacked Current cell state current cell state self.tf.keras.layers.add self.tf.keras.layers.multiply input gate g input gate i self.tf.keras.layers.multiply previous cell state input forget gate f name current cell state output hidden state output hidden state self.tf.keras.layers.multiply self.tf.keras.layers.Activation tanh current cell state output state o name output hidden state output char probs output char probs self.tf.keras.layers.Dense units self.vocab size activation softmax name output char probs output hidden state cell self.tf.keras.Model inputs char input previous hidden state input previous cell state input outputs output char probs output hidden state current cell state return cell This code will break tensorflow 2.0 I found the problem to be in tensorflow tensorflow python keras layers merge.py with the add and multiply functions. Please fix The error I received trying to feed input in raise RuntimeError Variable value not supported. Use RuntimeError Variable value not supported. Use var.assign var value to modify the variable or var var value to get a new Tensor object. Can the line https github.com tensorflow tensorflow blob 3a094e6d1c927967438c0b263db0bf42d16813ae tensorflow python keras layers merge.py L245 be changed to output output inputs i to fix this and similar within Subtract etc. It doesn t like the notation when applied to a tf.Variable 
27845,Wrong derivatives for complex second order derivatives., em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 OSX Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below tensorflow 1.12.0 Python version 3.6.8 You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION Describe the current behavior Derivatives of non holomorphic functions are incorrect when compared both against AD and finite differences. Describe the expected behavior Derivatives of non holomorphic functions should becorrect. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. Additional information https github.com google jax issues 603,Ran this on tensorflow 2.0.0 dev20190327 and I get the same incorrect output. Thanks for the minimal code snippet to reproduce the issue. I was able to reproduce the behavior in TF 1.13 and latest nightly build. Thanks for filing the issue If I replace tf.abs on your example with a manual implementation tf.sqrt real x real x imag x imag x the values are identical so I think this is a problem with the gradient for the ComplexAbs op. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27845 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27845 No a Thanks for fixing this guys 
27847,BUG tfdbg session cannot be used with SessionRunHooks, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 ArchLinux Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below b v1.13.0 rc2 5 g6612da8 1.13.1 Python version 3.7 Bazel version if compiling from source n a GCC Compiler version if compiling from source n a CUDA cuDNN version n a GPU model and memory n a The following code Throws I believe this issue was introduced in https github.com tensorflow tensorflow commit 1f26c65254268730b7409f517d1ed1b554d01e50 a year ago. flatten cannot handle fetches created by hooks. The fix will be to obtain empty fetches in a smarter way.,Fixed by https github.com tensorflow tensorflow commit e2d269edb9217411fc4119338df949e1a741432b Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27847 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27847 No a 
27848,Error filename logging with tf.logging.warn, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 windows 7 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below 1.13.1 Python version 3.7.0 You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION Describe the current behavior stdout prints Describe the expected behavior stdout prints Code to reproduce the issue Other info logs since the logging.warn of python3 is the wrapper of logging.warning code is here https github.com python cpython blob master Lib logging init .py L1459 this adds one frame making the get caller 4 https github.com tensorflow tensorflow blob v1.13.1 tensorflow python platform tf logging.py L49 of tf.logging miss the warn method. ,Added PR 27871 for the fix. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27848 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27848 No a 
27875,Second Order derivative of sparse softmax cross entropy with logits, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Non OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary PyPI TensorFlow version use command below 2.0 alpha Python version 3.6 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behaviour Currently trying to take second order derivatives of sparse softmax cross entropy with logits leads to the error Describe the expected behavior This should work. ,Just a quick show for a workaround and potentially how this might be handled internally Yes I agree that this is a problem. Are you interested in contributing a version of this as a pull request One issue we had is that because the fused op emits both the cross entropy and the linearization if we naively used RegisterGradient we d always compute the second derivative which is expensive but we can do the same trick we do in https github.com tensorflow tensorflow blob 2f3eb7b5c2fd927ec2b21ae972a39788cdce89c4 tensorflow python ops nn grad.py L530 to avoid computation if not needed. Sorry was on holiday. I can try to cook up something. Should I attempt what you have linked of the gradient of the normal Softmax with Logits botev I think https github.com tensorflow tensorflow pull 22231 is already doing this Closing this issue since the associated PR has been merged. Thanks It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 27875 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 27875 No a I am getting this error in Python as well similar to https github.com tensorflow tensorflow issues 27875 issue 433504286. Is there any update on this issue Is there any short term workaround for it 
27892,Crash on TF 1.13 when custom RNNCell which has Template . , em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Arch Linux Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary pip TensorFlow version use command below v1.13.0 rc2 5 g6612da8 1.13.1 Python version 3.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 10.1 7.5.0 GPU model and memory GTX 1080 8G You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION Describe the current behavior A code below works well on Python 3.6 with TF 1.12. On Python 3.7 with TF 1.13 however the code crashes. Describe the expected behavior Do not crash Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Traceback Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,Sorry for the breakage seems that the issue has already been fixed by https github.com tensorflow tensorflow commit eb741cedf304b411af6cae8ec2d60578cd3980a1. Which hasn t reach the release yet. I tested the same code in nightly release and the error is mitigated. For now I think you can update your code with following line since the template does not have any updates. Thanks I should have checked this on master version. Since the problem already has been fixed I ll close the issue. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27892 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27892 No a 
27949,Tensorflow 2.0 No gradients provided for any variable but only when tf.math.square AND tf.function is used ,I posted this in SO https stackoverflow.com questions 55730930 tensorflow 2 0 no gradients provided for any variable but only when tf math squ but now am wondering if it is actually a bug. , fchollet I think what s happening here is that the tf.math.square not wrapped in a lambda layer is messing the functional model s ability to be called so self.model X leaves unconnected inputs and outputs. I had the same issue with the tf.math.pow function as well. But the issue disappears if you put the tf.math operation inside the function with the tf.function annotation. robieta omalleyt12 I think this is related to another issue you re working on can you check cottrell could you try this with the 2.0 nightly i think this should be fixed omalleyt12 Could you please tell me how to install the 2 0 nightly Should I build it from source or I can just install it just by pip install jiarenyf I think https pypi.org project tf nightly 2.0 preview but it might not yet work in new python 3.7 and up. See posts like this one if you hit problems in your python version https github.com tensorflow tensorflow issues 28691 cottrell Thank you I could install the tf nightly up 2.0 preview in python 3.7 and ubuntu 18.04 and solve the problem of No gradients provided for any variable . I have having same issue with tf.math.sqrt along with MirroredStrategy and tf.function Hi mdasadul is this issue occurring with the latest nightly omalleyt12 yes. I intalled tf nightly gpu 2.0.0 dev20190522 . And my case is quite interesting. It s happening only when I am using mirrored strategy. I think it s something to do with mirrored strategy and tf.function mdasadul thanks for reporting this could you provide a minimal example that reproduces this I was trying to create a minimal example but couldn t recreate the issue with minimal example. I can share my code if you share your email address Thanks omalleyt12 Here is what the minimal example looks like although I am getting NotImplementedError when using distributed strategy. The model looks like this The training class without distributed strategy looks like as follows and I was able to save the model without distributed strategy When I am adding distributed strategy as follows I am getting NotImplementedError Thanks so much mdasadul can you post the stack trace of the error you are seeing anj s omalleyt12 Here is the stack trace Thanks mdasadul seemuch is working on adding support for saving a model under distribution strategy scope using the save model.save API. seemuch Can you update this issue once the fix is in Thanks anj s can you provide workaround to save model in distribution mode for current tf version My main goal is convert tf model subclassed model of tf.keras.Model which was trained using custom loop to tf lite. How can I do it currently without tf.saved model.save API I believe this issue is fixed in the latest 2.0 nightly. Although the tf.saved model.save train obj.model dist model line should be called out side of the distribution scope. I am going to close this issue for now since it should be fixed. If there is any more updates feel free to let me know. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27949 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27949 No a seemuch thank for update If you can please look at this issue https stackoverflow.com questions 56739596 how to convert trained in custom loop subclassed tf keras model to tflite I can t understand how I can convert subclassed tf.keras.Model to SavedModel due to the following warning Skipping full serialization of Keras model because its inputs are not defined. Although the tf.saved model.save train obj.model dist model line should be called out side of the distribution scope. It s not the true because I got the next error RuntimeError Need to be inside with strategy.scope for tensorflow.python.distribute.mirrored strategy.MirroredStrategy object at 0x7fd50d505ef0 Hitting a similar issue with ValueError Num gradients 0 generated for op name nvp layer 5 StatefulPartitionedCall op StatefulPartitionedCall input S input B input nvp layer 5 StatefulPartitionedCall args 2 input nvp layer 5 StatefulPartitionedCall args 3 input nvp layer 5 StatefulPartitionedCall args 4 attr key Tin value list type DT FLOAT type DT FLOAT type DT RESOURCE type DT RESOURCE type DT RESOURCE attr key Tout value list type DT FLOAT attr key gradient op type value s PartitionedCall 4641 attr key config value s attr key config proto value s n 007 n 003CPU 020 001 n 007 n 003GPU 020 0002 002J 0008 001 attr key executor type value s attr key f value func name inference call 4640 Is there some easy workaround beside not using tf.function I ve tried constructing most things inside tf.function call but can t seem to get around this one. I have a feeling some update of tf rc0 or tfp broke it. I can try to open a new issue with a minimal example if it will help but I think things might be changing in the versions so not sure it is worth it. This is a separate issue. Can you file a separate bug with instructions to reproduce On Thu Aug 29 2019 at 2 52 PM David Cottrell notifications github.com wrote Hitting a similar issue with ValueError Num gradients 0 generated for op name nvp layer 5 StatefulPartitionedCall op StatefulPartitionedCall input S input B input nvp layer 5 StatefulPartitionedCall args 2 input nvp layer 5 StatefulPartitionedCall args 3 input nvp layer 5 StatefulPartitionedCall args 4 attr key Tin value list type DT FLOAT type DT FLOAT type DT RESOURCE type DT RESOURCE type DT RESOURCE attr key Tout value list type DT FLOAT attr key gradient op type value s PartitionedCall 4641 attr key config value s attr key config proto value s n 007 n 003CPU 020 001 n 007 n 003GPU 020 0002 002J 0008 001 attr key executor type value s attr key f value func name inference call 4640 Is there some easy workaround beside not using tf.function I ve tried constructing most things inside tf.function call but can t seem to get around this one. I have a feeling some update of tf rc0 or tfp broke it. I can try to open a new issue with a minimal example if it will help but I think things might be changing in the versions so not sure it is worth it. You are receiving this because you were assigned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 27949 email source notifications email token AAABHRKJVUBUS2XG2IFJHS3QHBALJA5CNFSM4HG3NY3KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD5P5NJA issuecomment 526374564 or mute the thread https github.com notifications unsubscribe auth AAABHRMDZ3IZLRLP3YRBU4LQHBALJANCNFSM4HG3NY3A . Alex I am getting this error again I have one encoder model and output of that I am passing to three different decoders and adding loss of all three and then doing backward propagation. But I am getting this error ipython input 66 94a2de13802b 61 train step optimizer.apply gradients zip gradients variables usr local lib python3.6 dist packages tensorflow core python keras optimizer v2 optimizer v2.py 427 apply gradients grads and vars filter grads grads and vars usr local lib python3.6 dist packages tensorflow core python keras optimizer v2 optimizer v2.py 1025 filter grads v.name for v in grads and vars ValueError No gradients provided for any variable encoder embedding embeddings 0 encoder gru kernel 0 encoder gru recurrent kernel 0 encoder gru bias 0 decoder a embedding 1 embeddings 0 decoder a gru 1 kernel 0 decoder a gru 1 recurrent kernel 0 decoder a gru 1 bias 0 decoder a dense 3 kernel 0 decoder a dense 3 bias 0 decoder a bahdanau attention 1 dense 4 kernel 0 decoder a bahdanau attention 1 dense 4 bias 0 decoder a bahdanau attention 1 dense 5 kernel 0 decoder a bahdanau attention 1 dense 5 bias 0 decoder a bahdanau attention 1 dense 6 kernel 0 decoder a bahdanau attention 1 dense 6 bias 0 decoder b embedding 2 embeddings 0 decoder b gru 2 kernel 0 decoder b gru 2 recurrent kernel 0 decoder b gru 2 bias 0 decoder b dense 7 kernel 0 decoder b dense 7 bias 0 decoder b bahdanau attention 2 dense 8 kernel 0 decoder b bahdanau attention 2 dense 8 bias 0 decoder b bahdanau attention 2 dense 9 kernel 0 decoder b bahdanau attention 2 dense 9 bias 0 decoder b bahdanau attention 2 dense 10 kernel 0 decoder b bahdanau attention 2 dense 10 bias 0 decoder c embedding 3 embeddings 0 decoder c gru 3 kernel 0 decoder c gru 3 recurrent kernel 0 decoder c gru 3 bias 0 decoder c dense 11 kernel 0 decoder c dense 11 bias 0 decoder c bahdanau attention 3 dense 12 kernel 0 decoder c bahdanau attention 3 dense 12 bias 0 decoder c bahdanau attention 3 dense 13 kernel 0 dec... seemuch I have same problem. ValueError No gradients provided for any variable Variable 0 Variable 0 Variable 0 Variable 0 . I have a tf.pow in the computation. Others are just tf.add tf.matmul tf.sigmoid. So I guess this tf.pow or tf.math.pow causes the problem. The gradient chain is broken. I am also getting the same issue while writing custom layer functionality hansong999 UdiBhaskar can you open separate issues with the instructions to reproduce your problems I have the same problem my model consists of three layers of dnn and uses a cross entropy loss for multi tasks finally add these multi tasks loss for train but i had just got an error like that ValueError No gradients provided for any variable class model ......... def loss self outputs task labels losses ctr label tf.cast task labels ... 0 tf.float32 ctr tf.sigmoid outputs ... 0 ctr loss tf.keras.losses.binary crossentropy ctr label ctr losses.append ctr loss for i in range 1 self.num classes task label tf.cast task labels ... i tf.float32 ctr label cvr tf.sigmoid outputs ... i task loss tf.keras.losses.binary crossentropy task label ctr cvr losses.append task loss return losses tasks losses model.loss tasks outputs train task labels def loss all func return tf.add n tasks losses join op tf.keras.optimizers.Adamax 0.001 .minimize loss all func var list tf.compat.v1.trainable variables Please file a new bug for your issue. 
28003,TF 2.0 Bug or Feature keras includes duplicated shared variables in model.variables, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary pip nightly TensorFlow version use command below 2.0.0 dev20190415 Python version 3.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION Describe the current behavior The two following layers give different number of variable in model.variables . Keras duplicates shared variables in the variable list The first print The second prints Describe the expected behavior I would expect two layers have the same number of variables. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,I was able to reproduce the behavior with This is fixed with latest version of TensorFlow nightly build version 2.1.0 dev20191021 Output Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 28003 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 28003 No a 
28010, TF2.0 Not JSON Serializable error wasn thrown when using tf.keras.activations operators in keras model., System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 x64 1809 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary pip TensorFlow version use command below tensorflow gpu 2.0.0a0 Python version 3.6.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 10 GPU model and memory Geforce 1070 Describe the current behavior When I used tf.keras.activations operators in my keras model serialization of model was failed due to Not JSON Serializable error. Describe the expected behavior It should be serialized without any error. Code to reproduce the issue If you changed from Activation to relu it failed to serialize.,I have slightly edited your code to be consistent with and was able to reproduce the error using TF gpu 2.0 alpha. Output I got the same error but with tf.keras.backend.expand dims tf.keras.backend.squeeze tf.expand dims tf.squeeze returns W0501 00 06 16.129765 139790005454464 tf logging.py 161 Model failed to serialize as JSON. Ignoring... Not JSON Serializable b n nExpandDims x12 nExpandDims x1a x05input x1a x0eExpandDims dim x07 n x01T x12 x020 x01 n n x04Tdim x12 x020 x03 Additionally using operator instead of tf.keras.layers.Add makes this issue same. I am facing the same issue any suggestions or solutions for work around for now I am having the same issue when using tf.concat in an otherwise keras only model can t use keras Concatenations due to issue 30355 . Only work around I have found so far is to use model.save weights . When loading you need to redefine the model and then use model.load weights Having same issue with TF 1.14 and Python 3.6 with tf.transpose and tensorflow.python.keras.layers.concatenate W0717 09 24 35.629342 14908 summary ops v2.py 1110 Model failed to serialize as JSON. Ignoring... Not JSON Serializable b n ttranspose x12 tTranspose x1a ndense Relu x1a x0etranspose perm x0b n x05Tperm x12 x020 x03 x07 n x01T x12 x020 x01 W0717 13 56 19.515430 16984 summary ops v2.py 1110 Model failed to serialize as JSON. Ignoring... Not JSON Serializable b n x05Shape x12 x05Shape x1a x15concatenate 28 concat x07 n x01T x12 x020 x01 x0e n x08out type x12 x020 x03 Thus callbacks.ModelCheckpoint with save weights only False causes the training to stop because it fails to backup the model I get a useless model file of 6 KB . As mketcha mentionned a workaround is to use save weights only True . But it should only be temporary waiting a fix because saving only the weights forces to keep the code of the corresponding model somewhere and this is really annoying if the model code changes often. Same problem here. Is this being worked on TensorFlow 2.0.0 b1 Was able to repro and have a fix out for this should be in in a day or two Hello Any news Thanks Hello Any news Thanks Should be fixed https github.com tensorflow tensorflow commit 7cc180f107f142432358ac33787466de90afd776 Closing this issue since its fixed in latest tf nightly build 2.0.0 dev20190802 Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28010 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28010 No a Had the problem callbacks.ModelCheckpoint and ModelCheckpoint setting save weights only True solved the problem for me. Had the problem callbacks.ModelCheckpoint and ModelCheckpoint setting save weights only True solved the problem for me. yassinetb Did you have any solution for the error TypeError Not JSON Serializable tf.float32 from setting save weights only False I am facing the same issue using tf.keras.callbacks.ModelCheckpoint . Any solution 
28029,sample weight ignored, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes. OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04.5 LTS Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary Docker image from tensorflow tensorflow latest py3 jupyter TensorFlow version use command below b v1.13.1 0 g6612da8951 1.13.1 Python version 3.5.2 default Nov 12 2018 13 43 14 GCC 5.4.0 20160609 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior sample weight seems to be ignored by fit test on batch etc. Describe the expected behavior sample weight should affect the loss. Code to reproduce the issue ,In TF 2.0.0 alpha0 the behavior is as expected that led me to believe the behavior in 1.13.1 is a bug. Looks like behavior changed in https github.com tensorflow tensorflow commit bacd851cf8281b3a87d782bbaf1888df571337a6 diff 4f0d455edc07c640cbb22c8e39a61dd5. Presuming I cannot update to a nightly what is the workaround meanwhile abirkmanis I can reproduce the issue. Output from TF2.0 is 1.0 and 0.1 whereas output from TF1.13.1 is 1.0 and 1.0. Thanks Hi abirkmanis looks like this was fixed as you say IIRC previous behavior was to divide the sample weights by the mean of the sample weights which is why it looks like it is being ignored in this case. My suggestion is if the mean of your sample weights is not 1 then to either rescale the sample weights or rescale your loss fn in order to get the desired behavior Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28029 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28029 No a 
28045,Graph transformations propagate fixed sizes.cc Unhandled operator type CTCBeamSearchDecoder, System information Have I written custom code yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04 TensorFlow installed from source or binary source TensorFlow version use command below 1.13.1 Python version 3.7 Bazel version if compiling from source 0.24.1 GCC Compiler version if compiling from source 5.4.0 CUDA cuDNN version 10.0 7.5 Exact command to reproduce Also tried Describe the problem My frozen model contains ctc beam search decoder operator. When I try to convert .pb model to .tflite I get How could I avoid this problem I dind t find any issue contains such problem. ,Thanks for filing the issue I have a fix https github.com tensorflow tensorflow commit a90747cc3e8b292d4de3248652d93166524446f0 can you try that It works. Now I get error TensorFlow Lite currently doesn t support control flow ops Merge Switch but I saw another issue about it. Thank you for resolving my problem Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28045 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28045 No a 
28070,tf2.0a0 tf.nn.ctc loss with AttributeError Tensor.op is meaningless when eager execution is enabled., System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes. OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary pip install tensorflow gpu 2.0.0 alpha TensorFlow version use command below 2.0.0 alpha0 v1.12.0 9492 g2c319fb Python version 3.7.0 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 10.0 fogotten GPU model and memory Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. When running python loss.py error raised see the error.txt for details. Maybe there is some internal implementation error on the function namely tf.nn.ctc loss ... 1. helper.py 2. loss.py 3. error.txt Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. , shashvatshahi1998 gadagashwini ebrevdo Could you please help me thank you. jiarenyf refer to this issue 27739 for further updates as your code is showing same Attribute error. I found that changing the tf.nn.ctc loss to tf.compat.v1.nn.ctc loss is ok but when would the error in 2.0.0 alpha being fixed... Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28070 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28070 No a 
28083,Tensor Indexing Loses Shape Information, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 TensorFlow installed from source or binary binary TensorFlow version use command below v1.12.0 9492 g2c319fb415 2.0.0 alpha0 Python version 3.7.3 Describe the current behavior Sliced tensors e.g. my tensor lose their shape information. Using tf.slice does seem to work but for that I need to have all dimensions defined which is more often than not simply not the case. Describe the expected behavior Shape information is kept and adjusted to match the sliced data ,In order to expedite the trouble shooting process please provide a code snippet to reproduce the issue reported here. Thanks Code import tensorflow as tf print tf. version inp tf.keras.layers.Input shape 32 32 3 dtype tf.dtypes.float32 print inp t inp 2 print t tf.1.13 output 1.13.1 Tensor input 1 0 shape 32 32 3 dtype float32 Tensor strided slice 0 shape 2 32 3 dtype float32 tf.2.0 output 2.0.0 alpha0 Tensor input 1 0 shape None 32 32 3 dtype float32 Tensor strided slice 0 dtype float32 You can see the missing shape information which makes further computations significantly more complicated Thank you for providing the code snippet. I was able to reproduce. achandraa jvishnuvardhan can you please provide me with the file where I can contribute robieta can you take a look I think this is due to behavior of the under the hood keras graph losing some shape inference data from placeholders. This is fixed latest tf 2.0 nightly version 2.0.0 dev20190906 . Thanks Output Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28083 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28083 No a 
28102,The Keras examples should load data with allow pickle True, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No I was trying the tutorials keras basic text classification OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOS High Sierra 10.13.5 TensorFlow installed from source or binary binary TensorFlow version use command below 1.13.1 Python version 3.6.2 Numpy version 1.16.3 Describe the current behavior When I tried the following code from the basic text classification sample It failed with error According to the change of numpy in https github.com numpy numpy pull 13359 the default value of allow pickle was changed from True to False. Please update the sample code accordingly may pass in allow pickle True explicitly to np.load . Describe the expected behavior The basic text classification example could be followed without error. Other info logs When I downgrade numpy from 1.16.3 to 1.16.1 whose default value was still allow pickle True I could finish the basic text classification example successfully. ,You need to use np.load path with the new parameters np.load path allow pickle where allow pickle is a boolean. This is a recent change in numpy which appears to be for improved source integrity. I encountered the same error as you listed updating the method call in the respective .py files solves it. In your case find imdb.py and change np.load path in line 86 to np.load path allow pickle True You need to use np.load path with the new parameters np.load path allow pickle where allow pickle is a boolean. This is a recent change in numpy which appears to be for improved source integrity. I encountered the same error as you listed updating the method signature in the respective .py files solves it. Good suggestion I could make this change in my local environment first. I still suggest the official code could be updated thus other developers could avoid the issue. vamcily Think the issue has been fixed in https github.com tensorflow tensorflow commit 79a8d5cdad942b9853aa70b59441983b42a8aeb3 vamcily Think the issue has been fixed in 79a8d5c https github.com tensorflow tensorflow commit 79a8d5cdad942b9853aa70b59441983b42a8aeb3 Nice Which release would include this fix I tried the suggested solution but error still there You need to use np.load path with the new parameters np.load path allow pickle where allow pickle is a boolean. This is a recent change in numpy which appears to be for improved source integrity. I encountered the same error as you listed updating the method call in the respective .py files solves it. In your case find imdb.py and change np.load path in line 86 to np.load path allow pickle True I tried the suggested code change but the error still there what is the problem in your opinion hi there i use the solution before i solved it my numpy version is 1.16.3 and then Downgrate numpy to fix a problem pip install numpy 1.16.2 import numpy as np print np. version it fixed my problem the same as yours best luck I tried the suggested solution but error still there You need to use np.load path with the new parameters np.load path allow pickle where allow pickle is a boolean. This is a recent change in numpy which appears to be for improved source integrity. I encountered the same error as you listed updating the method call in the respective .py files solves it. In your case find imdb.py and change np.load path in line 86 to np.load path allow pickle True I tried the suggested code change but the error still there what is the problem in your opinion This method does work however I meet the same problem as yours. If you are using Jupyter Notebook you need to restart Jupyter Notebook after some changes made in local package file. Running code like from keras.datasets import imdb again is useless. The interesting point is that in Jupyter the codes showed as part of errors are real time when the loaded files are not then it may make us confused. response container BBPPID font family initial font size initial color initial In a Jupyter notebook at the very top add a coffee feel that contains pip install numpy 1.16.2Them restart the runtime. This forces that notebook to use ver1.16.2 if numpy. Waste no more time arguing what a good man should be just be one. Marcus Aurelius From notifications github.comSent April 28 2019 04 15To tensorflow noreply.github.comReply to reply reply.github.comCc colin colinwu.ca manual noreply.github.comSubject Re tensorflow tensorflow The Keras examples should load data with allow pickle True 28102 I tried the suggested solution but error still there You need to use np.load path with the new parameters np.load path allow pickle where allow pickle is a boolean. This is a recent change in numpy which appears to be for improved source integrity. I encountered the same error as you listed updating the method call in the respective .py files solves it. In your case find imdb.py and change np.load path in line 86 to np.load path allow pickle True I tried the suggested code change but the error still there what is the problem in your opinion This method does work however I meet the same problem as yours. If you are using Jupyter Notebook you need to restart Jupyter Notebook after some changes made in local package file. Running code like from keras.datasets import imdb again is useless. The interesting point is that in Jupyter the codes showed as part of errors are real time when the loaded files are not then it may make us confused. You are receiving this because you are subscribed to this thread.Reply to this email directly view it on GitHub or mute the thread. hi there i use the solution before i solved it my numpy version is 1.16.3 and then Downgrate numpy to fix a problem pip install numpy 1.16.2 import numpy as np print np. version it fixed my problem the same as yours best luck that finally solved for me I downgraded numpy version from 1.16.3 to 1.16.1 which default value is still allow pickle True. just remove current version then install the 1.16.1 version via pip uninstall numpy pip install upgrade numpy 1.16.1 enjoy I am still getting the same error in Google Colab I tried both pip uninstall numpy pip install upgrade numpy 1.16.1 and pip install numpy 1.16.2 import numpy as np print np.version also but still not working for Google Colab do we need to do something extra. x train y train x valid y valid imdb.load data num words n unique words usr local lib python3.6 dist packages numpy lib format.py in read array fp allow pickle pickle kwargs 690 if pickle kwargs is None 691 pickle kwargs 692 try 693 array pickle.load fp pickle kwargs 694 except UnicodeError as err ValueError Object arrays cannot be loaded when allow pickle False Did you restart your runtime after installing numpy 1.16.2 Click Runtime menu then Restart runtime . Colin Wu Life would be so much easier if I had the source code. On Mon 29 Apr 2019 at 11 02 subhobrata notifications github.com wrote I am still getting the same error in Google Colab I tried both pip uninstall numpy pip install upgrade numpy 1.16.1 and pip install numpy 1.16.2 import numpy as np print np.version also but still not working for Google Colab do we need to do something extra. x train y train x valid y valid imdb.load data num words n unique words usr local lib python3.6 dist packages numpy lib format.py in read array fp allow pickle pickle kwargs 690 if pickle kwargs is None 691 pickle kwargs 692 try 693 array pickle.load fp pickle kwargs 694 except UnicodeError as err ValueError Object arrays cannot be loaded when allow pickle False You are receiving this because you are subscribed to this thread. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 28102 issuecomment 487612628 or mute the thread https github.com notifications unsubscribe auth AAZPLFLNI6OJMAIAKJE3UUDPS4EZ7ANCNFSM4HIBIQ2Q . yes I restarted the runtime. For full code I created a notebook in Github. Below is the link https github.com subhobrata Keras issues blob master Keras issue.ipynb keras.datasets.imdb is broken in 1.13 and 1.14 by np 1.16.3. Its fixed in tf nightly. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28102 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28102 No a https stackoverflow.com questions 55890813 how to fix object arrays cannot be loaded when allow pickle false for imdb loa this here fix code import numpy as np np load old np.load np.load lambda a k np load old a allow pickle True k 
28114,For ConcatV2 axis really require int64 support , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device NA TensorFlow installed from source or binary source TensorFlow version use command below 0.12.0 Python version 2.7.15rc1 Bazel version if compiling from source 0.15.0 GCC Compiler version if compiling from source 7.3.0 CUDA cuDNN version NA GPU model and memory NA Describe the current behavior The testConcatAxisType test in concat op test is failing on an assertion for s390x Describe the expected behavior Assert values should match. Code to reproduce the issue On Intel On s390x Other info logs From above logs we can see that when we pass a constant axis of int64 type to ConcatV2 we get different results on s390x which causes assertion failure. After searching the historical records I found this PR 14251 which has introduced this test case. However when I scanned the TensorFlow code for checking how the concat operations work I found that it deals mostly with int32 type. Getting different results on s390x for 32 Vs 64 bit data types is nothing new. We have handled couple of cases in issue 11431 and 14017 and it happens because of NumPy s different behavior on Intel Vs Z PR 12963 . As my code understanding with concat is different I am curious to understand if we really need to process int64 type of axis is it a real time use case or its just added to fill the gap between ConcatV2 supported types Vs its kernel implementation I might be wrong but want to be sure that I proceed to find fix for the right thing. Please check and confirm., mrry aselle do you have any ideas I think this was fixed by the change to concat op.cc in https github.com tensorflow tensorflow commit 2d0531d72c7dcbb0e149cafdd3a16ee8c3ff357a which added these lines https github.com tensorflow tensorflow blame f4815551dc19b2149a70f20dba5a6c043e627a5b tensorflow core kernels concat op.cc L65 L87 . That change isn t part of TensorFlow 0.12.0 but it looks like it became available with 1.8.0. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28114 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28114 No a mrry The concat op test is failing for the architecture I am working on and we are hitting the same numpy bug which we encountered in 12963 https github.com tensorflow tensorflow pull 12963 . In this issue I just wanted to check if we can skip int64 support for my architecture if its not that critical. If I change the dtype to int32 in make tensor proto https github.com tensorflow tensorflow blob master tensorflow python framework tensor util.py L376 the test passes for us. I need to add following code after line https github.com tensorflow tensorflow blob master tensorflow python framework tensor util.py L485 Please suggest if you are aware of any better approach to achieve this. Thanks in advance 
28151,after quantization aware training when I convert to tflite quant int8 model still need to provide min max value for add operation why add operation need min max value , em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below 1.13 Python version 3.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 9.2 cudnn 7 GPU model and memory You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior after quantization aware training why i m still asked to provide min max value for add operation when convert to tflite int8 model tensorflow.lite.python.convert.ConverterError TOCO failed. See console for info. 2019 04 25 20 18 08.606359 I tensorflow lite toco graph transformations graph transformations.cc 39 Before Removing unused ops 2360 operators 3530 arrays 0 quantized 2019 04 25 20 18 08.663749 I tensorflow lite toco graph transformations graph transformations.cc 39 Before general graph transformations 2360 operators 3530 arrays 0 quantized 2019 04 25 20 18 08.993938 I tensorflow lite toco graph transformations graph transformations.cc 39 After general graph transformations pass 1 273 operators 505 arrays 1 quantized 2019 04 25 20 18 08.998229 I tensorflow lite toco graph transformations graph transformations.cc 39 Before pre quantization graph transformations 273 operators 505 arrays 1 quantized 2019 04 25 20 18 09.000372 I tensorflow lite toco graph transformations graph transformations.cc 39 After pre quantization graph transformations pass 1 160 operators 392 arrays 1 quantized 2019 04 25 20 18 09.002236 I tensorflow lite toco graph transformations graph transformations.cc 39 Before Group bidirectional sequence lstm rnn 160 operators 392 arrays 1 quantized 2019 04 25 20 18 09.004023 F tensorflow lite toco tooling util.cc 1708 Array slim mobilenetv2 Add which is an input to the Conv operator producing the output array slim mobilenetv2 Conv 6 Relu6 is lacking min max data which is necessary for quantization. If accuracy matters either target a non quantized output format or run quantized training with your model from a floating point checkpoint to change the input graph to contain min max information. If you don t care about accuracy you can pass default ranges min and default ranges max for easy experimentation. Fatal Python error Aborted Current thread 0x00007fb011618740 most recent call first File home syshang anaconda3 envs tfnightly0410 lib python3.7 site packages tensorflow lite toco python toco from protos.py line 33 in execute File home syshang anaconda3 envs tfnightly0410 lib python3.7 site packages absl app.py line 251 in run main File home syshang anaconda3 envs tfnightly0410 lib python3.7 site packages absl app.py line 300 in run File home syshang anaconda3 envs tfnightly0410 lib python3.7 site packages tensorflow python platform app.py line 40 in run File home syshang anaconda3 envs tfnightly0410 lib python3.7 site packages tensorflow lite toco python toco from protos.py line 59 in main File home syshang anaconda3 envs tfnightly0410 bin toco from protos line 10 in module Aborted core dumped Describe the expected behavior Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,any ideas any updates a similar problem do you solve it bro I meet the same prblem Based on feedback that the contrib quantize quantization aware training tool is a bit brittle and hard to use on some model architectures we have released a post training integer quantization tool https medium.com tensorflow tensorflow model optimization toolkit post training integer quantization b4964a1ea9ba that requires a small calibration dataset. Please take a look at the tutorial https github.com tensorflow tensorflow blob master tensorflow lite tutorials post training integer quant.ipynb and give it a try it should work much better And let us know if you run into any issues. Closing this issue since we are rethinking and working on an api to replace contrib quantize quantization aware training although post training quantization above should be sufficient for the majority of use cases . Thanks Suharsh Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28151 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28151 No a 
28158,Keras ValueError stops autograph building, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow no TensorFlow installed from source or binary pip TensorFlow version use command below 2.0.0 dev20190424 Python version 3.7.1 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version cudatoolkit 10.0.130 0 cudnn 7.3.1 cuda10.0 0 GPU model and memory GeForce RTX 2080 Ti Describe the current behavior Calling keras layer without calling build automatically infers the shapes of the trainable variables. This works both in eager mode and graph mode in the current 2.0 alpha version. However running the provided code in 2.0.0 dev20190424 version it gives the following error message Describe the expected behavior Code to reproduce the issue ,I could reproduce the issue with tf nightly. However there is no error with TF2.0.0 alpha0. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28158 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28158 No a Just submitted a fix that should allow your code to work without calling model.build. 
28161,MobileNet v2 model cannot be converted to TFLite missing OP ,It is not possible to convert retrained model built on top of the feature vector MobileNet v2 https tfhub.dev google tf2 preview mobilenet v2 feature vector 2 to TensorFlow Lite with tf.lite.TFLiteConverter.from concrete function method. During the conversion there are errors telling about not supported operations and data types see below . System information TensorFlow Version 2.0.0 alpha0 Eager mode True TensorFlow Hub version 0.4.0 Is GPU available True Code to reproduce the issue Colab project with output https colab.research.google.com drive 1fDWZfce2BJnU49xyrvwXYpX3EW3UK6QH scrollTo jyT0Zw6l8LE5 Stacktrace ,Tried also with the model tf.keras.applications.MobileNetV2 from Transfer Learning Colab Tutorial https colab.research.google.com github tensorflow docs blob master site en r2 tutorials images transfer learning.ipynb scrollTo 19IQ2gqneqmS It results with This is not Build Installation or Bug Performance issue. Please post this kind of support questions at Stackoverflow https stackoverflow.com questions tagged tensorflow . There is a big community to support and learn from your questions. GitHub is mainly for addressing bugs in installation and performance. Thanks Thanks for the response. I asked this question here assuming that if it is not possible to simply convert MobileNet to tflite which which is designed for it I believe this might be a bug. Esp when it works with no issues on TF1.x. Or maybe it should be reported to TF Hub repo If I m wrong here I m happy to move it to Stack Overflow then. There were some bugs in the alpha that have been resolved since. Can you try either from saved model from keras model or from concrete functions in the 2.0 nightly tf nightly 2.0 preview The API is documented here https www.tensorflow.org lite r2 convert python api. I ve just checked it with suggested changes and now saving model works correctly. Code I needed to use Info Updated module handle from version 2 to 3 2 doesn t work And at the end Instead of from concrete function Updated code https colab.research.google.com drive 1fDWZfce2BJnU49xyrvwXYpX3EW3UK6QH scrollTo o5JPT0yOkiK6 Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28161 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28161 No a 
28163,TFLite Interpreter fails to load quantized model on Android stock ssd mobilenet v2 , System information Android 5.1.1 on LGL52VL also tested on Android 9 Simulator Nexus 5 Latest Tensorflow in build.gradle compile org.tensorflow tensorflow lite Also please include a link to a GraphDef or the model if possible. http download.tensorflow.org models object detection ssd mobilenet v2 quantized 300x300 coco 2019 01 03.tar.gz Any other info logs I convert the stock quantized mobilenet v2 to TFLite using the following code converter tf.lite.TFLiteConverter.from frozen graph graphDefPath input arrays output arrays input shapes converter.optimizations tf.lite.Optimize.OPTIMIZE FOR SIZE converter.allow custom ops True converter.convert The resulting .tflite file works fine in Python including inference interpreter tf.lite.Interpreter model path modelFilePath interpreter.allocate tensors When I try to load the same tflite model file on Android the Interpreter constructor gives me error Didn t find op for builtin opcode CONV 2D version 2 MappedByteBuffer buffer fileChannel.map FileChannel.MapMode.READ ONLY startOffset declaredLength Interpreter.Options options new Interpreter.Options tfLite new Interpreter buffer options E flutter 7796 ERROR flutter lib ui ui dart state.cc 148 Unhandled Exception PlatformException error Unsupported value java.lang.IllegalArgumentException Internal error Cannot create interpreter Didn t find op for builtin opcode CONV 2D version 2 E flutter 7796 Registration failed. E flutter 7796 null E flutter 7796 0 StandardMethodCodec.decodeEnvelope package flutter src services message codecs.dart 564 7 E flutter 7796 1 MethodChannel.invokeMethod package flutter src services platform channel.dart 302 33 E flutter 7796 asynchronous suspension E flutter 7796 2 Tflutter.loadModelAndLabels package tflutter tflutter.dart 129 20 E flutter 7796 asynchronous suspension E flutter 7796 3 Detector.initialize package AIM services detector.dart 51 26 E flutter 7796 asynchronous suspension E flutter 7796 4 MonitorState. initStateAsync package AIM screens Monitor.dart 47 20 E flutter 7796 5 AsyncAwaitCompleter.start dart async patch async patch.dart 49 6 E flutter 7796 6 MonitorState. initStateAsync package AIM screens Monitor.dart 45 25 E flutter 7796 7 MonitorState.initState package AIM screens Monitor.dart 42 5 E flutter 7796 8 StatefulElement. firstBuild package flutter src widgets framework.dart 3853 58 E flutter 7796 9 ComponentElement.mount package flutter src widgets framework.dart 3724 5 E flutter 7796 10 Element.inflateWidget package flutter src widgets framework.dart 2968 14 E flutter 7796 11 Element.updateChild package flutter src widgets framework.dart 2771 12 E flutter 7796 12 SingleChildRenderObjectElement.mount package flutter src widgets framework.dart 4883 14 E flutter 7796 13 Element.inflateWidget package flutter src widgets framework.dart 2968 14 E flutter 7796 14 Element.updateChild package flutter src widgets framework.dart 2771 12 E flutter 7796 15 ComponentElement.performRebuild package flutter src widgets framework.dart 3757 16 E flutter 7796 16 Element.rebuild package flutter src widgets framework.dart 3572 5 E flutter 7796 17 ComponentElement. firstBuild package flutter src widgets framework.dart 3729 5 E flutter 7796 18 ComponentElement.mount package flutter src widgets framework.dart 3724 5 E flutter 7796 19 Element.inflateWidget package flutter src widgets framework.dart 2968 14 E flutter 7796 20 Element.updateChild package flutter src widgets framework.dart 2771 12 E flutter 7796 21 SingleChildRenderObjectElement.mount package flutter src widgets framework.dart 4883 14 E flutter 7796 22 Element.inflateWidget package flutter src widgets framework.dart 2968 14 E flutter 7796 23 Element.updateChild package flutter src widgets framework.dart 2771 12 E flutter 7796 24 SingleChildRenderObjectElement.mount package flutter src widgets framework.dart 4883 14 E flutter 7796 25 Element.inflateWidget package flutter src widgets framework.dart 2968 14 E flutter 7796 26 Element.updateChild package flutter src widgets framework.dart 2771 12 E flutter 7796 27 SingleChildRenderObjectElement.mount package flutter src widgets framework.dart 4883 14 E flutter 7796 28 Element.inflateWidget package flutter src widgets framework.dart 2968 14 E flutter 7796 29 Element.updateChild package flutter src widgets framework.dart 2771 12 E flutter 7796 30 SingleChildRenderObjectElement.mount package flutter src widgets framework.dart 4883 14 E flutter 7796 31 Element.inflateWidget package flutter src widgets framework.dart 2968 14 E flutter 7796 32 Element.updateChild package flutter src widgets framework.dart 2771 12 E flutter 7796 33 ComponentElement.performRebuild package flutter src widgets framework.dart 3757 16 E flutter 7796 34 Element.rebuild package flutter src widgets framework.dart 3572 5 E flutter 7796 35 ComponentElement. firstBuild package flutter src widgets framework.dart 3729 5 E flutter 7796 36 StatefulElement. firstBuild package flutter src widgets framework.dart 3871 11 E flutter 7796 37 ComponentElement.mount package flutter src widgets framework.dart 3724 5 E flutter 7796 38 Element.inflateWidget package flutter src widgets framework.dart 2968 14 E flutter 7796 39 Element.updateChild package flutter src widgets framework.dart 2771 12 E flutter 7796 40 ComponentElement.performRebuild package flutter src widgets framework.dart 3757 16 E flutter 7796 41 Element.rebuild package flutter src widgets framework.dart 3572 5 E flutter 7796 42 ComponentElement. firstBuild package flutter src widgets framework.dart 3729 5 E flutter 7796 43 ComponentElement.mount package flutter src widgets fra PS. The same Android code works fine for non quantized tflite models that I converted so I am confident that there should not be any silly bugs in my code. The problem seems to be triggered by Quantized Android TFlite,i am facing the same error yes i m also having problems with this the exact same problem. i m using the next lines for convertion Also.. What is the correct way to preprocess an image in order to make a non quantized model get the correct prediction in android Use the pip install U tf nightly and it should solve the Op problem. Even while converting. The tf nightly has the most latest build of tensorflow. And the most recent is two days ago from today. I have installed tf nightly and converted model but it still would not load I am trying to load a custom trained quantized model and the android sample code of TensorFlow gives me the same error as above. But if I use the float model instead it works perfectly. Does the android dependency implementation org.tensorflow tensorflow lite 1.13.1 not yet supports the CONV 2D conversion parameter You ll need org.tensorflow tensorflow lite 1.14.0 which was just released this past week or try the nightly. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28163 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28163 No a I no longer face the above error but now if I use my custom quantized model and I get this error java.lang.IllegalArgumentException Cannot convert between a TensorFlowLite buffer with 602112 bytes and a ByteBuffer with 150528 bytes. If I use a custom float model then there are no errors. My code is as follows How did you quantize your model Note that with post training quantization the default approach is to preserve float inputs outputs implicitly quantizing dequantizing as necessary. You can check the input tensor type using interpreter.getInputTensor 0 .dataType https github.com tensorflow tensorflow blob master tensorflow lite java src main java org tensorflow lite Tensor.java L49 or by using the visualization script https www.tensorflow.org lite guide faq how do i inspect a tflite file . My Model takes FLOAT32 as input but when I use the float code mentioned above the code works fine but the time taken increases dramatically 2.3 seconds per image as compared to my float model 0.8 seconds per image . Float Model file is which is of larger size 80MB as compared to post training Quantized Model size 20MB. Just to confirm you re saying that the quantized model which is 4x smaller takes 3x longer during inference One thing to try is to explicitly set the number of threads when executing your model e.g. compare single threaded performance . We re still working on multi threading support for the hybrid quantization path for post training quantization. With the new post training integer quantization path https medium.com tensorflow tensorflow model optimization toolkit post training integer quantization b4964a1ea9ba if you can provide a representative dataset at conversion time you should get both the size reduction and a substantial inference latency reduction this includes multi threading support . Yes you understood it correctly. Currently I am running the classifier class from a DB class as soon as media path gets inserted into my DB I pass the media path to classifier methods which then runs respective code to get the confidence probabilities. So this whole thing is running in a background thread synchronously. One thing which I don t get is why the smaller model takes more time than the larger one. The quantized model is supposed to be faster. Is there any other specific way to train the model for quantization so that I can use the byte code instead of the float as used in the sample application One thing which I don t get is why the smaller model takes more time than the larger one. In this case it s because you re using a hybrid quantized model which doesn t yet have multi threading support. I would try using the new post training integer quantization https medium.com tensorflow tensorflow model optimization toolkit post training integer quantization b4964a1ea9ba path which should get you both faster execution and a smaller model. If you don t mind could you attach the command you used to convert your model And the generated .tflite model Feel free to PM me directly if you prefer not to share openly. Thanks. 
28231,Tensor.graph is meaningless when eager execution is enabled. in TF 2.0 when compiling model, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 TensorFlow installed from source or binary pip install tensorflow gpu 2.0.0 alpha0 TensorFlow version use command below 2.0.0 alpha0 Python version 3.7.3 CUDA cuDNN version 10.0 GPU model and memory GeForce GTX 1080 Ti 11175MiB Describe the current behavior After migrating to TF 2.0 when compiling a custom model I wrote I get an error AttributeError Tensor.graph is meaningless when eager execution is enabled. The model compiled as expected in TF 1.13 Describe the expected behavior compile the model without raising any exceptions. Code to reproduce the issue Run this script https gist.github.com Jsevillamol 1db0121e4d8b2b95a13ea14f010169d0 passing as an argument this file https gist.github.com Jsevillamol 94bbf97e062fff7d6cc95200a6e6d78a to create an h5 keras model. Then load model the resulting h5 file and call model.compile adam categorical crossentropy . Other info logs model.summary ,I believe this bug has been fixed in nightly by omalleyt12 so upgrade to tf nightly 2.0 preview or tf nightly gpu 2.0 preview and your code will work. Specifically this commit fixed it https github.com tensorflow tensorflow commit 809a0332eb53eedf1c3257f2a2b5556ca9320e56 Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28231 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28231 No a 
28241,tf.image and eager execution bug,For tensorflow 1.13 and 2.0.0a tf.image requires explicit tf.constant instead of numpy arrays. https colab.research.google.com drive 1CrbsY4L7KJLP3IuB gs4KJiz2szuVDb9 , Ouwen The link is not accessible could you post the code instead yongtang my apologies I ve changed the settings on the colab notebook and posted the code above. I could reproduce the issue in TF2.0. I think either we need to update the code or update the doc saying the dtype of img should be a tensor. Thanks Added a PR 28274 for the fix. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28241 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28241 No a 
28323,Input shape validation error when using TimeDistributed wrapper, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac OS Mojave 10.14.4 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 alpha0 Python version Python 3.6.8 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A I have built a model consisting of a CNN feeding an LSTM. I ve wrapped the CNN in a TimeDistributed wrapper. I ve configured my training set x values to have a 5D shape batch size num steps image height image width channels . When trying to train the model I get the following error ValueError Error when checking input expected time distributed input to have 5 dimensions but got array with shape 4 28 28 1 . I ve tried a number of different model configurations and have debugged through the code. I can see where the ValueError is being thrown but I don t see any way to keep this from happening. Code details follow. I m not expecting a discrepancy between what the model thinks I m providing and what I m providing. I m providing a 5D data set and that s what it wants but it claims that I m only providing a 4D data set. Here is the code I m using. For the input data I m using MNIST images in 28x28x1 format. For the x data I m creating 20 records each consisting of a list of four 28x28x1 image vectors hence an input shape of 20 4 28 28 1 . The y data consists of a 1D list of labels. Output from training attempt Note that I get this same result regardless of which of the three combined model.add tfkl.TimeDistributed cnn... versions I have in my code. While debugging I found the place where the ValueError is thrown. It is in training utils.py line 336 if len data shape len shape lines 303 350 shown for context lines 303 350 of training utils.py the state of things when the debugger hits line 336 is From this one can see that the data shape getting to this point is correct but the code pulls out the the first record in the batch and compares its shape to the shape the TimeDistributed layer is expecting. The problem is that the TimeDistributed layer is expecting a batch representation in the shape None 4 28 28 1 but it s not there b c the code only pulled the input record at that batch location data shape data i .shape that is the code is using the 4 28 28 1 record s shape to compare with the TimeDistributed input shape None 4 28 28 1 which can never succeed Screen Shot 2019 05 01 at 2 50 51 PM https user images.githubusercontent.com 16819138 57046609 56ad3c80 6c2e 11e9 9073 51ed7631d526.png Screen Shot 2019 05 01 at 2 42 13 PM https user images.githubusercontent.com 16819138 57046632 6dec2a00 6c2e 11e9 9c3b eafe53f0b694.png . The way I see it the batch dimension shouldn t be represented in the tuple that is used to establish expected shape of data to the InputLayer the batch length comparison needs to ignore None in the shape tuples or the comparison should be if len np.array data .shape len shape ,made some progress on this. Added the designated line to the following block of code in training utils.py starting with line 296 After adding this line I can train the model.... I haven t verified that it actually works yet but I m no longer getting shape errors. The real issue is in this block lines 274 295 of training utils.py specifically in the section. My data input was a list of lists. So I got into this block and the code essentially reformatted my list of lists into a list of numpy arrays. BUT I still had the same shape. The other conditions e.g else data data.values if data. class . name DataFrame else data data data which happens if the inbound data is a numpy array actually adds the dimension to data that is needed to get through the .zip invocation with the original input shape. So the correct fix isn t the fix I cited in the comment above but to add it in the block cited in this comment as shown jvishnuvardhan what would I need to do to take on this issue and do the fix scooter4j If you have a solution that works well then fork tensorflow repository on your GitHub update the codes in your folder test it. If it is working as expected create a pull Request PR . All PR s are reviewed by TF Code owners and if it passes all the tests then your code will be merged into TF repository. Please let me know if anything is not clear to you. Thanks jvishnuvardhan ok. I m pretty confident that the fix shown in my last comment with code is what s needed. I ll try to get to that this week. The one question I have is how do I run unit tests for GPU if I don t have a GPU available Even the GPU based Dockerfile wouldn t emulate a GPU on my normal CPU system right scooter4j Good to know that you want to contribute. You could use GPU from the Google colab https colab.sandbox.google.com notebooks welcome.ipynb . It is free for public. In the Runtime dropdown menu in Google colab click change runtime type and select GPU for Hardware accelerator . Thanks. I could use Colab for manual validation not sure how to run your existing suite of automated tests against my change on GPU. That s ok maybe it ll become clear to me when I get in there. Plus I suppose I d see if I broke any tests when I create a PR and could adjust. scooter4j Have you created a PR If yes can share it here so that we can close when the PR merge. Thanks Any update on this I got exactly the same Issue. If scooter4j got this error I got this one Which is the same but with different image dimensions and batch size It has been 29 days with no activity and the awaiting response label was assigned. Is this still an issue scooter4j Is this resolved Did you raise any PR Please close if the issue was resolved already. Thanks Closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28323 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28323 No a 
28346,TrtGraphConverterV2 does not preserve output names in the signature def, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 TensorFlow installed from source or binary source TensorFlow version use command below master from April 22nd Python version 3.6.7 Bazel version if compiling from source 0.24 GCC Compiler version if compiling from source 7.4 CUDA cuDNN version 10.0 7.5.0 GPU model and memory GTX 1080 Ti Describe the current behavior If you use TrtGraphConverterV2 to convert a function in a saved model to use TRT it does not preserve the output names in the signature def of the saved model. If the saved function decorated with tf.function returned a dict output a a output b b the names output a and output b are in the saved model. After conversion with TrtGraphConverterV2 they are changed to the default names output 0 and output 1 . Describe the expected behavior The names of the outputs should not change. This breaks all code that loads the model and relies on the correct names. Code to reproduce the issue Take any saved model that contains a function returning a dict. Then run this Use saved model cli to inspect the saved model., olesalscheider In order to expedite the trouble shooting process please provide a code snippet to reproduce the issue reported here. Thanks You can use this code to reproduce the issue https gist.githubusercontent.com olesalscheider 366f33115016ac9d5f2976ec17124496 raw f5b68bf571f325742c1bc24658f0de04b3d3b33c wrong outputs.py The output names should be the same before and after conversion but they are not. olesalscheider Able to reproduce the issue. Our saved model has the following structured outputs output a TensorSpec shape dtype tf.float32 name output a output b TensorSpec shape dtype tf.float32 name output b Running TF TRT conversion... Our converted model has the following structured outputs output 0 TensorSpec shape dtype tf.float32 name output 0 output 1 TensorSpec shape dtype tf.float32 name output 1 Thanks for reporting this. I can reproduce the problem will make a fix soon. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28346 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28346 No a Is it actually fixed bixia1 do you know if this is still a problem 
28398,Passing tf.data.Dataset to model.predict raises ValueError The batch size argument must not be specified when using dataset as an input. , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution 19.04 TensorFlow installed from source or binary pip install tensorflow 2.0.0 alpha0 TensorFlow version use command below 2.0.0 alpha Python version 3.7.3 Describe the current behavior Running simple classification example with Keras interface Describe the expected behavior Predict results of fitted model with tf.data.Dataset using model.predict Code to reproduce the issue Other info logs , ispmarin I am able to reproduce the issue on colab with TensorFlow version 2.0.0 alpha0 jvishnuvardhan as far as I can tell this is an error in Keras not tf.data ... please re assign this problem to someone familiar with Keras thank you ispmarin I think this was resolved in tf nightly gpu 2.0 preview 2.0.0.dev20190723 . Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan e68b76451dd49bf282fb35d2f78faca7 tf 28398.ipynb . Last line in your code mdl.predict classes is expecting a numpy array where as your are providing a dataset . I am closing this issue as it was resolved. Please feel free to reopen the issue if the issue persists again. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28398 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28398 No a The error persists and now the documentation https www.tensorflow.org versions r2.0 api docs python tf keras Sequential predict classes says that predict can use tf.data besides other kinds of data but predict classes can only use numpy arrays. Is there a reason for predict classes only accepts numpy arrays This seems to break what is expected from the library. 
28441,Shape errors occur after compiling keras model with tf.keras.losses.CategoricalCrossentropy, System information OS Platform and Distribution Linux Ubuntu 18.04 TensorFlow installed from source or binary binary TensorFlow version 1.13.1 Python version 3.6.3 Describe the current behavior If I want to specify for example a reduction method of the loss function I will need to explicitly create an instance of tf.keras.losses.CategoricalCrossentropy and pass it to model.compile instead of passing the categorical crossentropy keyword. However compiling a model with an instance of tf.keras.losses.CategoricalCrossentropy results in the following shape error when calling model.fit afterwards Describe the expected behavior I was expecting similar behavior for both categorical crossentropy and tf.keras.losses.CategoricalCrossentropy . Code to reproduce the issue A short MNIST example. I realized that removing the to categorical transformation resolves the shape errors but the model does not learn properly anymore. Thank you , philgras I was able to replicate the output with the code snippet provided on Colab TensorFlow version 1.13.1 . philgras I think this is intended behavior. loss categorical crossentropy expects targets to be binary matrices 1s and 0s of shape samples classes . If you use loss tf.keras.losses.CategoricalCrossentropy from logits False it was expecting integer classes for targets. shape of the Targets y train y test caused the error. Thanks philgras I tried running the sample code snippet in TF 1.13.1 and was not able to repro the difference in behavior between using tf.keras.losses.CategoricalCrossentropy from logits False and categorical crossentropy . pavithrasv Here is the Github gist https colab.sandbox.google.com gist jvishnuvardhan 4a3250db8ba13303ecc968b225344d9e untitled155.ipynb that works without any issue. Here is the gist https colab.sandbox.google.com gist jvishnuvardhan 86caa7fd84915e2645316c5ccab56a59 untitled155.ipynb that throws an error. Between the two gists there was only one line change. In the second gist tf.keras.losses.CategoricalCrossentropy is expecting integer classes as input but categorical classes were given as input. Thanks Thank you jvishnuvardhan. I see the issue now. Looks like it has been fixed already as it does not repro in TF 2.0 alpha or the nightly release. Closing the PR now please feel free to re open if you see that the issue is not fixed. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28441 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28441 No a 
28444,tf.signal.inverse stft AttributeError int object has no attribute value in 2.0.0 alpha0,There appears to be a bug in tf.signal.inverse stft when testing for real frames.shape 1 .value is None where real frames.shape 1 is an integer and does not have a value . Reproducible code ,The issue is the usage of real frames.shape 1 .value . In TF 1.x real frames.shape returns a list of Dimensions which does have the value. In TF 2.0 real frames.shape returns int so value is not applicable any more. Created a PR 28461 for the fix. Thanks for the report timsainb and thanks for the fix yongtang. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28444 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28444 No a 
28501,tf2.0 tf.keras Embedding layer behave different than tf1.13, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information OS Platform and Distribution Ubuntu 16.04 TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 alpha0 Python version 3.6.7 Describe the current behavior embedding matrix 2 is not equal to model.predict 2 0 0 . Describe the expected behavior embedding matrix 2 should equal to model.predict 2 0 0 those two equals on tf 1.13.1 version., BrikerMan I was not able to reproduce the reported behavior. I executed the above code snippet in TF 1.13.1 and 2.0 alpha to receive identical results as given below Can you please try executing your code in google colab https colab.sandbox.google.com notebooks welcome.ipynb recent true and confirm Thanks ymodak I have tried it with colab it should be equal when using 1.13.1. I have changed the last line of my code and it works as expected. result is Here is demo colab https colab.research.google.com drive 1Y1ObMAg3H2O47on58t2hdirRUr6M172o Thanks for the report. And excellent example What s happening is that we don t pick up the initial weights in eager mode. Which definitely needs to be fixed. https github.com tensorflow tensorflow blob f0836d2a3bdc83b9487d703f30669723bd2662fb tensorflow python keras engine base layer.py L683 We the keras team will triage and report back. BrikerMan This appears to be fixed with latest TF 2.0 nightly 2.0.0 dev20190612 Can you please confirm BrikerMan This appears to be fixed with latest TF 2.0 nightly 2.0.0 dev20190612 Can you please confirm Fixed thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28501 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28501 No a 
28508,gfile.Copy does not overwrite dest file properly on posix filesystems, Describe the current behavior gfile.Copy overwrite True does not truncate the destination file before overwriting. That means if the src file is shorter than the dest file the resulting dest file contains the mix of the two. Describe the expected behavior gfile.Copy overwrite True results in having the exact same content of src file in the dest file. Code to reproduce the issue Tested with pip3 install tensorflow 1.13.1 python 3.5.2 b.txt should have aaa as the content not aaa nbb . Ref. https www.tensorflow.org api docs python tf gfile Copy ,CC Joeper214 who found this issue tsawada From the above referenced issue can see a PR was raised Can you please check if this is resolved after this change got merged. I built latest master 8a8a109e and my o trunc branch within tensorflow tensorflow devel docker container and confirmed that the problem reproduces on the master and not on o trunc . Oh the PR fixing this hasn t been yet merged due to my commit causing a merge conflict sorry about that Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28508 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28508 No a 
28541,tf.data.Dataset.padded batch does not actually pad data when working on multiple GPUs., System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device X TensorFlow installed from source or binary Binary TensorFlow version use command below v1.12.0 9492 g2c319fb415 2.0.0 alpha0 Python version 3.6 Bazel version if compiling from source X GCC Compiler version if compiling from source X CUDA cuDNN version 10.0 GPU model and memory Tesla K80 12GB RAM Exact command to reproduce See test below Describe the problem The problem appears when one wants to use a multi gpu compiled model with variable shaped data fed from a tf.data pipeline. Batching data using padded batch compulsory here since the data is of variable shape does not seem to really pad the tensors comprised in a batch. Calling fit or predict after doing so raises ValueError Input tensor shapes do not match for distributed tensor inputs PerReplica . See below a MVCE A python generator is used to generate tensors of variable shape 4 4 1 then 5 5 1 then 4 4 1 then 5 5 1 etc... A tf.data Dataset is built using tf.data.Dataset.from generator including a consistency check. A dummy model is built using the tf.keras functional API and a tf.distribute.MirroredStrategy on several GPUS. Source code logs MVCE , jvishnuvardhan the error is coming from TF distribution strategy which I am not familiar with. Could you please re assign this to someone from that team Thank you. Why do you think it s not padded if these statements pass Looks padded doesn t it Since all of the batches are ... 5 5 1. Hi I agree that the title is misleading The issue here comes when using a padded batch tf.data.dataset with a multi gpu model built using a MirroredStrategy . Doing so raises ValueError Input tensor shapes do not match for distributed tensor inputs PerReplica . This error prints along each sub batch one sub batch per worker . Consequently The MVCE above will print two tensors one of shape 1 4 4 1 the other of shape 1 5 5 1 My issue here is that I can not train a Distributed model on varying sized input. Yes it s an issue and we are currently working on fixing it. Thanks for reporting. FYI it should work if there were fully defined shapes instead of None in padded batch. isaprykin thanks good to know Automatically closing this out since I understand it to be resolved but please let me know if I m mistaken.Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28541 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28541 No a I don t think the issue is resolved yet Igor simply suggested a workaround for now. The bug is still there and needs to be fixed. I think this issue should now be fixed. Please re open if not. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 28541 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 28541 No a 
28570,TF 1.13.1 SequenceExample Dataset Estimator Saved Model No attr named Ncontext sparse in NodeDef, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 OS X 10.14 TensorFlow installed from source or binary binary TensorFlow version use command below 1.13.1 Python version 3.6.5 CUDA cuDNN version N A GPU model and memory N A Describe the current behavior In TF 1.13.1 using a Dataset API pipeline that parses a SequenceExample https www.tensorflow.org api docs python tf train SequenceExample with parse single sequence example https www.tensorflow.org api docs python tf io parse single sequence example exported as a SavedModel with strip default attrs set to True which is the default behavior for Estimators fails with the following error See the full stacktrace below. Describe the expected behavior The above error does not occur in TF 1.12 perhaps due to DatasetV2 changes with OptimizeDataset . After some experimentation I found the error does not occur when strip default attrs is set to False when exporting the SavedModel. Nor does it occur when using Example https www.tensorflow.org api docs python tf train Example in a similar capacity. Though the example below is intentionally mundane I ve found the Dataset API functions like padded batch to be extremely nice and was disappointed when I found I couldn t immediately use 1.13.1 saved models as I had been in previous versions. Code to reproduce the issue Other info logs Full stacktrace ,For anyone experiencing this issue you can work around it in the Estimator API by exporting your saved model using export savedmodel https www.tensorflow.org api docs python tf estimator Estimator export savedmodel with strip default attrs set to False . This isn t ideal since it may hurt the forwards compatibility of your exported model. Testing in later versions 1.14 1.15 this issue seems to be fixed now. Closing the ticket thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 28570 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 28570 No a 
28598,Tensorflow 2.0 rejection resample not producing the proper target distribution, System information Have I written custom code Yes OS Platform and Distribution Linux Ubuntu 16.04 TensorFlow installed from source or binary from binary pip3 TensorFlow version use command below 2.0 alpha Python version 3.5.2 CUDA cuDNN version 10.0 GPU model and memory GTX Titan V 12GB Issues I am trying to test rejection sampling with the following code. This is the result I got target dist 0.5 0.5 initial distribution 0.8333333333333334 0.16666666666666666 result counts 1500 600 final dist 0.7142857142857143 0.2857142857142857 Expectation The final distribution does not reflect the target distribution I set. I feel like this is a bug in the original algorithm because I can t seem to find what I did wrong in my code as shown below , celinew1221 It looks like you haven t used a template to create this issue. Please resubmit your issue using a template from here https github.com tensorflow tensorflow issues new choose . We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation. Sure. Can I just edit this post Looks like I am missing some system info. Done. Please read edited post. celinew1221 Able to reproduce the issue with the provided code. In the template it is mentioned that it is not a custom written code can you provide the link for this to investigate further. This is your target dist 0.5 0.5 This is your initial distribution 0.8333333333333334 0.16666666666666666 This is your result counts 1500 600 This is your final dist 0.7142857142857143 0.2857142857142857 Sorry I meant that I didn t create a custom made rejection resample function. Everything else is what I wrote. I ll edit that. Hi I had a similar issue. Initially I was using shuffle and repeat separately. This caused the rejection resample to produce incorrect distribution. Code The incorrect result is provided below However when I replaced the shuffle and repeat using the function given below it works as expected. Result with correct distribution is given below Issue is replicating with Tf nightly 2.2.0.dev20200311 Please find the gist here https colab.sandbox.google.com gist gadagashwini 0501e4610fd758d4ba6e72a95d35604b untitled443.ipynb . Thanks This issue should be fixed by https github.com tensorflow tensorflow commit f313fdce45d4933938a89980f6ca9bb2c8cbd27a Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 28598 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 28598 No a Well I m stil able to reproduce the issue by running the gist gadagashwini provided using 2.2.0 rc3 and I m also having a hard time getting it to properly resample using my own training scripts which I don t think is important to provide since even for a simple script from that gist it does not work . celinew1221 this Issue should probably be reopened I still have this issue with my own data TensorFlow 2.2 and I can reproduce the gist that is provided above even by version 2.4.0 dev20200717 it seems that the problem still exists. I think the issue needs to be reopened. jsimsa I can reproduce the same gist using Tensorflow 2.4.3. This is very likely not fixed yet This is the fix https github.com tensorflow tensorflow commit f15fbec84874d41592dd8622e784e455f5b580f4 diff 8c6fcb1b5540671afd6bfe9d0d10f84cb4a681b47929b49f016d7a8ee9c41a7c The fix has been released as part of TF 2.6. TF 2.4 was released in December 2020 and the TensorFlow project does not backport fixes to patch releases of older minor releases i.e. TF 2.4.x . 
28599,Restore from SavedModel not compatible with tf.distribute, System information OS Platform and Distribution e.g. Linux Ubuntu 16.04 Archlinux Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below v1.12.0 9492 g2c319fb415 2.0.0 alpha0 Python version 3.6.0 Describe the current behavior I m using Tensorflow Hub to restore a model from a SavedModel I do expect the restore to work even if it executed inside a distribution strategy scope but instead it raises an exception. Describe the expected behavior It should load the model correctly even it the restore from the savedmodel is performed inside a distribution strategy context. Code to reproduce the issue Other info logs The exception ,Thanks for the report. seemuch is looking into SavedModels distribution strategies. This works now with the latest 2.0 nightly. Please check out https www.tensorflow.org beta tutorials distribute save and load for more details. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28599 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28599 No a 
28606,tflite GPU Delegate sub operator not supported, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes But the model should be running OS Platform and Distribution Linux Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device Oneplus3 Android 8.0 TensorFlow installed from source or binary Source TensorFlow version use command below 1.13 Python version 3.6.8 Bazel version if compiling from source 0.24.1 GCC Compiler version if compiling from source 5.4.0 CUDA cuDNN version NIL GPU model and memory NIL Describe the current behavior The TFLite GPU Delegate benchmark tool provides support for sub operator to run on the GPU of the mobile. sub operator which was included to the model is not running on the GPU as part of GPU delegate but falls back to CPU. Describe the expected behavior Sub operator should be running on the GPU as per the documentation provided. Code to reproduce the issue Attached with this is the models and error logs of the models. The model is a modified version of the Deeplab GPU delegate model provided by google. Input size is 197 Graph Appending Code trial.tflite sub model Benchmark Tool Log trial.tflite sub model adb shell data local tmp benchmark model gpu graph data local tmp trial.tflite use gpu true Loaded model data local tmp trial.tflite resolved reporter INFO Initialized TensorFlow Lite runtime. INFO Created TensorFlow Lite delegate for GPU. ERROR Next operations are not supported by GPU delegate SUB Incorrect operation type passed First 74 operations will run on the GPU and the remaining 1 on the CPU. Applied GPU delegate. Initialized session in 744.972ms Running benchmark for at least 1 iterations and at least 0.5 seconds count 11 first 91729 curr 37009 min 36876 max 91729 avg 46306.5 std 16106 Running benchmark for at least 50 iterations and at least 1 seconds count 50 first 37205 curr 37165 min 36706 max 37530 avg 37075.8 std 158 Summary by node type Node type count avg ms avg cdf mem KB times called DELEGATE 1 37.034 99.906 99.906 0.000 1 SUB 1 0.035 0.094 100.000 0.000 1 Timings microseconds count 50 first 37200 curr 37161 min 36700 max 37520 avg 37070.4 std 158 Memory bytes count 0 2 nodes observed Average inference timings in us Warmup 46306.5 Init 744972 no stats 37075.8 TFLITE File The tflite file is attached below trial.tflite https github.com tensorflow tensorflow files 3167392 trial.tflite.zip Screenshot of modified part image https user images.githubusercontent.com 41156980 57543244 7a305f80 7371 11e9 960c efd3262ca719.png ,it s a tiny problem. sent a one line patch PR to fix it. impjdi Closing this issue since the associated PR has been merged. Feel free to reopen if the problem still persists. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28606 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28606 No a The problem still seems to exist with the recent tflite gpu experimental 0.0.1 and tflite gpu nightly 0.0.0 versions for android.The tflite models runs without errors in CPU mode but in GPU we encountered SIGSEV errors.The same tflite models also seems to work in PC linux TF13 . We used the latest tensorflow nightly to convert the tensorflow models pb to tensorflow lite. Also we were able to benchmark the models with android benchmark tool in CPU mode but it failed in GPU mode.The problem seems to be with the elementwise operators except mul in GPU when either one of the input is a constant tf.constant or an input from previous node. Benchmark Tool error Segmentation Fault using use gpu true Android error Models Error seems to be with elementwise operators like add or sub in GPU delegate .... 1. Add with Constant Fatal signal 11 SIGSEGV Error 1 https user images.githubusercontent.com 1130185 58099675 c3966f80 7bf9 11e9 881d 1ab4855466fc.png 2. Add and Sub with Constant Fatal signal 11 SIGSEGV Error 2 https user images.githubusercontent.com 1130185 58099681 c729f680 7bf9 11e9 9c54 59e211e18fe6.png 3. Sub with two runtime input Fatal signal 11 SIGSEGV Error 3 https user images.githubusercontent.com 1130185 58099690 ca24e700 7bf9 11e9 8362 8568b485c098.png How can we resolve this issue anilsathyan7 FYR after the one line change merged into master already I was able to run this model with GPU Delegated enabled command line benchmark model and label image. freedomtan How can i ensure that latest library is included in the android application Is it updated in both 0.0.1 experimental and gpu nightly branches I tried the demo app at https github.com tensorflow tensorflow tree master tensorflow lite java demo. It uses org.tensorflow tensorflow lite gpu 0.0.0 nightly aar from jcenter https github.com tensorflow tensorflow tree master tensorflow lite java demo building in android studio with tensorflow lite aar from jcenter . Does it include latest updates Or should i build from source Can you please mention the steps that you followed to verify the same I don t know how aar binaries are updated. I built benchmark model https github.com tensorflow tensorflow tree master tensorflow lite tools benchmark and label image https github.com tensorflow tensorflow blob master tensorflow lite examples label image label image.md from source code. The benchmark model has use gpu option already. I have patches for label image to use GPU delegate see my PR https github.com tensorflow tensorflow pull 27464 
28608,Tensorflow gives incorrect results for simple example, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below v1.13.1 2 g09e3b09e69 1.13.1 Python version 2.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version CUDA 10.0 GPU model and memory GeForce GTX 1080 You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior The actual output is 0.7853982 1.1071488 Describe the expected behavior The expected output is 0.7853982 0.7853982 Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. , kenryd I could reproduce this with TF1.13.1. However with TF2.0 the results are what you expected. GitHub Gist https colab.sandbox.google.com gist jvishnuvardhan 40edc993f3c1ce726a47dddd5bff7c6d untitled151.ipynb is here for TF2.0.0 alpha0. Thanks jvishnuvardhan In your link tf.Print is removed I believe it was deprecated and removed in 2.0 . However removing the tf.Print op in 1.13 also fixes the behavior so this does not address the problem. My concern is that this bug is some memory problem and just use 2.0 isn t a solution. It s also not possible for everyone to just upgrade to 2.0 since tensorflow is often integrated into large projects where upgrading takes a significant amount of time. The answer I get for the second value is 1.1071488 which is equal to atan2 2.0 1.0 so tensorflow may be reading memory erroneously. If so this is very concerning and undercuts any results I get in tensorflow how do I know they re correct if there are bugs like this Is there a way to avoid this specific bug in tensorflow 1.13 This looks like a grappler bug which was fixed a few months ago. Can you reproduce it on nightly alextp After installing tf nightly 1.14.1 it does indeed seem to be fixed. Thanks Do you by chance know which commit fixed this issue No not off the top of my head. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28608 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28608 No a 
28614,Keras RNN example from docs does not support statefulness when multilayer, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 TensorFlow installed from source or binary Binary TensorFlow version use command below 1.13.1 Python version 3.6.7 Anaconda CUDA cuDNN version 9.2 7.3.1 GPU model and memory GTX 1070 Ti Describe the current behavior Modifying the example code given here https www.tensorflow.org api docs python tf keras layers RNN to have stateful True leads to the following error Describe the expected behavior Code should run with no error Code to reproduce the issue ,Thank you for reporting the issue will fix it soon. This should be now fixed in https github.com tensorflow tensorflow commit 12250556493fe7757bd97f397e3483e7c0e022b1. 
28643,Keras fit generator fails in graph mode when input is dict, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow simple keras model combined from examples OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOS X 10.14 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device No TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 alpha0 Python version 2.7 Bazel version if compiling from source no GCC Compiler version if compiling from source no CUDA cuDNN version no CPU version GPU model and memory no CPU version Describe the current behavior When running provided code it fails only when fit generator executed in graph mode. In other cases fit and fit generator work well. Describe the expected behavior tf.keras.Model .fit generator should work properly with input of type dict. Code to reproduce the issue , shkarupa alex Able to reproduce the issue with the provided code. 62 model.run eagerly False 63 model.fit generator input fn 2 frames usr local lib python3.6 dist packages tensorflow python keras engine training.py in train on batch self x y sample weight class weight reset metrics 1248 else 1249 if not isinstance K.symbolic learning phase int 1250 ins x y sample weights True 1251 else 1252 ins x y sample weights TypeError unsupported operand type s for dict and list tf nightly is free from this issue but that is tf 1.x branch as i know I think this issue has been fixed by https github.com tensorflow tensorflow commit 254e39f810fd88e14c3743a5aaf903e5dd6bb397. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28643 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28643 No a 
28648,Keras fit generator using validation does not respect verbose argument, System information OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 1809 TensorFlow installed from source or binary pip TensorFlow version use command below 1.13 gpu Python version 3.6 Bug When using fit generator with validation the progress bar is printed regardless of the verbose setting. Cause Line 216 of tensorflow python keras engine training generator.py calls model iterator to do validation but does not pass the verbose parameter meaning that the default value of verbose 1 is used. Solution Add in the missing parameter e.g. , geometrikal Could you provide a standalone code to reproduce the issue Thanks From 28738 Hi geometrikal thanks for the issue As you have a solution this would be a good opportunity to submit a PR if you d like. Would you like to Otherwise I ll look into the fix Closing because this appears fixed in 1.14 Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28648 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28648 No a 
28710,steps per epoch parameter in fit not working with tf.keras.utils.Sequence, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 14.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary conda TensorFlow version use command below 1.13.1 Python version 3.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior setting the steps per epoch parameter in model.fit tf.keras to for example 10 should only perform 10 updates per epoch. When passing numpy arrays as inputs to model.fit this works as expected but when using a custom tf.keras.utils.Sequence instance steps per epoch does not have any effect. Describe the expected behavior When passing steps per epoch to model.fit as a parameter when training on data fed via a tf.keras.utils.Sequence instance the model should be updated exactly steps per epoch times in every epoch. Code to reproduce the issue Other info logs Produces I would have expected 5 5 instead of 25 25 ,Can reproduce this. Looks a bug to me. I was able to reproduce the mentioned output with TensorFlow version 1.13.1. I can t reproduce this issue in tf.nightly which indicate that this issue has been recently fixed but hasn t reach release yet. TF version 1.14.1 dev20190523 Epoch 1 10 1 5 ........................ ETA 0s loss 0.6458 5 5 0s 39ms step loss 0.7210 Epoch 2 10 1 5 ........................ ETA 0s loss 0.9641 5 5 0s 1ms step loss 0.7426 Epoch 3 10 1 5 ........................ ETA 0s loss 0.9364 5 5 0s 1ms step loss 0.7591 Epoch 4 10 1 5 ........................ ETA 0s loss 0.6825 5 5 0s 1ms step loss 0.6975 Epoch 5 10 1 5 ........................ ETA 0s loss 0.6900 5 5 0s 1ms step loss 0.6927 Epoch 6 10 1 5 ........................ ETA 0s loss 0.6488 5 5 0s 20ms step loss 0.6489 Epoch 7 10 1 5 ........................ ETA 0s loss 0.7134 5 5 0s 1ms step loss 0.7097 Epoch 8 10 1 5 ........................ ETA 0s loss 0.7574 5 5 0s 1ms step loss 0.7197 Epoch 9 10 1 5 ........................ ETA 0s loss 0.6375 5 5 0s 1ms step loss 0.6965 Epoch 10 10 1 5 ........................ ETA 0s loss 0.6357 5 5 0s 1ms step loss 0.6624 Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28710 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28710 No a I am using TensorFlow version 1.13.1. Is there any work around for this Can confirm ran a training run in 1.13.1 and had this issue upgraded to 1.14.0 without doing anything else and now works correctly. This bug is still present in 1.13.2. 
28725,Autograph fails for keyword only arguments, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux TensorFlow installed from source or binary pip install tf nightly gpu 2.0 preview TensorFlow version use command below v1.12.1 1847 gc095504 2.0.0 dev20190514 Python version 3.7.3 Describe the current behavior Autograph complains when compiling functions with keyword only arguments. Example output W0515 01 46 22.158518 139635868194560 ag logging.py 145 Entity function f at 0x7eff8120c1e0 could not be transformed and will be executed as is. Some features e.g. tensor dependent conditionals and loops may not work as expected. Error details can be found in the logs when running with the env variable AUTOGRAPH VERBOSITY 1. Please report this to the AutoGraph team. Cause Unexpected error transforming function f at 0x7eff8120c1e0 . If you believe this is due to a bug please set the verbosity to 10 on Linux export AUTOGRAPH VERBOSITY 10 and attach the full output when filing the bug report. Caused by inconsistent nodes None NoneType and None NoneType Describe the expected behavior Autograph works for keyword only arguments Code to reproduce the issue ,I have reproduce the mentioned output with TensorFlow version 2.0.0 dev20190515 on Colab. Thanks for filing the bug At a glance it looks related to the lone arg without a name probably its name field is None in the AST. We should have fix soon. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28725 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28725 No a 
28748,Counting parameters from Keras model doesn t work correctly with list of layers,Below is an example that keras model is not able to count correctly the number of parameters when custom keras layers are inside a list that is defined in an another custom layer. returns which is not correct. If the nested layers were outside a list then are counted correctly. which gives System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device No TensorFlow installed from source or binary Binary TensorFlow version use command below 1.13 Python version 3.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version CUDA 10.0 CuDNN 7.3.1 GPU model and memory Nvidia Titan V , lioutasb Able to reproduce the issue as mentioned tested the count of the number of parameters when custom keras layers are inside a list that is defined in an another custom layer. Layer type Output Shape Param dense Dense multiple 262656 test layer1 TestLayer1 multiple 0 Total params 262 656 Trainable params 262 656 Non trainable params 0 Below is the output for the nested layers that were outside a list. Layer type Output Shape Param dense 3 Dense multiple 262656 test layer1 1 TestLayer1 multiple 1574912 Total params 1 837 568 Trainable params 1 837 568 Non trainable params 0 muddham is there any workaround to count fast the number of parameters from a very large network Manually it would be a pain. muddham This is not mentioned before but this issue poses a more serious problem because this doesn t let training to be executed for the whole model. gives 2 which is wrong since it only sees the weight and bias matrices from the first dense layer. This means that if you try to train the model using the following code it will only optimize the first dense layer. I tested the same code on TF2.0 and it works as expected there so it seems only 1.13 has this bug. Keras actually overloads the setattr to check whenever a layer is added. So a list would not trigger it. Here is a workaround jnd77 I confirm that this workaround works as expected. Closing this out since I understand it to be resolved but please let me know if I m mistaken. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28748 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28748 No a 
28760,TF2.0 custom dynamic rnn with autograph and tensorflow datasets fails to run,When I followed the effective tensorflow2 instructions to try custom dynamic loop optimized with autograph I encountered the following errors. It seems the sequence length used in dynamic rnn became None. Everything is fine if I disable tf.function and replace DynamicRNN with DynamicRNNV2 which uses eager execution. , npuichigo Please provide details about what platform you are using operating system architecture . Also include your TensorFlow version. Also did you compile from source or install a binary Make sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in the Github new issue template https github.com tensorflow tensorflow issues new choose . We ask for this in the issue submission template because it is really difficult to help without that information. Thanks muddham Related information is listed below. System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 alpha0 Python version 3.6.8 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 10.0 7.5 GPU model and memory Tesla P100 16G Describe the current behavior Tensorflow dataset doesn t work well with custom dynamic loop for clause with autograph. Describe the expected behavior Custom for loop with eager execution use sequence length as condition. When adding tf.function decorator to it the sequence length used in dynamic rnn became None after two steps. Everything is fine if I disable tf.function and replace DynamicRNN with DynamicRNNV2 which uses eager execution. Code to reproduce the issue Other info logs npuichigo After I execute the provided code in google colab I get the below message. FATAL Flags parsing error Unknown command line flag f Pass helpshort or helpfull to see help on flags. An exception has occurred use tb to see the full traceback. SystemExit 1 In order to run in google colab you need to remove absl app. muddham npuichigo Able to reproduce the issue in 2.0.0 alpha0 tf nightly. I wasn t able to reproduce this issue with tf nightly 2.0 preview which means this issue was somehow fixed between alpha release and nightly. npuichigo could u try on nightly version Also by looking at your code I think the dynamic RNN doesn t make much sense from RNN model building perspective. The cell field you used which is a keras.layer.GRU is actually a whole GRU layer not just a cell. It will take care of looping through all the timesteps and you don t have to manage the looping by the for loop. Since the GRU LSTM can handle dynamic timestep size you actually can just feed the input tensor to it which might have a dynamic timestep size. So your implementation of DynamicRNN is just a builtin GRU layer. If you want to build your owner GRU layer with some custom behavior instead of using keras.layers.GRU as a cell you should use keras.layers.GRUCell which will take a 2D input one step at a time . qlzh727 Thank you for your advice I found the bug when I tried to implement a seq2seq decoder using custom dynamic rnn with autograph so the code above is just to illustrate the problem. I used tf nightly 2.0 preview and the bug seems to have been fixed. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28760 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28760 No a demo for custom dynamic rnn with autograph . 
28761,Can t set an initial state for the Bidirectional LSTM Layer of tf.keras 2.0 under eager execution mode, System information Have I written custom code No OS Platform and Distribution MacOS 10.14.4 TensorFlow installed from pip install tensorflow 2.0.0 alpha0 TensorFlow version v1.12.0 9492 g2c319fb415 2.0.0 alpha0 Python version 3.7.3 Describe the current behavior Get an Error when setting an initial state for the Bidirectional LSTM Layer of tf.keras 2.0 under eager execution mode Describe the expected behavior The code should work with eager execution mode. Code to reproduce the issue Other info logs The Error detail InvalidArgumentError Incompatible shapes 2 256 vs. 50 256 Op Add name bilstm2 add , Liu Da Able to reproduce the issue with the provided code. Thanks for reporting the issue will send out the fix very soon. This should be fixed by 2a8f9b1ccfaaebd6f9cf5b5eb972c2dafded4f5e now. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28761 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28761 No a qlzh727 Same problem in the Bidirectional code leads to a different issue in tf 1.14 graph mode Code to reproduce the issue Other info logs Do you think this can be solved in tf 1.x too at some point Thanks 
28782, TF2.0 Can t use tf.data.Dataset cache with distribution strategy, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 TensorFlow installed from source or binary pip tf nightly gpu 2.0 preview TensorFlow version use command below 2.0.0.dev20190514 Python version 3.7.3 CUDA cuDNN version 10 7.4.2.24 GPU model and memory V100 Describe the current behavior Using tf.data.Dataset cache and tf.distribute.MirroredStrategy will fail with Cache should only be read after it has been completed. Op MultiDeviceIteratorInit in TF 2.0 nightly. Describe the expected behavior Using tf nightly gpu 1.14.1.dev20190514 graph mode tf.data caching works using distribution strategy and doesn t throw an error. Code to reproduce the issue Other info logs ,Will it be possible to provide full minimal code snippet that can reproduce the issue on version 2.0.0.dev20190514. That would really help us to debug faster. Thanks achandraa Sorry for the incomplete code sample. This was the result of some late night debugging I added a complete code sample above. Hi Any update in this I meet the same problem today with newest released tf2.0. Same problem with me. If I cache on the validation data and use the validation step in the fit call I always get the same error. However if I do not include the validation step and remove the repeat from the validation dataset it works nicely as the validation runs until the validation data dataset is exhausted see the documentation of fit . As it is not necessary to shuffle the validation data the repeat is not really necessary here. So this way the problem is solved . So I guess I did not use the correct validation steps value after all but it is not really needed. Same problem with me. If I cache on the validation data and use the validation step in the fit call I always get the same error. However if I do not include the validation step and remove the repeat from the validation dataset it works nicely as the validation runs until the validation data dataset is exhausted see the documentation of fit . As it is not necessary to shuffle the validation data the repeat is not really necessary here. So this way the problem is solved . So I guess I did not use the correct validation steps value after all but it is not really needed. I just found my problem come from that I accidently used same tf dataset for both train and valid stage. In the same time the number of steps I set do not exhausted all data in the cache. After I use different tf.data.dataset and set a larger number of steps the problem solved. Thanks for darcula1993 . Using cache after take or skip solved my problem. I m experiencing the same issue. both suggestions by darcula1993 and sbl1996 dont help A minimal reproduction code Produces this error The environment is a Colab notebook with python 3.6 and TPU accelerator. TF version v2.0.0 rc2 26 g64c3d38 2.0.0 The problem disappears if the code is switched to TF 1.x and training runs well. I m experiencing the same issue. both suggestions by darcula1993 and sbl1996 dont help A minimal reproduction code Produces this error The environment is a Colab notebook with python 3.6 and TPU accelerator. TF version v2.0.0 rc2 26 g64c3d38 2.0.0 The problem disappears if the code is switched to TF 1.x and training runs well. try catche first then map Can you try with TF 2.1 I believe this issue should have been fixed by https github.com tensorflow tensorflow commit 08f41c6216d177933ba8eb48cd171a1e004e6ca2 Closing this issue since it s resolved. Feel free to reopen if still have problems. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 28782 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 28782 No a 
28797, TF2.0 Can t use keras validation data with distribution strategy , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 TensorFlow installed from source or binary pip tf nightly gpu 2.0 preview TensorFlow version use command below 2.0.0.dev20190517 Python version 3.7.3 CUDA cuDNN version 10 7.4.2.24 GPU model and memory 4 x V100 Describe the current behavior Using keras validation data and tf.distribute.MirroredStrategy will fail with BatchDataset object is not subscriptable in TF 2.0 nightly. Describe the expected behavior Without distribution strategy everything works fine. Code to reproduce the issue Other info logs , lgeiger Able to reproduce the issue with the provided code. TypeError BatchDataset object does not support indexing lgeiger I don t see any issue when I ran the code with TF2.0 gist is here https colab.sandbox.google.com gist jvishnuvardhan aca319db01cf71284a0dc7b8001be7f7 untitled191.ipynb as well as TF nightly gpu 2.0 preview gist is here https colab.sandbox.google.com gist jvishnuvardhan 6d1c69fedc2846663a94d388bd0796ce untitled192.ipynb . Thanks I m also unable to repro lgeiger could you try the lastest nightly This may be already fixed omalleyt12 I am still able to reproduce the error even with the latest nightly 2.0.0.dev20190523 . jvishnuvardhan If I re run both your notebooks inside colab I also get the same error message. jvishnuvardhan omalleyt12 Did you run this code on multiple GPUs Updating to the latest nightly 2.0.0.dev20190528 I receive a different error message I was able to reproduce this new error looking into a fix now. In the meantime a workaround should be to pass the validation steps arg to fit Similar error here https github.com tensorflow tensorflow issues 30843 issuecomment 514534349 This is fixed in the latest 2.0 nightly Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28797 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28797 No a 
28801,Linux No module named tensorflow estimator.python.estimator.tpu,I successfully developed and run a model using Tensorflow Probability with Pruning in a Windows machine but I get the following error in Linux Ubuntu 16.04 All the packages involved are up to date including pandas matplotlib and numpy. Pip conda reinstalling is not solving this issue. I m running Python 3.6.8 64bits Qt 5.11.2 PyQt5 5.11.3 on Linux Spyder 3.3.1., RubensZimbres Please provide details about TensorFlow version. Also did you compile from source or install a binary Thanks I have the same issue tensorflow probability seems to be incompatible with tf nightly installed via pip . Just to check you have installed both tf nightly and tf estimator nightly at the same time gadagashwini Tensorflow version 2.0.0a0 installed via pip mihaimaruseac I have installed both. It doesn t seem to be Spyder issue as the same problem happens in CLI. dir tfp brings me doc loader name package path spec RubensZimbres Tensorflow Probability 0.6.0 version is tested and stable against TensorFlow version 1.13.1. For more information please visit this link https github.com tensorflow probability releases . Thanks In this case downgrading Tensorflow to 1.13.1 works for tensorflow probability 0.6.0 import as well as tfp.distributions but for Model Optimization ImportError This version of TensorFlow Model Optimization requires TensorFlow version 1.14.0 Detected an installation of version 1.13.1. . No problem using Tensorflow 2.0.a0 for both libraries in Windows only in Ubuntu. Manually install the tensorflow estimator 1.14.0 with bazel works for me. https github.com tensorflow estimator Oh the tpu changes came in in 1.14. Automatically closing this out since I understand it to be resolved but please let me know if I m mistaken.Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28801 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28801 No a 
28845,GPU visible device selection causes segfault in tf nightly gpu, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow no OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary TensorFlow version use command below tf nightly gpu 1.14.1.dev20190519 Python version 2.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 10.0 GPU model and memory You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior The following code causes a segfault Describe the expected behavior No segfault. GPU pinning should work. cc jaingaurav,It looks like https github.com tensorflow tensorflow pull 28885 pulled whatever caused this issue into r1.14. Could you make sure the fix gets picked into r1.14 as well cc annarev I tried gdb on this and saw it failed at Thread 1 python received signal SIGSEGV Segmentation fault. 0x00007fff2608c6fa in stream executor StreamExecutorMemoryAllocator StreamExecutorMemoryAllocator stream executor Platform const absl Span stream executor StreamExecutor const from .env local lib python2.7 site packages tensorflow python pywrap tensorflow internal.so skye hawkinsp could this be related to the recent StreamExecutorMemoryAllocator changes alsrgv Could you verify that https github.com tensorflow tensorflow commit fc2863425d742585b6e7d8b0ae9d008d2bfbe72f fixes the issue I ll be cherry picking it into r1.14. cheshire the fix works thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28845 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28845 No a Thanks cheshire 
28849,Python3 type annotation does not work with tf.function for loop tf.while loop conversion, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device n a TensorFlow installed from source or binary binary TensorFlow version use command below 2.0 alpha Python version 3.6.7 Bazel version if compiling from source n a GCC Compiler version if compiling from source n a CUDA cuDNN version n a GPU model and memory n a Describe the current behavior If you use python3 type annotation such as x tf.Tensor tf.constant 0 I aliased tf.Tensor for various shape to keep my sanity for reinforcement learning problems in a tf.function and the function contains a for loop to be translated to tf.while loop that doesn t even have to use the tensor that s annotated the code will fail as if you did not turn on eager execution. Describe the expected behavior Python3 type hinting should not fail the code. Code to reproduce the issue Other info logs WARNING Logging before flag parsing goes to stderr. W0519 21 35 32.992043 140297958307648 tf logging.py 161 Entity function tf for tf break at 0x7f99a9edde18 could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH VERBOSITY 1. Please report this to the AutoGraph team. Cause AttributeError during conversion NoneType object has no attribute fields Traceback most recent call last File home jackshi .local lib python3.6 site packages tensorflow python autograph impl conversion.py line 393 in function to graph node node to graph node context File home jackshi .local lib python3.6 site packages tensorflow python autograph impl conversion.py line 436 in node to graph node converter.standard analysis node context is initial True File home jackshi .local lib python3.6 site packages tensorflow python autograph core converter.py line 493 in standard analysis graphs cfg.build node File home jackshi .local lib python3.6 site packages tensorflow python autograph pyct cfg.py line 813 in build visitor.visit node File usr lib python3.6 ast.py line 253 in visit return visitor node File home jackshi .local lib python3.6 site packages tensorflow python autograph pyct cfg.py line 672 in visit FunctionDef self.visit stmt File usr lib python3.6 ast.py line 253 in visit return visitor node File usr lib python3.6 ast.py line 257 in generic visit for field value in iter fields node File usr lib python3.6 ast.py line 171 in iter fields for field in node. fields AttributeError NoneType object has no attribute fields During handling of the above exception another exception occurred Traceback most recent call last File home jackshi .local lib python3.6 site packages tensorflow python autograph impl api.py line 369 in converted call experimental partial types partial types File home jackshi .local lib python3.6 site packages tensorflow python autograph impl api.py line 513 in to graph arg values arg types File home jackshi .local lib python3.6 site packages tensorflow python autograph impl conversion.py line 190 in entity to graph node name ns function to graph o program ctx arg values arg types File home jackshi .local lib python3.6 site packages tensorflow python autograph impl conversion.py line 396 in function to graph raise errors.InternalError conversion e tensorflow.python.autograph.pyct.errors.InternalError AttributeError during conversion NoneType object has no attribute fields During handling of the above exception another exception occurred Traceback most recent call last File home jackshi MagneticAccelerator descrete optimization tf scratch.py line 12 in module print tf for tf break File home jackshi .local lib python3.6 site packages tensorflow python eager def function.py line 426 in call self. initialize args kwds add initializers to initializer map File home jackshi .local lib python3.6 site packages tensorflow python eager def function.py line 370 in initialize args kwds File home jackshi .local lib python3.6 site packages tensorflow python eager function.py line 1313 in get concrete function internal garbage collected graph function self. maybe define function args kwargs File home jackshi .local lib python3.6 site packages tensorflow python eager function.py line 1580 in maybe define function graph function self. create graph function args kwargs File home jackshi .local lib python3.6 site packages tensorflow python eager function.py line 1512 in create graph function capture by value self. capture by value File home jackshi .local lib python3.6 site packages tensorflow python framework func graph.py line 694 in func graph from py func func outputs python func func args func kwargs File home jackshi .local lib python3.6 site packages tensorflow python eager def function.py line 317 in wrapped fn return weak wrapped fn . wrapped args kwds File home jackshi .local lib python3.6 site packages tensorflow python framework func graph.py line 686 in wrapper args kwargs File home jackshi .local lib python3.6 site packages tensorflow python autograph impl api.py line 390 in converted call return call unconverted f args kwargs File home jackshi .local lib python3.6 site packages tensorflow python autograph impl api.py line 188 in call unconverted return f args kwargs File home jackshi MagneticAccelerator descrete optimization tf scratch.py line 7 in tf for tf break for i in tf.range 5 File home jackshi .local lib python3.6 site packages tensorflow python framework ops.py line 449 in iter Tensor objects are only iterable when eager execution is TypeError Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map fn. ,I am able to reproduce the issue on colab with TF 2.0alpha . This is the error I got TypeError Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map fn. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28849 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28849 No a 
28856,TFLiteConverterV2 has no attribute from saved model , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary TensorFlow version use command below v1.12.0 9492 g2c319fb415 2.0.0 alpha0 Python version 3.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 9.0 7.1.4 GPU model and memory Describe the current behavior When running tf.lite.TFLiteConverter.from saved model saved model path an error occurs as described in the title. Describe the expected behavior The TF lite converter should work properly. Code to reproduce the issue Just follow the sample code in here https www.tensorflow.org lite r2 convert python api , from saved model wasn t added until after the alpha was released. Details are available in the comment here https www.tensorflow.org lite r2 convert python api. Can you upgrade to tf nightly 2.0 preview and rerun your code gargn I originally compiled tf 2.0 from source and had the same issue. Then I switched to tf 1.12. Here is the version of my tf 2.0 v2.0.0 alpha0 4 g2c2d508 2.0.0 alpha0 . I installed last night s tf nightly 2.0 preview using pip install tf nightly 2.0 preview specifically 2.0.0.dev20190529 and ran the code you provided without error. The 2.0 alpha branch you mentioned was cut in early February and has a slightly different API for from saved model as the documentation indicates. Automatically closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28856 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28856 No a 
28923,TF 2.0 save restore DenseFeatures, Code Follwing this guide i created a model to classify structured data https www.tensorflow.org alpha tutorials keras feature columns System information OS Platform and Distribution e.g. Linux Ubuntu 16.04 Win10 TensorFlow version use command below 2.0.0 alpha Python version 3.6 Describe the behavior Now when i try to save with the usual model.save path to my model.h5 i cannot restore the model since i get an exception Unknown layer DenseFeatures looking at some other https github.com tensorflow tensorflow issues 26835 url posts I tried with tf.keras.experimental.export saved model model experimental.h5 but here I get another exception init missing 1 required positional argument feature columns Tried to restore using but it always give me the feature columns error ,Tried with 2.0.0 alpha on Colab and was able to get the mentioned output. I tried with tensorflow 2.0.0beta1 the issue is still there. I spent time debugging issue in load model . Here is my theory As for the error message Unknown layer DenseFeatures the problem happens with this line of code https github.com tensorflow tensorflow blob master tensorflow python keras layers serialization.py L82. DenseFeatures class is not part of the dict returned by the call to globals . This problem can be solved not sure if it is a correct way by adding the following import to serialization.py from tensorflow.python.feature column.feature column v2 import DenseFeatures After that load model will complain about init missing 1 required positional argument feature columns . This happens in the init of DenseFeatures class. I think the root cause is that feature columns information is not serialized in the saved model file or is not read from the file correctly. From the deserialized model config dict I don t see feature columns. Could anyone from the team take a look Thank you. Digging more on this. based on this commit https github.com tensorflow tensorflow commit cd09510f9218220b872803c9ee70ba1dcd8c0f45 diff 99593c5667932dcaa61fb3e1b4921cc1 it seems like karmel was fixing this issue and the fix hasn t made it to tf2.0 beta1 yet Can you try using the TensorFlow format for saving and restoring That should work with the beta though the change in question may have missed the cut. If it doesn t work try with the nightly 2.0 release https pypi.org project tf nightly 2.0 preview With tf.version 2.0.0 dev20190724 I was not able to save a model using model.save output path save format tf . I used the keras functional api to build the model. 1159 1160 saving.save model self filepath overwrite include optimizer save format 1161 1162 def save weights self filepath overwrite True save format None anaconda3 envs tf2 lib python3.6 site packages tensorflow core python keras saving save.py in save model model filepath overwrite include optimizer save format 105 model filepath overwrite include optimizer 106 else 107 saved model save.save model filepath overwrite include optimizer 108 109 anaconda3 envs tf2 lib python3.6 site packages tensorflow core python keras saving saved model save.py in save model filepath overwrite include optimizer 83 model.optimizer None 84 85 save lib.save model filepath 86 87 if not include optimizer anaconda3 envs tf2 lib python3.6 site packages tensorflow core python saved model save.py in save obj export dir signatures 838 object saver util.TrackableSaver checkpoint graph view 839 asset info exported graph fill meta graph def 840 meta graph def saveable view signatures 841 saved model.saved model schema version 842 constants.SAVED MODEL SCHEMA VERSION anaconda3 envs tf2 lib python3.6 site packages tensorflow core python saved model save.py in fill meta graph def meta graph def saveable view signature functions 568 569 with exported graph.as default 570 signatures generate signatures signature functions resource map 571 for concrete function in saveable view.concrete functions 572 concrete function.add to graph anaconda3 envs tf2 lib python3.6 site packages tensorflow core python saved model save.py in generate signatures signature functions resource map 442 argument inputs signature key function.name 443 outputs call function with mapped captures 444 function mapped inputs resource map 445 signatures signature key signature def utils.build signature def 446 tensor dict to tensorinfo exterior argument placeholders anaconda3 envs tf2 lib python3.6 site packages tensorflow core python saved model save.py in call function with mapped captures function args resource map 393 Calls function in the exported graph using mapped resource captures. 394 export captures map captures to created tensors 395 function.graph.captures resource map 396 mapped inputs args export captures 397 Calls the function quite directly since we have new captured resource anaconda3 envs tf2 lib python3.6 site packages tensorflow core python saved model save.py in map captures to created tensors original captures resource map 315 be tracked by assigning them to an attribute of a tracked object 316 or assigned to an attribute of the main object directly. 317 .format interior 318 export captures.append mapped resource 319 return export captures AssertionError Tried to export a function which references untracked object Tensor StatefulPartitionedCall args 55 0 shape dtype resource .TensorFlow objects e.g. tf.Variable captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly. cysmnl Can you post a standalone code to reproduce the issue Thanks I am very confused about which API I should use see this simple Colab example https colab.research.google.com drive 1CXESHGbE8pPZqtBPcsS5UV2tj MYrxaH with different approaches of saving and restoring a tf.keras.Sequential model with feature columns DenseFeatures . Save Restore I am now quite unsure about where the issue is and what are the best practices for exporting and restoring in this case. I am currently submitting a fix for this issue which will allow DenseFeatures to be saved and loaded using Keras APIs model.save and tf.keras.models.load model . The best practices can be summed up to If you are planning to load the model back into Keras use the Keras APIs for saving and loading. If you re not planning on using the Keras APIs to further train the model then tf.saved model.save and tf.saved model.load is sufficient. I believe the docs need to be updated as the experimental function tf.keras.experimental.export saved model is now deprecated. batikus Is this resolved Please take a look at the feature column https www.tensorflow.org beta tutorials keras feature columns tutorial. I could save the model in tf and .h5 formats without any issue. Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan b6d63d35fd05058a3eb204891ca891c0 feature columns.ipynb . If you still have an issue please provide a simple standalone code to reproduce the issue. Thanks I am closing the issue as it was resolved in tf nightly 2.0 preview . Please feel free to open it if the issue persists again. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28923 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28923 No a UPD after investigation found that for SequenceFeatures should be open another issue because this fix does not cover SequenceFeatures behavior. The new issue is 32385 I am currently submitting a fix for this issue which will allow DenseFeatures to be saved and loaded using Keras APIs model.save and tf.keras.models.load model . k w w does this fix cover tf.keras.experimental.SequenceFeatures as well I save model by tf.keras.models.model.save MODEL PATH.h5 . Then while loading model by tf.keras.models.load model MODEL PATH.h5 I experience the similar error message as described in this issue Unknown layer SequenceFeatures I have not found separete issue for save load problems concerning SequenceFeatures but my problem is reproduced in tensorflow 2.0.0 rc0 and 2.0.0b1 and do not reproduced in tf nightly 2.0 preview . It is not clear now open new issue for SequenceFeatures or not. 
28941,Can t use padded batch on dataset with distributed strategy make dataset iterator,I can t use padded batch when I m trying to create a distributed iterator using the following code. It throws this exception System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device No TensorFlow installed from source or binary Binary TensorFlow version use command below 1.13 Python version 3.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version CUDA 10.0 CuDNN 7.3.1 GPU model and memory Nvidia Titan V ,I was able to get the mentioned error when tried on Colab with TensorFlow GPU version 1.13. padded batch should be supported. Can you try updating your TensorFlow version to 1.14 and try again anj s I confirm that the issue is fixed on 1.14. Thank you. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28941 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28941 No a 
28959, BaseSession. Callable. del doesn t detect closed session correctly, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Arch Linux Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device Not tested TensorFlow installed from source or binary Binary TensorFlow version use command below 1.13.1 Python version 3.7.3 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior BaseSession. Callable. del doesn t detect closed session correctly. Running the code I provided below will causes the following error message being printed The error message doesn t affect the correctness of the program but it is annoying to see. Describe the expected behavior The test program exits without printing the error message. Code to reproduce the issue Other info logs Here is my analysis This is the definition of BaseSession.close https github.com tensorflow tensorflow blob 592fa18bc134d9b9527f3d87ba51d6ae0378e3da tensorflow python client session.py L736 L747 If it is called on an alive session self. closed will be set to True but self. session will not change. This is the definition of BaseSession. del https github.com tensorflow tensorflow blob 592fa18bc134d9b9527f3d87ba51d6ae0378e3da tensorflow python client session.py L749 L764 This method will set self. session to None . This is the definition of BaseSession. Callable. del https github.com tensorflow tensorflow blob 592fa18bc134d9b9527f3d87ba51d6ae0378e3da tensorflow python client session.py L1467 L1473 It only detect self. session. session but not self. session. closed . So if a BaseSession. Callable is being destroyed after the corresponding session being closed but before the session being destroyed TF SessionReleaseCallable will be called on a closed session which will raise the exception mentioned above. There are two possible fixes 1. Set self. session to None in BaseSession.close . 2. Detect self. session. closed in BaseSession. Callable. del . Maybe related 3388 24367.,I was able to reproduce the issue on Colab with TensorFlow version 1.13. 
28995,Keras doesn t allow tf.data validation without validation steps, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac 10.14.4 Ubuntu 18.01 TensorFlow installed from source or binary binary TensorFlow version use command below 1.14.1.dev20190524 1.14.0rc0 1.14.0 Python version 3.6.8 CUDA cuDNN version GPU model and memory Describe the current behavior Using tf.data as validation data without defining validation steps fails with TypeError DatasetV1Adapter object does not support indexing . Using tf.data without steps per epoch works as expected when using it as training data instead. Describe the expected behavior I think the behaviour of training data and validation data in Keras model.fit should be consistent. This would make Keras a lot easier to use together with tf.data because it gets rid of the need for defining a exact number of steps. Code to reproduce the issue Other info logs , lgeiger Can you please provide tensorflow version . With Tensorflow tf nightly and tf 2.0.0 alpha the above code outputs expected output. Thanks lgeiger Can you please provide tensorflow version . With Tensorflow tf nightly and tf 2.0.0 alpha the above code outputs expected output. Thanks Sorry for the missing version I am using tf nightly 1.14.1.dev20190524 and tensorflow 1.14.0rc0 . lgeiger I tried with tf nightly 1.14.1.dev20190524 and tensorflow 1.14.0rc0 but the code executed without any error. Can you try once again and let us know if that still gives error. Thanks Can you try once again and let us know if that still gives error. Yep just tried it with tensorflow 1.14.0rc0 and it still gives an error. Which python version are you using lgeiger Thanks for confirming. I am using Python 3.6.7 version. I can still reproduce this with Python 3.6.8 TensorFlow 1.14.0 rc0 and TensorFlow Datasets 1.0.2 lgeiger I don t see the error you mentioned. I ran your code using tensorflow 2.0.0 alpha0 gist https colab.sandbox.google.com gist jvishnuvardhan 1f15082053c715c76643058dda5eae58 tf28995 gpu.ipynb tensorflow gpu 2.0.0 alpha0 gist https colab.sandbox.google.com gist jvishnuvardhan 4736ce8fd58e1bd2657728409a34752e tf gpu 28995 gpu.ipynb and tf nightly gpu 2.0 preview 2.0.0 dev20190530 gist https colab.sandbox.google.com gist jvishnuvardhan ba8cfffda7f0bb54ca55d41643f06381 tf nightly gpu 28995 gpu.ipynb . Please try to run it yourself. If you see any issues let us know. Thanks I don t see the error you mentioned. I ran your code using tensorflow 2.0.0 alpha0 gist tensorflow gpu 2.0.0 alpha0 gist and tf nightly gpu 2.0 preview 2.0.0 dev20190530 gist . Thanks for reproducing. As mentioned above the issue is related to 1.14.1.dev20190524 and 1.14.0rc0 . TF 2 works fine. lgeiger I ran it in 1.14.1.dev20190524 and don t see any error. Here is the gist https colab.sandbox.google.com gist jvishnuvardhan 6f99cbefa5cd7f90169f7098fe67f4f0 tf nightly 1 14 1 gpu 28995 gpu.ipynb . It will be helpful If you can create a gist and share. Thanks Here https colab.research.google.com drive 1gxC8Apn7xVExkaPo4ncBUU9YQKfe4ZK0 is a colab using 1.14.0rc0 Here https colab.research.google.com drive 1GGWq2mlYOLJ3fQjrcp9o5j25n4MUVRSt is a colab using 1.14.1.dev20190524 lgeiger I could reproduce the issue when i select cpu . If I select gpu as shown in my gist there is no error. Thanks lgeiger I could reproduce the issue when i select cpu . If I select gpu as shown in my gist there is no error. Thanks Strange. Good that you can reproduce it now jsimsa Is this intended behavior Can you please take a look at this issue Thanks This is a question for tf.keras folks not tf.data. I also encounter this problem tf.keras tf.data. My tensorflow version is tf13.1. jvishnuvardhan Any updates on this issue Facing the similar issue. omalleyt12 I can still reproduce this issue with TensorFlow 1.14.0 Yup can reproduce here as well with 1.14.0 . Suboptimal workarounds are 1. Set verbose 0 2. Set steps per epoch to something that is not None 3. Don t use validation data Hi lgeiger sorry for the very late reply. That indeed seems to be a bug for certain runtime condition. As benyeoh mentioned the quickest workaround is to set verbose 0 which suppress the print of the message and still give u the same training eval behavior. We are in the middle of a refactoring of the training logic and will take care of the issue in 2.0 release. Thanks for taking a look We are in the middle of a refactoring of the training logic and will take care of the issue in 2.0 release. Does that mean TF 1.14 won t receive a fix for this I understand that you are focusing on 2.0 at the moment but a fix would be very useful for people currently relying still relying on TF 1.x. We probably won t update 1.14 release for this bug. Also just check the docstring of model.fit validation data Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation data will override validation split. validation data could be tuple x val y val of Numpy arrays or tensors tuple x val y val val sample weights of Numpy arrays dataset or a dataset iterator. For the first two cases batch size must be provided. For the last case validation steps must be provided. Seems that the validation steps is needed if the validation data is a dataset. This is fixed with latest tf nightly version 1.15.0 dev20190808 Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28995 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28995 No a Sorry for opening this after almost a year but from docstring of model.fit steps per epoch Integer or None. Total number of steps batches of samples before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors the default None is equal to the number of samples in your dataset divided by the batch size or 1 if that cannot be determined. If x is a tf.data dataset and steps per epoch is None the epoch will run until the input dataset is exhausted . This argument is not supported with array inputs. Why not do the same If x is a tf.data dataset and steps per epoch is None the epoch will run until the input dataset is exhausted for validation data The docstring is a bit confusing validation data Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation data will override validation split. validation data could be tuple x val y val of Numpy arrays or tensors tuple x val y val val sample weights of Numpy arrays dataset For the first two cases batch size must be provided. For the last case validation steps must be provided. However validation steps Only relevant if validation data is provided and is a tf.data dataset. Total number of steps batches of samples to draw before stopping when performing validation at the end of every epoch. If validation data is a tf.data dataset and validation steps is None validation will run until the validation data dataset is exhausted. For 1.15 during model.fit when validation data is tf.data and validation steps is None the following error raises But this is not raised during model.evaluate with steps None Sorry for opening this after almost a year but from docstring of model.fit steps per epoch Integer or None. Total number of steps batches of samples before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors the default None is equal to the number of samples in your dataset divided by the batch size or 1 if that cannot be determined. If x is a tf.data dataset and steps per epoch is None the epoch will run until the input dataset is exhausted . This argument is not supported with array inputs. Why not do the same If x is a tf.data dataset and steps per epoch is None the epoch will run until the input dataset is exhausted for validation data The docstring is a bit confusing validation data Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation data will override validation split. validation data could be tuple x val y val of Numpy arrays or tensors tuple x val y val val sample weights of Numpy arrays dataset For the first two cases batch size must be provided. For the last case validation steps must be provided. However validation steps Only relevant if validation data is provided and is a tf.data dataset. Total number of steps batches of samples to draw before stopping when performing validation at the end of every epoch. If validation data is a tf.data dataset and validation steps is None validation will run until the validation data dataset is exhausted. For 1.15 during model.fit when validation data is tf.data and validation steps is None the following error raises ValueError When using data tensors as input to a model you should specify the steps per epoch argument. But this is not raised during model.evaluate with steps None same issue here Hello I am hitting same error TensorFlow 1.5 Python 2.7.17 inside docker container. Minimal logical code images labels read list data dir data list here data dir is full path of dir. containing image label files. data list is a text file 2 column containing names of image file and lable file. The output is array consisting of full path of images corresponding labels. queue tf.data.Dataset.from tensor slices images labels img contents tf.io.read file queue 0 Error location label contents tf.io.read file queue 1 File wasr wasr models image reader.py line 180 in read images from disk img contents tf.io.read file input queue 0 TypeError DatasetV1Adapter object does not support indexing I have tried reading various posts but this post has come closest to what I am looking for. Any help to fix the issue will be greatly appreciated. Thanks Shailesh Sorry for opening this after almost a year but from docstring of model.fit steps per epoch Integer or None. Total number of steps batches of samples before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors the default None is equal to the number of samples in your dataset divided by the batch size or 1 if that cannot be determined. If x is a tf.data dataset and steps per epoch is None the epoch will run until the input dataset is exhausted . This argument is not supported with array inputs. Why not do the same If x is a tf.data dataset and steps per epoch is None the epoch will run until the input dataset is exhausted for validation data The docstring is a bit confusing validation data Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation data will override validation split. validation data could be tuple x val y val of Numpy arrays or tensors tuple x val y val val sample weights of Numpy arrays dataset For the first two cases batch size must be provided. For the last case validation steps must be provided. However validation steps Only relevant if validation data is provided and is a tf.data dataset. Total number of steps batches of samples to draw before stopping when performing validation at the end of every epoch. If validation data is a tf.data dataset and validation steps is None validation will run until the validation data dataset is exhausted. For 1.15 during model.fit when validation data is tf.data and validation steps is None the following error raises ValueError When using data tensors as input to a model you should specify the steps per epoch argument. But this is not raised during model.evaluate with steps None same issue here same problem encountered. Could anyone in tensorflow team at least clarify what does the conflicting doc string mean If the bug is not fixed in TF1.14 what is the intended behavior according to above doc string It is pretty confusing. Any update on it 
29030,eigen get a0d250e79c79.tar.gz Error 404, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device n a TensorFlow installed from source or binary source TensorFlow version use command below master Python version 3.6 Bazel version if compiling from source 0.25 GCC Compiler version if compiling from source n a CUDA cuDNN version n a GPU model and memory n a Describe the current behavior Error 404. Related 28926.,I assume the issue could be fixed with PR 29017 hoonkai Please let us know if this issue is resolved once the PR has been merged Automatically closing due to lack of recent activity. Please update if this still an issue or new information is available. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29030 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29030 No a 
29052,ArithmeticOptimizer fails for stack with axis R and axis R 1 , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac OS 10.14.5 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary pip TensorFlow version use command below 1.13.1 Python version 3.7.3 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version N A GPU model and memory N A Describe the current behavior In the stack op the allowed range for axis is R 1 R 1 . However if you take a strided slice over the result with axis R 1 or axis R the ArithmeticOptimizer will fail with a warning. It also fails for scalars with any value in axis . The results are still correct but this disables the arithmetic optimizer for some all of the code. See the repro code for an example. Describe the expected behavior I would not expect ArithmeticOptimizer to fail. Code to reproduce the issue Scalar example Vector example Executes sucessfully but the ArithmeticOptimizer fails with a warning ,I tried running on Colab with TensorFlow version 1.13.1 but unfortunately did not reproduce the mentioned warning. Can you please check and let us know once again if the issue still persists. Thanks That seems odd. The bug seems to happen in the latest 1.13.1 docker container It does not seem to happen in nightly however So it might be fixed in the latest nightly. But it s odd why this does not happen in colab. Could there be a version difference between colab and docker pip Edit While nightly does seem to fix it in the repro example case it seems like my main program from which I found this minimal test case still has the problem. Might it be that nightly will not trigger the arithmetic optimizer in this case Turns out 1.13.1 was installed in the container running my main program. This is due to the nightly container not having a tensorflow pip package installed in the docker image. As one of my dependencies had a dependency on tensorflow it would install TF 1.13.1 from pip. Edit 2 The RemoveStackStridedSliceSameAxis has not been changed much since its implmentation so I doubt it s actually fixed there between 1.13.1 and nightly although I suppose it could be fixed in some function it depends on directly or indirectly . I have tried on Colab Docker and local machine with TF 1.13.1 and have noticed that only docker reproduces the issue. Nightly does not have the issue even when tried through docker. Are you sure the TF 1.13.1 on your local machine matches the pip version I just tried it using a base ubuntu container with tensorflow installed via pip and still get the warning Seems to be fixed in 1.14 Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29052 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29052 No a 
29073,Empty trainable variables in keras model tf 2.0 , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Arch Linux TensorFlow installed from source or binary binary TensorFlow version use command below v1.12.1 2504 g2be59a5191 2.0.0 dev20190522 Python version 3.6.8 CUDA cuDNN version CUDA Version 10.0.130 GPU model and memory Nvidia GeForce GTX 1080 Ti Describe the current behavior I want to create UNet using the Keras Model subclassing API The current code is When I evaluate the model using an input and check the trainable variables they are empty Describe the expected behavior The code should print the list of trainable variables of the Net. The output is correct so the call method is correctly called. ,Update After some debugging I found the cause of this issue. The problem is that keras does its magic under the hood. In the UNet init Keras intercepts the creation of the lists and initializes its attributes. Keras does not manage the append of layers to a list. Changing the above code in Solves the issue. In this way the variables are correctly added to the trainable variables. I think this is a serious bug. It should be fixed or at least documented. We agree that this should be better documented. Would you be interested in adding examples and references in the base Layer docstring Just ran this by allenlavoie this may have been fixed in newer versions of TensorFlow and is worth checking with the nightly. It was initially broken then fixed then there was a regression with some of qlzh727 layer refactoring changes then may have been fixed again . I wasn t able to reproduce the error with nightly version of TF. Basically adding the layer to the list attribute of the subclass model should work. Nested list should work too since we recursively find all the layer like objects. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29073 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29073 No a Update this is fixed in version v1.12.1 2994 g5fa098e3b9 2.0.0 dev20190530 
29099,Drastically different behavior between TF1 and TF2,I ve noticed drastically different behavior of the following code between 2.0.0 alpha0 and 1.13.1 . In 2.0.0 alpha0 the accuracy of the network consistently reaches 80 in the first few epochs. In 1.13.1 the accuracy consistently reaches only 20 after all 15 epochs. What is going on Here is the file with weights W bio.npy https drive.google.com file d 12O0XE98jl yYmiBbZMWyLMaeZiHH FXH view usp sharing ,The difference in behavior has something to do with using ReLU. If instead the activation function used for the hidden layer is sigmoid then the behavior becomes the same in TF1 and TF2 20 accuracy . danaugrs I hope you are using ReLU for the hidden layers now. Can you please let us know if we have to keep this issue open muddham I think you seriously misunderstood my last comment. I was just helping whoever is going to debug this difference in behavior between 2.0.0 alpha0 and 1.13.1 by pointing out that it probably lies in the ReLU implementation. The difference in behavior is only apparent when using ReLU. All the other activation functions I ve tried lead to the same behavior in both versions. The accuracy difference doesn t seem to be as severe now but it is definitely still present 65 vs. 85 However switching to distribution strategy which changes the execution path restores the accuracy in 2.0. We re standardizing on the latter path so this should be fixed before the 2.0 release. In the mean time something like will do the right thing. Thanks for the report and for the fantastic repro. cc tomerk qlzh727 omalleyt12 karmel danaugrs I think the issue was resolved in tf nightly and tf nightly 2.0 preview . I ran your code in both the versions and the accuracy after 15 Epochs is 78.57 and 78.29 with TF2.0 and tf nightly TF.1.15 . Here is gist https colab.sandbox.google.com gist jvishnuvardhan d57e90aed84f74b7c422bccb23abf48b tfnightly20preview 29099 keras perfomance relu.ipynb with TF2.0 and here is the gist https colab.sandbox.google.com gist jvishnuvardhan 415672186e6fcadd6093198dc33be64e tfnightly 29099 keras perfomance relu.ipynb with tf nightly . I am closing the issue as it was resolved recent nightly. Please feel free to open it if the issue persists again. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29099 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29099 No a 
29108,Gradient clipping by norm has different semantics in tf.keras.optimizers against keras.optimizers, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device No TensorFlow installed from source or binary binary TensorFlow version use command below 1.13.1 Python version 3.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version CUDA 10.0 cuDNN 7.4 GPU model and memory NVIDIA Tesla V100 32GB You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior The gradient clipping mechanism is implemented in the abstract class tf.keras.optimizers.Optimizer keras.optimizers.Optimizer so that every optimizer inherited from the class supports it. We find different semantics of the implementations between tf.keras and keras.io. In tf.keras the gradient norm is calculated and clipped per gradient tensor . https github.com tensorflow tensorflow blob bb8cf258bc87f68612a5e032d5b4def0d3c52566 tensorflow python keras optimizers.py L98 L99 But in keras.io the gradient norm is calculated globally across all gradient tensors . Code link https github.com keras team keras blob eab1b5bcdf105746ede02d2eb8a5cb3ca359b1b5 keras optimizers.py L96 L98 IMO tf.keras should adopt the latter method since 1 The paper introducing gradient clipping https arxiv.org abs 1211.5063 suggests the latter one and 2 tf.keras should be compliant with keras.io. ,I already provided comment in the PR. Closing this. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29108 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29108 No a Hi tanzhenyu thanks for your reply. But where can I find the comment you left chenchc in the PR go to files changes tab or the regular page you should be able to see it tanzhenyu I cannot see the comments in both tabs. Maybe they are invisible to my account img width 1015 alt Screen Shot 2019 06 26 at 12 37 12 src https user images.githubusercontent.com 6285919 60151408 33a69f80 980f 11e9 8cf6 360f5933b1f0.png chenchc My major comment is that I don t see where in this paper describing it should use global clip instead of local clip. 
29122,Entity could not be transformed and will be staged without change, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 TensorFlow installed from source or binary source TensorFlow version use command below 2.0 alpha Python version 3.6 CUDA cuDNN version 10 GPU model and memory GTX2080Ti Hi there could you please have a look at the issue A customized network module in Keras cannot work as for gradient backpropagation. Log W0529 19 29 46.658907 140071302989632 tf logging.py 161 Entity bound method Layer. call of tensorflow.python.keras.engine.training.Model object at 0x7f6404415fd0 could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH VERBOSITY 1. Please report this to the AutoGraph team. Cause Unexpected error transforming bound method Layer. call of tensorflow.python.keras.engine.training.Model object at 0x7f6404415fd0 . If you believe this is due to a bug please set the verbosity to 10 on Linux export AUTOGRAPH VERBOSITY 10 and attach the full output when filing the bug report. Caused by node gast.gast.Subscript object at 0x7f61f416e4e0 has ctx unset W0529 19 29 52.344057 140071302989632 tf logging.py 161 Entity function train G at 0x7f64c79a9ae8 could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH VERBOSITY 1. Please report this to the AutoGraph team. Cause Unexpected error transforming function train G at 0x7f64c79a9ae8 . If you believe this is due to a bug please set the verbosity to 10 on Linux export AUTOGRAPH VERBOSITY 10 and attach the full output when filing the bug report. Caused by node gast.gast.Name object at 0x7f61ad7cb588 has ctx unset W0529 19 29 52.960978 140071302989632 optimizer v2.py 928 Gradients does not exist for variables conv2d 24 kernel 0 conv2d 24 bias 0 conv2d 25 kernel 0 conv2d 25 bias 0 conv2d 26 kernel 0 conv2d 26 bias 0 conv2d 27 kernel 0 conv2d 27 bias 0 conv2d 28 kernel 0 conv2d 28 bias 0 conv2d 29 kernel 0 conv2d 29 bias 0 conv2d 30 kernel 0 conv2d 30 bias 0 conv2d 31 kernel 0 conv2d 31 bias 0 conv2d 32 kernel 0 conv2d 32 bias 0 conv2d 33 kernel 0 conv2d 33 bias 0 conv2d 34 kernel 0 conv2d 34 bias 0 conv2d 35 kernel 0 conv2d 35 bias 0 conv2d 36 kernel 0 conv2d 36 bias 0 conv2d 37 kernel 0 conv2d 37 bias 0 conv2d 38 kernel 0 conv2d 38 bias 0 conv2d 39 kernel 0 conv2d 39 bias 0 when minimizing the loss. x, weiminson In order to expedite the trouble shooting process please provide a code snippet to reproduce the issue reported here. Also Could you provide more details about the issue and context . Thanks weiminson In order to expedite the trouble shooting process please provide a code snippet to reproduce the issue reported here. Also Could you provide more details about the issue and context . Thanks gadagashwini thanks for your reply. I was trying to implement a network with the following structure convGenerator https user images.githubusercontent.com 41808551 58610418 bf034280 82ee 11e9 9a69 a90a45d4c248.png each of its decoder branches are connected to a decoder like this convDiscriminator1 https user images.githubusercontent.com 41808551 58610718 d7c02800 82ef 11e9 8524 ad94246eddf5.png However when I try to update its gradient in a function called train G using with tf.GradientTape persistent True as t TAB ground truth get Mask x real TAB x1 fake x2 fake G ground truth training True TAB x1 fake d logit D1 x1 fake training True TAB x2 fake d logit D2 x2 fake training True TAB G1 loss g loss fn x1 fake d logit TAB G2 loss g loss fn x2 fake d logit G2 grad t.gradient G2 loss G.trainable variables G optimizer.apply gradients zip G2 grad G.trainable variables the warning described above occurs saying that no gradient will be obtained from the branches. is there anything wrong with my code weiminson I tried to reproduce the issue but it looks code snippet is incomplete. Please provide complete code to. Thanks weiminson I tried to reproduce the issue but it looks code snippet is incomplete. Please provide complete code to. Thanks gadagashwini Thanks for your reply to reproduce the issue I ve uploaded the entire coding to https github.com weiminson debug SplitU Plz run tain.py to test. for the running environment plz refer to README. jvishnuvardhan Could you please help to try this case I ve been stuck on this issue for more than one week... Appreciate your reply. mdanatg Is this autograph related There is an Unexpected error transforming in the logs Yes the warning is definitely related to autograph and indicates a bug judging from the error message. That said I believe the gradient error is unrelated the warning simply states that autograph will leave that code unchanged and it s unlikely to cause that error. So I think we re dealing with two separate issues here it would be great if you had a code snippet or a simplified model of it that can reproduce the error. mdanatg I believe the code has been uploaded to https github.com weiminson debug SplitU The warning from optimizer is as expected. weiminson your code seems to be G2 grad t.gradient G2 loss G.trainable variables G optimizer.apply gradients zip G2 grad G.trainable variables G optimizer is getting all trainable variables with G.trainable variables while you are providing gradients only for the ones on the G2 branch hence it is warning you that it doesn t see any gradients for the variables in the G1 branch. The warning is to make sure you aren t doing this by mistake. mdanatg the autograph warning seems unrelated but may be worth checking if it needs to be there. The warning from optimizer is as expected. weiminson your code seems to be G2 grad t.gradient G2 loss G.trainable variables G optimizer.apply gradients zip G2 grad G.trainable variables G optimizer is getting all trainable variables with G.trainable variables while you are providing gradients only for the ones on the G2 branch hence it is warning you that it doesn t see any gradients for the variables in the G1 branch. The warning is to make sure you aren t doing this by mistake. mdanatg the autograph warning seems unrelated but may be worth checking if it needs to be there. rajatmonga Thanks for your reply. Actually I did try out a training mode where one of the branches was disabled and the other was left enabled mode 2 . Consequently it was still showing the same error. Besides I checked the name of the layers from that Error messages finding that these layers are names of layers of BOTH branches which implied that neither of the branches actually worked. Correct me if I m wrong. BTW do you know what this error message means Why train G could not be transformed I believe this is the root cause of the issue. W0529 19 29 52.344057 140071302989632 tf logging.py 161 Entity function train G at 0x7f64c79a9ae8 could not be transformed and will be staged without change. weiminson I tried to run the code you uploaded to the github repo but it fails with this error It seems to require the PET CT dataset but it s unclear how to install that. At any rate the two warnings are very unlikely to be related. To verify that please change line 156 as follows weiminson I had a closer look at train G and I believe the bug that caused the autograph warning is already fixed in the beta1 release pip3 install U tensorflow 2.0.0 beta1 I think that will require upgrading tensorflow addons too pip3 install U tensorflow addons Could you try running your code under these versions and check if any of the warnings remain 1 with what rajatmonga said. The warning from optimizer only suggests branch 2 is not included in the backprop path and seems to be legit in your use case. If you want to avoid such warning the simplest way is to change your code to use G.trainable variables to tape.watched variables so that it only updates branch 1. The actual error seems to be coming from NodeTransformer Since we expect the warnings to clear in the beta or nightly builds but we can t confirm from our side whether the warnings clear in the latest build I will close this issue for now. Please re open it if they still appear in the nightly build though. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29122 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29122 No a weiminson I had a closer look at train G and I believe the bug that caused the autograph warning is already fixed in the beta1 release pip3 install U tensorflow 2.0.0 beta1 I think that will require upgrading tensorflow addons too pip3 install U tensorflow addons Could you try running your code under these versions and check if any of the warnings remain mdanatg I had a similar issue. The warnings disappeared after using the beta1 release. Thanks. 
29178, TF2 Variable List Shape, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04 TensorFlow installed from source or binary pip install tf nightly 2.0 preview TensorFlow version use command below v1.12.1 2995 ge09b015 2.0.0 dev20190530 Python version 3.8 Describe the current behavior Right now the tf.Variable constructor accepts a list as a shape but if you do this then calls to .assign raise an error. Describe the expected behavior Either tf.Variable should automatically convert the list to a tf.TensorShape or it could raise a type error for a list shape but that seems significantly less ergonomic . Code to reproduce the issue The following code fails with If I use shape tf.TensorShape None 50 then this works just fine.,Have tried with TensorFlow version 2.0.0 dev20190531 on Colab and was able to reproduce the issue. alanhdu Hi I ve tried this on tensorflow 2.1 and tf nightly and haven t got this issue. Can you please check on your end alanhdu This is fixed in Tf nightly 2.2.0.dev20200312. Please find the gist here https colab.sandbox.google.com gist gadagashwini 39f97b21499f862848df44f6f24f680d untitled445.ipynb and let us know can we close this issue. Thanks Oh sorrry Yes it looks like support has been added. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 29178 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 29178 No a 
29187,TF 2.0 Cannot use recurrent dropout with LSTMs GRUs, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No one line modification to stock example OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 14.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below tensorflow gpu 2.0.0 alpha0 also fails with every other tf 2.0 build I have explored Python version 3.6 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version Tried multiple GPU model and memory Tried multiple Describe the current behavior The program crashes with a TypeError as below TypeError An op outside of the function building code is being passed a Graph tensor. It is possible to have Graph tensors leak out of the function building context by including a tf.init scope in your function building code. For example the following function will fail tf.function def has init scope my constant tf.constant 1. with tf.init scope added my constant 2 The graph tensor has name encoder unified gru ones like 0 This occurs when trying to backprop the gradients through the LSTM GRU with recurrent dropout enabled. Describe the expected behavior No error Code to reproduce the issue Since this problem shows up at the time of training one needs to have the entire training pipeline dataset model etc. setup to demonstrate this bug. As a result I used the Neural Machine Translation tutorial https www.tensorflow.org alpha tutorials text nmt with attention from TensorFlow and modified their model to include recurrent dropout . The entire code can be found in this Colab notebook https colab.research.google.com drive 1dLE58i2tttY6J Yr8dX8f57Ai0 54kTE run the code blocks all the way till the block where we re training the model to see the bug. Other info.logs pre x TypeError Traceback most recent call last ipython input 26 fa5128338d20 in module 8 9 for batch inp targ in enumerate dataset.take steps per epoch 10 batch loss train step inp targ enc hidden 11 total loss batch loss 12 6 frames usr local lib python3.6 dist packages tensorflow python eager def function.py in call self args kwds 436 Lifting succeeded so variables are initialized and we can run the 437 stateless function. 438 return self. stateless fn args kwds 439 else 440 canon args canon kwds self. canonicalize function inputs args kwds usr local lib python3.6 dist packages tensorflow python eager function.py in call self args kwargs 1286 Calls a graph function specialized to the inputs. 1287 graph function args kwargs self. maybe define function args kwargs 1288 return graph function. filtered call args kwargs pylint disable protected access 1289 1290 property usr local lib python3.6 dist packages tensorflow python eager function.py in filtered call self args kwargs 572 573 return self. call flat 574 t for t in nest.flatten args kwargs 575 if isinstance t ops.Tensor 576 resource variable ops.ResourceVariable usr local lib python3.6 dist packages tensorflow python eager function.py in call flat self args 625 Only need to override the gradient in graph mode and when we have outputs. 626 if context.executing eagerly or not self.outputs 627 outputs self. inference function.call ctx args 628 else 629 self. register gradient usr local lib python3.6 dist packages tensorflow python eager function.py in call self ctx args 413 attrs executor type executor type 414 config proto config 415 ctx ctx 416 Replace empty list with None 417 outputs outputs or None usr local lib python3.6 dist packages tensorflow python eager execute.py in quick execute op name num outputs inputs attrs ctx name 68 if any ops. is keras symbolic tensor x for x in inputs 69 raise core. SymbolicException 70 raise e 71 pylint enable protected access 72 return tensors usr local lib python3.6 dist packages tensorflow python eager execute.py in quick execute op name num outputs inputs attrs ctx name 58 tensors pywrap tensorflow.TFE Py Execute ctx. handle device name 59 op name inputs attrs 60 num outputs 61 except core. NotOkStatusException as e 62 if name is not None TypeError An op outside of the function building code is being passed a Graph tensor. It is possible to have Graph tensors leak out of the function building context by including a tf.init scope in your function building code. For example the following function will fail tf.function def has init scope my constant tf.constant 1. with tf.init scope added my constant 2 The graph tensor has name encoder unified gru ones like 0 pre ,Have tried with TensorFlow version 2.0.0 alpha and was able to reproduce the issue. Thanks for reporting the issue let me take a look. Thanks for reporting the issue it should now be fixed by https github.com tensorflow tensorflow commit 180f28a26660ca2e1ba27477f4f9592db5f9c4e8 Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29187 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29187 No a Btw the current colab might not apply the dropout correctly if you only enable the dropout recurrent dropout on the GRU layer. Under the hood the keras layer will check whether the current context is in training or inference and only apply the dropout during training. If the GRU layer was using by a keras model together with model.fit eval predict then the training context will be applied correctly. However if the user is writing their own custom training loop then the training context need to be set manually eg by The other alternative is that make sure the encoder decoder s call method is training state aware. eg the method could take a new kwarg training None and set to different value during training and inference. The training value need to be popagated to GRU s call method as well. qlzh727 Thanks a ton for your help on this Quick follow up has this been fixed in the GPU version as well I tried the nightly version from yesterday and it didn t seem to work. The issue still persists in the beta release I still have this issue in beta 2.0.0b1 For any of you that still facing the issue could u provide a snippet to reproduce the issue could u provide a snippet to reproduce the issue Similar error even without GRU. knobel dk I am bit confused about your message this issue was about the recurrent dropout for the LSTM GRU layer but your code doesn t have any LSTM GRU layer within it. Could you be more specific about the error you are facing knobel dk I am bit confused about your message this issue was about the recurrent dropout for the LSTM GRU layer but your code doesn t have any LSTM GRU layer within it. Could you be more specific about the error you are facing Thanks. Yes I have confused myself too. Those stateful Jupyter notebooks.. I fixed my problem by updating the TF2 version. Thanks. 
29189,Keras LSTM does not work with tf.distribute 2.0 , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Minor tweak to tutorial code. OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04. TensorFlow version use command below tf nightly gpu 2.0 preview Python version 3.6 CUDA cuDNN version 10.0 7.4 I copy pasted this tutorial code https www.tensorflow.org alpha tutorials distribute multi worker MNIST distributed training with TF2.0 but used tf.distribute.MirroredStrategy instead of MultiWorker . It worked. Then I changed the model architecture to a simple Embedding LSTM Dense architecture. It broke with the following errror This was executed on a remote cluster single machine with 2 GPUs. Note that I ve been seeing this error ever since the initial 2.0 alpha release. The code is as follows Again the common theme I ve observed is that if tf.keras.LSTM is part of my model and I m using tf.distribute it breaks with this error. Otherwise it works just fine., mckinziebrandon Ran the code in colab using GPU got the below error. AttributeError module tensorflow. api.v1.keras.losses has no attribute SparseCategoricalCrossentropy . muddham using TF 2.0 That s an error I get if I use TF 1.13. For example here is the link https www.tensorflow.org versions r2.0 api docs python tf keras losses SparseCategoricalCrossentropy for its documentation. Also muddham are you able to run this on a multi GPU setup The above code works as expected for tensorflow 2.0 CPU but breaks when run with tf nightly gpu 2.0 preview or tensorflow gpu 2.0.0 alpha0 on 2 GPUS. muddham Any update on this mckinziebrandon Is this still an issue gadagashwini Just confirmed that yes this is still an issue on the current TF gpu nightly 2.0 . Can you use Distribution Strategies directly with the Keras model instead of passing through Estimator There are likely bad effects here resulting from the mixing of many APIs including v1 optimizers and if you can swap to the fully v2 version Dist Strat keras optimizers keras model keras LSTM this should work. karmel I can try that sure. However if that s the case that people should not mix them then perhaps TensorFlow should not suggest doing so in their own tutorials recall that this post is a fairly trivial modification to an official tutorial . It does look like a lot has been updated regarding the tutorials on distributed training so I ll check those out too. Will update here after I try your suggestion. Ah that tutorial has been replaced with this one https www.tensorflow.org beta tutorials distribute multi worker with keras and rchao is removing references to the old. Can you start from the new one Sure starting from the new one I made the following incremental changes. Reporting my results for each change. In summary I end up getting basically the same error as before. Everything run below was with tensorflow gpu 2.0.0 beta1. Trying to Run the Tutorial with CNN First I went through the updated tutorial linked to by karmel and ran into more issues than before with the tutorial code itself when making certain minor changes. Again below are the sequence of tweaks I tried and what I observed. 1. Copy paste tutorial code and run without modification. Works as expected. Question here I noticed the tutorial does not mention single worker multi gpu explicitly and also doesn t mention tf.distribute until the multi worker case. Is it not required to use tf.distribute when using multi gpu on a single worker If so would be very helpful to clarify that in the docs. 2. Specify strategy tf.distribute.MirroredStrategy and wrap the build and compile cnn model with strategy.scope . Works as expected . 3. Specify TF CONFIG as described by the tutorial but only run on single worker. FAILS with error Note that TensorFlow itself acknowledged as usual the two GPUs on the machine when the program started It seems to somehow have forgotten them. Not sure. 4. Increase number of workers from 1 to 2 FAILS with error This happens when creating MultiWorkerMirroredStrategy. This was particularly surprising because the tutorial you linked seems to suggest that this should be a warning not an error. Searching further around the docs it is quite unclear how to fix this error. Replacing CNN with RNN Let s start over back to the updated tutorial again. Insert my tweaks to use an RNN instead of CNN as described in my original post. Concretely Instead of build and compile cnn model use Instead of loading dataset from tfds use If I then do I get essentially the same error as in my original post UPDATE something that might be useful to know is that if I replace tf.keras.LSTM with tf.keras.RNN cell tf.compat.v1.nn.rnn cell.LSTMCell ... everything works as expected for single worker multi gpu. The issue is definitely within tf.keras.LSTM itself. For multi worker multi gpu I still get the collective ops error that the tutorial suggests should be a warning . This error goes away if I wraps things with tf.estimator. Overall qlzh727 karmel are these being worked on I understand there are a ton of changes for TF2.0 but having Keras LSTM be incompatible with tf.distribute seems like a fairly significant issue. Hi mckinziebrandon we also got bug report from internal and we are trying to address it. For now i think you can work around the issue by using tf.keras.RNN cell tf.keras.LSTMCell ... which should give the same math result. Hi I am trying to run mnist example using distributed training tf 2.0.I am getting below similar error. I request to help me with workaround for this error. tf.distribute.experimental.CollectiveCommunication.NCCL File home administrator venv lib python3.5 site packages tensorflow python distribute collective all reduce strategy.py line 80 in init communication communication File home administrator venv lib python3.5 site packages tensorflow python distribute collective all reduce strategy.py line 110 in init self. initialize strategy cluster resolver File home administrator venv lib python3.5 site packages tensorflow python distribute collective all reduce strategy.py line 116 in initialize strategy self. initialize multi worker cluster resolver File home administrator venv lib python3.5 site packages tensorflow python distribute collective all reduce strategy.py line 208 in initialize multi worker device filters job s task d task type task id File home administrator venv lib python3.5 site packages tensorflow python eager context.py line 541 in configure collective ops raise RuntimeError Collective ops must be configured at program startup RuntimeError Collective ops must be configured at program startup mvp2301 you error seems to be totally different issue please file a separate bug with more details. qlzh727 Hi running into the same issue. I was running on tf.keras.layers.RNN cell tf.keras.layers.LSTMCell ... until recently but decided to switch to tf.keras.layers.LSTM for the CuDNN implmentation. Is the LSTM layers the only access point for the CuDNN LSTM In fact I also opened a feature request on it but that s a different matter https github.com tensorflow tensorflow issues 30535 I would also like to point out that I am getting this error without even being on a distributed strategy. Using LSTM Dataset TF 2.0... InvalidArgumentError Traceback most recent call last ipython input 11 c81bfda0c541 in module 7 optimizer optimizer 8 batch seq x len x len y 9 global batch size 240 10 11 t1 time.time anaconda3 envs seq3 lib python3.6 site packages tensorflow python eager def function.py in call self args kwds 426 Lifting succeeded so variables are initialized and we can run the 427 stateless function. 428 return self. stateless fn args kwds 429 else 430 canon args canon kwds anaconda3 envs seq3 lib python3.6 site packages tensorflow python eager function.py in call self args kwargs 1333 Calls a graph function specialized to the inputs. 1334 graph function args kwargs self. maybe define function args kwargs 1335 return graph function. filtered call args kwargs pylint disable protected access 1336 1337 property anaconda3 envs seq3 lib python3.6 site packages tensorflow python eager function.py in filtered call self args kwargs 587 588 return self. call flat 589 t for t in nest.flatten args kwargs expand composites True 590 if isinstance t ops.Tensor 591 resource variable ops.ResourceVariable anaconda3 envs seq3 lib python3.6 site packages tensorflow python eager function.py in call flat self args 669 Only need to override the gradient in graph mode and when we have outputs. 670 if context.executing eagerly or not self.outputs 671 outputs self. inference function.call ctx args 672 else 673 self. register gradient anaconda3 envs seq3 lib python3.6 site packages tensorflow python eager function.py in call self ctx args 443 attrs executor type executor type 444 config proto config 445 ctx ctx 446 Replace empty list with None 447 outputs outputs or None anaconda3 envs seq3 lib python3.6 site packages tensorflow python eager execute.py in quick execute op name num outputs inputs attrs ctx name 65 else 66 message e.message 67 six.raise from core. status to exception e.code message None 68 except TypeError as e 69 if any ops. is keras symbolic tensor x for x in inputs anaconda3 envs seq3 lib python3.6 site packages six.py in raise from value from value InvalidArgumentError 2 root error s found. 0 Invalid argument Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices job localhost replica 0 task 0 device GPU 0 vs job localhost replica 0 task 0 device CPU 0. The edge src node is while 20 exit 94 and the dst node is while 0 RetVal node se q3 seq encoder while body 195 decoder c lstm 2 StatefulPartitionedCall se q3 seq decoder encoder bidirectional cond 1 then 958 while exit 6952 2016 1 Invalid argument Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices job localhost replica 0 task 0 device GPU 0 vs job localhost replica 0 task 0 device CPU 0. The edge src node is while 20 exit 94 and the dst node is while 0 RetVal node se q3 seq encoder while body 195 decoder c lstm 2 StatefulPartitionedCall 0 successful operations. 0 derived errors ignored. Op inference train step 54702 Function call stack train step train step jkamalu can u provide the full code snippet to reproduce your issue mvp2301 you error seems to be totally different issue please file a separate bug with more details. Thanks for reply.created a new bug issue 30551 qlzh727 I will when I find the time and am able to produce it on a smaller example. In the meantime check out this issue which may be related the logs were the same for me when I got the cannot place graph error https github.com tensorflow tensorflow issues 29525 Hi qlzh727 I finally got around to finding a minimal example that shows that the LSTM implementation breaks in some cases without the distributed strategy. Here you go https github.com jkamalu tensorflow bugs blob master LSTMGraphPlacement.py It has to do the the inclusion of the loop. Without the loop it works fine. My real use case of the while loop is to perform dynamic decoding but I get the same error as you will run into here. mckinziebrandon I am currently able to run a model containing the tf.keras.layers.LSTM layer with the MirroredStrategy on 3 GPUs in parallel as of the July 23 2.0 gpu nightly. See this comment by qlzh727 on my now closed issue pointing to two recent commits that may have resolved a myriad of problems https github.com tensorflow tensorflow issues 30639 issuecomment 513599641 Thanks for reporting the issue the issue should be solved by ca7acec and f0fd2be. Could u try the code again with the tf 2.0 nightly builds Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29189 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29189 No a does this already fixed Yes. Hi I still encountered a similar error with the official 2.0 version and tf nightly. Here is the code snippet https gist.github.com matthew z e5848d545b60792dd84bfb9470ea541f model.fit works well but model.evaluate raises this error matthew z thanks for reporting the issue I can t reproduce it. Seems that to be a different issue since the your code does not use distribution strategy. Do u mind create new a ticket for that with the details about your local environment Thanks. qlzh727 Thank you for the reply. I have filed a new issue 33258 with detailed environment info tested on GCE machine and image . qlzh727 I have this issue when using TimeDistributed LSTM with mask zero True this is my model model tf.keras.Sequential embeding layer layers.Embedding self.vocab size self.word vector dim weights word embeding matrix trainable False mask zero True model.add TimeDistributed embeding layer model.add TimeDistributed tf.keras.layers.LSTM 50 model.add tf.keras.layers.Bidirectional costumized lstm.Costumized LSTM 50 model.add tf.keras.layers.Bidirectional costumized lstm.Costumized LSTM 100 model.add layers.Dense 3 activation softmax opt tf.keras.optimizers.Adam learning rate 0.001 model.compile optimizer opt loss categorical crossentropy metrics accuracy self.f1 m self.precision m self.recall m self.model model and this is the error C Users jalil PycharmProjects untitled1 venv Scripts python.exe C Users jalil PycharmProjects untitled1 main file.py 2019 11 14 14 11 36.983144 I tensorflow stream executor platform default dso loader.cc 44 Successfully opened dynamic library cudart64 100.dll 2019 11 14 14 11 45.638679 I tensorflow stream executor platform default dso loader.cc 44 Successfully opened dynamic library nvcuda.dll 2019 11 14 14 11 46.216495 I tensorflow core common runtime gpu gpu device.cc 1618 Found device 0 with properties name GeForce GTX 970M major 5 minor 2 memoryClockRate GHz 1.038 pciBusID 0000 01 00.0 2019 11 14 14 11 46.216676 I tensorflow stream executor platform default dlopen checker stub.cc 25 GPU libraries are statically linked skip dlopen check. 2019 11 14 14 11 46.217282 I tensorflow core common runtime gpu gpu device.cc 1746 Adding visible gpu devices 0 2019 11 14 14 11 50.885396 I tensorflow core platform cpu feature guard.cc 142 Your CPU supports instructions that this TensorFlow binary was not compiled to use AVX2 2019 11 14 14 11 51.214275 I tensorflow core common runtime gpu gpu device.cc 1618 Found device 0 with properties name GeForce GTX 970M major 5 minor 2 memoryClockRate GHz 1.038 pciBusID 0000 01 00.0 2019 11 14 14 11 51.214484 I tensorflow stream executor platform default dlopen checker stub.cc 25 GPU libraries are statically linked skip dlopen check. 2019 11 14 14 11 51.218182 I tensorflow core common runtime gpu gpu device.cc 1746 Adding visible gpu devices 0 2019 11 14 14 11 51.905201 I tensorflow core common runtime gpu gpu device.cc 1159 Device interconnect StreamExecutor with strength 1 edge matrix 2019 11 14 14 11 51.905307 I tensorflow core common runtime gpu gpu device.cc 1165 0 2019 11 14 14 11 51.905366 I tensorflow core common runtime gpu gpu device.cc 1178 0 N 2019 11 14 14 11 51.906228 I tensorflow core common runtime gpu gpu device.cc 1304 Created TensorFlow device job localhost replica 0 task 0 device GPU 0 with 4757 MB memory physical GPU device 0 name GeForce GTX 970M pci bus id 0000 01 00.0 compute capability 5.2 WARNING tensorflow From C Users jalil PycharmProjects untitled1 venv lib site packages tensorflow core python keras backend.py 3983 where from tensorflow.python.ops.array ops is deprecated and will be removed in a future version. Instructions for updating Use tf.where in 2.0 which has the same broadcast rule as np.where Train on 35000 samples validate on 6447 samples Epoch 1 1000 2019 11 14 14 12 33.178251 W tensorflow core grappler optimizers implementation selector.cc 310 Skipping optimization due to error while loading function libraries Invalid argument Functions inference backward cudnn lstm with fallback 671418 672877 and inference backward cudnn lstm with fallback 671418 672877 specialized for StatefulPartitionedCall at inference distributed function 675292 both implement lstm 81cdaa4a fa6f 4675 abbb 02fb4cd0189b but their signatures do not match. 2019 11 14 14 12 33.544669 I tensorflow stream executor platform default dso loader.cc 44 Successfully opened dynamic library cublas64 100.dll 2019 11 14 14 12 34.397677 I tensorflow stream executor platform default dso loader.cc 44 Successfully opened dynamic library cudnn64 7.dll 32 35000 .............................. ETA 2 03 422019 11 14 14 12 35.151804 W tensorflow core framework op kernel.cc 1622 OP REQUIRES failed at cudnn rnn ops.cc 1498 Unknown CUDNN STATUS BAD PARAM in tensorflow stream executor cuda cuda dnn.cc 1424 cudnnSetRNNDataDescriptor data desc.get data type layout max seq length batch size data size seq lengths array void padding fill 2019 11 14 14 12 35.152137 W tensorflow core common runtime base collective executor.cc 216 BaseCollectiveExecutor StartAbort Unknown CUDNN STATUS BAD PARAM in tensorflow stream executor cuda cuda dnn.cc 1424 cudnnSetRNNDataDescriptor data desc.get data type layout max seq length batch size data size seq lengths array void padding fill node cond 64 then 0 CudnnRNNV3 2019 11 14 14 12 35.152541 W tensorflow core common runtime base collective executor.cc 216 BaseCollectiveExecutor StartAbort Unknown function node forward cudnn lstm with fallback 672876 specialized for sequential time distributed 1 lstm StatefulPartitionedCall at inference distributed function 675292 function node forward cudnn lstm with fallback 672876 specialized for sequential time distributed 1 lstm StatefulPartitionedCall at inference distributed function 675292 CUDNN STATUS BAD PARAM in tensorflow stream executor cuda cuda dnn.cc 1424 cudnnSetRNNDataDescriptor data desc.get data type layout max seq length batch size data size seq lengths array void padding fill node cond 64 then 0 CudnnRNNV3 sequential time distributed 1 lstm StatefulPartitionedCall 32 35000 .............................. ETA 2 25 41Traceback most recent call last File C Users jalil PycharmProjects untitled1 main file.py line 102 in module main model instance.train model train batch data train batch labels test batch data test batch labels File C Users jalil PycharmProjects untitled1 main model.py line 103 in train model history self.model.fit x np.array train batch data y np.array train batch labels validation data np.array test batch data np.array test batch labels epochs 1000 callbacks tensorboard callback File C Users jalil PycharmProjects untitled1 venv lib site packages tensorflow core python keras engine training.py line 734 in fit use multiprocessing use multiprocessing File C Users jalil PycharmProjects untitled1 venv lib site packages tensorflow core python keras engine training v2.py line 324 in fit total epochs epochs File C Users jalil PycharmProjects untitled1 venv lib site packages tensorflow core python keras engine training v2.py line 123 in run one epoch batch outs execution function iterator File C Users jalil PycharmProjects untitled1 venv lib site packages tensorflow core python keras engine training v2 utils.py line 86 in execution function distributed function input fn File C Users jalil PycharmProjects untitled1 venv lib site packages tensorflow core python eager def function.py line 439 in call return self. stateless fn args kwds File C Users jalil PycharmProjects untitled1 venv lib site packages tensorflow core python eager function.py line 1822 in call return graph function. filtered call args kwargs pylint disable protected access File C Users jalil PycharmProjects untitled1 venv lib site packages tensorflow core python eager function.py line 1141 in filtered call self.captured inputs File C Users jalil PycharmProjects untitled1 venv lib site packages tensorflow core python eager function.py line 1224 in call flat ctx args cancellation manager cancellation manager File C Users jalil PycharmProjects untitled1 venv lib site packages tensorflow core python eager function.py line 511 in call ctx ctx File C Users jalil PycharmProjects untitled1 venv lib site packages tensorflow core python eager execute.py line 67 in quick execute six.raise from core. status to exception e.code message None File string line 3 in raise from tensorflow.python.framework.errors impl.UnknownError Derived CUDNN STATUS BAD PARAM in tensorflow stream executor cuda cuda dnn.cc 1424 cudnnSetRNNDataDescriptor data desc.get data type layout max seq length batch size data size seq lengths array void padding fill node cond 64 then 0 CudnnRNNV3 sequential time distributed 1 lstm StatefulPartitionedCall Op inference distributed function 675292 Function call stack distributed function distributed function distributed function do you have any suggestions please by the way on CPU it works fine the problem appears when using the GPU version. I also tried every GPU version of TensorFlow and the problem is still there. jalilasadi do u mind opening another ticket with more details about your issue We don t want to track multiple issue in the same ticket if they are different. 
29191,tf.function spuriously fails for branched super calls, tf.function fails spuriously under Python 3.7.3 for the following example Colab https colab.research.google.com drive 1 pS1K0biwse1oTIuuEv0lZYavbE61HAP This produces the following error Observations Everything works correctly without the tf.function decoration The branch in call seems necessary to trigger the bug. Skipping the branch doesn t trigger it. Replacing True with False doesn t trigger it. The bug can be triggered even if the condition evaluates to False for example replacing True with x 0 for x 42 Replacing tf.constant 42. with 42. doesn t trigger the bug Tested on TensorFlow 2.0 nightly 2.0.0 dev20190529 on Ubuntu 16.04 with Python 3.7.3 ,change to there will be no errors zakizhou Sure but the point is that super without any arguments is perfectly valid in Python 3 https www.python.org dev peps pep 3135 and as such this should not result in a failure. Furthermore the sensitivity to the conditional hints at further issues in tf.function I am able to reproduce the issue with Tensorflow nightly 2.0.0 dev20190529. Thanks I believe this is related to 26029 the Python3 style super is indeed not handled correctly yet we re working to fix that. It s indeed strange that the issue doesn t replicate reliably I ll have a closer look. mdanatg this continues to be an issue even with the recent attempt at supporting argument less super. In the new super in original context https github.com tensorflow tensorflow blob 612ceb6c488e228fa5246d2452799cf2691ef5f1 tensorflow python autograph operators py builtins.py L92 L130 implementation the technique used for getting self seems a bit fragile. In particular existence of conditionals and presumably other scoping constructs around the super call causes it to break in various ways. I ve included a couple of samples in this colab notebook https colab.research.google.com drive 1aoqSo9StvVHDPHhH7Ao4WYm t6UycnWd ethereon Thank you for follow up and the samples Indeed our current resolution for handling super is not yet complete and will only work if the call is in a method or outside control flow as you pointed out . kkimdev has been working on addressing this. We intentionally oversimplified the solution to assume the original caller of super is exactly one frame up the call stack in the converted code and we re working on a follow up improvement that removes that incorrect assumption and instead walks the call stack to find the caller which can be any number of levels above. We hope to have this ready shortly and will ping this thread when ready. In the mean time the following should be a reliable wokraround Again all these workarounds should hopefully not be needed soon. This should not be more robustly handled with the commit that will land soon. If you have a chance to give it a try I d love to know if we missed any corner case. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29191 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29191 No a 
29310,keras.model.load weights does not consider custom model layer , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 TensorFlow installed from source or binary conda Anaconda TensorFlow version use command below 1.13 Issue still existed in the latest source code I failed to load the official Resnet h5 weights with self implemented Resnet using the tf.keras.model . I found the behaviour of load weights in network.py does not considered the sub tf.keras.model . For example the Resnet has a ConvBlock and a IdentityBlock . Instead of use a function to define them I used tf.keras.model to define them so it followed the latest standard. Instead of simply pass the self.layers to load weights from hdf5 group by name . The load weights function is network.py should collect all layers inside the sub model then pass to load weights from hdf5 group by name . network.py https github.com tensorflow tensorflow blob master tensorflow python keras engine network.py line 1415 Suggested naive fix def get all layers model tf.keras.Model layers for layer in model.layers if isinstance layer tf.keras.Model layers.extend get all layers layer else layers.append layer return layers ,Confirmed again this issue is existed Will it be possible to provide a minimal code snippet that can reproduce the issue. It will really help us to understand the issue more clearly. Thanks For example The tf.keras.Model.load weights will not load the weights for the ConvBlock in above code. Because the Load weights only passed the self.layers to load weights from hdf5 group by name network.py https github.com tensorflow tensorflow blob master tensorflow python keras engine network.py 1415 edwardyehuang Can you provide a standalone code to reproduce the issue The current code shows NotImplementedError When subclassing the Model class you should implement a call method. . Thanks edwardyehuang Can you provide a standalone code to reproduce the issue The current code shows NotImplementedError When subclassing the Model class you should implement a call method. . Thanks The example above is not a full code resnet.zip https github.com tensorflow tensorflow files 3429272 resnet.zip Cannot believe you guys still cannot understand my issue... For example Conv2d is the subclass of tf.keras.layer Resnet and ConvBlock are the subclass of tf.keras.Model A Resnet class contains Conv2d and ConvBlock instances A ConvBlock contains Conv2d instances When doing the load h5 weight by name for Resnet. It will only load the weights for the tf.keras.layer instances in Resnet. The tf.keras.layer instances in ConvBlock are ignored Hmm...AFAIK .layers has issues for functional and subclassed models. But if with the solution you proposed it works fine then we d appreciate if you want to contribute. 31049 Closing this. Let s move discussion to the PR. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29310 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29310 No a edwardyehuang it seems that load weights .h5 by name True not work when changing the one layer of subclassed model name in faster rcnn it still raise erro about shape dismatch Layer 4 named b box head including some layers and i change the one layer name from rcnn class logits to new rcnn class logits according to the KERAS site but when loading it still load the weigths of rcnn class logits any helps thanks and i can get the weights .h5 name ubuntu 16 python 3 tf2 
29342,tf.config.set soft device placement seems to have no effect, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOSX 10.13.6 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below VERSION 2.0.0 dev20190527 GIT VERSION v1.12.1 2821 gc5b8e15064 Python version 3.5 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version CUDA 10.0 it s just a Colab GPU instance GPU model and memory Tesla P4 15079MiB Describe the current behavior The tf.config.set soft device placement function seems to have no effect when I create an integer variable and try to place it on a GPU I still get an exception. Describe the expected behavior I expect soft placement to fallback to using the CPU. No error. Code to reproduce the issue Other info logs The code above causes the following exception ,Also if I activate soft placement and I try to place an operation on a GPU device that does not exist I still get an exception. I expected TF to fallback to using the CPU Raises this exception Hey have you had a look at this example from the GPU guide at tensorflow.org tf.debugging.set log device placement True Create some tensors a tf.constant 1.0 2.0 3.0 4.0 5.0 6.0 b tf.constant 1.0 2.0 3.0 4.0 5.0 6.0 c tf.matmul a b print c To me it looks like TF is choosing which device is then suitable to execute this op If enabled an op will be placed on CPU if any of the following are true 1. there s no GPU implementation for the OP 2. no GPU devices are known or registered 3. need to co locate with reftype input s which are from CPU from TF config https github.com tensorflow tensorflow blob master tensorflow python framework config.py Hi lufol Thanks for your answer. Yes I saw this doc that s actually why I filed this bug. I don t see what soft placement would change in this example. I expect soft placement to change something when the user requests a specific device but there is no op for that device or the device does not exist. This would be useful if you want to write a program and deploy it on machines that may or may not have GPUs for example. So on a machine without any GPU I expect the following code to work without any error and just fallback to placing the variable on the CPU Perhaps I m misunderstanding what set soft device placement is designed for Okay I got it the semantics of soft placement have changed since TF 1. In TF 1 the following code works fine on a machine without any GPU I just tried it Prints 123 no problem. That s why I was surprised that it didn t work in TF 2.0. I m actually not sure when you would ever need to call set soft device placement False in TF 2.0 what s the use case From what I understand you either use tf.config.set soft device placement True to let tf automatically decide which device to use. or you use tf.config.set soft device placement False and decide for each tensor where to place it like this with tf.device CPU 0 ... So I guess in your case the first option would be best. Hi lufol I just ran some tests and it really seems like tf.config.set soft device placement False makes no difference at all. Whether it s True or False the default behavior is applied. I m quite puzzled. Have tried to reproduce on Colab with TF 2.0.0 dev20190527 with set soft device placement as True as well as False and was able to get same result in both the scenarios as mentioned in the issue. Yes ageron you are right. But I guess at least the doc does not explicitly says to execute the tensors on CPU if tf.config.set soft device placement False according to the doc If enabled an op will be placed on CPU if any of the following are true 1. there s no GPU implementation for the OP 2. no GPU devices are known or registered 3. need to co locate with reftype input s which are from CPU Maybe one could add something to the doc explaining the behaviour if tf.config.set soft device placement is set to False Behaviour is reproducable here https colab.research.google.com drive 1rtGjX9O 3rEI7yarMHMHBvu0l222dFXd . jaingaurav any comments Sorry for the delay. There have been multiple discussions about this internally. It seems that soft placement is respected by tf.function but not by the tf.device placement. We d like to resolve this however given it is a behavior change we re unlikely to be able to change the default in 1.0. Thanks jaingaurav. I m just thinking about TF 2.x I understand that it must keep the same behavior in TF 1.x. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29342 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29342 No a 
29361,CuDNN LSTM fails with XLA on 2080 Ti and CUDA 10.1, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04 TensorFlow installed from source or binary From Official Tensorflow GPU powered Docker TensorFlow version use command below 1.13.1 Python version 3.5.2 2.7 CUDA cuDNN version 10.1 GPU model and memory NVIDIA GeForce 2080 Ti and NVIDIA Tesla T4 Describe the current behavior When running with XLA on the code given below fails with the following exception Reproduces in the following environments Official Tensorflow Docker tensorflow tensorflow latest gpu NVIDIA Tensorflow Docker nvcr.io nvidia tensorflow 19.03 py3 Fails with the same error on Keras code CuDNN LSTM keras implementation too. Describe the expected behavior Given code completes successfully. Code to reproduce the issue If you remove any of the tf ops above error doesn t reproduce.,I have similar problem when I doing normal training it works but when I use FP 16 it gives me exactly the same error............ I have tried on Colab with TensorFlow version 1.13.1 and was able to reproduce the issue. thomasjoerg do you have cycles to look at this At first glance this looks like an invalid reduce multi output fusion that fails to codegen. The CHECK failure indicates that the shapes do not line up. I skimmed the relevant code but didn t spot an obvious bugs. I may have some cycles tomorrow otherwise early next week to debug this. Sorry I don t know whether my question is relevant to this but I do have similar error msg. All the functionality and model which works at tf 1.10 has issue in tf 1.13 in NCHW format. But NHWC works just fine. For details pls refer https stackoverflow.com questions 56437502 mixed precision training report ret check failure shapeutilequalfirst reduce I can reproduce the issue. Thank you for the high quality bug report The issue is fixed at head . You are running TF 1.13 stable . Would it be feasible for you to upgrade to TF 1.14 Yes sure that d be great thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29361 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29361 No a I can reproduce the issue. Thank you for the high quality bug report The issue is fixed at head . You are running TF 1.13 stable . Would it be feasible for you to upgrade to TF 1.14 thomasjoerg Could you provide a file name or commit hash that points to the issue the fix I m experiencing the same issue and as porting tf version may be hard in my use case I m wondering if it s easier to apply the same patch. Thank you 
29391,Dropout in GRU LSTM in Tensorflow 2.0 doesn t reset dropout masks on call, System information Tensorflow GIT VERSION v1.12.1 3283 geff4ae822a VERSION 2.0.0 dev20190604 Colab environment Describe the current behavior In RNNs the dropout masks should be reset after every call. However in training mode where the dropouts are activated GRU and LSTM implementation in tensorflow 2.0 seems to be re using the same dropout masks leading to deterministic behavior. Simple RNN seems to be doing the right thing re sampling dropout masks after each call. Describe the expected behavior The expected behaviour should be the same as SimpleRNN re sample dropout masks on each call . Code to reproduce the issue The following code produces the correct behaviour in tensorflow 1.13.1 output but in tensorflow 2.0 it doesn t output A quick fix is to call reset dropout mask and reset recurrent dropout mask between calls however this looks like a breaking change. output ,Have tried on Colab with TF CPU versions 1.13.1 and 2.0.0 dev20190604 and was able to reproduce the issue. Thanks for reporting the issue. There was a similar issue and has been recently fixed in 180f28a26660ca2e1ba27477f4f9592db5f9c4e8. Can u try with the latest nightly again https github.com tensorflow tensorflow issues 29187 For version v1.12.1 3447 g5a0f1bbfb7 2.0.0 dev20190606 the issue seems resolved. Thanks for verifying this closing bug now. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29391 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29391 No a sbagroy986 the CPU and GPU implementation is covered under same code path which means they should all be covered by the change. Can u report a issue with details if you feel this hasn t be fixed Also please make sure you install the latest pip package which should include the fix. 
29393, 2.0alpha0 AutoGraph tf.function does not automatically transform nested class methods, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0alpha0 Python version 3.6.5 Describe the current behavior When we define multiple methods for a class and only decorate one of them with tf.function the nested methods are not automatically transformed and some errors raise. Describe the expected behavior We only need decorate the outermost method. Code to reproduce the issue pre coding utf 8 Author Lin Lan ryan.linlan gmail.com from future import absolute import from future import division from future import print function import tensorflow as tf class Foo tf.keras.Model def init self super Foo self . init self.dense tf.keras.layers.Dense 20 self.embeddings tf.Variable tf.random.normal 100 5 dtype tf.float32 tf.function def call self inputs embeddings tf.nn.embedding lookup self.embeddings inputs return self. inner embeddings tf.function def inner self embeddings batch tf.shape embeddings 0 ta tf.TensorArray tf.float32 size batch for i in tf.range batch this self.dense embeddings i tf.newaxis ta ta.write i this return ta.stack foo Foo res foo 0 2 4 6 8 pre Other info logs TypeError Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map fn. Also decorating the method inner eliminate the error. ,It took me a lot of time to find this bug or intended behavior from my original code. It would be better to add a warning in the doc of tf.function . This section https www.tensorflow.org alpha guide effective tf2 refactor your code into smaller functions is confusing given this issue. Also the third paragraph of this section https www.tensorflow.org alpha guide autograph the tffunction decorator . Have tried with TF version 2.0.0 alpha on Colab and was able to reproduce the issue as mentioned in the description. You shouldn t need to decorate self. inner. mdanatg why isn t it being caught here This is a bug. We ll have it fixed in the nightly soon. Hi mdanatg Any updates regarding to this issue Yep It will likely be fixed today or sometime next week at the latest. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29393 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29393 No a 
29429,fit generator and predict generator ignores steps per epoch parameter when using sequence, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04.2 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below 1.13.1 Python version 3.6.8 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 10.0 GPU model and memory TITAN You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior Hi When running fit generator and predict generator with the steps per epoch parameter and steps in predict using a custom sequence the whole dataset is used instead of the number of batches passed in the steps per epoch parameter. According to the documentation steps per epoch Total number of steps batches of samples to yield from generator before declaring one epoch finished and starting the next epoch. It should typically be equal to the number of samples of your dataset divided by the batch size. Optional for Sequence if unspecified will use the len generator as a number of steps. The code below works as expected in tensorflow 1.12.0 and 2.0.0a0 but not in 1.13.1 Seems like the convert to generator like function in file engine training generator.py overrides the steps per epoch parameter with len data without checking if it is not None in case of a sequence Thanks Describe the expected behavior Code to reproduce the issue Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,This is fixed with latest version of TF Nightly. Output in tf nightly version 1.15.0 dev20190821 is printed below Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29429 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29429 No a 
29434,Python3 Issue with Keras Custom Layer, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04.2 LTS Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary on tx2. followed this https devtalk.nvidia.com default topic 1038957 jetson tx2 tensorflow for jetson tx2 TensorFlow version use command below 1.13.1 Python version 3.6.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version cuda 10.0 cudnn7.3 GPU model and memory TX2 Nvidia jetson You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior I am trying to load a model hdf5 which has a keras custom layer. I also see the same error when creating a keras model from scratch. Conv2D however works alright. Note I use python3. Is there a modification needed for custom layer However the script fails with the following error Describe the expected behavior Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. https gist.github.com mpkuse c2daacc3a5b8e07697ea7c595f900413 Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,I could verify the issue is encountered for both Python 2 and 3. Created a PR 29567 for the fix. Since I am running tensorflow with precompiled binaries from Nvidia on tx2 is there a simpler way to get your fix to my environment Will it be necessary for me to recompile tensorflow again I understand you have changed the following in file tensorflow python ops random ops.py Can you tell me how if possible I can patch my tensorflow mpkuse Not familiar with your environment. You can modify the python script in place though it will be replaced if installed by pip again. These changes worked for me on Nvidia Jetson TX2. Thanks a ton yongtang Closing the issue since its resolved. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29434 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29434 No a 
29438,TimeDistributed wrapper around DepthwiseConv2D broken AttributeError in 1.13, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Linux Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from pip3 TensorFlow version b v1.13.1 0 g6612da8951 1.13.1 Python version CUDA cuDNN version 10.0.130 7.4.1 GPU model and memory NVIDIA K80 11GB Describe the current behavior TimeDistributed wrapper around DepthwiseConv2D fails with AttributeError tuple object has no attribute dims Describe the expected behavior Wrapper should succesfully apply to layer previously worked in TF 1.11.0. Code to reproduce the issue ,I am able to reproduce the issue with tf 1.13.1 on colab. Thanks Thanks for reporting this. Roughly looking through stack trace I think the issue is that TimeDistributed layer does not wrap input shape as TensorShape before calling Conv2D.build I will see if I can fix it Sorry I meant DepthWiseConv2D Fixed and should be available in tf nightly. Let me know if it doesn t work for you. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29438 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29438 No a 
29472, TF2.0 tf.lite.converter.convert error Cannot find the Placeholder op that is an input to the ReadVariableOp. watch my second problem,I test this code for save model in tf nightly 2.0 gpu in ubuntu 19.04 tf.saved model.save model saved model dir and get a error AttributeError TypeError object has no attribute message my code like this coding utf 8 from future import absolute import division print function unicode literals import tensorflow as tf import os import numpy as np import matplotlib.pyplot as plt tf. version Setup Input Pipeline Download the flowers dataset. URL https storage.googleapis.com download.tensorflow.org example images flower photos.tgz zip file tf.keras.utils.get file origin URL fname flower photos.tgz extract True base dir os.path.join os.path.dirname zip file flower photos Use ImageDataGenerator to rescale the images. Create the train generator and specify where the train dataset directory image size batch size. Create the validation generator with similar approach as the train generator with the flow from directory method. IMAGE SIZE 224 BATCH SIZE 64 datagen tf.keras.preprocessing.image.ImageDataGenerator rescale 1. 255 validation split 0.2 train generator datagen.flow from directory base dir target size IMAGE SIZE IMAGE SIZE batch size BATCH SIZE subset training val generator datagen.flow from directory base dir target size IMAGE SIZE IMAGE SIZE batch size BATCH SIZE subset validation for image batch label batch in train generator break image batch.shape label batch.shape Save the labels in a file which will be downloaded later. print train generator.class indices labels n .join sorted train generator.class indices.keys with open labels.txt w as f f.write labels Create the base model from the pre trained convnets Create the base model from the MobileNet V2 model developed at Google and pre trained on the ImageNet dataset a large dataset of 1.4M images and 1000 classes of web images. First pick which intermediate layer of MobileNet V2 will be used for feature extraction. A common practice is to use the output of the very last layer before the flatten operation the so called bottleneck layer . The reasoning here is that the following fully connected layers will be too specialized to the task the network was trained on and thus the features learned by these layers won t be very useful for a new task. The bottleneck features however retain much generality. Let s instantiate an MobileNet V2 model pre loaded with weights trained on ImageNet. By specifying the include top False argument we load a network that doesn t include the classification layers at the top which is ideal for feature extraction. IMG SHAPE IMAGE SIZE IMAGE SIZE 3 Create the base model from the pre trained model MobileNet V2 base model tf.keras.applications.MobileNetV2 input shape IMG SHAPE include top False weights imagenet Feature extraction You will freeze the convolutional base created from the previous step and use that as a feature extractor add a classifier on top of it and train the top level classifier. base model.trainable False Add a classification head model tf.keras.Sequential base model tf.keras.layers.Conv2D 32 3 activation relu tf.keras.layers.Dropout 0.2 tf.keras.layers.GlobalAveragePooling2D tf.keras.layers.Dense 5 activation softmax Compile the model You must compile the model before training it. Since there are two classes use a binary cross entropy loss. model.compile optimizer tf.keras.optimizers.Adam loss categorical crossentropy metrics accuracy model.summary print Number of trainable variables .format len model.trainable variables Train the model TODO markdaoust delete steps per epoch in TensorFlow r1.14 r2.0 epochs 2 history model.fit train generator epochs epochs validation data val generator Learning curves Let s take a look at the learning curves of the training and validation accuracy loss when using the MobileNet V2 base model as a fixed feature extractor. acc history.history accuracy val acc history.history val accuracy loss history.history loss val loss history.history val loss plt.figure figsize 8 8 plt.subplot 2 1 1 plt.plot acc label Training Accuracy plt.plot val acc label Validation Accuracy plt.legend loc lower right plt.ylabel Accuracy plt.ylim min plt.ylim 1 plt.title Training and Validation Accuracy plt.subplot 2 1 2 plt.plot loss label Training Loss plt.plot val loss label Validation Loss plt.legend loc upper right plt.ylabel Cross Entropy plt.ylim 0 1.0 plt.title Training and Validation Loss plt.xlabel epoch plt.show Fine tuning In our feature extraction experiment you were only training a few layers on top of an MobileNet V2 base model. The weights of the pre trained network were not updated during training. One way to increase performance even further is to train or fine tune the weights of the top layers of the pre trained model alongside the training of the classifier you added. The training process will force the weights to be tuned from generic features maps to features associated specifically to our dataset. Un freeze the top layers of the model All you need to do is unfreeze the base model and set the bottom layers be un trainable. Then recompile the model necessary for these changes to take effect and resume training. base model.trainable True Let s take a look to see how many layers are in the base model print Number of layers in the base model len base model.layers Fine tune from this layer onwards fine tune at 100 Freeze all the layers before the fine tune at layer for layer in base model.layers fine tune at layer.trainable False Compile the model Compile the model using a much lower training rate. model.compile loss categorical crossentropy optimizer tf.keras.optimizers.Adam 1e 5 metrics accuracy model.summary print Number of trainable variables .format len model.trainable variables Continue Train the model history fine model.fit train generator epochs 2 validation data val generator Convert to TFLite Saved the model using tf.saved model.save and then convert the saved model to a tf lite compatible format. error code save failed ............ 1 saved model dir save fine tuning tf.saved model.save model saved model dir converter tf.lite.TFLiteConverter.from saved model saved model dir tflite model converter.convert with open model.tflite wb as f f.write tflite model Download the converted model and labels from google.colab import files files.download model.tflite files.download labels.txt acc history fine.history accuracy val acc history fine.history val accuracy loss history fine.history loss val loss history fine.history val loss plt.figure figsize 8 8 plt.subplot 2 1 1 plt.plot acc label Training Accuracy plt.plot val acc label Validation Accuracy plt.legend loc lower right plt.ylabel Accuracy plt.ylim min plt.ylim 1 plt.title Training and Validation Accuracy plt.subplot 2 1 2 plt.plot loss label Training Loss plt.plot val loss label Validation Loss plt.legend loc upper right plt.ylabel Cross Entropy plt.ylim 0 1.0 plt.title Training and Validation Loss plt.xlabel epoch plt.show and this is mistake W0606 11 03 53.153013 140230779684672 saved model.py 748 Skipping full serialization of Keras layer tensorflow.python.keras.engine.input layer.InputLayer object at 0x7f894c11be80 because it does not have an input spec defined. W0606 11 03 53.165678 140230779684672 saved model.py 748 Skipping full serialization of Keras layer tensorflow.python.keras.engine.input layer.InputLayer object at 0x7f89a64fdba8 because it does not have an input spec defined. Traceback most recent call last File home mint miniconda3 lib python3.7 site packages tensorflow python keras saving saved model.py line 712 in serialize all attributes save model default signature File home mint miniconda3 lib python3.7 site packages tensorflow python keras saving saved model.py line 850 in wrap layer functions fn.get concrete function File home mint miniconda3 lib python3.7 site packages tensorflow python eager def function.py line 681 in get concrete function self. initialize args kwargs add initializers to initializer map File home mint miniconda3 lib python3.7 site packages tensorflow python eager def function.py line 359 in initialize args kwds File home mint miniconda3 lib python3.7 site packages tensorflow python eager function.py line 1401 in get concrete function internal garbage collected graph function self. maybe define function args kwargs File home mint miniconda3 lib python3.7 site packages tensorflow python eager function.py line 1689 in maybe define function graph function self. create graph function args kwargs File home mint miniconda3 lib python3.7 site packages tensorflow python eager function.py line 1582 in create graph function capture by value self. capture by value File home mint miniconda3 lib python3.7 site packages tensorflow python framework func graph.py line 728 in func graph from py func func outputs python func func args func kwargs File home mint miniconda3 lib python3.7 site packages tensorflow python eager def function.py line 309 in wrapped fn return weak wrapped fn . wrapped args kwds File home mint miniconda3 lib python3.7 site packages tensorflow python keras saving saved model.py line 994 in call and return conditional losses return layer call inputs training training layer.get losses for inputs File home mint miniconda3 lib python3.7 site packages tensorflow python keras layers normalization.py line 651 in call outputs self. fused batch norm inputs training training File home mint miniconda3 lib python3.7 site packages tensorflow python keras layers normalization.py line 533 in fused batch norm self.add update mean update File home mint miniconda3 lib python3.7 site packages tensorflow python util deprecation.py line 507 in new func return func args kwargs File home mint miniconda3 lib python3.7 site packages tensorflow python keras engine base layer.py line 1117 in add update updates process update x for x in updates File home mint miniconda3 lib python3.7 site packages tensorflow python keras engine base layer.py line 1117 in listcomp updates process update x for x in updates File home mint miniconda3 lib python3.7 site packages tensorflow python keras engine base layer.py line 1113 in process update reachable tf utils.get reachable from inputs relevant inputs update File home mint miniconda3 lib python3.7 site packages tensorflow python keras utils tf utils.py line 134 in get reachable from inputs raise TypeError Expected Operation Variable or Tensor got str x TypeError Expected Operation Variable or Tensor got None During handling of the above exception another exception occurred Traceback most recent call last File home mint ai tensorflow flowerlite flowers tf lite.py line 204 in module tf.saved model.save model saved model dir File home mint miniconda3 lib python3.7 site packages tensorflow python saved model save.py line 812 in save checkpoint graph view File home mint miniconda3 lib python3.7 site packages tensorflow python saved model signature serialization.py line 65 in find function to export functions saveable view.list functions saveable view.root File home mint miniconda3 lib python3.7 site packages tensorflow python saved model save.py line 139 in list functions self. serialization cache File home mint miniconda3 lib python3.7 site packages tensorflow python keras engine base layer.py line 2234 in list functions for serialization fns saved model.serialize all attributes self serialization cache File home mint miniconda3 lib python3.7 site packages tensorflow python keras saving saved model.py line 712 in serialize all attributes save model default signature File home mint miniconda3 lib python3.7 site packages tensorflow python keras saving saved model.py line 818 in wrap layer functions original attrs replace child layer functions layer serialization cache File home mint miniconda3 lib python3.7 site packages tensorflow python keras saving saved model.py line 891 in replace child layer functions layer fns serialize all attributes child layer serialization cache File home mint miniconda3 lib python3.7 site packages tensorflow python keras saving saved model.py line 712 in serialize all attributes save model default signature File home mint miniconda3 lib python3.7 site packages tensorflow python keras saving saved model.py line 818 in wrap layer functions original attrs replace child layer functions layer serialization cache File home mint miniconda3 lib python3.7 site packages tensorflow python keras saving saved model.py line 891 in replace child layer functions layer fns serialize all attributes child layer serialization cache File home mint miniconda3 lib python3.7 site packages tensorflow python keras saving saved model.py line 716 in serialize all attributes message .format layer e.message AttributeError TypeError object has no attribute message the save code is ok at 2.0alpha0 but dismiss some function like this tf.lite.TFLiteConverter.from saved model this is my tf2.0 version import tensorflow as tf print tf. version 2.0.0 dev20190605 I think it s bug is it ,Have tried on Colab with TF version 2.0.0 dev20190605 and was able to get mentioned output. tms2003 I don t see any issue in saved model as it ran through without any errors. Please check the gist https colab.sandbox.google.com gist jvishnuvardhan 8415f5b8b3f5f8f1ed4f7470437cdb9f tf 29472 saved model.ipynb . Please provide a small reproducible code if there are any more issues. Thanks I test this code for save model in tf nightly 2.0 gpu in ubuntu 19.04 tf.saved model.save model saved model dir and get a error the save code is ok at 2.0alpha0 but dismiss some function like this tf.lite.TFLiteConverter.from saved model this is my tf2.0 version I think it s bug is it It s ok in beta0 but I get error when covert my code converter tf.lite.TFLiteConverter.from saved model saved model dir tflite model converter.convert and error traceback most recent call last File D aisource tf flowertflite flowers tf lite.py line 253 in module tflite model converter.convert File D ProgramData Miniconda3 envs tf20 lib site packages tensorflow lite python lite.py line 348 in convert self. funcs 0 File D ProgramData Miniconda3 envs tf20 lib site packages tensorflow python framework convert to constants.py line 166 in convert variables to constants v2 raise ValueError Cannot find the Placeholder op that is an input ValueError Cannot find the Placeholder op that is an input to the ReadVariableOp. The same error in https colab.research.google.com gist tms2003 e65a1e3bc1c5e978a4436ad377b0a92a tf 29472 saved model.ipynb ValueError Traceback most recent call last ipython input 7 5d4cfeefed96 in module 1 2 tflite model converter.convert 3 4 1 frames usr local lib python3.6 dist packages tensorflow python framework convert to constants.py in convert variables to constants v2 func 164 input name get name map name to node input name .input 0 165 if map name to node input name .op Placeholder 166 raise ValueError Cannot find the Placeholder op that is an input 167 to the ReadVariableOp. 168 Build a map of Placeholder ops that are inputs to ReadVariableOps to the ValueError Cannot find the Placeholder op that is an input to the ReadVariableOp. so anybody can help me tms2003 I can reproduce the same error as you listed. Could you edit the title accordingly. Thanks Hi I see the same error when trying to convert a RNN model to tflite but not when converting a fully connected or CNN model Does anyone know if tf.keras.layers.LSTM and tf.keras.layers.GRU ops have issues with conversion to TFLite models When the tflite converter encounters the lstm node it find a Enter op instead of Placeholder op and causes the conversion to fail. For my case the node and the corresponding op where is fails are node name lstm while ReadVariableOp 1 op ReadVariableOp input lstm while ReadVariableOp Enter input lstm while Identity attr key dtype value type DT FLOAT input op name lstm while ReadVariableOp Enter op Enter input lstm recurrent kernel attr key T value type DT RESOURCE attr key frame name value s lstm while while context attr key is constant value b true attr key parallel iterations value i 32 Is there a remedy around this or tf.keras.layers.LSTM are just not usable with TFLite There is currently limited support for LSTMs in TFLite. The documented path is available here https github.com tensorflow tensorflow blob master tensorflow lite experimental examples lstm g3doc README.md . We are working on improving our support of control flow based operations and models. i am getting this error while saving my model and converting it into tfLite ValueError Traceback most recent call last ipython input 31 214edbbd817b in module 1 saved model dir Documents fine tuning 2 tf.saved model.save model saved model dir 3 4 converter tf.lite.TFLiteConverter.from saved model saved model dir 5 tflite model converter.convert AppData Local Continuum anaconda3 envs hello tf lib site packages tensorflow python saved model save.py in save obj export dir signatures 820 object saver util.TrackableSaver checkpoint graph view 821 asset info exported graph fill meta graph def 822 meta graph def saveable view signatures 823 saved model.saved model schema version 824 constants.SAVED MODEL SCHEMA VERSION AppData Local Continuum anaconda3 envs hello tf lib site packages tensorflow python saved model save.py in fill meta graph def meta graph def saveable view signature functions 508 resource initializer ops 509 with exported graph.as default 510 object map resource map asset info saveable view.map resources 511 for resource initializer function in resource initializer functions 512 asset dependencies AppData Local Continuum anaconda3 envs hello tf lib site packages tensorflow python saved model save.py in map resources self 243 and capture not in self.captured tensor node ids 244 copied tensor constant op.constant 245 tensor util.constant value capture 246 node id len self.nodes 247 node CapturedConstant AppData Local Continuum anaconda3 envs hello tf lib site packages tensorflow python framework constant op.py in constant value dtype shape name 244 245 return constant impl value dtype shape name verify shape False 246 allow broadcast True 247 248 AppData Local Continuum anaconda3 envs hello tf lib site packages tensorflow python framework constant op.py in constant impl value dtype shape name verify shape allow broadcast 282 tensor util.make tensor proto 283 value dtype dtype shape shape verify shape verify shape 284 allow broadcast allow broadcast 285 dtype value attr value pb2.AttrValue type tensor value.tensor.dtype 286 const tensor g.create op AppData Local Continuum anaconda3 envs hello tf lib site packages tensorflow python framework tensor util.py in make tensor proto values dtype shape verify shape allow broadcast 452 else 453 if values is None 454 raise ValueError None values not supported. 455 if dtype is provided forces numpy array to be the type 456 provided if possible. ValueError None values not supported. I am also getting the same error opt conda lib python3.6 site packages tensorflow lite python lite.py in from keras model file cls model file input arrays input shapes output arrays custom objects 760 set tensor shapes input tensors input shapes 761 762 graph def freeze graph sess input tensors output tensors 763 return cls graph def input tensors output tensors 764 opt conda lib python3.6 site packages tensorflow lite python util.py in freeze graph sess input tensors output tensors 236 output arrays get tensor name tensor for tensor in output tensors 237 return tf graph util.convert variables to constants sess graph def 238 output arrays 239 else 240 return sess.graph def opt conda lib python3.6 site packages tensorflow python util deprecation.py in new func args kwargs 322 in a future version if date is None else after s date 323 instructions 324 return func args kwargs 325 return tf decorator.make decorator 326 func new func deprecated opt conda lib python3.6 site packages tensorflow python framework graph util impl.py in convert variables to constants sess input graph def output node names variable names whitelist variable names blacklist 300 source op name get input name map name to node source op name 301 if map name to node source op name .op VarHandleOp 302 raise ValueError Cannot find the variable that is an input 303 to the ReadVariableOp. 304 ValueError Cannot find the variable that is an input to the ReadVariableOp. Does anyone has any solution tms2003 Can you try TF2.0 and let us know. I ran it in TF2.0 and I don t see any error. Here https colab.sandbox.google.com gist jvishnuvardhan 8dc40269e8223c9eb66f286f94e1d621 tf 29472 saved model.ipynb is the gist. I just changed epochs from 2 to 1 just to save some TF runtime. If you think the issue was resolved by TF2.0 then please close this issue. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29472 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29472 No a Hi I see the same error when trying to convert a RNN model to tflite but not when converting a fully connected or CNN model Does anyone know if tf.keras.layers.LSTM and tf.keras.layers.GRU ops have issues with conversion to TFLite models When the tflite converter encounters the lstm node it find a Enter op instead of Placeholder op and causes the conversion to fail. For my case the node and the corresponding op where is fails are node name lstm while ReadVariableOp 1 op ReadVariableOp input lstm while ReadVariableOp Enter input lstm while Identity attr key dtype value type DT FLOAT input op name lstm while ReadVariableOp Enter op Enter input lstm recurrent kernel attr key T value type DT RESOURCE attr key frame name value s lstm while while context attr key is constant value b true attr key parallel iterations value i 32 Is there a remedy around this or tf.keras.layers.LSTM are just not usable with TFLite mrudulaathi You able to resolve this one gunjanddave You should be able to resolve your conversion issues by setting the flag experimental new converter to True . If that does not resolve your issues please file a new bug with your error and details on how to reproduce your error. gunjanddave You should be able to resolve your conversion issues by setting the flag experimental new converter to True . If that does not resolve your issues please file a new bug with your error and details on how to reproduce your error. Perfect it worked. Thanks. image https user images.githubusercontent.com 105509376 168322331 b6a07cd5 5ca6 4cd6 ac5f f787eff4bebd.png I am facing the same problem even after setting the experimental new converter to True . Can anyone help me I am trying to quantize a custom layer Full int8 
29525, TF 2.0 constant folding failed invalid argument unsupported type 21, System information TensorFlow installed from source or binary binary TensorFlow version use command below tf nightly gpu 2.0 preview 2.0.0.dev20190606 Python version 3.6.5 Code to reproduce the issue pre import numpy as np import tensorflow as tf class Model tf.keras.Model def init self super Model self . init self.dense tf.keras.layers.Dense 10 def call self inputs return self.dense inputs model Model def forward x batch size x.shape 0 ys tf.TensorArray tf.float32 size batch size for i in tf.range batch size y model x i tf.newaxis ys ys.write i y return ys.stack def train x forward func with tf.GradientTape as tape ys forward func x loss tf.reduce mean ys grads tape.gradient loss model.trainable weights return grads def big train x with tf.GradientTape as tape batch size x.shape 0 ys tf.TensorArray tf.float32 size batch size for i in tf.range batch size y model x i tf.newaxis ys ys.write i y ys ys.stack loss tf.reduce mean ys grads tape.gradient loss model.trainable weights return grads x np.random.rand 10 5 .astype np.float32 codes buggy tf.function train x forward tf.function big train x codes normal tf.function train x tf.function forward train x tf.function forward train x forward big train x def test code tf.print tf.print f code exec code test codes buggy 0 test codes buggy 1 test codes normal 0 test codes normal 1 test codes normal 2 test codes normal 3 pre Other info logs Print pre tf.function train x forward 2019 06 07 16 46 23.314712 E tensorflow core grappler optimizers meta optimizer.cc 502 constant folding failed Invalid argument Unsupported type 21 2019 06 07 16 46 23.357137 E tensorflow core grappler optimizers meta optimizer.cc 502 constant folding failed Invalid argument Unsupported type 21 2019 06 07 16 46 23.460568 I tensorflow stream executor platform default dso loader.cc 42 Successfully opened dynamic library libcublas.so.10.0 tf.function big train x 2019 06 07 16 46 24.139754 E tensorflow core grappler optimizers meta optimizer.cc 502 constant folding failed Invalid argument Unsupported type 21 2019 06 07 16 46 24.180814 E tensorflow core grappler optimizers meta optimizer.cc 502 constant folding failed Invalid argument Unsupported type 21 tf.function train x tf.function forward train x tf.function forward train x forward big train x pre Related to 28626 .,Have the same issue on a TF2.0 GPU beta0. It really influences performance. Hi vejvarm What kind of performance do you mean Training speed or accuracy Hi llan ml sorry for not ellaborating on that. By performance I mean the training speed. If I remember correctly with the warning it took about 2 seconds batch while without it I m at 2 to 4 batches second. So roughly 4 to 8 times slowdown with the warning. Not really sure about a specific number but it was significant. As of accuracy I haven t had the time to run the model for long enough to see if it has some inpact on that. llan ml I tried to reproducing the issue on colab with latest tf nightly gpu 2.0 preview but i did not get any error. Can you try once and let us know if that still an issue. Thanks llan ml I tried to reproducing the issue on colab with latest tf nightly gpu 2.0 preview but i did not get any error. Can you try once and let us know if that still an issue. Thanks Just tried it and to my knowledge it is still there as of 2.0.0.dev20190614 . It s just not written dirrectly to the cell output as it is not an error but a warning. It can be found in the runtime logs of the notebook gadagashwini I tested with 2.0.0.dev20190615 and the error still appears. same issue on tf gpu 1.14 now rmlarsen this looks like a grappler issue can you triage I am having a similar issue also on TensorFlow 2.0 beta with GPU enabled. Interestingly hiding the GPU away from Tensorflow using export CUDA VISIBLE DEVICES 1 before running the script enables the code to run but still prints out the error message this Issue is about and feels slower than it should while using the GPU results in a memory leakage that end up with the system crashing due to the GPU memory being saturated. System information OS Platform and Distribution Linux Mint 19.1 TensorFlow installed from binary using pip TensorFlow version v2.0.0 beta0 16 g1d91213fe7 Python version 3.6.8 CUDA cuDNN version 10.0 7 GPU model and memory QUADRO P 1000 with 4 GB of dedicated RAM 16 GB of system RAM Code to reproduce the issue Error with GPU enabled Error with GPU disabled Hi I did some additional testing based on my previous bug yielding example and would like to report on it in hope that it may help track down and ultimately fix the issue at stake. Setting and consequences What I did was getting rid of sequences masking for the BiLSTM layers thus using a less general model expecting batches of same length sequences. In this case I no longer encounter GPU memory leakage at least not something that would make my computer crash on the first run of fitting the model however an optimization warning is raised and I have no idea whether it relates to the initial issue or not. It shows up both with and without enabling the use of the GPU and for each use of the model not just for the fitting process . Warning message Code In the code below I allow distinct batches to contain sequences of different length however I also made a test using a strict parameter i.e. setting the InputLayer s shape to length input dim with length an integer instead of None which yields exactly the same error message. I hope this helps solving the initial issue. Please let me know if there is any additional info I can provide or test I can run to help. At the moment not being able to fit models with LSTM layers using properly masked variable length sequences is quite an issue to put code into production under TensorFlow 2.0. I know this is the whole point of a beta release having a not yet quite stable version out to identify issued that need solving before the actual release but the programming logic has been so greatly altered as compared with TF 1.x that it would also be unpractical not to start taking the step getting used to Eager execution demands an important effort after having extensively used the low level placeholder session API ... Note this issue is quite similar to the newly opened 30263 Additional test results sorry for the multiplication of messages I really want to provide as much info as possible hoping it can help solve the issue Changing my code to feed the model with a numpy array containing the batched sequences lengths then converting it to a sequence mask Tensor using tf.sequence mask within the model partly fixes the issue. E tensorflow core grappler optimizers meta optimizer.cc 502 constant folding failed Invalid argument Unsupported type 21 still shows up twice when first calling the fit method of the model built both when using GPU or CPU only The model can be run and fit both with and without the GPU. After the first call to fit the error message no longer shows up and I can see the loss decreasing along the iterations up to some point . Code Conclusion So I guess the initial issue the error showing up is not solved. There is additionally the issue previously pointed out and also object of issue 30263 of an optimization error and apparent failure to fit models when using fixed length sequences. However the GPU memory issue I was personally encountering seems to have been related to the use of a Tensor instead of a numpy array as input to my model. I don t know whether this is by design in which case it might be worth it to add warnings when users do that or a separate issue but I was able to fix it with better code design. I also run into this issue when using masking on a GRU LSTM layer though running on CPU does not prevent the memory from blowing up and crashing the machine. In fact even when running on GPU system memory maxes out though it looks as though the printed errors imply that GPU memory has been completely filled as well. Removing the masking however allows training to occur without issue though the constant folding failed Invalid argument Unsupported type 21 message still occurs. Hi Could anyone from the TF team confirm that this issue is being researched worked out It appears that it does not show in every setting thus the difficulty to pass front row issues screeners as in the newly opened 30533 but it causes major performance issues to people who are confronted to it see my performance tests on issue 30263 . It should also be noted that this not only affects TF 2.0 but also 1.14 when Eager execution is enabled... Hi I can confirm that this is not inherent to the LSTM implementation. I have just reproduced the same error with GRU. Maybe it has something to do with the gpu optimized CuDNN implementations Thank you for sharing this. The issue seems to be at the grappler level which if I am not mistaken is indeed the mechanism that chooses the backend kernel to use which can be a CuDNN one... Interestingly I am encountering this issue in TF 1.14 in TF 2.0b1 installed through pip but not in TF 2.0b1 installed from source using the r2.0 branch and not always in TF 2.0b1 installed from source using yesterday s state of the master branch. Using this issue s code on the latter installation I have a distinct bug namely repeated prints similar to W tensorflow core grappler costs virtual scheduler.cc 794 Output node gradients while grad while grad gradients zeros 1 switch 43 has alread seen this input node gradients while grad while grad merge 25 possibly due to Swith Merge in previous nodes. Skip to increment num inputs ready. Edit I should note that I am running on the gpu nightly pip build as of the time stamp on this comment. Another interesting piece of narrowing information. In the piece of code below everything runs without a hitch if the for loop tf.while loop behind the scenes is removed. That is... Without for loop tf function routine runs twice code runs ad infinitum With for loop tf function routine runs twice graph placement issue and and code breaks Here s the code https github.com jkamalu tensorflow bugs blob master LSTMGraphPlacement.py Another thing worth noting is that this issue appears even without the while loop with tensorflow GPU distributed strategies as seen https github.com tensorflow tensorflow issues 29189 pandrey fr a note if the cudnn implementation is not important to you I don t know why it wouldn t be but just in case you can wrap the LSTMCell layer in the RNN layer and it works fine... another hint that this error might be in the optimized implementation. The warning should go away in the next nightly. I m looking into the original issue with unsupported types in constant folding. The issue is that the error handling in many places in Grappler is much too conservative. In this case we bail completely out of folding because we fail to convert a constant of an unknown type early. I ll work on making the code more robust in this sense. The particular error in this case was due to ZerosLike being overloaded for DT VARIANT types https github.com tensorflow tensorflow blob master tensorflow core kernels constant op.cc L267 I am submitting a fix now. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29525 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29525 No a Fix submitted https github.com tensorflow tensorflow commit 24174643a75e819b8ce01fd70d45d03616e50071 Great thank you rmlarsen As announced by rmlarsen the fix which is now included in the nightly build removes the error message however it appears at least in my case that LSTM layers with masking still won t be moved to the GPU when Eager is enabled at least I am still trying to figure out whether it is the case with Eager disabled which is somehow confusing. Do you have any idea why this is the case Do you mean they won t be moved to the GPU or that the graph won t be built with the CuDNN implementation My bootleg LSTM layers see below exist on the GPU with the standard implementation I verify this by watching nvidia smi . I use masking right padding so TF v2.0 CuDNN compatible but end up having to use RNN wrapped LSTMCell instances which don t use the CuDNN implementation. It should be noted that in a while loop for dynamic decoding the GPU enabled CuDNN compatible tf.keras.layers.LSTM implementation does not function nor does this specific setup work even without the while loop on multiple GPUs via a distributed strategy. To be honest I am not quite sure... What I did was using a tf.keras.callbacks.TensorBoard callback to trace the fitting of my model in Eager mode and I found out that on TensorBoard the LSTM unit is represented in a different color than the other bits when I set the visualization parameter to device used with the other bits color being labeled GPU 0 . I also verified that when I use custom layers of mine that make use of masking they are clearly drawn to have been placed on the GPU. If you have any advice as to how to properly keep track of where operations are being performed maybe also when Eager execution is disabled I would be glad to use them Hi rmlarsen I just wanted to let you know that errors resembling this decrease in speed were reintroduced by later nightly builds. This isn t a request for a fix I downgraded to the July 24 nightly and everything works fine now but I thought you might like to know just in case it s a simple thing. With the same code multi gpu setting on TF v2 with LSTM ... On the July 24 build... model trains quickly on all GPUs and is correct and gives spurious error messages 2019 08 13 11 31 15.551518 E tensorflow core grappler optimizers meta optimizer.cc 502 implementation selector failed Invalid argument Invalid format of input node name Expected forward node name index 2019 08 13 11 31 32.258063 W tensorflow core grappler optimizers implementation selector.cc 310 Skipping optimization due to error while loading function libraries Invalid argument Functions inference cudnn lstm with fallback 186209 and inference standard lstm 185862 specialized for model lstm 2 StatefulPartitionedCall at inference step 311955 both implement lstm 03256996 2770 4288 91ed 338407bd3cc3 but their signatures do not match. On the August 12 build... model trains on all GPUs and is correct but takes 50 times more time. Not an exaggeration. 2019 08 13 09 39 40.107723 E tensorflow core grappler optimizers meta optimizer.cc 502 function optimizer failed Invalid argument Input 1 of node se q3 seq encoder while body 1 TensorListPushBack 42 was passed float from se q3 seq encoder while body 1 decoder c lstm 3 StatefulPartitionedCall 9 incompatible with expected variant. 2019 08 13 09 39 53.596733 E tensorflow core grappler optimizers meta optimizer.cc 502 function optimizer failed Invalid argument Input 1 of node se q3 seq encoder while body 1 TensorListPushBack 42 was passed float from se q3 seq encoder while body 1 decoder c lstm 3 StatefulPartitionedCall 9 incompatible with expected variant. 2019 08 13 09 39 58.604309 W tensorflow core common runtime process function library runtime.cc 686 Ignoring multi device function optimization failure Invalid argument Input 1 of node se q3 seq encoder while body 1 TensorListPushBack 69 was passed float from se q3 seq encoder while body 1 decoder c lstm 2 StatefulPartitionedCall 9 incompatible with expected variant. 
29535,model.trainable False does nothing in tensorflow keras, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary source TensorFlow version use command below 1.13 Python version 2.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 10.1.105 GPU model and memory m60 8gb It seems setting model.trainable False in tensorflow keras does nothing except for to print a wrong model.summary . Here is the code to reproduce the issue ,tf.keras.models.Model documentation does not mention this attribute. tf.keras.models.Model inherits attribute from Network. Network inherits the attribute from Layer Attribute seems to be defined here https github.com tensorflow tensorflow blob 9699b90e16b3538168dde330e1ac8745fe8b73b8 tensorflow python keras engine network.py L194 L195 and here https github.com tensorflow tensorflow blob c572ea2806176d036e4d81b5de22f7251449d371 tensorflow python keras engine base layer.py L164 L166 Setting the trainable attribute to False affects the models trainable weights attribute. https github.com tensorflow tensorflow blob c572ea2806176d036e4d81b5de22f7251449d371 tensorflow python keras engine base layer.py L766 L772 This Property seems to be the cause When trainable is set to false the trainable weights property resolve to empty list. This causes the layer summary to display the parameter count to display incorrectly. https github.com tensorflow tensorflow blob 9699b90e16b3538168dde330e1ac8745fe8b73b8 tensorflow python keras utils layer utils.py L234 L239 setting model.trainable False is the suggested way in even the Tensorflow documentation for transfer learning https www.tensorflow.org tutorials images transfer learning When trainable is set to false the trainable weights property resolve to empty list. This causes the layer summary to display the parameter count to display incorrectly. It is unclear to me how trainable weights being set to an empty list makes layer summary to display the parameter count incorrectly. Can you explain a bit more please https github.com tensorflow tensorflow blob 9699b90e16b3538168dde330e1ac8745fe8b73b8 tensorflow python keras utils layer utils.py L234 L239 I think the problem is actually that the individual layers in the model stay trainable when the model is set to be non trainable. i mentioned that in the code bit i posted as minimal reproducible example. All layers remain trainable both before and after compiling the model Tried on Colab with TF 1.13.1 and was able to get mentioned model.summary with trainable params as 0. I can confirm that all three model.trainable False layer.trainable False for layer in model.layers and model.trainable True result in different behavior https github.com tensorflow tensorflow blob c572ea2806176d036e4d81b5de22f7251449d371 tensorflow python keras engine base layer.py L750 L754 This observer pattern does not seem to be functioning as expected. Any update on this This is fixed in 1.14 can you update the version and try it again Nitinsiwach Please check a gist with 1.14 here https colab.sandbox.google.com gist jvishnuvardhan 936cbc0184d3ae96bdc682f34a32d9a2 tf29535 model tranables.ipynb . Thanks Automatically closing this out since I understand it to be resolved but please let me know if I m mistaken.Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29535 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29535 No a Encountered this error in tensorflow 1.14 from a colab gpu runtime with the following code import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense Activation BatchNormalization Reshape Conv2D LeakyReLU Input from tensorflow.keras.optimizers import Adam print tf. version network Sequential Conv2D 64 4 strides 2 2 padding same input shape 28 28 1 LeakyReLU Conv2D 128 4 strides 2 2 padding same BatchNormalization axis 1 LeakyReLU Reshape 7 7 128 Dense 1024 BatchNormalization axis 1 LeakyReLU Dense 1 Activation sigmoid network.compile loss binary crossentropy optimizer Adam 2e 4 beta 1 0.5 network.summary network.trainable False network.summary aladoro Please create a new issue by providing your platform details and the standalone code. Thanks This first summary shows there is no trainable weights. Wheras second summary shows all weights are trainable. If doing it this way both summaries show no trainable weights. don tpanic Please create a new issue with more details on the issue and also provide platform details version used etc. Thanks This first summary shows there is no trainable weights. Wheras second summary shows all weights are trainable. If doing it this way both summaries show no trainable weights. yes this is because confusingly the behavior of model.trainable is different in keras vs tf.keras in keras it only impacts the variable of the model layer without impacting the variable of all the sub layers contrary to what happens in tf.keras. Is it expected to have such different behavior between keras and tf.keras In the second summary you effectively creates a new model which set model.trainable True by default which explains the different behavior. I find setting the trainable flag of the whole model very confusing so I now only set it per sub layers. For example in tf.keras if you do model.trainable False and then model.layers 4 .trainable True the latter instruction has no effect as it does not change the trainable flag at the top of the model. babaish Thank you good point useful to know I also think the role of model.trainable could be clarified and the overriding of trainability when the model is created. See this gist https gist.github.com microprediction 7aa395cec8a0ea2586936084a635cfba showing that layer trainability is ignored. The behavior is a little counter intuitive. 
29542,Loss of shape information when using dilation rate 1 in Conv layers, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N a TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 beta0 Python version 3.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 10.0 7.5 GPU model and memory 11gb GTX1080Ti You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior I was using the code below with 1.12 1.13.1 and tf2.0 alpha. But it fails to run in the latest tf2.0 beta. As you can see there is nothing fancy going on in the code. This is the block of code that produces this error Strangely shapes of y pool y 1 are correctly inferred complete code to reproduce the issue available below Describe the expected behavior The code should work as it did in the earlier release ie tf2.0 alpha Code to reproduce the issue Other info logs , gadagashwini jvishnuvardhan dynamicwebpaige Interesting thing I found out was this happens to all Conv2D layers with dilation rate 1 Explicitly setting shapes to the tensors seems to be a dirty workaround as of now Output Did you use tf.function decorator here is colab notebook showing the problem https colab.research.google.com drive 1UtkZRNyBBRJqDgDo3VMMP2otxSJXcv88 llan ml no I did not use it fchollet Problem in compute output shape or should the set shape be inside call Duplicate of issue 28400 The regression happened somewhere between tf nightly gpu 2.0 preview 2.0.0.dev20190410 and tf nightly gpu 2.0 preview 2.0.0.dev20190504. It also affects 1.14 RC0 when Eager mode is enabled. Dilations worked fine in 2.0 Alpha 29843 has some debugging information. I will close the other two as duplicates. Perhaps related to https github.com tensorflow tensorflow issues 26797 where the shape in dilated conv is not just lost but wrong. are there some news updates k w w is working on a fix. Thanks for finding this bug The fix has been submitted waiting for the changes to be copied in . For now to get around the shape issue run the code below after the Conv layer has just been created before calling the layer Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29542 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29542 No a how where can i see your merge request to know when it gets merged and a nightly is usable Here s the commit https github.com tensorflow tensorflow commit 0b4c19a3c1d1d3e7c016760aeea099fecb5fa544 thanks 
29545,Subclassing tf.keras.models.Model save model saves h5py but not h5 hdf5 file types, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Google Colab TensorFlow installed from source or binary Pip TensorFlow version use command below 2.0.0b Python version 3.6.7 GPU model and memory Tesla T4 Describe the current behavior Using tf 2.0.0b gpu on google colab. While using the subclassing API for a subclassed layer and model I was unable to use the model.save model function for h5 or hdf5 file types but I could successfully save and load the model if it was saved as h5py file type. In the toy example being used it worked correctly although this may not be the case. Note that the get config method was implemented in the custom layer and custom model. Describe the expected behavior Either the save model should always work I believe this is a feature goal and the documentation should reflect this or if the save is likely to produce incorrect results it should raise an error and the documentation should continue to suggest that custom models can only be saved with the save weights feature. Code to reproduce the issue Other info logs This will raise an error that only sequential or functional models can be saved , Ryandry1st I tried reproducing the issue but looks code snippet is incomplete. Please provide minimal code snippet to reproduce issue reported here. Also provide the error message that you have got. Thanks The only code not provided was the actual data for the train and test data objects. It is possible to reproduce with the MNIST fashion MNIST or really any other dataset simply change the 10 from the line model res mod 10 activation softmax to whatever you are using it for. Assuming you have loaded train and test data then the code executes up to the save model function the error is produced by attempting to save the model as either h5 or hdf5 while h5py works fine and produces the following error NotImplementedError Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models because such models are defined via the body of a Python method which isn t safely serializable. Consider saving to the Tensorflow SavedModel format by setting save format tf or using save weights . The entire traceback is attached in the text file. Custom Model Save Error.txt https github.com tensorflow tensorflow files 3276997 Custom Model Save Error.txt The point of this issue is that if it cannot be safely serialized then saving as an h5py either is handling it improperly and should raise an error similar to the above OR it works as it should with h5py based on only this one test case and limited testing besides checking weights and accuracy and the documentation should include this information and probably adopt the method recommend it. Any luck on this issue One option for the data is of course just randomly generating x and y values. It does not matter what data is fed into the network only that it is a trained subclassed network that is attempted to save The code snippet you have provided looks incomplete. Can you please check with latest tf 2.0 nightly version Here is a more basic code snippet that throws the error. I do want to stress though that the only difference is that I am providing some randomly generated x and y data and removing layers from the custom model for simplicity. There is no real new code or any ideas and it was no stretch to reach the warning using the previous code and using random data as was suggested a month ago. You will see that this throws the error but the following would not. So somewhere along the way it should either be adopted the error removed or it should throw a warning using this file format as well. I also tried using the preview with the more complicated code and found that it throws a new error which appears to be due to improperly recalling custom objects. This may be some of the premise behind the warning but it did work when using the beta version and no longer works with the nightly preview. In TensorFlow 2.0 you should save the model with save format h5 extension otherwise it will default to saving to the SavedModel format. Another way to save to H5 is to set the extension to .h5 or .hdf5 or .keras . The H5 format does not support saving Subclassed models so the initial error stating that only Sequential and Functional models may be saved is intended. The documentation describes the argument on this page https www.tensorflow.org versions r2.0 api docs python tf keras Model save I have an internal change that should fix the ValueError Python inputs incompatible with input signature but I can t be sure because the examples above do not raise the error. I ll update this post when the change is copied to github. I agree with the save format my question comes down to the use of h5py which seems to allow the model to be saved and it is able to load the model correctly then too. So the question is should it not allow h5py save format with subclassed models or if it does work correctly then shouldn t this be a standard Calling model.save custom model.h5py still saves out in the SavedModel format even though the extension says .h5py so it s not actually using h5py . That is fine however that does not solve the actual issue. The issue is that there is no error or warning if this method is used to save the model even though it explicitly states that you should not be able to save a subclassed models yet it clearly allows it if you change the saved model extension to be .h5py . So regardless of what is occurring in the background there is a problem that subclassed models are being saved unless this can be adopted for all instances. This problem has not been solved in rc0 and should be reopened. I would like to get a better understanding of the issue is. Perhaps docstring for the save format is unclear Currently it says The NotImplementedError error from an earlier post above states that HDF5 cannot save out subclassed models. The SavedModel format is able to handle these models so you should not get an error. I see so the SavedModel format is able to handle these models. I believe this should be documented then in the warning as the warning seems to suggest that there is not a clear method for saving these models outside of saving the weights independently from the model which is obviously more complicated and prone to error Specifying that the SavedModel format is able to handle custom models sounds reasonable and would remove the ambiguity. I ll add that to the warning message. Thanks for the suggestion Thank you for clearing that up appreciate it Now what is the best way to save model created in subclassed Were there some examples for how to save and load models thank you I understand that so long as you use the save model format which is done by calling model.save NameOfYourModel save format tf This should be the most clear method of successfully saving a subclassed model while also being clear of the format being used. If you specify the file format e.g. .h5 .keras then it will not work for a subclassed model. If you specify anything else as I was experiencing with .h5py it will default to using save format tf Closing this issue since I understand it to be resolved by updating the warning but please let me know if I m mistaken. The current warning is clearly mentions what to do to save subclass model. Please check the gist here https colab.research.google.com gist jvishnuvardhan 7fed70726f8e6def50d408e13a3416d7 untitled783.ipynb . Current Warning message is as follows NotImplementedError Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models because such models are defined via the body of a Python method which isn t safely serializable. Consider saving to the Tensorflow SavedModel format by setting save format tf or using save weights. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 29545 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 29545 No a Hi Ryandry1st may I have help on this similar issue please using tf 2.2.0 https github.com tensorflow tensorflow issues 41543 I have tried and loaded back with still i get and I tried with tf 2.0.0 version getting same error I need of help please Thanks Found it Thanks Actually recreating the model with didn t work for me First we have to save weights from the built model Then we have to initiate a new instance for the subclass Model then compile and train on batch with one record and load weights of built model This work perfectly in TensorFlow 2.2.0 Glad you got that figured out hanzigs the way you are doing it is one that I use as well when I have custom classes layers which require the subclassing method. It seems like with the definition of base config it should work to save the entire model as a SavedModel format but this has not been my experience. As you said I just save the weights and then instantiate a new model and load the weights. Ryandry1st Similar issue I m trying to save a subclass model and convert it to TensorFlowJS layers model format. This only works if I m able to get the model to be saved as h5 but I get the error about not being able to save a non sequential or functional model. The insistence for me to have the layers model format is so I can retrain using TFJS. Any suggestions I believe you cannot save the model in its entirety as an h5 format because this ignores the information of the control flow ops that are in the subclasses model. Instead you need to save the weights in h5 format then make a new model the same way train on batch as shown above and then load the weights. I have not had to deploy a subclasses model in TF JS so I could not say if this is usable or not. Hi Adding to hanzigs When I train the model and the save the weights using model.save weights path And load it in another session using model1 DCN model1.compile optimizer tf.keras.optimizers.Adam learning rate model1.load weights path The prediction got in the first session from model and the prediction got from the second session model1 is completely different. How do I solve this NOTE If I save the model using model.save os.path.join model path my model save format tf Then I get the below warning. WARNING absl Found untraced functions such as ranking layer call fn ranking layer call and return conditional losses dense layer call fn dense layer call and return conditional losses ranking layer call fn while saving showing 5 of 10 . These functions will not be directly callable after loading. This warning cannot be ignored because when I load the model and try to evaluate it with the same test set I get the following error ValueError Exception encountered when calling layer dcn type DCN . Could not find matching concrete function to call loaded from the SavedModel. What should I do if I get this error when saving the model using ModelCheckpoint with save weights only False The error NotImplementedError Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models because such models are defined via the body of a Python method which isn t safely serializable. Consider saving to the Tensorflow SavedModel format by setting save format tf or using save weights . Thanks 
29558,model.fit does not reshuffle the dataset between epochs, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOSX 10.13.6 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below VERSION 2.0.0 dev20190606 GIT VERSION v1.12.1 3447 g5a0f1bbfb7 Python version 3.6.8 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior When calling model.fit dataset epochs 2 with a finite shuffled dataset the model is trained on the same dataset order at each epoch. Describe the expected behavior I expect the dataset to be reshuffled after each epoch. Right now it s not even when I use reshuffle each iteration True in the dataset s shuffle method. This argument seems to only shuffle between iterations within one epoch. Code to reproduce the issue Other info logs The output is as follows I ve added comments As you can see the order of the data is perfectly identical during the 1st and 2nd epochs. It is only reshuffled at each iteration within the same epoch. So the only way to ensure that the data will be reshuffled at each epoch is to use dataset.repeat n epochs then model.fit dataset steps per epoch ... epochs n epochs . It feels like unnecessary complexity., jsimsa can you comment on the best way to get dataset re shuffling here It seems like the reshuffling arg should apply when we move to the next epoch This is a known issue that we are working on fixing by RC0. The gist is that reshuffle each iteration was introduced when iterations were achieved by repeat and that s what the flag controls . I am not sure what Keras does internally to perform epochs but in 2.0 it should create a new iterator for each epoch and reshuffling the order between different iterators will be controlled by a tf.data.Option that will be available by RC0. Is this issue still open If so I would like to help otherwise it should be closed joshz123 I believe that this issue is fixed in TF 2. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 29558 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 29558 No a 
29593,tf.estimator.train and evaluate fails to print anything loss accuracy when used with tf.keras.estimator.model to estimator tf2.0 beta , em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow NO OS Platform and Distribution e.g. Linux Ubuntu 16.04 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 beta0 Python version 3.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 10.0 7.5 GPU model and memory 11gb GTX 1080ti Describe the current behavior When training an estimator which is got from a tf.keras.Model instance the tf.estimator.train and evaluate method fails to log anything on the stdout. However if set the above flag i can loss being logged similar to that of API v1 Describe the expected behavior log the metrics and loss as described in the official docs https www.tensorflow.org beta guide migration guide Code to reproduce the issue Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. , gadagashwini jvishnuvardhan any updates srihari humbarwadi Can you check with TF2.0 and let us know whether it was resolved your issue I ran it in TF2.0 and I see the following output partial that includes loss at different steps. Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan ceabd9bc9d5f4447a94d8c26344669a1 tf29593.ipynb and let us know what you think. If your issue was resolved by TF2.0 then please close the issue. thanks Looks like its working now thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29593 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29593 No a 
29608,Cannot import tensorflow.summary since 2019 06 08 nightly, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 gLinux Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below v1.12.1 3679 g3040de1372 2.0.0 dev20190608 Python version 2.7 or 3.6 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior Importing tensorflow.summary raises ModuleNotFoundError Describe the expected behavior Importing that module should succeed Code to reproduce the issue Other info logs cc mihaimaruseac is this related to recent Pip changes ,I think it is sorry about that. Will get to look into this in around 1 hour Cool nothing pressing we ll just stay pinned to 2019 06 06 for now. mihaimaruseac the tf.summary import mechanism is pretty arcane let me know if I can be helpful with any debugging fixing here. It seems I cannot reproduce with a simple virtualenv maybe I m missing something Got it was able to reproduce the exact example you gave mihaimaruseac Right. Those discrepancies are exactly why we test so many import permutations in our smoke test https github.com tensorflow tensorboard blob 7a4ef58a4e8e6a9db24b7b72ac2102e832dd590b tensorboard pip package build pip package.sh L181 L193 Currently import attr and import from work but import as fails. I have a fix testing against tf1. and tf.2 and on both pythons to make sure it works properly. Fix is in 2f0e7e39109ff7f40021425494f3c9d84a29b59c sorry it took so long. Fixed in tf nightly 2.0 preview 2.0.0.dev20190613 . Thank you Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29608 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29608 No a 
29624,on epoch end not triggered when workers 0 in fit generator, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac OS X 10.14 TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 beta0 issue also happens on TF 1.12 and 1.13 Python version Python 3.6.7 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior When running fit generator on the main thread i.e. with use multiprocessing False and workers 0 on epoch end doesn t get triggered Describe the expected behavior Having looked at the source code I would expect on epoch end to be triggered at the end of each epoch Code to reproduce the issue I create a simple generator by subclassing from tf.keras.utils.Sequence I then create a simple model I set up my training and validation generators and run fit generator I get the following output If I change workers from 0 to 1 then on epoch end is triggered i.e. on epoch end print statements appear., marwan116 I was able to reproduce the issue with TF v2.1. However it seems to be fixed with the latest nightly TF v2.2.0 dev20200327. Please find the gist of it here https colab.research.google.com gist amahendrakar 2fceed384fd9cfb8966ba9b507b8fcde tf nightly.ipynb . Thanks amahendrakar thanks for taking the time to check this. Ok cool so this looks like it is working now what about TF1 1.15 .. marwan116 Seems like the issue still persists with TF 1.15.2. Please find the gist here https colab.research.google.com gist amahendrakar a338353b2125128441160f36acd0cd49 29624 1 15.ipynb . Thanks Hi marwan116. We usually don t patch old releases when we fix bugs. We only do patch releases for security issues. When do those we investigate other PRs opened against the corresponding release branch and cherry pick additional fixes but this is limited. Our recommendation is to always use the latest version. If someone identifies the fix commit for this and makes a cherrypick against the r1.15 branch we would be able to apply it when we do the release. Otherwise unfortunately we don t have enough cycles to cherry pick all fix commits on all live branches I am closing this issue as it was resolved in recent tf nightly . This will be available in stable TF2.2 which will be released in near future. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 29624 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 29624 No a 
29628,TF 2.0 tf.image.non max suppression always output the length of max output size,tensorflow version tensorflow 2.0.0 beta0 and the output is I also tried tf.image.non max suppression padded function with parameter pad to max output size False and pad to max output size True the output shape is the same,I have tried on Colab with TF CPU version 2.0.0 beta and was able to reproduce the issue. JoelTsui Can you please try with nightly version pip install tf nightly and let us know the whether the issue persists.I have tried and please find the gist here https colab.sandbox.google.com gist ravikyram 26b4d9836f53dc6a3032888d77804266 untitled616.ipynb . Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 29628 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 29628 No a 
29656,Bug on gather nd with gradient., em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary pip TensorFlow version use command below tf2 gpu beta Python version 3.6.8 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior A simple test code gives following error message Describe the expected behavior the grads should be 0 0 0 1 but error occurs. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,I just found very weird behavior. If I replace v to v 0 like below it gives expected result... It seems the direct access to variables with gather nd ruines something unexpected... Sangwon91 I could able to reproduce the reported issue with Tensorflow 2.0.0.beta0. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29656 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29656 No a 
29715,Session crashed when I use TFLiteConverter with tf.lite.OpsSet.TFLITE BUILTINS INT8 option., System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes. OS Platform and Distribution e.g. Linux Ubuntu 16.04 Colaboratory Ubuntu 18.04.2 LTS Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device NA TensorFlow installed from source or binary pre installed TensorFlow version use command below 2.0.0 beta0 Python version Python 3.6.7 Bazel version if compiling from source NA GCC Compiler version if compiling from source NA CUDA cuDNN version CUDA v10.0.130 cuDNN 7.6.0 GPU model and memory K80 Describe the current behavior I tried to build fully quantized AutoEncoder model using TF2.0 beta and keras on Colaboratory https colab.research.google.com gist ohtaman 09c66e27f240d151f6f17ac4a9f62e54 post training integer quantization error.ipynb But when I run TFLiteConverter.convert method the Jupyter kernel crashes and the session is restarted. Describe the expected behavior Finish conversion without any errors. Code to reproduce the issue Please check Colaboratory https colab.research.google.com gist ohtaman 09c66e27f240d151f6f17ac4a9f62e54 post training integer quantization error.ipynb Other info logs ,Tried executing the attached Colab code snippet with TF GPU version 2.0.0 beta and was able to replicate the issue. Hmm when I run the colab just now and it works fine for me am i reproducing the error incorrectly githubcolab https user images.githubusercontent.com 1450614 59544225 01b74400 8ec5 11e9 9e8c 2b5452b9e5d0.png Can you verify that this is still an issue Yes session crashes in my environment. The lower left Japanese message means it. I also checked that same error appears with tensorflow gpu 2.0 beta1 . image https user images.githubusercontent.com 329750 59547868 b2364e00 8f80 11e9 8e7f b3386613e1db.png I found that the cause of this error is the upsampling implemented by myself. If I use keras.layers.UpSampling2D 2 interpolation bilinear it works well. In the colab notebook I implemented upsampling by myself because UpSampling2D with interpolation nearest is not supported by the quantization tool yet. ohtaman I am closing the issue as it was resolved in tf nightly . Please feel free to open it if the issue persists again. Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 701764218387356183515f772bbef27f post training integer quantization error.ipynb . Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 29715 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 29715 No a 
29727,EagerFunc tensor conversion error dtype mismatch, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below 1.13.1 Python version 3.7 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version 10.0 7.4.2 GPU model and memory 1080Ti Describe the current behavior GIST Link 1 https gist.github.com kami93 27da32514c4c9b085bf4fd20dcf6ef57 GIST Link 2 https gist.github.com kami93 49355db2491ff7b4a03a22fbfbce09f7 I am trying to use tfe.pyfunc to combine eager block in the operation graph See Link 1 . There are four input arguments to the eager mode function of dtype tf.float32 and tf.int32. When I try to calculate the gradient of a tensor w.r.t some variables that the calculations involving them are happen in the eager block the error in the Link 2 is raised TypeError Cannot convert provided value to EagerTensor. Provided value 0.0 Requested dtype int32 . Describe the expected behavior The gradient is calculated and returned after calling tf.Session.run Code to reproduce the issue See Link 1. Other info logs See Link 2. I posted a Pull Request for the fix of this issue. https github.com tensorflow tensorflow pull 29728 https github.com tensorflow tensorflow pull 29728 , kami93 Let us know is this still an issue. Thanks Automatically closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29727 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29727 No a 
29730,MirroredStrategy does not work with CudnnLSTM, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 LTS Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary source TensorFlow version use command below 1.13.1 v1.13.1 2 ga5c387b5ed Python version 3.6.7 Bazel version if compiling from source 0.21.0 GCC Compiler version if compiling from source 7.4.0 CUDA cuDNN version 10.1 GPU model and memory 1080 Ti 11GB Describe the current behavior Unable to train a model using MirroredStrategy with CudnnLSTM. Fails with the following error Describe the expected behavior Code should run. Code to reproduce the issue ,Have tried on Colab with TF 1.13.1 CUDA 10.0 and was able to reproduce the issue. I tested with 1.14 in a colab and it seems to work. Please try with 1.14 and re open if still an issue. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29730 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29730 No a 
29751,KeyError ParallelInterleaveDataset in Tf 1.13 mkl gpu ,OS Platform and Distribution Linux Ubuntu 16.04 TensorFlow installed from Anaconda dist 3.6. TensorFlow version 1.13 mkl 1.13 gpu Python version Python 3.6.6 Anaconda Inc. Describe the current behavior There is an import error on tf.train.import meta graph while importing one of the models from the model zoo coral ready model. The import works fine on 1.12 but not 1.13 ver Describe the expected behavior The model should be otherwise loaded with the placeholder and graph Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. The model was taken from here Model Link http download.tensorflow.org models object detection ssd mobilenet v2 quantized 300x300 coco 2019 01 03.tar.gz Model Website https coral.withgoogle.com models Other info logs UPDATE 1 The following code snippet is used for importing the model downloaded from the model zoo ,Will it be possible for a small code snippet that can reproduce the issue. It will really help us to proceed faster. Thanks achandraa Have updated the issue with a snippet. The simple importer is what I have used. I had a similar issue earlier and it is still open 27752 https github.com tensorflow tensorflow issues 27752 issue 432019580 Jiri did we remove this op If so I m guessing it was just experimental The op was renamed to ExperimentalParallelInterleaveDataset . Do we have a solution for this I m trying to reload the .meta file for a model that I downloaded from this colab notebook https colab.research.google.com github tensorflow tensor2tensor blob master tensor2tensor notebooks asr transformer.ipynb scrollTo Qz5u2O5LvShm. The following code throws this error The solution for this is to update the serialized checkpoint replacing occurrences of ParallelInterleaveDataset with ExperimentalParallelInterleaveDataset . jsimsa Thank you for your reply What about ParallelInterleaveDatasetV2 instances Leaving them unchanged causes the same error. Changing them to ExperimentalParallelInterleaveDatasetV2 cases DecodeError Error parsing message All experimental tf.data ops renamed as part of https github.com tensorflow tensorflow commit bf70c447d68070e81c1dbc2ebf0b6b0d8ae2c597 need to be updated accordingly. Notably ParallelInterleaveDatasetV2 should not be renamed. I suspect that when you replaced ParallelInterleaveDataset to ExperimentalParallelInterleaveDataset you also accidentally renamed ParallelInterleaveDatasetV2 to ExperimentalParallelInterleaveDatasetV2 . Any updates on this keyerror There is also another KeyError called FixedLengthDatasetV2 . Do you mean FixedLengthRecordDatasetV2 That operation has not been renamed so I suspect that is a different issue. Please create a separate issue with details on how to reproduce the error. Hello jsimsa. Thanks a lot for your help. What s the best way to update the serialized checkpoint I tried replacing occurrences as you suggested using vim but now I have a corrupted checkpoint file. Also do I have to update ParallelInterleaveDataset to ExperimentalParallelInterleaveDataset in Tensorflow files such as dataset ops.py gen dataset ops.py interleave ops.py and readers.py . I think the best way would be to write a simple program that reads the checkpoint file parses the proto the file contains I believe this would be metagraph https github.com tensorflow tensorflow blob master tensorflow core protobuf meta graph.proto then walks the data structure to update all occurrences of the renamed transformations and saves the updated proto to a new file. If you do not mind sharing your checkpoint file I can write the utility for you and share it via my github. jsimsa That would be great. I really couldn t find a way to manipulate metagraph to remove those nodes and replace with a simple placeholder. TF suggests to use graph transform tools for which you need to bazel build it. How can I share the meta ckpt files with you How big is it Could you use Google Drive or Dropbox and share a link to it How big is it Could you use Google Drive or Dropbox and share a link to it The link is in the issue description. Is using tf nightly an option for you Instead of providing a tool for fixing the checkpoint I am leaning towards fixing TensorFlow to be able to read the checkpoint. However this change would only be available in the nightly build. jsimsa I think rather than fixing in nightly build its better to update the model zoo models stripped off these experimental nodes. yes nightly build does help but hope it gels well with other tf operations. It would be really great if you could provide a snippet on how you strip off such nodes. I never really go to use the graph transform tools that level since we need to bazel build it jsimsa Any leads on the error prateethvnayak it is not possible to fix the model zoo model in a way that would work with all versions of Tensorflow. Note that the tf nightly build of TensorFlow should now correctly handle the model thanks to https github.com tensorflow tensorflow commit de9c460b06b437a44ebb3d795d9ff3cc20a8c4f1. Could you please verify that the error is indeed no longer present Thank you. I m not familiar with this code so unassigning myself. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29751 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29751 No a In tensorflow 1.12 also has the problem. How could I give a workaround. Thanks Is the proposed find replace solution above still recommended I m running into this problem when attempting to write the imported meta graph from this object detection api model http download.tensorflow.org models object detection ssd mobilenet v2 quantized 300x300 coco 2019 01 03.tar.gz My tf version is 1.14.0 NOTE I m writing this file to view the output ops in tensorboard so that I can then use tf 1.14.0 s freeze graph function to then save a frozen graph of the model and perform inference with it. Yes the best know solution is to update the serialized checkpoint replacing occurrences of ParallelInterleaveDataset with ExperimentalParallelInterleaveDataset. jsimsa Thanks for the information. Do you know of a resource explaining how to do this I noticed earlier in this thread that some were having trouble doing the replacement. jdcast jsimsa I had the same issue but somehow fixed it by the following code on TF 1.14.0. I hope this would be helpful to you. In tensorflow 1.12 also has the problem. How could I give a workaround. Thanks Have you solve the problem I have the same problem with you. jdcast jsimsa I had the same issue but somehow fixed it by the following code on TF 1.14.0. I hope this would be helpful to you. I know this is an old issue but would anyone know what the correct conversion would be for ExperimentalFunctionBufferingResource I can t find it in the file linked. I m trying to import a checkpoint trained in V1.12 into V2.8. 
29824,tf.reduce max returns wrong value, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04.2 LTS TensorFlow installed from source or binary source TensorFlow version use command below 1.12.2 Python version 3.6.7 Bazel version if compiling from source 0.17.2 GCC Compiler version if compiling from source gcc version 7.3.0 Ubuntu 7.3.0 27ubuntu1 18.04 CUDA cuDNN version no gpu GPU model and memory no gpu The current behavior I have implemented a computation graph consisting of a custom keras layer GaussianSimilaritiesLayer and a tf.reduce max function. You can find the code below. When GaussianSimilaritiesLayer.call return statement looks like below the script outputs 0.13499996 This is the value of exp arg from GaussianSimilaritiesLayer.call . The function should return e 0.13499996. The expected behavior When GaussianSimilaritiesLayer.call return statement looks like below the script outputs 0.87371594 which is the desired value.,Seems to be a bug in tf.reduce max if i replace tf.reduce max with for example tf.nn.relu output is correct. I will look into the tf.reduce max implementation. The behavior is really weird I guess that multiplying by 1 changes the dtype of the tensor somehow. I get the desired result also when returning I could reproduce the issue with Tf 1.12.2 on Google Colab.Thanks This works as expected in 1.14 pwasows Can you upgrade to TF 1.14 as it is working as expected Thanks pwasows Can you upgrade to TF 1.14 as it is working as expected Thanks I cannot upgrade to 1.14 but can live with the 1 tf.math.exp exp arg workaround Automatically closing this out since I understand it to be resolved but please let me know if I m mistaken.Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29824 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29824 No a 
29856,tf.keras.layers.UpSampling2D interpolation bilinear has a smearing defect on the right bottom edges, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow I ve provided a link to a Colab notebook demonstrating the issue below comparing keras upsampling to what it should look like with a correct implementation as seen in tf.image.resize. OS Platform and Distribution e.g. Linux Ubuntu 16.04 Google Colab Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below v2.0.0 beta0 16 g1d91213fe7 Python version 3 Bazel version if compiling from source n a GCC Compiler version if compiling from source n a CUDA cuDNN version n a GPU model and memory n a You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior Upsampling using tf.keras.layers.UpSampling2D results in unnatural smearing of the right and bottom edges of the image. This problem is amplified when the upsampling is repeated. Describe the expected behavior Keras layers should use sensible default behaviour and not have this smearing issue. This causes serious problems for autoencoders GANs and cost months of time. Correct behaviour is seen with tf.image.resize o size size method tf.image.ResizeMethod.BILINEAR . Keras upsampling should use this as the default instead of the current defective behaviour. Note In TensorFlow 1.x the tf.image.resize method had an align corners parameter that toggled between defective and proper behaviour and was set to False defective behaviour by default. In TensorFlow 2 this parameter has been removed and the correct behaviour align corners True behaviour is now the default. The keras layer should follow the same path. Code to reproduce the issue Here is a Colab notebook that demonstrates the issue https colab.research.google.com drive 1rgCzJcMo4DN 9 hutr9l2vSrTRPfcd6K Other info logs ,Actually I think it s even worse than that it seems bilinear doesn t work https colab.research.google.com drive 1BG1gRC86Hj9CqyLTD9quyW0vtNJWZK3j Have tried with code snippet provided and was able to reproduce the issue on Colab with TensorFlow version 2.0beta. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29856 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29856 No a 
29862, TF 2.0 Converting keras model to estimator ignores data types, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 ProductName Mac OS X ProductVersion 10.14.2 BuildVersion 18C54 TensorFlow installed from source or binary pip install ... TensorFlow version use command below v1.12.1 3259 gf59745a381 2.0.0 beta0 Python version 3.6.8 Describe the current behavior When converting Keras model to estimator it converts all integer inputs to floats or doubles. This will result in data type errors in the converted estimator. I am not entirely sure if this is a bug or feature because this function is indeed called in the conversion https github.com tensorflow estimator blob c956dd32561bac645a1cd870d3c8cfe8e9fe969b tensorflow estimator python estimator keras.py L62 However this is blocking using integer inputs in the converted estimator. Describe the expected behavior Input data types are preserved when converting keras model to estimator. Code to reproduce the issue Small example to reproduce the issue Because input is converted to float last line results in ,Have tried on Colab with TF 2.0beta and was able to get mentioned error. Triage notes we have a fix for this going in right now. tanzhenyu will update when it s in the nightlies. This works in tf nightly 2.0 preview since the fix was submitted yesterday. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29862 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29862 No a 
29863,Error when using unique with counts function. , em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em On Windows Subsystem for Linux. Python 3.6.7. Tensorflow 1.13.1. Code Error System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary TensorFlow version use command below Python version Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior Describe the expected behavior Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. , zendevil Looks like above code snippet is incomplete to reproduce the issue. Please provide the full code snippet to reproduce. Thanks There are some variables that are defined in a file called google res which I import. You probably can recreate error without that file. Good luck debugging this. zendevil I am unable to reproduce the issue with above code snippet. Looks some entities are not defined like search terms main. Can you help us to reproduce the issue. Thanks I think the issue is that bool support is not available for unique with counts. Added a PR 29986 for the fix. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29863 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29863 No a 
29867,tf.range with tf.constant int32 limit and dtype tf.float32 fails,Tensorflow Version tf nightly gpu 2.0 preview 2.0.0.dev20190611 or CPU equivalent linux Also tested on tf nightly 2.0 preview 2.0.0.dev20190607 windows Try running the following two The first one fails with ,Added a PR 29987 for the fix. In general this is an error that is easy to fix in user code so I d rather fix it there. It s ambiguous what to do with a floating point range. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29867 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29867 No a It works for numpy 
29871,TF 2.0 Upgrade Script Unable to handle the operator for matrix multiplication, System information OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 TensorFlow installed from source or binary pip TensorFlow version use command below 2.0.0 beta1 Python version 3.7.3 Describe the current behavior Running the tf upgrade v2 script with a file containing the operator results in an exception see below . Code to reproduce the issue Using the file tmp.py with the following content import numpy as np def mul a b z a b Then run . tf upgrade v2.exe infile C any path tmp.py outfile C another path tmp.py Other info logs Traceback most recent call last File c users pzobel pycharmprojects tf2 0 beta test venv lib site packages pasta base annotate.py line 1194 in visit super AstAnnotator self .visit node File c users pzobel pycharmprojects tf2 0 beta test venv lib site packages pasta base annotate.py line 132 in visit super BaseVisitor self .visit node File C Program Files Python37 lib ast.py line 262 in visit return visitor node File c users pzobel pycharmprojects tf2 0 beta test venv lib site packages pasta base annotate.py line 47 in wrapped f self node args kwargs File c users pzobel pycharmprojects tf2 0 beta test venv lib site packages pasta base annotate.py line 690 in visit BinOp op symbol ast constants.NODE TYPE TO TOKENS type node.op 0 KeyError class ast.MatMult During handling of the above exception another exception occurred Traceback most recent call last File C Program Files Python37 lib runpy.py line 193 in run module as main main mod spec File C Program Files Python37 lib runpy.py line 85 in run code exec code run globals File C Users pzobel PycharmProjects TF2 0 Beta Test venv Scripts tf upgrade v2.exe main .py line 9 in module File c users pzobel pycharmprojects tf2 0 beta test venv lib site packages tensorflow tools compatibility tf upgrade v2 main.py line 139 in main args.input file output file upgrade File c users pzobel pycharmprojects tf2 0 beta test venv lib site packages tensorflow tools compatibility tf upgrade v2 main.py line 40 in process file upgrader.process file in filename out filename File c users pzobel pycharmprojects tf2 0 beta test venv lib site packages tensorflow tools compatibility ast edits.py line 900 in process file temp file File c users pzobel pycharmprojects tf2 0 beta test venv lib site packages tensorflow tools compatibility ast edits.py line 960 in process opened file self.update string pasta .join lines in filename File c users pzobel pycharmprojects tf2 0 beta test venv lib site packages tensorflow tools compatibility ast edits.py line 916 in update string pasta t pasta.parse text File c users pzobel pycharmprojects tf2 0 beta test venv lib site packages pasta init .py line 25 in parse annotator.visit t File c users pzobel pycharmprojects tf2 0 beta test venv lib site packages pasta base annotate.py line 1194 in visit super AstAnnotator self .visit node File c users pzobel pycharmprojects tf2 0 beta test venv lib site packages pasta base annotate.py line 132 in visit super BaseVisitor self .visit node File C Program Files Python37 lib ast.py line 262 in visit return visitor node File c users pzobel pycharmprojects tf2 0 beta test venv lib site packages pasta base annotate.py line 47 in wrapped f self node args kwargs File c users pzobel pycharmprojects tf2 0 beta test venv lib site packages pasta base annotate.py line 220 in visit Module self.generic visit node File C Program Files Python37 lib ast.py line 270 in generic visit self.visit item File c users pzobel pycharmprojects tf2 0 beta test venv lib site packages pasta base annotate.py line 1194 in visit super AstAnnotator self .visit node File c users pzobel pycharmprojects tf2 0 beta test venv lib site packages pasta base annotate.py line 132 in visit super BaseVisitor self .visit node File C Program Files Python37 lib ast.py line 262 in visit return visitor node File c users pzobel pycharmprojects tf2 0 beta test venv lib site packages pasta base annotate.py line 95 in wrapped f self node args kwargs File c users pzobel pycharmprojects tf2 0 beta test venv lib site packages pasta base annotate.py line 411 in visit FunctionDef self.visit stmt File c users pzobel pycharmprojects tf2 0 beta test venv lib site packages pasta base annotate.py line 1194 in visit super AstAnnotator self .visit node File c users pzobel pycharmprojects tf2 0 beta test venv lib site packages pasta base annotate.py line 132 in visit super BaseVisitor self .visit node File C Program Files Python37 lib ast.py line 262 in visit return visitor node File c users pzobel pycharmprojects tf2 0 beta test venv lib site packages pasta base annotate.py line 47 in wrapped f self node args kwargs File c users pzobel pycharmprojects tf2 0 beta test venv lib site packages pasta base annotate.py line 530 in visit Assign self.visit node.value File c users pzobel pycharmprojects tf2 0 beta test venv lib site packages pasta base annotate.py line 1196 in visit raise AnnotationError e pasta.base.annotate.AnnotationError class ast.MatMult ,I have tried to reproduce the issue on my system and was able to do it. soupytwist FYI. I ll send you a PR. Automatically closing this out since I understand it to be resolved by the PR https github.com google pasta pull 67 merged already but please let me know if I m mistaken.Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29871 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29871 No a That will resolve it but until pasta is released with the change and I add an updated dependency users might have to reinstall pasta manually. It s probably ok to close. I am running into the same issue but not sure how to resolve it. My pasta library is updated but I am still getting the same error and can t upgrade to TensorFlow 2. Hi TrentBrick it seems that the latest release of pasta is from May 29 https github.com google pasta releases and therefore martinwicke s fix is not in the latest release yet. Please install pasta from source or manually add the fix https github.com google pasta pull 67 to your code until they added a new release. Hi I ve just released pasta v0.1.8 which contains the latest fixes. Sorry for the delay I don t yet see a v0.1.8 tag or release on GitHub am I just not seeing it I don t yet see a v0.1.8 tag or release on GitHub am I just not seeing it Pushed the tag up now forgot this. Thanks 
29872, TF 2.0 categorical column with vocabulary list no longer usable with tf.functiion, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes slightly OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Fedora 30 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below v2.0.0 beta0 16 g1d91213fe7 2.0.0 beta1 Python version 3.5 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Describe the current behavior The initialization of the lookup table fails with Describe the expected behavior As with v2.0.0 alpha.0 feature columns with constant vocabulary lists should be usable in tf.function graphs. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs details summary Traceback summary pre Traceback most recent call last File bug.py line 21 in module print func features File ... venv lib python3.5 site packages tensorflow python eager def function.py line 416 in call self. initialize args kwds add initializers to initializer map File ... venv lib python3.5 site packages tensorflow python eager def function.py line 359 in initialize args kwds File ... venv lib python3.5 site packages tensorflow python eager function.py line 1360 in get concrete function internal garbage collected graph function self. maybe define function args kwargs File ... venv lib python3.5 site packages tensorflow python eager function.py line 1648 in maybe define function graph function self. create graph function args kwargs File ... venv lib python3.5 site packages tensorflow python eager function.py line 1541 in create graph function capture by value self. capture by value File ... venv lib python3.5 site packages tensorflow python framework func graph.py line 716 in func graph from py func func outputs python func func args func kwargs File ... venv lib python3.5 site packages tensorflow python eager def function.py line 309 in wrapped fn return weak wrapped fn . wrapped args kwds File ... venv lib python3.5 site packages tensorflow python framework func graph.py line 706 in wrapper raise e.ag error metadata.to exception type e TypeError in converted code bug.py 19 func return feature layer features ... venv lib python3.5 site packages tensorflow python keras engine base layer.py 667 call outputs call fn inputs args kwargs ... venv lib python3.5 site packages tensorflow python feature column feature column v2.py 473 call tensor column.get dense tensor transformation cache ... venv lib python3.5 site packages tensorflow python feature column feature column v2.py 3123 get dense tensor transformation cache state manager ... venv lib python3.5 site packages tensorflow python feature column feature column v2.py 3714 get sparse tensors transformation cache.get self state manager None ... venv lib python3.5 site packages tensorflow python feature column feature column v2.py 2562 get transformed column.transform feature self state manager ... venv lib python3.5 site packages tensorflow python feature column feature column v2.py 3692 transform feature return self. transform input tensor input tensor ... venv lib python3.5 site packages tensorflow python feature column feature column v2.py 3686 transform input tensor name lookup .format self.key .lookup input tensor ... venv lib python3.5 site packages tensorflow python ops lookup ops.py 1388 index table from tensor table StaticHashTableV1 init default value ... venv lib python3.5 site packages tensorflow python ops lookup ops.py 284 init super StaticHashTable self . init default value initializer ... venv lib python3.5 site packages tensorflow python ops lookup ops.py 174 init self. init op self. initialize ... venv lib python3.5 site packages tensorflow python ops lookup ops.py 177 initialize return self. initializer.initialize self ... venv lib python3.5 site packages tensorflow python ops lookup ops.py 424 initialize self. values ... venv lib python3.5 site packages tensorflow python ops gen lookup ops.py 785 lookup table import v2 table handle keys values name name ctx ctx ... venv lib python3.5 site packages tensorflow python ops gen lookup ops.py 820 lookup table import v2 eager fallback attrs attrs ctx ctx name name ... venv lib python3.5 site packages tensorflow python eager execute.py 71 quick execute raise e ... venv lib python3.5 site packages tensorflow python eager execute.py 61 quick execute num outputs TypeError An op outside of the function building code is being passed a Graph tensor. It is possible to have Graph tensors leak out of the function building context by including a tf.init scope in your function building code. For example the following function will fail tf.function def has init scope my constant tf.constant 1. with tf.init scope added my constant 2 The graph tensor has name dense features kind embedding kind lookup Const 0 pre details This bug was likely introduced by https github.com tensorflow tensorflow commit 2f0b7b1c2f5638a157af76383bd5a42bd3cc2938 diff 0972bc0b553e347b626ce302457971dfR383 Somewhat related issue https github.com tensorflow tensorflow issues 27086 ,I am able to reproduce the issue with Tensorflow 2.0.0.beta1 on Google colab. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29872 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29872 No a 
29881,The call method of DenseFeatures and SequenceFeatures use deprecated attribute num buckets, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow no OS Platform and Distribution e.g. Linux Ubuntu 16.04 macOS 10.13.6 TensorFlow installed from source or binary from pip install TensorFlow version use command below v1.12.1 3259 gf59745a381 2.0.0 beta0 Python version v3.6.7 6ec5cf24b7 Oct 20 2018 03 02 14 Describe the current behavior By simply calling the call method of DenseFeatures and SequenceFeatures defined with categorical column with identity and sequence categorical column with identity feature columns along with embedding column we get warnings that the deprecated attributes num buckets are used instead of the non deprecated num buckets I guess . Also note on the code example below that a third warning about a deprecated method add dispatch support. locals .wrapper arises. I do not understand it but there are instructions for updating given in the warning see the code below. Describe the expected behavior I think that we should not get warnings about deprecated objects when we are not calling any deprecated method attribute etc. I think that somewhere in the code of the call method of DenseFeatures and SequenceFeatures there is a use of num buckets that should be replaced by num buckets . Following the updating instructions about the third warning may be enough to get rid of it. Code to reproduce the issue produces the following three warnings ,I could see the warning message on colab with Tf 2.0.0.beta0. Thanks Similar problem here from beta1 tensorflow python ops math grad.py 1250 add dispatch support. locals .wrapper from tensorflow.python.ops.array ops is deprecated and will be removed in a future version. Instructions for updating Use tf.where in 2.0 which has the same broadcast rule as np.where I ve got exactly the same issue I hope someone fixes it Same here 1 most annoying comment ever 3 Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29881 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29881 No a The warnings are still here in tf2.0.0 rc0. Actually there is even one new warning when using tf2.0.0 rc0 I didn t see it the first time I tested. For the same code as above now the output is See the new warning Is it possible to re open this issue Or should I open a new one Edit it says I unassigned bananabowl but I don t know how to do that I just posted this new comment and then edited it. Edit 2 mentioning martinwicke as he is the author of the commit which caused this issue to be closed. All warnings have disappeared in tf2.0.0. This one can stay closed for good. 
29916,Second order gradient of tf.contrib.eager.function is broken,When using 2nd order gradients i.e. Nested GradientTape we cannot utilize tf.defun and tf.function. System information TensorFlow installed from source or binary binary TensorFlow version use command below 1.13.1 Code to reproduce the issue ,Have tried on Colab with TF 13.1 and was able to reproduce the issue. Maybe related https github.com tensorflow tensorflow issues 29942 fps7806 I could reproduce the issue with TF1.13.1 but I don t see any error with TF.1.14.0. I created a gist https colab.sandbox.google.com gist jvishnuvardhan bc4e6c61ebbe156c20fab10a9de3e51c tf 29916.ipynb . Please check it. thanks It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue Automatically closing this out since I understand it to be resolved but please let me know if I m mistaken.Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29916 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29916 No a 
29917, TensorFlow 2.0 Taking a slice of a Keras Input layer gives a tensor with unknown shape, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac OS X 10.14.5 TensorFlow installed from source or binary binary TensorFlow version use command below v2.0.0 beta0 16 g1d91213fe7 2.0.0 beta1 Python version 3.6.8 CUDA cuDNN version don t have a GPU GPU model and memory don t have a GPU Describe the current behavior If I create an input layer with Keras and take a slice of it the slice has unknown shape This is a problem when I want to use this slice in something else e.g. here with a softmax Describe the expected behavior I d expect slice to have shape TensorShape None 10 5 . Code to reproduce the issue ,I was able to reproduce the above mentioned issue on Colab with Tensorflow 2.0.0.beta1. Thanks I have the problem any solution aaronlyt I found a temporary fix to get my code running. You can call set shape on a tensor to enforce a specific shape on it. In my example it would look like this Thanks it works I believe the object returned from Input is simply a Tensor so this looks like a bug or missing feature in the slide shape function aselle is this known The code snippet works in TF 1.14 . However fails with TF 2.0 nightly 2.0.0 dev20190628 This is fixed with latest tf 2.0 nightly version 2.0.0 dev20190809 . Thanks omalleyt12 was the fixing commit before or after 2.0 branch cut I d like to make sure the release has the fix. martinwicke oops sorry for not updating this this and other related loss of shape bugs were fixed before the cut Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29917 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29917 No a 
29965, mean squared error gives incorrect results in conjunction with sample weights, System information OS Platform and Distribution e.g. Linux Ubuntu 16.04 Arch Linux TensorFlow installed from source or binary Binary TensorFlow version use command below 1.13.1 Python version 3.7.3 There is a difference between passing the string mean squared error or passing tf.keras.losses.mean squared error as loss function in conjunction with sample weights. Output ,It seems like mean squared error is simply not weighted by sample weight . It gives the same result as tf.keras.losses.mean squared error as a non weighted metric. In TensorFlow 2.0 Beta all loss functions seem to behave in the way mean squared error behaves. This is the output in TensorFlow 2.0.0 beta1 I don t know much about the internal workings and I didn t look at the code but this is my interpretation The behavior of mean squared error in TensorFlow 1.13 and all loss functions in TensorFlow 2.0 can be described the following way The losses per sample and per time step if sample weight mode temporal are weighted by sample weight . Then they are used for SGD everything good so far. Now the losses are averaged to provide them as a metric to the user. This is done by adding them up and dividing them by the number of losses the batch size or the batch size times the number of time steps if sample weight mode temporal . Consider a fixed batch size the last batch is just containing a few samples and is filled up with zero entries. Sample weights are used to mask these zero entries from SGD. The weighted losses are now consisting of the few losses from the samples and a lot of zeros. The user will see a much lower loss on the last batch. Or in my case with less and less samples per batch I sort the samples by sequence length before batching it looked like the loss was going down if only Although confusing for the user this is true to the mathematics behind the model. Loss functions in TensorFlow 1.13 except mean squared error instead of dividing by the number of losses divide by the sum of all sample weights. This avoids misinterpretation from the user but also hides the true loss value. You can add your loss functions additionally as weighted metrics and they will be calculated the intuitive way. So the TensorFlow 2.0 behavior is superior because it allows for both. Although a warning in the documentation of the sample weight parameter explaining how to interpret the loss values would be helpful. timakro I was able to reproduce the issue with TF1.13. But in the most recent version TF1.14 the loss function is working as expected. Please check the gist https colab.sandbox.google.com gist jvishnuvardhan 07cb3ebf319020ff34764bfc568f5ff4 tf29965 metric error.ipynb here. Please let me know what you think. Thanks Thanks seems fixed. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29965 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29965 No a 
29971,Backpropogation error with tf.math.top k, Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 TensorFlow installed from source or binary Source TensorFlow version use command below 1.13 Python version 3.5.7 Also reproduced with 3.6 CUDA cuDNN version 10.1 GPU model and memory Quadro P3000 Describe the current behavior When the input argument in tf.math.top k losses is a tensor of shape 0 0 backpropogation fails on the reshape step. Describe the expected behavior There should be no backprop on collecting 0 elements from tensor with 0 values. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Trying to perform Online Hard Example Mining with two lists of losses of shape None None Batch num losses When there are either zero positive examples or zero negative examples trips the error on the backprop step even though there should be no backprop for a selection of zero elements or a tensor of zero elements Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. , tokotchd In order to expedite the trouble shooting process please provide a code snippet to reproduce the issue reported here. Thanks After building a minimum working example it seems that the only condition required for this error is backpropogation on top k function when k 0. As a result the above code runs successfully 50 of the time and fails with the following stacktrace the other 50 of the time. I could reproduce the issue on colab with TF gpu 1.13.1. I cannot reproduce this using nightly. Can you find a smaller reproducing example Say just the shape of the input to top k a call to top k and a call to gradients alextp you need some sort of trainable variables layers otherwise the call to gradients will not propagate through top k. I had the same problem. how to solve this problem alextp I ran his code three times and I get this error. maybe you should run the code he provided a few times to get this error. tokotchd This is fixed in Tf 1.14.0. Please take a look at colab gist here https colab.research.google.com drive 1Q7HkreVszvG6ygXqlQrUoZI0ZRHzsX4m . You want to give a try. Thanks I just ran this 20 times with and without GPUs on colab and did not get the error. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29971 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29971 No a 
30028,Python package is missing ModuleSpec in tensorflow. spec in tf 1.14.0, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow no OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux 4.9.125 linuxkit x86 64 with Ubuntu 18.04 bionic Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device no TensorFlow installed from source or binary preinstalled in docker image TensorFlow version use command below v1.14.0 rc1 22 gaf24dc91b5 1.14.0 Python version 3.6.8 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Describe the current behavior In TF 1.14.0 the module spec in tensorflow. spec is None Describe the expected behavior This is different from tf 1.13.1 where it works as expected Missing spec causes some problems e.g. pkgutil now fails when trying to find tensorflow. Note that the first call to find loader is successful it only fails after tensorflow is imported Code to reproduce the issue See above Other info logs I ve tested this using official tf docker image tensorflow tensorflow 1.14.0 py3 and also using python docker image python 3.6 with tensorflow installed with pip.,importlib throws the same error. Can t do custom imports in 1.14. Another data point this also happens in nightly Sorry for the breakage This is caused by adding a module wrapper that prints deprecation messages. I will send a change to fix it. Closing this issue since the associated PR has been merged. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30028 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30028 No a Is there a work around in the mean time We ll get the 1.14.1 patch release this week. Update Instead of a 1.14.1 patch release we will get a 1.15 version in a few weeks. Sorry for the extra delay this causes. 
30029,Autoconversion to Tensor in functions, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 19.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary pip TensorFlow version use command below 1.14.0 Python version 3.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Describe the current behavior calling certain functions with non Tensors but convertible to fails since the object is not converted to a tensor. The function actually only expect Tensors. e.g. tf.math.real 1.0 fails because 1. has no attribute dtype which it will only have after the conversion to a tensor . Describe the expected behavior My understanding goes that functions that take Tensors such as tf.math.real can also take anything that can be converted to a Tensor such as Python floats or objects with a registered conversion function. Namely are equivalent . I would expect tf.math.real 1. to return the real part of the tensor tf.convert to tensor 1. . This causes a big problem with the actual beautiful registration of conversion functions. The code contains only a minimal example. Am I mistaken with the expected behavior And if so this may could be made clear in the docs that only Tensors are taken vs Tensor like objects. E.g. the docs of tf.math.round and tf.math.real leave no clue in which to use tensors and in which Tensor like objects. Code to reproduce the issue short version but of course also fails for any custom defined Tensor like object ,Added a PR 30049 for the fix. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30029 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30029 No a 
30040,Dimensions check in BinaryCrossEntropy, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device NA TensorFlow installed from source or binary Binary TensorFlow version use command below 2.0.0 beta1 Python version 3.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Describe the current behavior Suppose we build a model for binary classification problem and we want to use BinaryCrossEntropy loss provided in tf.keras.losses . Here is an example Describe the expected behavior When the dimensions of y true and y pred are different in that case the loss function should raise an error for dimension mismatch or the model will fail silently and no one would be ab e to debug it until unless they are aware of this behavior Code to reproduce the issue Check above ,Added a PR 30053 for the shape check. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30040 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30040 No a 
30052,ValueError Arguments and signature arguments do not match when using dataset api keras functional api and checkpoints callback tf2.0 , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOS 10.13.6 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device NA TensorFlow installed from source or binary binary TensorFlow version use command below GIT VERSION v1.12.1 4759 g9856697d8b TF VERSION 2.0.0 dev20190622 Python version 3.6.4 Bazel version if compiling from source NA GCC Compiler version if compiling from source NA CUDA cuDNN version NA GPU model and memory NA Describe the current behavior Calling the function on a Keras model when specifying a Dataset and a callback will crash after the first epoch with this error . The error happens only when specifying both the training Dataset and validation Dataset. The error happens because of the checkpoint callback. Describe the expected behavior The model should not crash continue training and successfully save the checkpoints. Code to reproduce the issue Other info logs ,I get the same issue. I tried on colab with Tensorflow 2.0.0 dev20190623. I am able to reproduce the issue. Thanks get the same issue without specifying a ModelCheckpoint callback Traceback most recent call last File home deeplearning .vscode extensions ms python.python 2019.6.22090 pythonFiles ptvsd launcher.py line 43 in module main ptvsdArgs File home deeplearning .vscode extensions ms python.python 2019.6.22090 pythonFiles lib python ptvsd main .py line 434 in main run File home deeplearning .vscode extensions ms python.python 2019.6.22090 pythonFiles lib python ptvsd main .py line 312 in run file runpy.run path target run name main File usr local lib python3.6 runpy.py line 263 in run path pkg name pkg name script name fname File usr local lib python3.6 runpy.py line 96 in run module code mod name mod spec pkg name script name File usr local lib python3.6 runpy.py line 85 in run code exec code run globals File home deeplearning work Deeplearning TensorFlow DeepWritingID DeepHWS online 2 run.py line 62 in module sys.exit main File home deeplearning work Deeplearning TensorFlow DeepWritingID DeepHWS online 2 run.py line 56 in main train model.fit next it batch size params.batch size epochs 3 File usr local lib python3.6 site packages tensorflow python keras engine training.py line 643 in fit use multiprocessing use multiprocessing File usr local lib python3.6 site packages tensorflow python keras engine training arrays.py line 664 in fit steps name steps per epoch File usr local lib python3.6 site packages tensorflow python keras engine training arrays.py line 383 in model iteration batch outs f ins batch File usr local lib python3.6 site packages tensorflow python keras backend.py line 3510 in call outputs self. graph fn converted inputs File usr local lib python3.6 site packages tensorflow python eager function.py line 572 in call return self. call flat args File usr local lib python3.6 site packages tensorflow python eager function.py line 671 in call flat outputs self. inference function.call ctx args File usr local lib python3.6 site packages tensorflow python eager function.py line 427 in call len args len list self.signature.input arg ValueError Arguments and signature arguments do not match 97 98 I get the same issue when I use the callback parameter Get error when start training 2 epoch. Hi mahzoon I got the same error as yours. But it only happens when I set save weights only False in the checkpoint callback. If and only if I set save weights only True it will work as usual. Also I m confused by this warning W tensorflow python util util.cc 280 Sets are not currently considered sequences but this may change in the future so consider avoiding using them. It didn t show up after I set save weights only True . rchao can you take a look There seems to be some delta in the inputs as created and as we try to do them. Can we improve the error message while fixing this We should be able to print out what the mismatch is in that error message. mahzoon thanks for reporting the issue we were able to repro and found that the mismatched argument is keras learning phase. We re actively working on training loop refactoring which will resolve the issue. Will post back here once we get more updates. In jupyter notebook I noticed that if I don t have any metric it works after the first fail. Just fyi in case it helps Hi mahzoon I got the same error as yours. But it only happens when I set save weights only False in the checkpoint callback. If and only if I set save weights only True it will work as usual. Also I m confused by this warning W tensorflow python util util.cc 280 Sets are not currently considered sequences but this may change in the future so consider avoiding using them. It didn t show up after I set save weights only True . I got the same issue like zihaozhihao setting save weights only False would cause the problem. same issue here running on rc0 I also hit this issue in my personal project TF VERSION v2.0.0 rc2 26 g64c3d38 Python version 3.5.3 I am not sure if it is the same bug but it states ValueError Arguments and signature arguments do not match 1229 1230 It occurs on model.fit generator at the beginning of the training. But if I use fit generator with class weights argument the error would prompt on the end of the epoch and seems to be related to validation dataset evaluation ValueError You must feed a value for placeholder Tensor block1c bn block1c bn trainable 0 dtype bool If using without validation data argument the model works fine. Setting save weights only True doesn t help. The workaround is to train 1 epoch without validation data save the model load afterwards. After this manipulation train can be done as usual. I am currently dealing with the same problem here when using keras backend during the learning phase. I wondered if any solution has been found yet This is fixed with latest tf nightly build 2.1.0 dev20200109 See gist https colab.sandbox.google.com gist ymodak 2e7ca2c039dd0c97a683d4132bb3465a github 30052ipynb.ipynb Feel free to reopen if still have problems. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 30052 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 30052 No a tried tf nightly build 2.1.0 dev20200109 and the lastest TensorFlow GPU also Keras 2.3.1 problem not resolved when doing predict or predict on batch from model loaded from a file. ValueError Arguments and signature arguments do not match. got 92 expected 93 can someone confirm this or my problem could be unrelated. tried tf nightly build 2.1.0 dev20200109 and the lastest TensorFlow GPU also Keras 2.3.1 problem not resolved when doing predict or predict on batch from model loaded from a file. ValueError Arguments and signature arguments do not match. got 92 expected 93 can someone confirm this or my problem could be unrelated. I got similar issue I was using K.function input output updates I m having same issue with tensorflow gpu 2.1.0 . and python 3.6.9 Any updates ValueError Arguments and signature arguments do not match. got 37 expected 39 I m having same issue with tensorflow gpu 2.1.0 . and python 3.6.9 Any updates ValueError Arguments and signature arguments do not match. got 37 expected 39 The problem solved after replacing import keras by import tensorflow.keras and making the necessary modifications 
30056,multi gpu training tf.compat.v1.scatter sub operation throws exception, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Extended from the stock example https www.tensorflow.org beta tutorials distribute keras Working example is attached to this bug report OS Platform and Distribution Linux Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device No TensorFlow installed from source or binary Binary TensorFlow version use command below 2.0.0 beta1 Python version Python 3.5.2 Bazel version if compiling from source N A 0.23.2 GCC Compiler version if compiling from source N A 5.4.0 20160609 CUDA cuDNN version 10.0.130 7.4.1 GPU model and memory 4x Titan Xp 12Gb Standalong no SLI connections Describe the current behavior Describe the expected behavior tf.compat.v1.scatter sub or equivalent scatter operations should support multi gpu training. Code to reproduce the issue 1. The script attached is used to train a model using multiple GPUs. The code is modified from the stock example for quick experiment. https www.tensorflow.org beta tutorials distribute keras Other info logs Run the script using the following command Save the following script as center loss mnist.py ,The script will run successfully without using the with stragety.scopt with the following changes Change from To I have reproduced the issue in Colab using TF GPU 2.0.0 beta1.Thanks FYI The same issue is present in tensorflow 1.14 as well. It comes from the usage of scatter sub in multi gpu MirroredStrategy. While you guys fix the issue is there a workaround for this that we can use Thanks Is there a chance to resolve this in final release of 1.15 This issue is still there and will not be fixed in 1.15 since that has already been cut. One person had started working on it but the fix got delayed due to some other issues. You can try 2 workarounds if the tensors you re trying to subtract are not too big you can try to convert them to dense and just subtract regularly using assign sub etc. This may have some performance implications You can implement this yourself using merge call reduce update something like this this is exactly how we are going to implement this within mirrored variable as well This issue should have been fixed in tf 2.2. You can just call the scatter method on the variable. tf.compat.v1.scatter sub wouldn t work though. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 30056 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 30056 No a 
30096,Keras load model fails to load models with BatchNormalization layer when saved in non eager mode, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Y OS Platform and Distribution e.g. Linux Ubuntu 16.04 macOS 10.13.6 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device NA TensorFlow installed from source or binary binary TensorFlow version use command below tf version 2.0.0 dev20190622 tf git version v1.12.1 4759 g9856697d8b Python version 3.6.4 Bazel version if compiling from source NA GCC Compiler version if compiling from source NA CUDA cuDNN version NA GPU model and memory NA Describe the current behavior This issue is very similar to 3628 but it only happens in non eager mode in TF 2.0. You can save a model having batch norm layer in non eager mode successfully. However you cannot load the model in any mode. Load model fails with Seems like the model is not saved correctly. Describe the expected behavior Save model should produce a correct SavedModel in non eager mode so that it can be loaded later. Code to reproduce the issue Other info logs ,I have tried on Colab with TF version 2.0.0 dev20190622 and was able to reproduce it on only non eager mode. Thanks for reporting this issue This should now be fixed with this change https github.com tensorflow tensorflow commit 769900b011cf6a31f8e8f919e97b70900f9825d8 diff 5738cc9149537416bc9a07c7cfefae42 Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30096 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30096 No a Thank you k w w I confirm that the issue is resolved in the tf nightly 
30113,tf.image.encode png doesn t support 16 bit and inconsistent behavior in eager mode, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Win 10 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below tested with 1.9.0 1.12.0 and 1.14.0 Python version 3.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior Creating a numpy array with uint16 datatype and passing it to tf.image.encode png yields different results in eager execution mode. The first time the array is passed it somehow gets transformed to a uint8 array and for the following encodings it works as expected. Using a tf.session the uint16 input is always transformed to uint8 Describe the expected behavior Just return a bytestring of a 16bit PNG Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,I am able to reproduce the issue with eager execution using TF 1.12 TF 1.9 but in session i am getting the below error. RuntimeError The Session graph is empty. Add operations to the graph before calling run . But in TF1.14 i am able to reproduce the issue with session mode but i am getting below error with eager execution. ValueError tf.enable eager execution must be called at program startup. Apologies for the delay in response. I get following results using TF 1.14. Can you please confirm Thanks Eager Mode Session Mode Hi thanks for having a look at it. The PNG header of the encoded image should be something around 60 70 bytes from looking at the outputs. So for the 10x10px uint8 image the expected byte length is 100 70 and for the uint16 it should be around 2 100 70. With a fresh install of TF 1.14.0 and Python 3.6 I ve got Eager Mode 178 278 Session Mode 178 178 But it should be 278 always in both modes. So in session mode the encoding doesn t work and in eager mode it works after the first encoding which is currently my workaround. Note I executed it in a notebook and after a kernel restart I get the 178 278 result for eager execution and if I execute the cell again it will be 278 278. So I guess there is some kind of initialization going on that sets the correct datatype after the first faulty execution of tf.image.encode . It seems the problem exists also in TF 2.0 if the usage hasn t changed . I could reproduce the issue with 2.0 and master and could see the issue lies in the generated code of encode png eager fallback and to args to matching eager . The issue is that in encode png eager fallback eager tensor for input image is created with preferred dtype passed as uint8 since EncodePng s defined T type is UINT8 . However input image itself is a numpy array with dtype of uint16. So the second time it runs input image is converted to a tensor as uint16. The reason is that the second time encode png eager fallback is not called so T type of UINT8 is not used implicitly. I think the behavior may need to be fixed one way or another though I it might also break many other places if the fix is not careful. Not sure the best way to get around it though 53RT if you convert input numpy array to tensor first then the example will run correctly. tensorflow api owners akshaym do you understand what is happening here Sorry for the delayed response. Eager internally has 2 paths and you ve found a place where they don t agree a discrepancy I will remove soon . Unfortunately this means removing the behavior of being 278 is unfortunately going to go away in eager and all the above cases will return 178. As suggested by yongtang the best thing to do is to convert the numpy array to a tensor explicitly. 53RT Looks like its fixed in TF nightly 2.1 . Please find the gist https colab.sandbox.google.com gist gadagashwini 7bd1e30565e22d6f8a9087006016e52f untitled327.ipynb and let us know. Thanks It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue 53RT Issue is fixed in Tf nightly. Can you confirm. Thanks gadagashwini sorry for not responding. Yes I could reproduce the results and PNG encoding decoding did worked for 8 and 16 bit with Thanks for fixing it Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 30113 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 30113 No a 
30122,Getting validation steps from dict breaks keras fit TF 2.0.0 beta1, System information TensorFlow installed from pip TensorFlow version 2.0.0 beta1 Python version 3.6 Describe the current behavior When calling fit on a Keras model in 2.0.0 beta1 a KeyError 0 is thrown when trying to call fit with validation data given as a dict this worked in the alpha e.g. The issue seems to have been introduced here https github.com tensorflow tensorflow commit 25380986954692d947bd230e4f5d3a2d11dd3064 as the line val samples or steps val inputs and val inputs 0 .shape 0 or None assumes an array will be found and fails for dicts. Describe the expected behaviour val inputs Either a list or dictionary of arrays or a dataset instance. ,Please provide us complete code to reproduce the issue.Thanks The following is sufficient to reproduce the issue This runs with 2.0.0 alpha0 but fails with 2.0.0 beta1 I am able to reproduce the issue in Colab works with TF 2.0.0 alpha0 and fails with 2.0.0 beta1.Thanks Added a PR 30258 for the fix. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30122 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30122 No a 
30141, Iterator Model Prefetch Batch Shuffle ParallelInterleaveV2 returned OutOfRange without setting end of sequence,See https github.com tensorflow tensorflow issues 29060 issuecomment 505539468 for more details System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 OSX and Linux TensorFlow installed from source or binary pipenv install pre tensorflow 2.0.0 beta1 TensorFlow version use command below 2.0.0 beta1 Python version 3.6.8 Describe the current behavior I m running into this issue when caching before interleave Describe the expected behavior Able to cache the filenames dataset Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. See above Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,Thank you for reporting this devstein. I am able to reproduce the problem with the below code I m looking into the root cause and will message back when I have a fix. The issue is related to using parallel interleave. As a temporary workaround try leaving num parallel calls unset. aaudiber Appreciate the quick response I ll try that workaround for now. The fix is merged to master in https github.com tensorflow tensorflow commit 3398e887f5daa1e22c59eaa1c6f5ca731698ad2f aaudiber Thanks 
30149,Autograph Failed to parse source code error when using lambda in for loop, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOSX 10.13.6 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below VERSION 2.0.0 dev20190625 GIT VERSION v1.12.1 4885 g71241a6afd Python version 3.6.8 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior I get an autograph error when running the following code see the full stacktrace below The error is ValueError Failed to parse source code of function lambda at 0x11194c488 Describe the expected behavior Everything works fine when I define the dataset on the previous line like this Code to reproduce the issue See above. Other info logs Full stack trace with AUTOGRAPH VERBOSITY 10 2019 06 25 22 24 13.172683 I tensorflow core platform cpu feature guard.cc 142 Your CPU supports instructions that this TensorFlow binary was not compiled to use AVX2 FMA 2019 06 25 22 24 13.197405 I tensorflow compiler xla service service.cc 168 XLA service 0x7fa5e8a657c0 executing computations on platform Host. Devices 2019 06 25 22 24 13.197445 I tensorflow compiler xla service service.cc 175 StreamExecutor device 0 undefined undefined Converted call function lambda at 0x134b81488 args VariantDataset shapes types tf.int64 kwargs Not whitelisted method wrapper call of function object at 0x134b81488 default rule Not whitelisted function lambda at 0x134b81488 default rule Entity function lambda at 0x134b81488 is not cached for key code object lambda at 0x13b5f7ed0 file ipython input 1 8a83c4c9b193 line 4 subkey tensorflow.python.autograph.core.converter.ConversionOptions object at 0x13b641588 frozenset Converting function lambda at 0x134b81488 WARNING Logging before flag parsing goes to stderr. E0625 22 24 13.215670 140735810999168 ag logging.py 133 Error converting function lambda at 0x134b81488 Traceback most recent call last File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph pyct parser.py line 78 in parse entity return parse str source preamble len len future features source File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph pyct parser.py line 140 in parse str module node gast.parse src File Users ageron miniconda3 envs tf2 lib python3.6 site packages gast gast.py line 240 in parse return ast to gast ast.parse args kwargs File Users ageron miniconda3 envs tf2 lib python3.6 ast.py line 35 in parse return compile source filename mode PyCF ONLY AST File unknown line 5 for window in ds.flat map lambda window window.batch 5 SyntaxError unexpected EOF while parsing During handling of the above exception another exception occurred Traceback most recent call last File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph pyct parser.py line 118 in parse entity return parse str source preamble len len future features source File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph pyct parser.py line 140 in parse str module node gast.parse src File Users ageron miniconda3 envs tf2 lib python3.6 site packages gast gast.py line 240 in parse return ast to gast ast.parse args kwargs File Users ageron miniconda3 envs tf2 lib python3.6 ast.py line 35 in parse return compile source filename mode PyCF ONLY AST File unknown line 5 for window in ds.flat map lambda window window.batch 5 SyntaxError unexpected EOF while parsing During handling of the above exception another exception occurred Traceback most recent call last File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph impl api.py line 635 in to graph return conversion.convert entity program ctx File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph impl conversion.py line 322 in convert free nonglobal var names File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph impl conversion.py line 240 in convert with cache entity program ctx File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph impl conversion.py line 441 in convert entity to ast nodes name entity info convert func to ast o program ctx File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph impl conversion.py line 601 in convert func to ast node source parser.parse entity f future features future features File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph pyct parser.py line 123 in parse entity source to n nBut that did not work. .format source File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph pyct parser.py line 66 in raise parse failure .format entity source comment ValueError Failed to parse source code of function lambda at 0x134b81488 which Python reported as from future import absolute import from future import division from future import print function from future import unicode literals for window in ds.flat map lambda window window.batch 5 If this is a lambda function the error may be avoided by creating the lambda in a standalone statement. Tried to strip down the source to from future import absolute import from future import division from future import print function from future import unicode literals for window in ds.flat map lambda window window.batch 5 But that did not work. ERROR Error converting function lambda at 0x134b81488 Traceback most recent call last File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph pyct parser.py line 78 in parse entity return parse str source preamble len len future features source File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph pyct parser.py line 140 in parse str module node gast.parse src File Users ageron miniconda3 envs tf2 lib python3.6 site packages gast gast.py line 240 in parse return ast to gast ast.parse args kwargs File Users ageron miniconda3 envs tf2 lib python3.6 ast.py line 35 in parse return compile source filename mode PyCF ONLY AST File unknown line 5 for window in ds.flat map lambda window window.batch 5 SyntaxError unexpected EOF while parsing During handling of the above exception another exception occurred Traceback most recent call last File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph pyct parser.py line 118 in parse entity return parse str source preamble len len future features source File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph pyct parser.py line 140 in parse str module node gast.parse src File Users ageron miniconda3 envs tf2 lib python3.6 site packages gast gast.py line 240 in parse return ast to gast ast.parse args kwargs File Users ageron miniconda3 envs tf2 lib python3.6 ast.py line 35 in parse return compile source filename mode PyCF ONLY AST File unknown line 5 for window in ds.flat map lambda window window.batch 5 SyntaxError unexpected EOF while parsing During handling of the above exception another exception occurred Traceback most recent call last File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph impl api.py line 635 in to graph return conversion.convert entity program ctx File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph impl conversion.py line 322 in convert free nonglobal var names File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph impl conversion.py line 240 in convert with cache entity program ctx File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph impl conversion.py line 441 in convert entity to ast nodes name entity info convert func to ast o program ctx File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph impl conversion.py line 601 in convert func to ast node source parser.parse entity f future features future features File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph pyct parser.py line 123 in parse entity source to n nBut that did not work. .format source File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph pyct parser.py line 66 in raise parse failure .format entity source comment ValueError Failed to parse source code of function lambda at 0x134b81488 which Python reported as from future import absolute import from future import division from future import print function from future import unicode literals for window in ds.flat map lambda window window.batch 5 If this is a lambda function the error may be avoided by creating the lambda in a standalone statement. Tried to strip down the source to from future import absolute import from future import division from future import print function from future import unicode literals for window in ds.flat map lambda window window.batch 5 But that did not work. Error transforming entity function lambda at 0x134b81488 Traceback most recent call last File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph pyct parser.py line 78 in parse entity return parse str source preamble len len future features source File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph pyct parser.py line 140 in parse str module node gast.parse src File Users ageron miniconda3 envs tf2 lib python3.6 site packages gast gast.py line 240 in parse return ast to gast ast.parse args kwargs File Users ageron miniconda3 envs tf2 lib python3.6 ast.py line 35 in parse return compile source filename mode PyCF ONLY AST File unknown line 5 for window in ds.flat map lambda window window.batch 5 SyntaxError unexpected EOF while parsing During handling of the above exception another exception occurred Traceback most recent call last File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph pyct parser.py line 118 in parse entity return parse str source preamble len len future features source File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph pyct parser.py line 140 in parse str module node gast.parse src File Users ageron miniconda3 envs tf2 lib python3.6 site packages gast gast.py line 240 in parse return ast to gast ast.parse args kwargs File Users ageron miniconda3 envs tf2 lib python3.6 ast.py line 35 in parse return compile source filename mode PyCF ONLY AST File unknown line 5 for window in ds.flat map lambda window window.batch 5 SyntaxError unexpected EOF while parsing During handling of the above exception another exception occurred Traceback most recent call last File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph impl api.py line 635 in to graph return conversion.convert entity program ctx File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph impl conversion.py line 322 in convert free nonglobal var names File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph impl conversion.py line 240 in convert with cache entity program ctx File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph impl conversion.py line 441 in convert entity to ast nodes name entity info convert func to ast o program ctx File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph impl conversion.py line 601 in convert func to ast node source parser.parse entity f future features future features File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph pyct parser.py line 123 in parse entity source to n nBut that did not work. .format source File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph pyct parser.py line 66 in raise parse failure .format entity source comment ValueError Failed to parse source code of function lambda at 0x134b81488 which Python reported as from future import absolute import from future import division from future import print function from future import unicode literals for window in ds.flat map lambda window window.batch 5 If this is a lambda function the error may be avoided by creating the lambda in a standalone statement. Tried to strip down the source to from future import absolute import from future import division from future import print function from future import unicode literals for window in ds.flat map lambda window window.batch 5 But that did not work. During handling of the above exception another exception occurred Traceback most recent call last File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph impl api.py line 528 in converted call experimental optional features options.optional features File Users ageron miniconda3 envs tf2 lib python3.6 site packages tensorflow core python autograph impl api.py line 639 in to graph entity e. class . name str e tensorflow.python.autograph.impl.api.ConversionError converting function lambda at 0x134b81488 ValueError Failed to parse source code of function lambda at 0x134b81488 which Python reported as from future import absolute import from future import division from future import print function from future import unicode literals for window in ds.flat map lambda window window.batch 5 If this is a lambda function the error may be avoided by creating the lambda in a standalone statement. Tried to strip down the source to from future import absolute import from future import division from future import print function from future import unicode literals for window in ds.flat map lambda window window.batch 5 But that did not work. W0625 22 24 13.223130 140735810999168 ag logging.py 146 Entity function lambda at 0x134b81488 could not be transformed and will be executed as is. Please report this to the AutoGraph team. When filing the bug set the verbosity to 10 on Linux export AUTOGRAPH VERBOSITY 10 and attach the full output. Cause converting function lambda at 0x134b81488 ValueError Failed to parse source code of function lambda at 0x134b81488 which Python reported as from future import absolute import from future import division from future import print function from future import unicode literals for window in ds.flat map lambda window window.batch 5 If this is a lambda function the error may be avoided by creating the lambda in a standalone statement. Tried to strip down the source to from future import absolute import from future import division from future import print function from future import unicode literals for window in ds.flat map lambda window window.batch 5 But that did not work. WARNING Entity function lambda at 0x134b81488 could not be transformed and will be executed as is. Please report this to the AutoGraph team. When filing the bug set the verbosity to 10 on Linux export AUTOGRAPH VERBOSITY 10 and attach the full output. Cause converting function lambda at 0x134b81488 ValueError Failed to parse source code of function lambda at 0x134b81488 which Python reported as from future import absolute import from future import division from future import print function from future import unicode literals for window in ds.flat map lambda window window.batch 5 If this is a lambda function the error may be avoided by creating the lambda in a standalone statement. Tried to strip down the source to from future import absolute import from future import division from future import print function from future import unicode literals for window in ds.flat map lambda window window.batch 5 But that did not work. 2019 06 25 22 24 13.243343 W tensorflow compiler jit mark for compilation pass.cc 1541 One time warning Not using XLA CPU for cluster because envvar TF XLA FLAGS tf xla cpu global jit was not set. If you want XLA CPU either set that envvar or use experimental jit scope to enable XLA CPU. To confirm that XLA is active pass vmodule xla compilation cache 1 as a proper command line flag not via TF XLA FLAGS or set the envvar XLA FLAGS xla hlo profile. 0 1 2 3 4 1 2 3 4 5 2 3 4 5 6 3 4 5 6 7 4 5 6 7 8 5 6 7 8 9 ,I have reproduced the issue in Colab using TF VERSION 2.0.0 dev20190625.Thanks This is related to a limitation in Python s inspect.getsource which can t always get the source code of lambda functions. Specifically getsource returns the entire source code line which isn t always well formed Python code as you could see from this example. The workaround is to declare the lambda function on a single line as the OP indicates. Normally the error message should describe that albeit in more detail but it should definitely suggest the workaround of declaring the lambda on a separate line ageron can you confirm that the error message included that guidance Related we should remove the extraneous imports from the error message. The message should spell just Yes I can confirm that the message If this is a lambda function the error may be avoided by creating the lambda in a standalone statement. was part of the very long error message. But it s neither at the beginning nor at the end so it s easily overlooked. I would recommend shortening the error message except when AUTOGRAPH VERBOSITY 10 to something like this Alternatively isn t it possible to parse this line to extract the lambda After all it s right there. I agree will simplify the error message. Yes we do attempt to parse the line but in this case is it not well formed Python code in our example it s a for loop without a body. One could imagine a partial parser which attempts to parse as much as possible of the code that is well formed but the Python parser doesn t know how to do that and even then there may still be situations of ambiguity where the results would be incorrect. A much more robust fix would be to fix the parser so that it records the exact extents of the lambda with column numbers. Currently it only records the line number which is the root of the problem. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30149 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30149 No a 
30182,autograph should handle for loops over range in a manner that is compatible with XLA compilation, System information TensorFlow version you are using 1.14 Are you willing to contribute it Yes No No Describe the feature and the current behavior state. Consider the following Python code bad loop is the intuitive way to write this loop. However it fails to compile with xla In contrast good loop calculates the correct result Autograph seems to always convert range into tf.range even in for loops. This means that XLA can t compile the function. However the equivalent loop written as a naive while loop works. Ideally Autograph would detect such uses of range in for loops and convert them into the style of good loop automatically rather than requiring users to do this. This would let us write cleaner code. Will this change the current api How No Who will benefit with this feature Users who want to write normal Python code with Autograph. Any Other info. ,I have tried on colab with TF version 1.14 and was able to reproduce the issue.Thanks Looks like a few bugs are being compound here I ll list them along with recommendations and plans to address 1. I think you are correct in this case there is no way but to detect the use of tf.range at first I thought this would be a mere performance optimization but it seems to be required for XLA. It wasn t already enabled because the detection of tf.range op is not terribly robust but I think this example justifies it. Will follow up with a fix soon. In the mean time using tf.range 3 should work see below . 2. range is normally not converted to tf.range this only happens when its argument is a Tensor xla.compile will auto cast all arguments to tensors hence range will receive a Tensor even though you only specify just 3. Even so using range tf.constant 3 is not officially supported and I recommend using tf.range which is more explicit anyway. 3. filed 30235 It appears that tf.range only works in XLA if you specify it with an inline constant tf.range tf.constant 3 even though bad function should be equivalent to that it looks like xla.compile will not recognize the constant argument and raise an error. For example the following code will work Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30182 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30182 No a Thank you mdanatg 
30222,SelectV2 and Reciprocal in TFLite, System information OS Platform and Distribution Ubuntu TensorFlow installed from source or binary binary TensorFlow version or github SHA if from source TF 2.0 beta1 Provide the text output from tflite convert Any other info logs At the beginning I tried to convert without this magic line And I got almost the same error log BUT it was additional operation that couldn t be converted. I attached the end of that log As far as understood correctly tf.where doesn t work and it is related to SelectV2 because when I remove line with tf.where x 0 left right everthing is okay. It s strange because TFLite docs said that tf.where should work. Also I figured out that Reciprocal is linked to 1.0 x expression. ,Current workaround for tf.where Current workaround for Reciprocal Find every 1.0 x expression and replace it. In my case I had x y 1.0 1.0 a b and I replace it with x y a b 1.0 a b . I suppose error happened because TF is looking for 1.0 x and optimize it in some another way rather than simple x y . Need to fix. Hi Oktai15 we re investigating direct converter support for reciprocal stay tuned for updates. Hello Oktai15 now the direct support for reciprocal is ready. FYI https github.com tensorflow tensorflow commit df41bbafb50e1ea99404257cf7fa6f60bd063b5f Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 30222 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 30222 No a SelectV2 op is now available in Tensorflow Lite. 
30248,tf.io.write file not working in tf.function decorated function, System information OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04.2 and Windows 10 TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 beta1 Python version 3.7 Describe the current behavior tf.io.write file creates file in eager execution but produces no output file when decorated with tf.function . Describe the expected behavior tf.io.write file should create an output file whether or not being decorated with tf.function . Code to reproduce the issue ,I have tried on colab with TF version 2.0 beta1 and was able to reproduce the issue.Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30248 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30248 No a This also happens in 1.14 and sadly inhibits visualizing images augmented in tf.data.Dataset pipelines since they are run as a Graph Unfortunately the fix didn t make it into the 1.14 release and will only be available in 2.0. In 1.14 write file should still be usable using the old style tf.control dependencies . junghau I ran the code shared and do not face any error please find the gist here https colab.research.google.com gist Saduf2019 92c7fe5bb5088ca162cd180e8fc90dcb untitled.ipynb . 
30315,TFlite conversion of Conv1D with dilation 1, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Google Colab TensorFlow installed from source or binary binary TensorFlow version use command below tensorflow 2.0.0 beta1 Python version python3 Describe the current behavior After converting a Conv1D op to tensorflow lite the interpreter cannot allocate tensors tensorflow lite kernels space to batch nd.cc 96 NumDimensions op context.input kInputDimensionNum 3 4 Node number 0 SPACE TO BATCH ND failed to prepare. Describe the expected behavior Tflite model should be able to load and execute. Code to reproduce the issue Other info logs The problem does not occur when dilation rate 1,This issue is solved with PRs 28410 27867 28179. Thanks Was able to reproduce the issue with TF v2.1 https colab.research.google.com gist amahendrakar 5216cdcb0582cc62e6a6518ea5334e2e 2 1 template.ipynb and TF nightly https colab.research.google.com gist amahendrakar 14ab31228c0693ab78b78ed73f0145d0 tf nightly.ipynb scrollTo ieAW NK5iqpf i.e. v2.2.0 dev20200327. Please find the attached gist. Thanks karimnosseir can you take a look amahendrakar Can you share the sample code The example in the original issue works when i tried it. Thanks karimnosseir Sure below are the links of the gist using TF v2.1 https colab.research.google.com gist amahendrakar 5216cdcb0582cc62e6a6518ea5334e2e 2 1 template.ipynb TF nightly https colab.research.google.com gist amahendrakar 14ab31228c0693ab78b78ed73f0145d0 tf nightly.ipynb scrollTo ieAW NK5iqpf karimnosseir Hi what tensorflow version were you using Thanks amahendrakar I can reproduce it on the 2.1 but works with nightly. Can you please retry. suicao i was using tf nightly I can confirm that this works with karimnosseir Works without any issues with the latest TF nightly i.e. v2.2.0 dev20200415. Please find the gist here https colab.research.google.com gist amahendrakar 67ba5ecd35433f7e61e5102f8d74611a 30315 tf nightly.ipynb scrollTo ieAW NK5iqpf . Thanks Thanks for confirming. I am closing the issue. Please feel free to reopen create a new one if you have any problems. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 30315 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 30315 No a Issue still exists in TF 2.2 stable. 
30361,Error when computing Jacobian in eager mode, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux version 3.10.0 957.10.1.el7.x86 64 mockbuild x86 040.build.eng.bos.redhat.com gcc version 4.8.5 20150623 Red Hat 4.8.5 36 GCC Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary pip install TensorFlow version use command below v1.13.1 0 g6612da8951 1.13.1 Python version 3.6.4 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A CPU GPU model and memory N A CPU Describe the current behavior Error occurs when trying to compute the Jacobian in the following code Describe the expected behavior tf.Tensor id XXX shape 2 1 dtype float32 numpy array 1. 1. dtype float32 Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. Additional info the expected behavior occurs when creating the GradientTape with persistent True and replacing the last line of code with g.jacobian y x experimental use pfor False . However that workaround is extremely slow with real data. Log of the code that reproduces the issue , sarshalom I am able to reproduce the issue with Tensorflow version 1.13.1 on Colab. Apologies for the delay in response. Looks like this is fixed in TF 1.14.0 I get following output with eager execution enabled Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30361 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30361 No a 
30378,Problems with keras model saving when there s a loss added with add loss, System information System windows 10 wsl with ubuntu 18 LTS Tensorflow Version 2.0.0b1 in CPU mode default installed from pip Python version 3.6.8 It also happens in real linux environments actually it s easy to simulate each these errors Describe the current behavior I m having many problems when saving loading keras models with custom loss added with add loss . I ll describe each one of the scenarios below I think they re all related Code to reproduce the issue When not using a keras layer as loss it produces a non valid JSON Traceback most recent call last File home nguerinjr Documents deep coding project teste.py line 8 in module tf.keras.experimental.export saved model model model.tf File home nguerinjr Documents deep coding project venv lib python3.7 site packages tensorflow python keras saving saved model.py line 169 in export saved model export model json model saved model path File home nguerinjr Documents deep coding project venv lib python3.7 site packages tensorflow python keras saving saved model.py line 177 in export model json model json model.to json File home nguerinjr Documents deep coding project venv lib python3.7 site packages tensorflow python keras engine network.py line 1449 in to json model config default serialization.get json type kwargs File usr lib python3.7 json init .py line 238 in dumps kw .encode obj File usr lib python3.7 json encoder.py line 199 in encode chunks self.iterencode o one shot True File usr lib python3.7 json encoder.py line 257 in iterencode return iterencode o 0 File home nguerinjr Documents deep coding project venv lib python3.7 site packages tensorflow python util serialization.py line 69 in get json type raise TypeError Not JSON Serializable obj TypeError Not JSON Serializable b n x03add x12 x03Add x1a x0fconv2d Identity x1a x05add y x07 n x01T x12 x020 x01 Now a code that uses keras layers Code to reproduce the issue Traceback most recent call last File home nguerinjr Documents deep coding project teste.py line 16 in module tf.keras.experimental.export saved model model model.tf File home nguerinjr Documents deep coding project venv lib python3.7 site packages tensorflow python keras saving saved model.py line 166 in export saved model input signature File home nguerinjr Documents deep coding project venv lib python3.7 site packages tensorflow python keras saving saved model.py line 236 in save v1 format export mode mode keys.ModeKeys.TRAIN has saved vars export args File home nguerinjr Documents deep coding project venv lib python3.7 site packages tensorflow python keras saving saved model.py line 299 in export mode compile clone compile clone File home nguerinjr Documents deep coding project venv lib python3.7 site packages tensorflow python keras models.py line 538 in clone and build model clone clone model model input tensors input tensors File home nguerinjr Documents deep coding project venv lib python3.7 site packages tensorflow python keras models.py line 326 in clone model model input tensors input tensors layer fn clone function File home nguerinjr Documents deep coding project venv lib python3.7 site packages tensorflow python keras models.py line 202 in clone functional model model. insert layers ancillary layers relevant nodes relevant nodes File home nguerinjr Documents deep coding project venv lib python3.7 site packages tensorflow python keras engine network.py line 1633 in insert layers for node in layer.inbound nodes ValueError min arg is an empty sequence When using a keras layer it loses their inbound nodes I ve debugged it . It puts them apart from other layers but loses information of the objects it s not a question of passing or not custom objects as param . Now trying to use non experimental saves loads. Code to reproduce the issue A first annoying thing is that it s based on .ext. Even though it only saves in tf2 put any extension in load it verifies these extensions. It not a clear way of working in my opinion. Maybe some additional information on the files saved could make it not use the extensions. Traceback most recent call last File home nguerinjr Documents deep coding project teste.py line 26 in module tf.keras.models.load model model.keras.tf custom objects lambda lbd File home nguerinjr Documents deep coding project venv lib python3.7 site packages tensorflow python keras saving save.py line 141 in load model return saved model.load from saved model v2 filepath compile File home nguerinjr Documents deep coding project venv lib python3.7 site packages tensorflow python keras saving saved model.py line 1225 in load from saved model v2 model. training config pylint disable protected access File home nguerinjr Documents deep coding project venv lib python3.7 site packages tensorflow python training tracking base.py line 458 in method wrapper result method self args kwargs File home nguerinjr Documents deep coding project venv lib python3.7 site packages tensorflow python keras engine training.py line 337 in compile self. compile weights loss and weighted metrics File home nguerinjr Documents deep coding project venv lib python3.7 site packages tensorflow python training tracking base.py line 458 in method wrapper result method self args kwargs File home nguerinjr Documents deep coding project venv lib python3.7 site packages tensorflow python keras engine training.py line 1494 in compile weights loss and weighted metrics self.total loss self. prepare total loss masks File home nguerinjr Documents deep coding project venv lib python3.7 site packages tensorflow python keras engine training.py line 1595 in prepare total loss raise ValueError The model cannot be compiled ValueError The model cannot be compiled because it has no loss to optimize. The second is related to the loading is that it does not accept no loss to compile. Keras accepts it you can see in the examples . It accepts because it accounts for the non default losses added. This is another saving loading bug. Both experimental and non experimental functions seem to have some problems in saving loading. I think if you simulate some custom scenarios with keras you ll find a bunch of other errors. So it s worth to take a whole look at them specially considering that Keras is the default prototyping tool in tf2 . In the current situation i ve not thought of a simple and easy way to save keras models with custom components those which are not in the list of arguments of compile ,Hi nguerinjr Thanks for creating this issue. I ve also struggled with this but it s possible https github.com tensorflow tensorflow commit 1ad6aae48d97937d7caed2627a1ff1f0889b2cc7. The trick is here https github.com tensorflow tensorflow blob 1ad6aae48d97937d7caed2627a1ff1f0889b2cc7 tensorflow python keras saving hdf5 format test.py L821 to connect the loss calculation with the rest of the network. It s not ideal I know. There really should be an auxiliary outputs type argument in Model or add loss should retrace graph but the workaround isn t terrible in this case. Hope this helps. nguerinjr Can you please confirm if ppham27 s workaround is working for you. Hi ppham27 rmothukuru Yes that s working in the scenario of the codes I ve put here. I ll implement this workaround in my real code where I have a bunch of custom objects. As you ve mentioned ppham27 it s not ideal. Since there are those problems in the code it s a good deal to make things work this way. Thanks Great for what it s worth I have a pending internal change that will trace the history when using add loss and add metric so you don t need that line. Ideally it will land at the end of next week but I can t say for certain since I m not actually on the TensorFlow team. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30378 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30378 No a 
30383,intel tensorflow MKL throws could not initialize a memory descriptor CPU GPU work fine , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Red Hat Enterprise 7.6 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below 1.13.1 Python version 3.6 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N a Describe the current behavior Tensorflow with MKL DNN intel tensorflow throws exception could not initialize a memory descriptor in file tensorflow core kernels mkl concat op.cc 380 Describe the expected behavior No exception thrown. Code to reproduce the issue pip install intel tensorflow tar xvzf testcase 2367.tar.gz attached cd testcase 2367 python testcase 2367.py Other info logs This inference network runs fine on tensorflow CPU and tensorflow GPU. Only MKL DNN tensorflow fails. Yes this is similar to issue 23145 but it is definitely not fixed in r1.13.1. It is also not fixed in r1.14 which I confirmed by compiling from source although the line number changes . testcase 2367.tar.gz https github.com tensorflow tensorflow files 3357086 testcase 2367.tar.gz ,It seems mkldnn failed to create the memory description Further debug shows that the value of one input parameter mkl common format to create mkldnn memory descriptor is mkldnn blocked Which makes the construction of md failed and report the above error message. The memory format blocked of this op concat is inherited from the input tensor as for the input tensor in this case the input op is MklSlice check the function ComputeMklSlice it do create the memory in blocked format. Maybe that s the problem. plopresti Find a work around to fix the problem and get the result with your script outMax max 0.4230261 outMax min 0.4230261 Is the result make sense Leslie Fang Could you elaborate about your work around And do you think this is a bug in MKL DNN TensorFlow or our script network Thanks Should this be assigned to Intel tensorflow Paging agramesh1 plopresti We will look into this issue and get back to you soon. nhasabni Any update on this Can should I open an issue against MKL DNN Let me know if there is anything else I can do. Thanks plopresti Code fixing the issue is coming up soon. It is not an issue of MKL DNN. We are fixing it inside TensorFlow. Please stay tuned for the next few days. Thanks for reporting and trying TensorFlow on Intel architectures plopresti I believe the patch has been merged to the master branch now. Could you have a try this fix not in master as of yet it s available in the 1.15 branch. Can you please try in this branch preethivenkatesh plopresti Master has the merged code https github.com tensorflow tensorflow commit 15bd4863bd63968ad0396493d2c41bd0e5d8390b as a result of merging https github.com tensorflow tensorflow pull 31777 Confirmed that we can now run inference on this case and others we are using. Thank you Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30383 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30383 No a 
30417,Mixed precision mode in keras AutoCastVariable object is not subscriptable, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 LTS TensorFlow installed from source or binary official Tensorflow docker tensorflow tensorflow 1.14.0 gpu py3 TensorFlow version use command below 1.14.0 Python version 3.6.8 Describe the current behavior The following error occurs in a given code below when trying to enable mixed precision mode with keras.mixed precision.experimental.set policy infer float32 vars It looks like ResourceVariable which is returned by self.add weight with mixed precision is OFF supports slice operations while AutoCastVariable returned by self.add weight when mixed precision is ON doesn t. It s possible to workaround this issue by converting this variable into tensor as shown on line 15 but it s not clear if it s a straightforward way to perform slice op on Variable. Describe the expected behavior Should work without any error. Code to reproduce the issue ,Reproduced the Error with TF Version 1.14 Thank you for the bug report. This should be fixed by 604988b5d4e8cec6564db6502e6e40eefac8fc67. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30417 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30417 No a 
30423,2.0 beta s zero output for mask produces unexpected zero filling behaviors for GRU LSTM., em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04.2 LTS in GOOGLE COLAB Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device None TensorFlow installed from source or binary binary TensorFlow version use command below v2.0.0 beta0 16 g1d91213fe7 2.0.0 beta1 Python version Python 3.6.8 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version CUDA Version 10.0 GPU model and memory Tesla T4 16G You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior The call methods of tf.keras.layers.GRU and others e.g. LSTM does not produce output as expected for zero output for mask when the layer is created with return sequences True . Regardless of the value zero output for mask the result of call fills zeros for masked timestamps. Describe the expected behavior It should produce outputs as the masked timestamps should be filled zeros if zero output for mask is True. If false the masked timestamps should be filled with previous outputs. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. I suspect that tf2.0 alpha s GRU s call is changed in tf2.0 beta version. PLEASE refer FINAL line of both snippets. beta version snippet of GRU s call in recurrent v2.py alpha version snippet of GRU s call in recurrent.py UnifiedGRU , jangmino Please provide us the error log. And also provide us the complete code to replicate the issue on our environment. Thanks jangmino Please provide us the error log. And also provide us the complete code to replicate the issue on our environment. Thanks I downloaded my colab file as ipynb and packed as zip file. There was no error but only UNEXPECTED output. You can watch the output by running my colab. Reproducing Code Unexpected zero filling behaviors for zero ouputput for mask TF 2 0 beta .zip https github.com tensorflow tensorflow files 3376099 Reproducing Code Unexpected zero filling behaviors for zero ouputput for mask .TF 2 0 beta.zip I was able to reproduce the reported issue on Colab with tensorflow gpu 2.0.0.beta1.Thanks jangmino Could you check whether the issue with latest TF. I ran your code with pip install tf nightly gpu 2.0 preview . Here is the gist https colab.sandbox.google.com gist jvishnuvardhan f4e161a207b4a835f0a5227dd688b9e4 reproducing code unexpected zero filling behaviors for zero ouputput for mask tf 2 0 beta.ipynb . Please close the issue if you think the issue was resolved. Thanks Automatically closing this out since I understand it to be resolved but please let me know if I m mistaken.Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30423 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30423 No a 
30471, TF2.0 Bug when export BatchNormalization layer, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 beta0 Python version python 3.6.8 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version cuda 10 GPU model and memory GTX 1080ti Describe the current behavior Describe the expected behavior There should be no error message. Code to reproduce the issue Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,Was able to reproduce the reported issue with Tensorflow 2.0.0.beta0 and It works as expected with Tenosrflow 2.0.0.beta1. Thanks My problem is solved. Thank you very much. gadagashwini Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30471 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30471 No a 
30474, TF2.0 Bug allowing misuse of the batch dimension of a convolution layer, tensorflow 1.14.0 rightfully complains about the following minimal example with ValueError could not broadcast input array from shape 20 6 6 32 into shape 10 6 6 32 . tensorflow 2.0.0 beta1 however happily runs it and prints 20 6 6 32 . As per discussion https groups.google.com a tensorflow.org forum topic testing txsgcR3cubQ this seems to be a bug in TF 2.0.,I am able to reproduce the issue on Colab with Tensorflow 1.14.0 and works as expected with 2.0.0.beta1. gadagashwini I am able to reproduce the issue on Colab with Tensorflow 1.14.0 and works as expected with 2.0.0.beta1. You mean the other way around right I.e. the ValueError we get with 1.14.0 should be the correct behavior while not raising this exception in 2.0.0 beta1 seems to be the problem. gadagashwini I am able to reproduce the issue on Colab with Tensorflow 1.14.0 and works as expected with 2.0.0.beta1. You mean the other way around right I.e. the ValueError we get with 1.14.0 should be the correct behavior while not raising this exception in 2.0.0 beta1 seems to be the problem. Yes Dobiasd I could able to get the ValueError with Tensorflow 1.14.0 and no exception with Tensorflow 2.0.0.beta1 .Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30474 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30474 No a Dobiasd With tf nightly 2.0 preview the code seems to work correctly yongtang With tf nightly 2.0 preview the code seems to work correctly Thanks for the confirmation. I guess this was to be expected since the fix https github.com tensorflow tensorflow commit 37fcf0a0e04b2014864936397c25e6c398135772 includes an explicit test for this. 1 
30476,Op Less bfloat16 registered twice,In the 1.14 branch the CPU kernel op Less is registered twice for the type bfloat16 see https github.com tensorflow tensorflow blob r1.14 tensorflow core kernels cwise op less.cc L19 first on line 19 and then on line 21 . This has the effect of throwing an error InvalidArgumentError Multiple OpKernel registrations match NodeDef whenever a bfloat16 comparison on a CPU is attempted. ,Added a PR 30479 for the fix. The fix looks good. Note that the same problem is present in master https github.com tensorflow tensorflow blob master tensorflow core kernels cwise op less.cc L19 Please fix the same issue with bfloat16 registration for less equal in 1.14 https github.com tensorflow tensorflow blob master tensorflow core kernels cwise op less equal.cc https github.com tensorflow tensorflow blob r1.14 tensorflow core kernels cwise op less equal.cc L19 Automatically closing this out since I understand it to be resolved by the PR 30479 merged already but please let me know if I m mistaken.Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30476 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30476 No a 30479 resolves less op issue. I do not see that it addresses less equal op issue. mangushev Sorry I didn t see the comment about less equal previously. Have added a PR 31459 to fix LessEqual . 
30486,Keras TimeDistributed on a Model creates duplicate layers and is inconsistent with TimeDistributed on a Layer, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 TensorFlow installed from source or binary Binary TensorFlow version use command below tensorflow gpu 1.14.0 Python version 3.6.7 CUDA cuDNN version 10 GPU model and memory GTX1060M Describe the current behavior Wrapping a model in a TimeDistributed layer creates duplicate nodes in the graph. If we follow the docs link https keras.io getting started functional api guide all models are callable just like layers and create a simple Dense Model wrapped in a TD Layer You end up with this bad td https user images.githubusercontent.com 24449147 60799985 d4e60c00 a1a6 11e9 84e8 e0b6ddd2a789.png Firstly if you follow the documentation approach you end up with an additional Dense Layer Bottom Left . This eats up memory and happens because you build the inner model then rebuild it again when you build the TimeDistributed model. This can be avoided by parameterizing your model link https www.tensorflow.org beta guide keras custom layers and models building models but it can be a very painful workaround if your model is complex. But for demonstrations sake here s the model as an object Here s the improved graph better td https user images.githubusercontent.com 24449147 60801864 58552c80 a1aa 11e9 9550 24068fffc43c.png So the additional Dense layer is gone but there are still two dense layers inside. They have different contents too. better td internal https user images.githubusercontent.com 24449147 60801763 25ab3400 a1aa 11e9 9721 00482828ed58.png Describe the expected behavior If you compare to what you get if you just wrap the Dense layer itself good td https user images.githubusercontent.com 24449147 60800184 5c337f80 a1a7 11e9 8dd3 901054a537d9.png I d expect wrapping a model should result in a very similar looking graph to wrapping a layer Code to reproduce the issue Full code for creating the graphs https gist.github.com LukeBolly 0efeca3db275dee97c5f0fbf1f400b5a ,If you define your custom model as a Layer instead of a Model it produces the expected graph without the additional nodes LukeBolly this seems to be fixed in the latest nightly could you please try updating Otherwise it may be an issue with TensorBoard but the layers in the outer Model look good to me as far as training in Keras This is fixed with tf nightly 2.1.0 dev20200109 version. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 30486 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 30486 No a 
30533, TF2.0 trainable True on tf.keras.Embedding result in constant folding failed Invalid argument Unsupported type 21 , em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 x64 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device No TensorFlow installed from source or binary Binary TensorFlow version use command below tensorflow gpu 2.0.0 beta1 Python version 3.7.3 Bazel version if compiling from source No GCC Compiler version if compiling from source No CUDA cuDNN version CUDA 10.0.130 411.31 cuDNN 7.5.1.10 GPU model and memory GTX 1060 6GB You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior When enabling training on Embedding layer using keras subclassed model an error is raised during model.fit 2019 07 09 11 00 33.231719 E tensorflow core grappler optimizers meta optimizer.cc 502 constant folding failed Invalid argument Unsupported type 21 This has a huge impact on training speed that scales with the amount of data in my full code each epoch take more than 1 minute to complete dataset of 869 tokenized strings for train an encoder decoder model the code for reproduce this error dont have a noticeable impact because there is only one sample already tokenized. The error only is showed when an Embedding layer with trainable True is followed by an LSTM GRU layer Describe the expected behavior LSTM GRU supporting zero masking and having no impact of training speed Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. What the minimal code example above show in the console for me Edit 01 i think is somewhat ralated to this https github.com tensorflow tensorflow issues 29525 issuecomment 509154149 on https github.com tensorflow tensorflow issues 29525,I manage to throw out some warnings leaving exclusively 2019 07 09 17 19 34.150417 E tensorflow core grappler optimizers meta optimizer.cc 502 constant folding failed Invalid argument Unsupported type 21 The training speed impact of this error is still there and the unique way i found for disable it completely is making trainable False on the layer constructor self.embedding tf.keras.layers.Embedding 300 2 trainable False here is the new code As you can see now instead of use mask zeros True in the Embedding layer constructor i leave it false as default and use the tf.keras.layers.Masking this give me the masking feature without any errors. The only one still there is the topic of the Issue. ElPapi42 I tried reproducing the issue and i do not see any errors while fitting the model. Please see https colab.sandbox.google.com drive 11g3kQiNYk4XTzp4h550 gPGCbUdemGGR scrollTo ewDZ7 374EJO for your reference.Thanks ravikyram I tried reproducing the issue too the reported warnings can be found in the collab runtime logs ElPapi42 I tried reproducing the issue and i do not see any errors while fitting the model. Please see https colab.sandbox.google.com drive 11g3kQiNYk4XTzp4h550 gPGCbUdemGGR scrollTo ewDZ7 374EJO for your reference.Thanks I dont have access to the link request an account with access permisions results https user images.githubusercontent.com 51902062 61029611 0cd49580 a3d9 11e9 93a7 608bef26a063.png Please see screenshot.Thanks ravikyram I tried reproducing the issue too the reported warnings can be found in the collab runtime logs ravikyram Please look at the runtime logs. The fact that this kind of issue will only show up in logs when ran in collab has already been noted in 30263 and this is an actual issue reported on multiple occasions but seemingly unsolved as of now... Adding Eugene who is the expert of grappler. rmlarsen rings any bells Any advance on this its really annoying I manage to throw out some warnings leaving exclusively 2019 07 09 17 19 34.150417 E tensorflow core grappler optimizers meta optimizer.cc 502 constant folding failed Invalid argument Unsupported type 21 The training speed impact of this error is still there and the unique way i found for disable it completely is making trainable False on the layer constructor self.embedding tf.keras.layers.Embedding 300 2 trainable False here is the new code As you can see now instead of use mask zeros True in the Embedding layer constructor i leave it false as default and use the tf.keras.layers.Masking this give me the masking feature without any errors. The only one still there is the topic of the Issue. I discover something in the code above i make sigmoid the activation of the lstm for get rid of 2019 07 19 09 01 52.787220 W tensorflow core grappler optimizers implementation selector.cc 199 Skipping optimization due to error while loading function libraries Invalid argument Functions inference backward cudnn lstm 395 571 and inference backward cudnn lstm 395 571 specialized for Adadelta gradients lstm lstm 1 StatefulPartitionedCall grad StatefulPartitionedCall at inference keras scratch graph 1552 both implement lstm ca0befc9 b0cc 4e66 adc9 7da601846478 but their signatures do not match activation sigmoid avoid use the cdnn implementation of the LSTM layer. Playing around with the code i notice that leaving activation tanh allowing cDDN implementation suppress the error Unsupported Type 21 but creates a new one related to the signatures the one that push me before to make activation sigmoid on lstm layer here is the new code that not produce Unsupported Type 21 error This can point that the Unsupported Type 21 error is produced on the Standard LSTM GRU implementation of Tensorflow The problem with the new error is that tells skip the optimization completely and that sounds bad PD as you can see the embedding layer has trainable True and Unsupported type 21 is not showing Obviously the Unsupported Type 21 Error is something that must be checked there is something broke This was fixed in https github.com tensorflow tensorflow commit 24174643a75e819b8ce01fd70d45d03616e50071 should be in nightly build For the implementation selector problems I think there is work in progress by qlzh727. Can you give the pip command for install tf2.0 nightly build any advances on this issue Any patches or solutions please we need some help . thank per advance This was fixed in 2417464 should be in nightly build pip install tf nightly gpu 2.0 preview or pip install tf nightly 2.0 preview depending whether you want GPU support or not you could also want and get a nightly build of tf 1.15 instead That being said the fix that was deployed three weeks ago merely avoids the warning message which saves a little time and avoids wary print outs but I believe the underlying issue has not been fixed yet hopefully this is indeed work in progress . ElPapi42 Is this still an issue Can you check with TF2.0 and let us know whether the issue persists with latest TF version. I ran with TF2.0 and don t see any constant folding error. Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 6ebdf409b24e112f7979fc0e31d5ee8e untitled541.ipynb Thanks I think this was solved but i dont have time to check right now. The project where this appeared is in an unusable state right now must work on it for test. I am closing the issue as it was resolved. Please feel free to open it if the issue persists again. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 30533 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 30533 No a 
30554,Can t set an initial state of the tf.keras.layers.Bidirectional, System information Have I written custom code yes OS Platform and Distribution macOS 10.14.15 Darwin 18.6.0 x86 64 i386 64bit TensorFlow installed from binary TensorFlow version 1.14.0 Python version 3.6.0 Describe the current behavior tf.keras.layers.Bidirectional wrapped around a tf.keras.layers.LSTM instance raises a TypeError when trying to pass it an initial state through the initial state argument of the call . Describe the expected behavior No error is raised and the joint initial state is properly distributed by the Bidirectional between the underlying forward and backward LSTM instances. Code to reproduce the issue Other info logs Similar issue https github.com tensorflow tensorflow issues 28761 is resolved in the tensorflow 2.0 code. The corresponding code fix in https github.com tensorflow tensorflow commit 2a8f9b1ccfaaebd6f9cf5b5eb972c2dafded4f5e seems to be relevant for this issue in tensorflow 1.x too.,I have tried on colab with TF version 1.14 and was able to reproduce the issue.Thanks From basic unexhaustive testing and as pointed out by aakhundov simply copy pasting the definition of the tf.keras.layers.Bidirectional.call from branch r2.0 to branch r1.14 seems to fix the issue without breaking things around. I guess a PR should be opened to do so and CI testing should validate this. aakhundov as you identified the issue maybe you could should do it pandrey fr I ve tried the copy paste that you d suggested and it indeed fixed the issue at least in my code snippet above. Regarding the PR I see the changes in the Bidirectional code from https github.com tensorflow tensorflow commit 2a8f9b1ccfaaebd6f9cf5b5eb972c2dafded4f5e already reflected in the master branch. Does this mean that the fix will appear in the next 1.x release and we just have to wait until then Hmm I do not know since the master branch seems to cover both 1.x and 2.x releases its versioning feels quite blurry to me . I would have gone for a PR to the r1.14 branch but I do not know whether that is the right behaviour. jvishnuvardhan any advice aakhundov I cannot reproduce the issue with tf nightly. Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 33931741ea0f5c161b043dbd0d0fab89 tf keras bidirectional 30554.ipynb . This might have been resolved. Thanks jvishnuvardhan it works with tf nightly indeed as the fix is already in the master branch. Can the fix also be incorporated into 1.14 somehow or should we just wait for 1.15 to be released and use tf nightly in the mean time Thank you aakhundov I think I would use tf nightly. qlzh727 Any thoughts on incorporating this in 1.14 Thanks I think the 1.14 release has already been published and we probably won t change it unless there is a dramatic issue. Given the fact that its fixed in nightly and 1.15 is not far away probably just wait for next release qlzh727 sure will wait for 1.15. Thank you Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30554 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30554 No a 
30564,Output of sysconfig.get link flags does not seem to be suitable for Mac, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac High Sierra 10.13.6 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary Binary TensorFlow version use command below v1.14.0 rc1 22 gaf24dc91b5 1.14.0 Python version Python 3.6.6 v3.6.6 4cf1f54eb7 Jun 26 2018 19 50 54 Bazel version if compiling from source GCC Compiler version if compiling from source Apple LLVM version 9.0.0 clang 900.0.39.2 CUDA cuDNN version GPU model and memory You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior When running the example of creating a custom op from https www.tensorflow.org guide extend op on a Mac the compilation stage see https www.tensorflow.org guide extend op build the op library fails with the following error message This appears to be because the specified linker option l libtensorflow framework.1.dylib returned by sysconfig.get link flags is not valid for ld on the Mac. The man page for ld says the following I believe that the correct format for this flag is ltensorflow framework.1 since the linker will prepend lib and append .dylib . A workaround for this is to replace this line With this Alternatively there is I believe a fix for the underlying cause here https github.com pio neil tensorflow pull 1 files Describe the expected behavior The linking process should be performed successfully and produce a library file called zero out.so. Code to reproduce the issue On a Mac create a file called zero out.cc with the following code copied from https www.tensorflow.org guide extend op Then run You should see an error like this ,I have seen the same issue. Suggested fix works for me The issue has been fixed through PR 30656 Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30564 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30564 No a 
30574,Decode wav sample rate output cannot be passed to tf.signal.linear to mel weight matrix, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Combined output of decode wav with the sample in signal mfccs from log mel spectrograms https www.tensorflow.org versions r2.0 api docs python tf signal mfccs from log mel spectrograms OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device No TensorFlow installed from source or binary Conda binary TensorFlow version use command below 2.0.0 dev20190702 git version unknown Python version 3.6.7 Bazel version if compiling from source No GCC Compiler version if compiling from source No CUDA cuDNN version GPU model and memory Surface Book Nvidia GPU Describe the current behavior The output of decode wav is tuple of wav data sample rate Sample rate is int32 but linear to mel weight matrix expects a float32 sample rate. If the sample rate is cast using tf.cast sample rate float32 and then a TypeError is thrown with the message TypeError Using a tf.Tensor as a Python bool is not allowed. Use if t is not None instead of if t to test if a tensor is defined and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor. Describe the expected behavior Sample rate output of decode wav can be used as input to linear to mel weight matrix. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. TypeError Traceback most recent call last ipython input 61 aaa96a9258a6 in module 1 Build datasets 2 train ds build data pairs from dir train dir 3 test ds build data pairs from dir test dir ipython input 60 92023d8b5863 in build data pairs from dir source dir 54 55 Convert to MEL 56 clean mel ds clean path ds.map load and mel file num parallel calls AUTOTUNE 57 noisy mel ds noisy path ds.map load and mel file num parallel calls AUTOTUNE 58 c users benhe .conda envs homl2 lib site packages tensorflow core python data ops dataset ops.py in map self map func num parallel calls 1887 return DatasetV1Adapter 1888 ParallelMapDataset 1889 self map func num parallel calls preserve cardinality False 1890 1891 deprecation.deprecated None Use tf.data.Dataset.map c users benhe .conda envs homl2 lib site packages tensorflow core python data ops dataset ops.py in init self input dataset map func num parallel calls use inter op parallelism preserve cardinality use legacy function 3333 self. transformation name 3334 dataset input dataset 3335 use legacy function use legacy function 3336 self. num parallel calls ops.convert to tensor 3337 num parallel calls dtype dtypes.int32 name num parallel calls c users benhe .conda envs homl2 lib site packages tensorflow core python data ops dataset ops.py in init self func transformation name dataset input classes input shapes input types input structure add to graph use legacy function defun kwargs 2677 resource tracker tracking.ResourceTracker 2678 with tracking.resource tracker scope resource tracker 2679 self. function wrapper fn. get concrete function internal 2680 if add to graph 2681 self. function.add to graph ops.get default graph c users benhe .conda envs homl2 lib site packages tensorflow core python eager function.py in get concrete function internal self args kwargs 1418 Bypasses error checking when getting a graph function. 1419 graph function self. get concrete function internal garbage collected 1420 args kwargs 1421 We re returning this concrete function to someone and they may keep a 1422 reference to the FuncGraph without keeping a reference to the c users benhe .conda envs homl2 lib site packages tensorflow core python eager function.py in get concrete function internal garbage collected self args kwargs 1412 if self.input signature 1413 args kwargs None None 1414 graph function self. maybe define function args kwargs 1415 return graph function 1416 c users benhe .conda envs homl2 lib site packages tensorflow core python eager function.py in maybe define function self args kwargs 1716 graph function self. function cache.primary.get cache key None 1717 if graph function is None 1718 graph function self. create graph function args kwargs 1719 self. function cache.primary cache key graph function 1720 return graph function args kwargs c users benhe .conda envs homl2 lib site packages tensorflow core python eager function.py in create graph function self args kwargs override flat arg shapes 1602 arg names arg names 1603 override flat arg shapes override flat arg shapes 1604 capture by value self. capture by value 1605 self. function attributes 1606 c users benhe .conda envs homl2 lib site packages tensorflow core python framework func graph.py in func graph from py func name python func args kwargs signature func graph autograph autograph options add control dependencies arg names op return value collections capture by value override flat arg shapes 784 converted func 785 786 func outputs python func func args func kwargs 787 788 invariant func outputs contains only Tensors CompositeTensors c users benhe .conda envs homl2 lib site packages tensorflow core python data ops dataset ops.py in wrapper fn args 2671 attributes defun kwargs 2672 def wrapper fn args pylint disable missing docstring 2673 ret wrapper helper args 2674 ret structure.to tensor list self. output structure ret 2675 return ops.convert to tensor t for t in ret c users benhe .conda envs homl2 lib site packages tensorflow core python data ops dataset ops.py in wrapper helper args 2616 nested args nested args 2617 2618 ret autograph.tf convert func ag ctx nested args 2619 If func returns a list of tensors nest.flatten and 2620 ops.convert to tensor would conspire to attempt to stack c users benhe .conda envs homl2 lib site packages tensorflow core python autograph impl api.py in wrapper args kwargs 220 except Exception as e pylint disable broad except 221 if hasattr e ag error metadata 222 raise e.ag error metadata.to exception type e 223 else 224 raise TypeError in converted code ipython input 60 92023d8b5863 27 load and mel file linear to mel weight matrix tf.signal.linear to mel weight matrix c users benhe .conda envs homl2 lib site packages tensorflow core python ops signal mel ops.py 155 linear to mel weight matrix lower edge hertz upper edge hertz dtype c users benhe .conda envs homl2 lib site packages tensorflow core python ops signal mel ops.py 74 validate arguments if sample rate 0.0 c users benhe .conda envs homl2 lib site packages tensorflow core python framework ops.py 692 bool raise TypeError Using a tf.Tensor as a Python bool is not allowed. TypeError Using a tf.Tensor as a Python bool is not allowed. Use if t is not None instead of if t to test if a tensor is defined and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor. , jbgh2 I tried reproducing the issue on Colab with Tensorflow 2.0.0 dev20190709 but i got the below error InvalidArgumentError Expected tf.Tensor False shape dtype bool to be true. Summarized data b No files matched pattern .wav . Please help us to reproduce the issue. Thanks gadagashwini You need to have at least one .wav file in your working directory otherwise it will obviously not run. jbgh2 I tried executing the .wav file but I am getting the following error ValueError sample rate was a non constant Tensor. Must be a Python float or a constant Tensor. . Thanks Bug repro in Colab using TF Versions 2.0.0 beta1 https colab.research.google.com drive 139qvpTBQH079 z RFfeTcbJqixV2oL G As far as I can tell the bug is because validate arguments expects Python types not Tensors and decode wav returns sample rate as a Tensor. I ve included code to repro both problems 1 Sample Rate from decode wav is an int32 tensor that is compared to a Python 0.0 2 Cast sample rate to float32 tensor causes bool comparison error Thanks for the report I have a fix for this out for review. Fixed in 03ff87b. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30574 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30574 No a 
30609, more spurious deprecation warnings,Similar to 27897 System information OS Platform and Distribution Linux 4.14.79 x86 64 with Ubuntu 18.04 bionic TensorFlow installed from pip install tensorflow 2.0.0 beta1 TensorFlow version use command below v2.0.0 beta0 16 g1d91213fe7 2.0.0 beta1 Python version 3.6.8 This is happening in colab.sandbox.google.com Describe the current behavior When using new APIs that replaced old APIs you deprecation warnings as if you were still using the old API. Describe the expected behavior If I use the new APIs I should not get deprecation warnings. Code to reproduce the issue ,I have discovered that if I build the model Sequentially the warnings at the end go away. I am getting the same error as adammichaelwood. Is there some new Feature Columns API on the way or are these actually spurious as indicated I have not seen any official TF 2.0 responses yes only a workaround. rohan100jain Rohan can you take a look how FeatureColumn APIs can be updated so that they don t print out deprecation warnings https github.com tensorflow tensorflow commit f70c46f8cd2a91b390455827cda65b5b5fe92ef0 should fix the spurious warnings. Closing bug now. Please re open open a new one if you still see warnings. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 30609 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 30609 No a 
30639,tf.while loop with tf.keras.layers.LSTM broken, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below the july 12 p36 gpu 2.0 nightly preview Python version 3.6 CUDA cuDNN version 10 7 GPU model and memory 3 GeForce GTX w 8 GB Describe the current behavior First I want to mention that the LSTM not working with distributed strategies is already being looked into here https github.com tensorflow tensorflow issues 29189 I wanted to highlight this as a separate issue because it likely has a different source... Basically when dynamically decoding a sequence with an LSTM and tf.while loop the code breaks see logs below for more detail . This does not happen with an RNN LSTMCell configuration but the LSTM is the only CuDNN access point aside from GRU which also does not work in this configuration . Describe the expected behavior The code should use the optimized CuDNN LSTM implementation and behave as the RNN LSTMCell approach i.e. not fail. Code to reproduce the issue https github.com jkamalu tensorflow bugs blob master LSTMGraphPlacement.py Other info logs 2019 07 12 11 37 10.386140 W tensorflow compiler jit mark for compilation pass.cc 1558 One time warning Not using XLA CPU for cluster because envvar TF XLA FLAGS tf xla cpu global jit was not set. If you want XLA CPU either set that envvar or use experimental jit scope to enable XLA CPU. To confirm that XLA is active pass vmodule xla compilation cache 1 as a proper command line flag not via TF XLA FLAGS or set the envvar XLA FLAGS xla hlo profile. 2019 07 12 11 38 30.548248 E tensorflow core grappler optimizers meta optimizer.cc 502 function optimizer failed Invalid argument Input 1 of node se q3 seq encoder while body 195 TensorListPushBack 49 was passed int32 from se q3 seq encoder while body 195 decoder c lstm 3 StatefulPartitionedCall 9 incompatible with expected variant. 2019 07 12 11 38 37.853257 E tensorflow core grappler optimizers meta optimizer.cc 502 function optimizer failed Invalid argument Input 1 of node se q3 seq encoder while body 195 TensorListPushBack 49 was passed int32 from se q3 seq encoder while body 195 decoder c lstm 3 StatefulPartitionedCall 9 incompatible with expected variant. 2019 07 12 11 38 39.689929 W tensorflow core common runtime process function library runtime.cc 672 Ignoring multi device function optimization failure Invalid argument Input 1 of node se q3 seq encoder while body 195 TensorListPushBack 77 was passed int32 from se q3 seq encoder while body 195 decoder c lstm 2 StatefulPartitionedCall 9 incompatible with expected variant. 2019 07 12 11 38 45.280991 I tensorflow stream executor platform default dso loader.cc 44 Successfully opened dynamic library libcublas.so.10.0 2019 07 12 11 38 45.755520 W tensorflow core framework op kernel.cc 1622 OP REQUIRES failed at partitioned function ops.cc 113 Invalid argument Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices job localhost replica 0 task 0 device GPU 0 vs job localhost replica 0 task 0 device CPU 0. The edge src node is while 20 exit 94 and the dst node is while 0 RetVal 2019 07 12 11 38 45.755562 W tensorflow core common runtime base collective executor.cc 216 BaseCollectiveExecutor StartAbort Invalid argument Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices job localhost replica 0 task 0 device GPU 0 vs job localhost replica 0 task 0 device CPU 0. The edge src node is while 20 exit 94 and the dst node is while 0 RetVal node se q3 seq encoder while body 195 decoder c lstm 2 StatefulPartitionedCall If 9 else 2424 gradients while grad while grad body 11561 gradients TensorArrayV2Read TensorListGetItem grad TensorListLength TensorListPopBack 1920 2019 07 12 11 38 45.755854 W tensorflow core common runtime base collective executor.cc 216 BaseCollectiveExecutor StartAbort Invalid argument Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices job localhost replica 0 task 0 device GPU 0 vs job localhost replica 0 task 0 device CPU 0. The edge src node is while 20 exit 94 and the dst node is while 0 RetVal node se q3 seq encoder while body 195 decoder c lstm 2 StatefulPartitionedCall I 11 38 49.971 NotebookApp Saving file at SEQ3 LSTM CUDA.ipynb , jkamalu Thank you for bringing this up. In the code mentioned by you in the link https github.com jkamalu tensorflow bugs blob master LSTMGraphPlacement.py we could not find tf.while loop. If the issue is with tf.while loop Can you please provide the code with tf.while loop. Thanks anush o Thanks for the reply. I m going off of the docs which say that a for loop which iterates over a tensor is converted to a tf.while loop with tf.function. See https www.tensorflow.org beta guide effective tf2 for while tf.while loop break and continue are supported With that understanding the tf.while loop is the post conversion of the python for loop I have that iterates over the tf.constant ... in the code I linked. Thanks for reporting the issue the while loop placement issue should be solved by ca7acecce5066f518b775da339b6258fafd3db23 and f0fd2bed4d5358ebdb8bae5243c1a86fd7967f27. Could u try the code again with the tf 2.0 nightly builds Thanks. jkamalu As qlzh727 mentioned it was resolved. I ran your code in tf nightly gpu 2.0 preview and I don t see any error. Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 5e4a2c43934f1f126563b47e3dcc6797 tf 30639 lstm.ipynb . I am closing this issue as it was resolved but please let me know if I m mistaken.Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30639 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30639 No a 
30685, TensorArray objects used as Dataset.reduce state lose inferred shapes, System information TensorFlow version use command below 2.0 Python version 3 Describe the current behavior TensorArray objects passed as accumulators to Dataset.reduce lose inferred shapes. Subsequent calls to TensorArray.concat returns a fully unknown shape. Describe the expected behavior The element shape of the TensorArray should be partially known consistent with the behavior of an equivalent tf.while loop . Code to reproduce the issue , jsimsa could you triage the issue aaudiber could you please take a look mdanatg Thank you for reporting this and for including simple repro instructions The issue is with how we use TensorArraySpec to convert the TensorArray to from Tensor components across the reduce boundary. As you noticed it loses the shape information. I have a CL out to fix it and will update once it is merged Now that 6cd69820a7ec68363647bf918d312b5d10e0e07a has merged inferred TensorArray shapes will be preserved across all tf.data operations. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30685 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30685 No a 
30686, tf.reduce called on the result of tf.TensorArray.concat of unknown rank return corrupted Tensor, System information TensorFlow version use command below 2.0 Python version 3 Describe the current behavior See the reproducing code. When tf.reduce mean is called on the output of a TensorArray.concat which had a fully dynamic shape including rank it produces a Tensor of scalar shape but whose value is a 1 element vector. Describe the expected behavior The resulting Tensor should have a scalar value consistent with its static shape. Code to reproduce the issue Note that the bug does not reproduce if the unknown shape Tensor is passed directly to reduce mean which would suggest some interaction between the two. , jaingaurav could you triage the issue morgangiraud pointed out that the issue manifests for tf.reduce sum as well and possibly other reduce ops. This issue is fixed in Tf nightly 2.2.0.dev20200312 version. Please find the gist here https colab.sandbox.google.com gist gadagashwini f6bffcfcc013b2316bd29f3d6b2d2714 untitled453.ipynb . Can we close this issue. Thanks It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 30686 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 30686 No a 
30691,Strange bug with uint64 and tf.constants, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary pip TensorFlow version use command below 1.13 Python version 2.7 Describe the current behavior Run the following code the output is while it should have been However the following code prints the correct output This is really weird ,If you set multiply constants and which are not used. Tensorflow would go through a FoldNode operation to do same convertion. I think that where the problem happens. Find a workaround to this problem Looking for root cause I could reproduce the issue in TF1.13 and even in tf nightly TF1.15 . However your code https colab.sandbox.google.com gist jvishnuvardhan 487e9949fbe04c58700ced437a153e32 tf 30691 ops.ipynb runs without any issue when tf nightly 2.0 preview 2.0.0.dev20190718 used.Thanks jvishnuvardhan Sorry to interrupt tf nightly 2.0 preview 2.0.0.dev20190718 is built based on master branch or any other branchs Leslie Fang f nightly 2.0 preview are built using TF2.0 https github.com tensorflow tensorflow tree r2.0 . Thanks piiswrong was this issue resolved by following Leslie Fang workaround Are you interested in trying TF2.0 Thanks Closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30691 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30691 No a 
30711,Keras custom metrics raises error when update state returns an op., em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 macOS Mojave 10.14.4 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below 1.14.0 Python version 3.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior I am trying to build a custom metric for Keras which worked with tensorflow 1.12. Now after upgrading to python 1.14 I get the error shown below. I am returning the result of tf.group in the update state method of the metric which is of course an Op. What puzzles me is that tensorflow.python.keras.utils.metric utils.update confusion matrix variables which is used by many of the other builtin metrics like Precision does the exact same thing. To make sure that the error is not caused by my own implementation I copied the implementation of tf.keras.metrics.Precision into my own file and tried to run it. It get the same error however when I substitute this custom metric with the builtin it works. The code to reproduce this is shown below. Describe the expected behavior The custom metric should work as expected. Code to reproduce the issue Other info logs ,Same error in my custom metric on yesterday nightly build of tf 2.0 dekromp I tried reproducing the issue with TF 1.14 and i didn t get any error. please take a look at Gist of Colab link https colab.research.google.com drive 1BUqCQgo2nuH9VQZyn4m1K6 1ANqLVWcA .Thanks dekromp I tried reproducing the issue with TF 1.14 and i didn t get any error. please take a look at Gist of Colab link https colab.research.google.com drive 1BUqCQgo2nuH9VQZyn4m1K6 1ANqLVWcA .Thanks Your link is private. anush o Hi anush o thanks for looking into this. I made a small error in my code example above which I fixed now . The code was still using tf.keras.metrics.Precision instead of Precision Sorry for that . You should be able to reproduce the error now. In the meantime I tried the above snippet on another computer Ubuntu 16.04 with a fresh conda environment and get the same error. I was able to replicate the issue with TF version 1.14. Thanks dekromp You can remove return statements and group ops from custom metrics. It is not required. Built in metrics have a different requirement because of an issue with TPUs. Once that is fixed we will remove the return from update state from built in metrics as well. Thank you pavithrasv That did the trick. Thanks for helping out. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30711 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30711 No a 
30713,Gradient tape with tf.math.reduce euclidean norm disconnects, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOS Mojave Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary Conda TensorFlow version use command below 2.0.0b1 Python version 3.7.3 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version N A GPU model and memory N A Running on CPU Describe the current behavior Using GradientTape to track the gradients using tf.math.reduce euclidean norm directly the gradient disconnects and returns None . Describe the expected behavior I expect a gradient to be returned. If I decompose the function into the three constituent sequential operators square the elements sum them reducing the dimension and square rooting the resulting tensor I get the gradient connecting up as expected. Code to reproduce the issue import tensorflow as tf x tf.constant 3.0 1.2 17 0 Calculate euclidian distance of an nD vector by tf implementation with tf.GradientTape as t2 t2.watch x z2 tf.math.reduce euclidean norm x axis 1 dz2 dx t2.gradient z2 x print z nGradients .format z2 dz2 dx Calculate euclidian distance of an nD vector by decomposed operations with tf.GradientTape as t t.watch x x sq tf.math.square x x sum sq tf.math.reduce sum x sq axis 0 z tf.math.sqrt x sum sq dz dx t.gradient z x print z nGradients .format z dz dx Other info logs The example code gives z 17.30433464050293 Gradients None z 17.30433464050293 Gradients 0.17336696 0.06934679 0.9824128 0. With the reduce euclidean distance operator the gradient is dropped.,I have tried on colab with TF version 2.0 beta1 and was able to reproduce the issue.Thanks FYI It s just not implemented yet https github.com tensorflow tensorflow blob c9443f07d2a5a5febcbaa415e27f871553661036 tensorflow python ops math grad.py L53 I think this could be confusing to users as it returns None without any explanation. This is a simple example but for a complex real world numerical script it could take good amount of time figuring out which part was the problem. Would it make sense to have a notion of not implemented which raises an exception or prints an error warning message instead of current ops.NotDifferentiale That is a very serious bug. ops.NotDifferentiable is to be used only on purpose when an operation is not differentiable at all for operations which are differentiable you do get an exception if you neither register a gradient nor mark it as notdifferentiable. However at this point removing this annotation would likely break existing code so we should fix this by implementing the gradient. Can you take a stab at it kkimdev sg started. Should be fixed on the master branch https github.com tensorflow tensorflow commit f2d7bc27a00f2ee08335c00782f98af0d04a08e2 Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30713 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30713 No a 
30736,GPU device creation fails when using the CUDA Malloc Allocator, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow no OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary source TensorFlow version use command below 1.14 master Python version 3.6 Bazel version if compiling from source 0.24.1 GCC Compiler version if compiling from source 7.4.0 CUDA cuDNN version 10.0 GPU model and memory RTX 2080 Describe the current behavior GPU device creation fails when the cuda malloc allocator is selected with the error No allocator statistics . This is because of an allocator stats check introduced between releases 1.13.1 and 1.14 in the gpu common runtime BaseGPUDeviceFactory CreateGPUDevice function https github.com tensorflow tensorflow blob r1.14 tensorflow core common runtime gpu gpu device.cc L1310 L1312 . This check fails when using the cuda malloc allocator because the virtual Allocator method GetStats was never overridden. Describe the expected behavior GPU device creation should succeed if the user specifies use of the cuda malloc allocator. Code to reproduce the issue Use the TF GPU ALLOCATOR environment variable to select the cuda malloc allocator export TF GPU ALLOCATOR cuda malloc Then in a python shell try to use the gpu ,Submitted PR 30737 as a proposed solution to implement the GetStats function thereby not failing the if stats check in gpu device.cc Looks like my PR was successfully merged closing the issue. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30736 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30736 No a Im encountering the same issue 48571 
30750,tf.sparse.to dense does not work on sparse tensor with string values. , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04.2 LTS Bionic Beaver Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below 1.14.0 Python version 3.6.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Describe the current behavior Trying to convert a SparseTensor of type string into the corresponding dense tensor using tf.sparse.to dense throughs an exception TypeError Expected string passed to parameter default value of op SparseToDense got 0 of type int instead. Error Expected string got 0 of type int instead. Describe the expected behavior Like with using an integer valued SparseTensor I expect tf.sparse.to dense to return a dense tensor with string values. Code to reproduce the issue There is also a colab notebook where you can execute the code directly https colab.research.google.com drive 1OxZCVWdnEYAkmGDZ7hAvP2mkFbxYX4Zh https colab.research.google.com drive 1OxZCVWdnEYAkmGDZ7hAvP2mkFbxYX4Zh Other info logs I came across this issue while reading in some TFRecords that included VarLenFeatures with string typed lists. Everything worked fine with the VarLenFeatures with integers. Right now I have not even found a way to convert the SparseTensor s string values into a new Tensor as a workaround. ,Add a PR 30781 for the fix. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30750 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30750 No a 
30769,tf. version raises an AttributeError exception, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOSX 10.13.6 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below Python version 3.6.8 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior tf. version raises an AttributeError exception. Describe the expected behavior tf. version should return the version tf.version.VERSION to respect PEP 396 https www.python.org dev peps pep 0396 . Code to reproduce the issue Other info logs Here s the exception ,To me it seems version should be initialized here tensorflow python framework versions.py However if I run the code as ageron on the tf nightly version I get the same issue. Here the full error log The process of the declaration of the version is described nicely at stackoverflow https stackoverflow.com questions 50427475 checking tensorflow version in python tf version vs tf version rq 1 Maybe this helps find the source of this problem quicker. The issue is that module wrapping hides all symbols starts with from all so version is hided as well. Created a PR 30778 for the fix. ageron Issue is fixed in latest TF 2.0 nightly version. Can we close the issue. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30769 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30769 No a 
30802,Using tf.function while enumerating a dataset causes an infinite loop, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOS 10.14.5 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 beta1 Python version 3.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Describe the current behavior Using tf.function when enumerating a dataset will cause an infinite loop. Describe the expected behavior Using tf.function when enumerating a dataset should not change the looping behavior. Code to reproduce the issue The code snippet below will hang after the last function call. I m not printing anything because calling tf.print results in a syntax error on colab and I know that these snippets are being run on colab by you. When printing the variable i it s clear that the loop just never stops i.e. i increase indefinitely. Other info logs Output of the above snippet ,I was able to reproduce the issue on Colab with Tensorflow 2.0.0.beta1. Please have a look at gist of Colab https colab.research.google.com drive 1UXGUopt4bVq NUslCJVxw BHeOlq0Rkz link. Thanks AutoGraph does not currently override enumerate so it has the wrong behavior in graph mode. It would be a nice feature to add and it shouldn t be too hard to do it. In the mean time to enumerate over datasets please use ds.enumerate . It works both inside and outside tf.function mdanatg thank you that worked for me. I m gonna let this issue stay open as the underlying problem hasn t been fixed yet but feel free to close if you need to. mdanatg I m interested in working on this I m confused on how to get started. I ve read the documentation on AutoGraph tf.function decorator as well as the code https github.com tensorflow tensorflow blob r2.0 tensorflow python eager def function.py L725 L1027 and I can t spot which code is responsible for overriding such action. Pardon my inexperience. ilhamfp that s great happy to help you get started The dynamic dispatch can indeed make things a bit confusing. The place to add this is in the Python builtin overloads file https github.com tensorflow tensorflow blob master tensorflow python autograph operators py builtins.py The file contains a number of overloads check out len for instance that you can use as a model. Then once you have the overload of enumerate you just need to add it to the lists at the bottom of the file SUPPORTED BUILTINS and BUILTIN FUNCTIONS MAP there s a bit of duplication there . That should be it the overload for function calls https github.com tensorflow tensorflow blob master tensorflow python autograph impl api.py L390 placed in the api module for various reasons should pick it up at Python runtime. AutoGraph does not currently override enumerate so it has the wrong behavior in graph mode. Hi mdanatg . Just out of curiosity what do you mean by wrong behavior in graph mode With tensorflow 2.0alpha0 I often use enumerate to iterate over a list of layers within a call function as follows It would be grateful if you could explain a bit more. llan ml I was referring to calling enumerate with a tf.data.Dataset argument. Called with Python lists it was working fine. See the original post before 31038 calling test loop with enumerate with decorator in the original example caused an infinite loop which was incorrect. At any rate enumerate should now be fully supported for datasets. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30802 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30802 No a mdanatg AutoGraph does not currently override enumerate so it has the wrong behavior in graph mode. It would be a nice feature to add and it shouldn t be too hard to do it. Could you guys add this to the documentation https www.tensorflow.org api docs python tf data Dataset for tf.Dataset Seems like a quick addition that would prove super valuable. Thanks tgsmith61591 This issue should be resolved enumerate should now work correctly with datasets in tf.function with TF 1.5. Have you been still experiencing issues 
30804,tf.keras.layers.Conv2D fails because it captures tensor from inner function, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04 TensorFlow installed from source or binary source TensorFlow version use command below v1.12.1 6461 gc6352706d6 1.14.0 Python version 3.6.7 Bazel version if compiling from source 0.26.1 GCC Compiler version if compiling from source 7.4.0 CUDA cuDNN version 10.0 7.5 GPU model and memory Nvidia Geforce GTX 1080 Ti 11 GB Describe the current behavior Since commit 546308e322a6b95542ba9f3cbb14136128aaad1e a tf.keras.layers.Conv2D in my training code fails with the following error This error only occurs with dilation rate set to 2. With dilation rate 1 the training script runs. It also works if I revert 546308e322a6b95542ba9f3cbb14136128aaad1e . Describe the expected behavior The training runs with dilation rate 2 . Code to reproduce the issue So far I could not come up with a small testcase to reproduce the issue. I will continue to try but until then I only have the backtrace and the problematic commit., olesalscheider In order to expedite the trouble shooting process please provide complete code snippet to reproduce the issue reported here. Thanks You can use this as a testcase olesalscheider When i executed the given code with dilation rate 2 i got the error KeyError conv2d 4 dilation rate . Can you confirm if the same error is faced. Thanks No I get this error with the testcase Which version of tensorflow do you use I used the current master branch. I was able to reproduce the issue with pip install tf nightly gpu 2.0 preview 2.0.0.dev20190724 . Here is the gist https colab.sandbox.google.com gist jvishnuvardhan 9fbcb2e8c3b7bb622aeb61455dfff6cb untitled324.ipynb . Thanks Here is the gist https colab.sandbox.google.com gist jvishnuvardhan a2d5481c9d0f4393de6ad872824aa974 tf 30804.ipynb with tf nightly and the error is different KeyError conv2d dilation rate . Thanks olesalscheider this issue should be fixed in tf nightly. Will close it for now. Feel free to re open it if the issue appears again. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30804 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30804 No a 
30808,Serialization of keras object fails if called with different input sizes, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04 TensorFlow installed from source or binary source TensorFlow version use command below v1.12.1 6461 gc6352706d6 1.14.0 Python version 3.6.7 Bazel version if compiling from source 0.26.1 GCC Compiler version if compiling from source 7.4.0 CUDA cuDNN version 10.0 7.5 GPU model and memory Nvidia Geforce GTX 1080 Ti 11 GB Describe the current behavior When I try to save a function of tf.Module as saved model that calls another function with different input shapes it fails with the following error Describe the expected behavior The model can be saved successfully. Code to reproduce the issue The following testcase allows to reproduce the issue , olesalscheider I tried reproducing the issue on Colab with Tensorflow version 1.14.0. I didn t receive any error. Please have a look at Colab https colab.research.google.com drive 18DPNelOKIdJiXq2q8 oNHiTadqcuwRY link. Let us know is this expected behavior.Thanks This is a regression in the current master branch with tf2 API . The code above used to work with older Tensorflow versions including 1.14.0. I had to request access to the Colab link because my Google account has a different email address . I will look at it once I have access. I could reproduce the issue with tensorflow 2.0.0.beta1. Please find the gist here https colab.research.google.com drive 14 RzRKLpMb1OBQuk5LEOxAI4INA1yUwE . Thanks olesalscheider I think the ValueError is correct as there is a mismatch in the shape. If you use strides 1 1 then shapes are same and there is not error. Please throw little more details on why you think this as a bug Thanks You mean there is a mismatch in the input to Inner.call Yes that is true. But this just contains a fully convolutional layer which can operate on inputs of different sizes. Also using the model works fine Executing for example infer.infer tf.random.uniform 1 64 64 8 instead of saving the model runs without error. If the shape mismatch was a problem it should not run. I expect that I can save a model if I can run it Thanks for submitting this report I ve submitted a fix which I ll request to be cherrypicked into 2.0. I think the issue was resolved. Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 65552c805709d58ec94c99b2143e285a tf 30808 savedmodel.ipynb . Thanks Automatically closing this out since I understand it to be resolved but please let me know if I m mistaken.Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30808 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30808 No a 
30811,Saving of BatchNormalization layer fails, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04 TensorFlow installed from source or binary source TensorFlow version use command below v1.12.1 6461 gc6352706d6 1.14.0 Python version 3.6.7 Bazel version if compiling from source 0.26.1 GCC Compiler version if compiling from source 7.4.0 CUDA cuDNN version 10.0 7.5 GPU model and memory Nvidia Geforce GTX 1080 Ti 11 GB Describe the current behavior When I try to save a BatchNormalization layer as in the example code it fails with the following error Describe the expected behavior Saving succeeds without error. Code to reproduce the issue The following testcase can be used to reproduce the issue , olesalscheider I tried executing the code on Colab with Tensorflow 1.14.0. But I did not get any error. Please take a look at gist of Colab https colab.research.google.com drive 1A4jaDTzJ4M b8g86FnIaAv 7Dl55JCrK . Thanks Oh I forgot to mention The code above used to work with older versions of Tensorflow I think including 1.14.0 . This is a regression in the current master branch with tf2 API . I am able to reproduce the issue with Tensorflow 2.0.0.beta1. Please take a look at gist here https colab.research.google.com drive 1YgQAH apOpqzuY bdXiF8sCYCPvNbRQs . Thanks I am getting the exact same error. Anybody have any way to fix this I am using the nightly previews. I believe this should be fixed in TF 2.0 RC 0. Closing this but if you are still having issues please reopen this issue. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30811 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30811 No a 
30838,Unable to quantize buffer or min max value for input 1 in op MUL in subgraph 0 node 8, System information OS Linux Ubuntu 19.04 TensorFlow installed from source or binary binary TensorFlow version use command below v1.14.0 rc1 22 gaf24dc9 1.14.0 Python version 3.7.3 CUDA cuDNN version 10.0.130 7.4.1 GPU model and memory Tesla V100 SXM2 16GB Describe the current behavior Running the official tutorial on post training quantization https www.tensorflow.org lite performance post training quantization https github.com tensorflow tensorflow blob master tensorflow lite tutorials post training quant.ipynb results in following error message Unable to quantize buffer or min max value for input 1 in op MUL in subgraph 0 node 8 Describe the expected behavior No error Code to reproduce the issue Run the attached notebook with attached graph.pb and csv https drive.google.com open id 1JI45QqwhgR2v4i8GCShkH5eod0TTB7FI Download YTF Dataset as from here https www.cs.tau.ac.il wolf ytfaces ,I believe this is triggered Mul takes a constant input. In the attached graph this is found here image https user images.githubusercontent.com 35263153 64412735 5f0bfd00 d090 11e9 8ab8 7dbce93e78b7.png It can also be reproduced more simply with This fails with the error Meet the same problem. Any solution This issue was fixed in the master branch and in branch 1.15 seemingly by this commit https github.com tensorflow tensorflow commit a372bb0e9d77b3532eec1bf24a44bbf342673968 diff 69178ece8d1001b2ef7f9b329e9a525b. Closing per last comment. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 30838 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 30838 No a 
30843,Unable to train model on multiple GPUs using MirroredStrategy in TF2.0,I have two Tesla T4 GPUs from Google. I am using this example https www.tensorflow.org beta tutorials load data text split the dataset into text and train batches in order to train it on multiple GPUs with defined the model inside tf.distribute.MirroredStrategy as described here https www.tensorflow.org beta guide distribute strategy using tfdistributestrategy with keras . The model trains without tf.distribute.MirroredStrategy on single GPU normally but not when I want to train it on multiple GPUs. You can find the error log below. Another dummy example program in the docs for using MirroredStrategy is here https www.tensorflow.org beta guide distribute strategy using tfdistributestrategy with keras . On executing this it trains and utilises both GPUs. Between these two examples I noticed one thing which is the use of padded batch and batch . It works fine in the latter case. I also have a model of mine where I am using padded batch . This model also can not be trained and although the error there is a bit different. Please suggest any ways to get rid of this problem. THANK YOU Train on None steps Epoch 1 10 W0718 14 21 33.186410 139786698037056 cross device ops.py 764 Efficient allreduce is not supported for 1 IndexedSlices W0718 14 21 38.304410 139786698037056 cross device ops.py 764 Efficient allreduce is not supported for 1 IndexedSlices 2019 07 18 14 21 39.456127 E tensorflow core grappler optimizers meta optimizer.cc 502 implementation selector failed Invalid argument Invalid format of input node name replica 1 sequential bidirectional StatefulPartitionedCall replica 1 StatefulPartitionedCall 8 Expected forward node name index 2019 07 18 14 21 40.179202 W tensorflow core grappler optimizers implementation selector.cc 199 Skipping optimization due to error while loading function libraries Invalid argument Functions inference backward standard lstm 356864 357366 and inference backward standard lstm 356864 357366 specialized for StatefulPartitionedCall 1 at inference distributed function 360209 both implement lstm 3542e53d 8ce9 47ba b422 dc9202236064 but their signatures do not match. 2019 07 18 14 21 40.669817 W tensorflow compiler jit mark for compilation pass.cc 1483 One time warning Not using XLA CPU for cluster because envvar TF XLA FLAGS tf xla cpu global jit was not set. If you want XLA CPU either set that envvar or use experimental jit scope to enable XLA CPU. To confirm that XLA is active pass vmodule xla compilation cache 1 as a proper command line flag not via TF XLA FLAGS or set the envvar XLA FLAGS xla hlo profile. 2019 07 18 14 21 40.752732 I tensorflow stream executor platform default dso loader.cc 42 Successfully opened dynamic library libcublas.so.10.0 2019 07 18 14 21 51.150641 I tensorflow core kernels data shuffle dataset op.cc 111 Filling up shuffle buffer this may take a while 17050 of 50000 2019 07 18 14 22 00.643784 I tensorflow core kernels data shuffle dataset op.cc 162 Shuffle buffer filled. 2019 07 18 14 22 00.664615 W tensorflow core framework op kernel.cc 1546 OP REQUIRES failed at partitioned function ops.cc 113 Invalid argument Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices job localhost replica 0 task 0 device GPU 1 vs job localhost replica 0 task 0 device CPU 0. The edge src node is while 22 exit 102 and the dst node is while 2 RetVal 2019 07 18 14 22 00.664706 W tensorflow core common runtime base collective executor.cc 216 BaseCollectiveExecutor StartAbort Invalid argument Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices job localhost replica 0 task 0 device GPU 1 vs job localhost replica 0 task 0 device CPU 0. The edge src node is while 22 exit 102 and the dst node is while 2 RetVal node replica 1 sequential bidirectional StatefulPartitionedCall metrics accuracy div no nan ReadVariableOp 1 50 2019 07 18 14 22 00.664771 W tensorflow core common runtime base collective executor.cc 216 BaseCollectiveExecutor StartAbort Invalid argument Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices job localhost replica 0 task 0 device GPU 1 vs job localhost replica 0 task 0 device CPU 0. The edge src node is while 22 exit 102 and the dst node is while 2 RetVal node replica 1 sequential bidirectional StatefulPartitionedCall replica 1 metrics accuracy AssignAddVariableOp 1 41 2019 07 18 14 22 00.665117 W tensorflow core framework op kernel.cc 1546 OP REQUIRES failed at partitioned function ops.cc 113 Invalid argument Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices job localhost replica 0 task 0 device GPU 1 vs job localhost replica 0 task 0 device CPU 0. The edge src node is while 22 exit 100 and the dst node is while 0 RetVal 2019 07 18 14 22 00.665167 W tensorflow core common runtime base collective executor.cc 216 BaseCollectiveExecutor StartAbort Invalid argument Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices job localhost replica 0 task 0 device GPU 1 vs job localhost replica 0 task 0 device CPU 0. The edge src node is while 22 exit 102 and the dst node is while 2 RetVal node replica 1 sequential bidirectional StatefulPartitionedCall 2019 07 18 14 22 00.666533 W tensorflow core framework op kernel.cc 1546 OP REQUIRES failed at partitioned function ops.cc 113 Invalid argument Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices job localhost replica 0 task 0 device GPU 0 vs job localhost replica 0 task 0 device CPU 0. The edge src node is while 22 exit 101 and the dst node is while 1 RetVal 2019 07 18 14 22 00.679274 W tensorflow core framework op kernel.cc 1546 OP REQUIRES failed at partitioned function ops.cc 113 Invalid argument Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices job localhost replica 0 task 0 device GPU 0 vs job localhost replica 0 task 0 device CPU 0. The edge src node is while 22 exit 100 and the dst node is while 0 RetVal Traceback most recent call last File colab.py line 94 in module model.fit train data epochs 10 validation data test data File home rishabh .local lib python2.7 site packages tensorflow python keras engine training.py line 643 in fit use multiprocessing use multiprocessing File home rishabh .local lib python2.7 site packages tensorflow python keras engine training distributed.py line 681 in fit steps name steps per epoch File home rishabh .local lib python2.7 site packages tensorflow python keras engine training arrays.py line 294 in model iteration batch outs f actual inputs File home rishabh .local lib python2.7 site packages tensorflow python keras distribute distributed training utils.py line 813 in execution function return out.numpy for out in distributed function input fn File home rishabh .local lib python2.7 site packages tensorflow python eager def function.py line 428 in call return self. stateless fn args kwds File home rishabh .local lib python2.7 site packages tensorflow python eager function.py line 1335 in call return graph function. filtered call args kwargs pylint disable protected access File home rishabh .local lib python2.7 site packages tensorflow python eager function.py line 589 in filtered call t for t in nest.flatten args kwargs expand composites True File home rishabh .local lib python2.7 site packages tensorflow python eager function.py line 671 in call flat outputs self. inference function.call ctx args File home rishabh .local lib python2.7 site packages tensorflow python eager function.py line 445 in call ctx ctx File home rishabh .local lib python2.7 site packages tensorflow python eager execute.py line 67 in quick execute six.raise from core. status to exception e.code message None File home rishabh .local lib python2.7 site packages six.py line 737 in raise from raise value tensorflow.python.framework.errors impl.InvalidArgumentError 2 root error s found. 0 Invalid argument Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices job localhost replica 0 task 0 device GPU 1 vs job localhost replica 0 task 0 device CPU 0. The edge src node is while 22 exit 102 and the dst node is while 2 RetVal node replica 1 sequential bidirectional StatefulPartitionedCall defined at usr lib python2.7 threading.py 801 metrics accuracy div no nan ReadVariableOp 1 50 1 Invalid argument Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices job localhost replica 0 task 0 device GPU 1 vs job localhost replica 0 task 0 device CPU 0. The edge src node is while 22 exit 102 and the dst node is while 2 RetVal node replica 1 sequential bidirectional StatefulPartitionedCall defined at usr lib python2.7 threading.py 801 0 successful operations. 1 derived errors ignored. Op inference distributed function 360209 Function call stack distributed function distributed function, rishabhsahrawat In order to expedite the trouble shooting process please provide a code snippet to reproduce the issue reported here and also mention the TF version being used. Thanks anush o thank you for your questions. I am using TF2.0 as I mentioned it in the Title. I am using the exact same code as this example https www.tensorflow.org beta tutorials load data text split the dataset into text and train batches provided by TF except before the model definition I added 2 lines for the multiple GPU support. The lines are mirrored strategy tf.distribute.MirroredStrategy with mirrored strategy.scope ... This is my code file with in txt format could not upload .py file here. colab.txt https github.com tensorflow tensorflow files 3410258 colab.txt This has been fixed in the latest nightly can you try with 2.0 nightly and see if it s still failing same as https github.com tensorflow tensorflow issues 30513 guptapriya Hi Priya thank you for your helpful response. After updating TF to latest 2.0 nightly from here https pypi.org project tf nightly gpu 2.0 preview history it starts training on both GPUs however once the first epoch is finished it throws following error on providing test data consisting test dataset to validation data argument in model.fit . However no error when validation data is defined None in model.fit File home rishabh .local lib python2.7 site packages tensorflow core python keras engine training.py line 710 in fit use multiprocessing use multiprocessing File home rishabh .local lib python2.7 site packages tensorflow core python keras engine training distributed.py line 680 in fit steps name steps per epoch File home rishabh .local lib python2.7 site packages tensorflow core python keras engine training arrays.py line 436 in model iteration steps name validation steps File home rishabh .local lib python2.7 site packages tensorflow core python keras engine training arrays.py line 174 in model iteration steps per epoch File home rishabh .local lib python2.7 site packages tensorflow core python keras engine training arrays.py line 490 in get num samples or steps steps per epoch File home rishabh .local lib python2.7 site packages tensorflow core python keras engine training utils.py line 425 in check num samples if hasattr ins 0 shape TypeError function object has no attribute getitem It seems like a bug to me. Please give your suggestions. I also tried training another program of my own which also arises the same behaviour. Can you try setting validation steps in fit and see if that helps This is still a bug and we will look into it. thank you for your reply. I tried your suggestion and now validation data can also be used in model.fit function. Since validation steps require number of batches and lets say length of test data is 5000 not completely divisible by the batch size 64 so in this case it will skip the last data of length 8 that doesn t make any batch. Now my question is if there is a way to also include that or should I wait for the bug to be fixed. Sorry if the question is too naive. Hi great to know it worked for you. Once we do fix the bug you should be able to run the validation until the end. In the meantime perhaps you can try rounding up instead of rounding down when calculating the number of steps and see if that works BTW would you mind filing a separate bug for the error you saw in https github.com tensorflow tensorflow issues 30843 issuecomment 514534349 Will help us track it. Thanks Actually ignore the new bug part i found a similar bug here https github.com tensorflow tensorflow issues 28797 Hi Priya guptapriya thank you for your response again. I tried using the rounded up value and it worked. I thought it only takes integer values as input. D Glad to know it worked Thanks for testing out 2.0 Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30843 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30843 No a 
30847,DistributedDataset iteration fails with data of type string, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04.5 LTS TensorFlow installed from source or binary conda TensorFlow version use command below 1.14.0 Python version 3.6.8 CUDA cuDNN version 10.0 GPU model and memory 8 x Tesla P100 PCIE 16GB Describe the current behavior I have noticed an issue while iterating over a DistributedDataset using a tf.distribute.MirroredStrategy that contains data of type string with eager execution enabled. Iterating works perfectly well but a RuntimeError is raised once the end of the dataset is reached cf logs below . Describe the expected behavior The exception is never raised if the dataset does not contain string data iteration stops and the rest of the code is executed. Code to reproduce the issue Other info logs ,Can you explain the behavior you would expect I am not sure I understand I expect to be able to iterate over a dataset containing strings without having an exception being raised once the last batch has been seen as it is happening without distribution of the dataset . same problem with mine. i am using a dataset that contains a string tensor as one of the inputs of the model. the stirng tensor input is not a part of training but it is necessary to be in the dataset for writing predictions to file. and i am using distribute strategy. everytime when it reaches the end of the dataset it shows a RuntimeError . i am using tensorflow gpu 1.14.0 python 3.7.3 Ubuntu 18.04 and 3 GPUs of TITAN Xp here is my test code and here is the logs This seems related to the handling of last partial batch in distributed datasets. we will take a look I encounter the same error in Tensorflow 2.0. Any update Same issue in Tensorflow 2.0. Same issue here. I m breaking the loop before the last iteration. Any elegant solution to this Tensorflow version 2.0.0 Python version 3.7.5 Is there a known workaround for this Workaround If you are okay with not training on the partial batch you can drop the remainder batched dataset dataset.batch batch size drop remainder True Unfortunately in TF 2.1 bsaund s workaround does not work for me nor does this appear to have anything to do with the string data type Apologies for the delay here fixing this took a while but this has been fixed as of end of Feb. It should not happen in tf nightly or the TF 2.2 rcs. Please re open if you still see it there. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 30847 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 30847 No a guptapriya Can you please give reference of commit that fixes the issue There were a series of commits that build up to this. from what i can tell this was the last one https github.com tensorflow tensorflow commit 97cdd4d16a81a349696f10451b7d564bfa99664f diff afbab14e6c4bbefd105d7ad3a3b67b7b 
30849,tf.data.experimental.make csv dataset cannot decompress files, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow custom OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac OS 10.14.5 but also tested in RedHat 7 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary pip3 TensorFlow version use command below 1.13.1 Python version 3.7.3 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory CPU Describe the current behavior tf.contrib.data.make csv dataset cannot decompress gzip files Describe the expected behavior When compression type is set to GZIP it should decompress a gzip file Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. As a sanity check works as expected returning Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,I think the issue is that make csv dataset will try to probe the column name automatically if not present. In order to probe the column automatically it use python s csv module to check for the first line of the file. But the compression type was not accounted for in this situation. Once column is available then internally CsvDataset process the file correctly though. Created a PR 30867 for the fix. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30849 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30849 No a 
30869, NoneType object has no attribute fetch cloud tpu metadata when using TPU, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary Binary TensorFlow version use command below 1.14.0 Python version 3.6 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version 10.0 GPU model and memory Cloud TPU v2 Describe the current behavior I ve been using T2T training script using cloud TPU. After upgrading to TF 1.14.0 from TF 1.13.1 I m getting the error below The training script continues w o any further issue despite the error. Describe the expected behavior No error. Code to reproduce the issue Run https github.com tensorflow tensor2tensor speech recognition problems using transformer model on TPUs. Other info logs N A,I also meet this problem have you solved it w4 sjcho HeimingX Do you have issues with this colab https colab.sandbox.google.com github tensorflow tensor2tensor blob master tensor2tensor notebooks hello t2t.ipynb scrollTo wfF8 cW OXPN . Please let us know the location item which is breaking. Thanks jvishnuvardhan jhseu Also seeing this with tensorflow 1.14 and t2t 1.14 happy to help. This is running in batch on ctpu v3 with tf 1.14 runtime in graph mode . As you can see the error above self. cluster. fetch cloud tpu metadata is failing because self. cluster is None this is being passed to CloudTPUPreemptedHook in tpu estimator.py https github.com tensorflow estimator blob master tensorflow estimator python estimator tpu tpu estimator.py L3237 . See there has recently been a change to tpu estimator.py https github.com tensorflow estimator commit 6a8b06637a73d070ca5ccbeeb5fdc280c8aa9994 that modifies to where the latter wraps the former to include a check for the TPUClusterResolver object not being None here which seems like one solution i.e. avoid adding the problematic hook if we don t have a cluster resolver object . Another would be to ensure the cluster resolver gets initialized when the run config is constructed in t2t trainer lib.py here https github.com tensorflow tensor2tensor blob 2036ffe309b86bda367b1e687fafb114534500f9 tensor2tensor utils trainer lib.py L232 looking at the logic there the portion initializing the cluster resolver won t be reached when using TPUs via GKE because KUBE GOOGLE CLOUD TPU ENDPOINTS will be set. Idk if it will break something else but I m just going to try moving the initialization of the cluster resolver outside of the logic about whether master and KUBE GOOGLE CLOUD TPU ENDPOINTS are set. Or re build tf with the modification to tpu estimator.py mentioned above. The tpu runtime tf version would still be stock 1.14 but afaict that pertains to ops not session hooks. jvishnuvardhan jhseu w4 sjcho HeimingX So it does look like there s a side effect in moving the cluster resolver instantiation from its current place but patching in https gist.github.com cwbeitel 2ab6bb9c5c418d390bedb23b8c959c0b file patch tpu estimator py the check add preemption hook change mentioned above gets rid of the error and training appears to proceed normally. Yeah this ll be fixed in TF 1.15. You can also pass the cluster to the RunConfig instead of passing master directly and it should work. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30869 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30869 No a 
30892,tf.keras.models.load model fails on Sequential model, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04 TensorFlow installed from source or binary Binary TensorFlow version use command below tf nightly gpu 2.0 preview 2.0.0 dev20190718 Python version 3.7 Describe the current behavior Attempt to run tf.keras.models.load model on a tf.keras.Sequential with lazily generated inputs produces an error. Describe the expected behavior No error is thrown when running the script below. model2 is a clone of model . Code to reproduce the issue Other info logs Running the script above produces the following error ,Related to https github.com tensorflow tensorflow issues 28668. cc ymodak. I was able to reproduce the issue on Colab with Tensorflow gpu 2.0.0 dev20190718 version. from future import absolute import division print function unicode literals import tensorflow as tf if name main tf.config.set soft device placement True tf.debugging.set log device placement True Creates some tensors a tf.constant 1.0 2.0 3.0 4.0 5.0 6.0 b tf.constant 1.0 2.0 3.0 4.0 5.0 6.0 c tf.matmul a b print c I met the same problem with Tensorflow gpu 2.0.0 dev20190718. No such error if I ran it in the python shell same problem using tf1.14 model is this https github.com didi delta blob master delta models asr model.py L75 Same problem with MWE hartikainen Model cannot be saved because the input shapes have not been set. Usually input shapes are automatically determined from calling .fit or .predict . To manually set the shapes call model. set inputs inputs . When I add input shape input shape image shape to the first layer there is no error as you described. Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 19028b5d44b1dcb7b8593a1d932654a6 untitled559.ipynb . Thanks I am closing the issue as it was resolved. Please feel free to open it if the issue persists again. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 30892 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 30892 No a Hey jvishnuvardhan. I don t think your comment resolves the issue. I m aware that explicitly specifying the input shape makes the example work but my issue was specific to the case where we feed in an input to the Sequential model only after it s initialization and thus the input shape is inferred later on. If you decide that this is a TensorFlow feature and not a bug then I m fine with closing the issue. Still personally I think it s a bug and tensorflow should support my example s way of model saving loading since it has all the information needed to do that. Unfortunately I m not able to reopen the issue so in case you think this is still a bug could you do it Also tensorflow bot s links don t work note that I haven t answered to the link in this issue yet image https user images.githubusercontent.com 2308543 66641417 a0a14200 ec12 11e9 8528 e95a9d4a865b.png hartikainen I think it is difficult to infer shape without manually specifying or running .fit or .predict atleast once. May be I am wrong. I will reopen this issue. Thanks jvishnuvardhan I think it should be possible once the first forward pass is done i.e. the line model inputs in my example is important. After that all the inputs and weights should be fully defined. Thanks for reopening I ve the same problem here. My model was created trained and saved using tf.keras and I m not able to tf.keras.models.load model it. I saved the model using HDF5 entire model option as explained in this link https www.tensorflow.org tutorials keras save and load the load model function fails with a weird error IndexError Traceback most recent call last ... usr local lib python3.6 dist packages tensorflow python keras engine network.py in from config cls config custom objects 1150 assert layer name in created layers 1151 layer created layers layer name 1152 layer output tensors layer. inbound nodes node index .output tensors 1153 output tensors.append nest.flatten layer output tensors tensor index 1154 IndexError list index out of range andmax It looks like different error. Can you please open a new issue with your details error trace and standalone code to reproduce the issue I think it will help community who has similar issue as you. Thanks Thanks jvishnuvardhan I followed your suggestion and created a specific issue https github.com tensorflow tensorflow issues 33357 hartikainen Looks like this was resolved in recent tf nightly . Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 0612e4ebc2548bc5a06727336bf5d541 30892.ipynb . Thanks Please feel free to close the issue if this was resolved for you. Thanks Neat this looks good in the colab so I ll close it. Thanks a lot jvishnuvardhan Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 30892 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 30892 No a 
30909,TF 2.0 nightly tf.keras.estimator.model to estimator got an unexpected keyword argument use v2 estimator , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow YEs OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOS 10.14.5 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device No TensorFlow installed from source or binary No TensorFlow version use command below tf nightly 2.0 preview 2.0.0.dev20190721 Python version Python 3.6.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Describe the current behavior The following code was working with some earlier release but now it is crahsing Describe the expected behavior Should work out of the box as before Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,It seems the issue was fixed in tf nightly 2.0 preview 2.0.0.dev20190722 but only work with strategy None and not strategy tf.distribute.MirroredStrategy . I will open a separet ticket. Closing Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30909 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30909 No a 
30910, TF 2.0 keras Conv2D looses shapes with dilation rate other than 1, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 macOS Mojave TensorFlow installed from source or binary binary TensorFlow version use command below v2.0.0 beta0 16 g1d91213fe7 2.0.0 beta1 Python version 3.7 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior When I use Conv2D layer with dilation rate other than 1 output looses shapes Describe the expected behavior I expect to see calculated shape with None only in first dimension Code to reproduce the issue Other info logs Output is Tensor dilation 1 Identity 0 shape None 512 512 256 dtype float32 Tensor dilation 6 Identity 0 shape None None None 256 dtype float32 ,I could able to reproduce the issue with Tensorflow 2.0.0.beta1 on Colab. Please take a look at gist of Colab https colab.research.google.com drive 1swIU2bAUBTdgeEcriw SAJiQtiXtMBLh . lytkarinskiy This was resolved in tf nightly builds. Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 8d0b507a3f1d1972012542f6c440c110 tf 30910 dilation.ipynb . I am closing the issue. Feel free to open if the issue persists. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30910 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30910 No a 
30957,Segfault when passing empty Tensor to cholesky solve, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04 Linux 93fd36b9ffb5 4.9.125 linuxkit 1 SMP Fri Sep 7 08 20 28 UTC 2018 x86 64 x86 64 x86 64 GNU Linux Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary Binary TensorFlow version use command below v1.14.0 rc1 22 gaf24dc91b5 1.14.0 Python version Python 3.6.8 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior When passing an empty Tensor as the second argument to the tf.cholesky solve function a segfault is encountered. Valgrind reports the following Describe the expected behavior Although we clearly shouldn t be passing an empty Tensor to this method I would expect an exception to occur in Python explaining that the input is invalid rather than a segfault. Code to reproduce the issue Run the following code Here the rhs argument to cholesky solve ends up as an empty Tensor it has a shape of 0 1 which appears to cause the segfault. ,I am able to reproduce the issue.Thanks nfergu There was some updates related to this issue. Can you please check whether the issue still persists. Please use tf nightly and let us know the status. Thanks I think it was resolved. Closing due to lack of recent activity. Please open new ticket if you see similar issue. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 30957 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 30957 No a 
30964,Redefinition of name when importing keras callbacks or optimizers, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 TensorFlow installed from source or binary binary TensorFlow version use command below 1.14.0 Python version 2.7.15 CUDA cuDNN version 10.0 Describe the current behavior When importing all keras callbacks or optimizers via from tensorflow.keras.callbacks import or from tensorflow.keras.optimizers import the variable name gets redefined to tensorflow.keras.callbacks and tensorflow.keras.optimizers . Describe the expected behavior Variable main should keep its value main so it can be used to determine if the python code is imported as a module library or the entry point of a script. Code to reproduce the issue Other info logs Only occurs with tensorflow.keras not with the standalone Keras.,I am able to reproduce the issue with Tenosrflow 1.14.0 on Colab. Take a look at Colab gist here https colab.research.google.com drive 1et k56o8CMt8lWEmwjS6imZKtudiAc1J . Thanks I don t have access to the gist that you are referring to. I can reproduce this on a virtual Ubuntu 18.04 machine with Python 2 and 3 and on a CentOS 7 server with Python 2. This is fixed with tf nightly version 1.15.0 dev20190731 Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30964 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30964 No a 
30992,INT8 TensorRT Quantization Fails to Calibrate, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 nvcr.io nvidia tensorrt 19.02 py3 https docs.nvidia.com deeplearning sdk tensorrt container release notes rel 19 02.html rel 19 02 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device NA TensorFlow installed from source or binary binary TensorFlow version use command below v1.12.1 6532 g9aaf74d733 1.15.0 dev20190718 Python version 3.5.2 Bazel version if compiling from source NA GCC Compiler version if compiling from source NA CUDA cuDNN version 10.0.130 7.4.2 as per the above linked container GPU model and memory Tesla V100 32GB Relevant output from pip freeze I am trying to quantize a Tensorflow frozen model to FP32 FP16 and INT8 using this https github.com tensorflow models blob master research tensorrt tensorrt.py script and a few helper functions from here https github.com tensorflow tensorrt blob master tftrt examples object detection graph utils.py . Describe the current behavior The current behavior is that the graph first gets converted to a TRT graph with TRTEngineOps but then when it gets calibrated for the INT8 quantization an error occurs. Logs follow Describe the expected behavior The expected behavior is for this error to not occur and for the INT8 calibrated and quantized graph to be produced correctly. Code to reproduce the issue Command executed You can find the do.py script and the utilities.py script referenced from it attached and the tiny yolov3 frozen.pb frozen model here https drive.google.com file d 1Bgj9h6TJLwedtrhnritRs9eYm iczG4v view . do.py.txt https github.com tensorflow tensorflow files 3427406 do.py.txt utilities.py.txt https github.com tensorflow tensorflow files 3427407 utilities.py.txt , issues with quantized model file sizes and inference speeds here 30717 nseidl Did you check whether the issue persists with latest versions TF1.14 or latest nightly builds of TF Thanks Can confirm that simply upgrading to tf nightly gpu 1.15.0.dev20190725 solved the issue. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30992 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30992 No a 
30993,Missing shuffle argument on validation call during training,On TensorFlow 1.14 OS Ubuntu 16.04 when I call fit of a tf.Keras model using HDF5Matrix on both training and validation data I set the argument shuffle batch and it works for training batches however it fails when starting validation batches TypeError TypeError while preparing batch. If using HDF5 input data pass shuffle batch . The problem seems to be the shuffle argument missing on the validation call during training https github.com tensorflow tensorflow blob master tensorflow python keras engine training arrays.py L424 It seems the recursive call of model iteration does not set the argument from parent training call into the validation call. Simply shuffle shuffle in argument list should fix the issue. Best Andre. , andmax Will it be possible to provide the minimal code to reproduce the reported issue. Thanks Yes based on keras cifar10 example I am bale to reproduce the reported issue with TF version 1.14.0 on Colab find the gist here https colab.research.google.com drive 1UuloCJ4vLmILAVws7QxSI4N6IWxYJHCx . Thanks This issue has not been fixed in TF 1.15 and 2.0. This issue is simple to resolve just place shuffle shuffle in model iteration call for val results in https github.com tensorflow tensorflow blob r1.15 tensorflow python keras engine training arrays.py L428 In this way shuffle will not be default to True in validation call raising the TypeError about not being shuffle batch here https github.com tensorflow tensorflow blob r1.15 tensorflow python keras engine training arrays.py L379 This is fixed with TF 2.1.0 Thanks Close this issue as it s fixed in TF 2.1. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 30993 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 30993 No a 
31024,tensorflow.keras.optimizers.Adadelta Inconsistencies With keras.optimizers.Adadelta , System information OS Platform and Distribution Linux 4.9.0 8 amd64 x86 64 with debian 9.9 . TensorFlow version v1.14.0 9 gc407b045b8 1.14.0 . Python version 3.6.6 . Describe the current behavior Take a look at the following two Kaggle kernel 1. https www.kaggle.com ilhamfp31 keras mnist 2. https www.kaggle.com ilhamfp31 tensorflow keras mnist Both of them run the same code with the same OS python version and TensorFlow version. The only difference is the first one importing from keras and the second one from tensorflow.keras. Optimizer Adadelta produces a different result because of the default configuration is not the same. The difference lies in the learning rate. print keras.optimizers.Adadelta .get config lr 1.0 rho 0.95 decay 0.0 epsilon 1e 07 print tensorflow.keras.optimizers.Adadelta .get config name Adadelta learning rate 0.001 decay 0.0 rho 0.95 epsilon 1e 07 Describe the expected behavior keras.optimizers.Adadelta and tensorflow.keras.optimizers.Adadelta should produce the same result as shown by optimizer Adam. Looking at the source code both Adam optimizer code in optimizer.py https github.com tensorflow tensorflow blob r1.14 tensorflow python keras optimizers.py and optimizer v2 adam.py https github.com tensorflow tensorflow blob r1.14 tensorflow python keras optimizer v2 adam.py default parameter is consistent with keras optimizer code https github.com keras team keras blob master keras optimizers.py . This is not true with Adadelta. While the code in optimizer.py https github.com tensorflow tensorflow blob r1.14 tensorflow python keras optimizers.py is consistent the code in optimizer v2 adadelta.py https github.com tensorflow tensorflow blob r1.14 tensorflow python keras optimizer v2 adadelta.py is not. Code to reproduce the issue Code needed to reproduce the issue is available by downloading it from the given Kaggle kernel link. Simply click three grey dot on the upper right corner and click Download code . ,I have created a pull request https github.com tensorflow tensorflow pull 31025 for this issue. Discussion on the PR thread https github.com tensorflow tensorflow pull 31025 addresses this issue. Hence closing it. Feel free to reopen if necessary. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31024 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31024 No a 
31040,TFLite conversion fails when using BatchNorm after Reshape Check failed dim x dim y , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04 Windows 10 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below v1.12.1 6931 g2b5ece29d3 1.15.0 dev20190724 Python version 3.6.8 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior Trying to convert a graph containing a reshape layer followed by batch normalization appears to trigger an incorrect op reordering. In particular the toco converter relocates the mul operation from the BN layer to before the reshape layer at which point the layers do not have compatible dimensions. This results in the Check failed dim x dim y error. From looking at the Graphviz video this change is introduced in frame 38. Describe the expected behavior The converter should not reorder operations across reshape if it would cause the dimensions to no longer match. In addition the check failure message should provide more information about the location of the error such as the originating layer names which are visible in the Graphviz outputs . Code to reproduce the issue Other info logs This occurs on all version of TF I ve tested 1.13.1 1.14.0 tf nightly . It s possible that this sequence of operations just isn t supported but this should be explicitly stated if so. The error message is quite vague and made the problematic sequence of ops extremely difficult to track down in a large graph. Relevant log messages ,I also meet this problem when use dense reshape BN activation when swap the order of BN and reshape it will work well. It refer to QAQ https stackoverflow.com questions 56110234 dimensions must match error in tflite conversion with toco But I don t want this. Win10 tensorflow 1.13.1 and I use tf.lite.TFLiteConverter.from session to convert. Do you have any other way to solve thie problem Same here. I ve a sort of Unet with embedding. After embedding linear layer I reshape the tensor to square 16x16x162 and proceed. I faced two problems actually one is addressed by https github.com tensorflow tensorflow issues 30124 but just changing the None 16 16 162 to 1 16 16 162 did the trick as follows x tf.reshape x 1 xsize 1 xsize 2 xsize 3 instead of x tf.reshape x xsize But then I add a BNorm and the Check failed dim x dim y 162 vs. 41472 Dimensions must match thing happened. Looking at the numbers 162 and 41472 16x16x162 it seems that it has something to do with the bnorm probably acting before the reshape not after as stated in the issue. I tried also to disentangle the two adding some variant of x x 1 x x 1 and 1 but no luck. Of course in TF everything is fine this happens only during the conversion. hey TOCO will be deprecated soon. Could you try use the new MLIR converter and see if issue still persist To use it please download the latest tf nightly or recent stable release of tf and then add this to your code converter.experimental new converter True . see user guide here https www.tensorflow.org lite convert python api I just tried with tf2.1 on windows updated a few days ago. Inserting the converter.experimental new converter True that error disappeared. Now I have other issues like error tf.ResizeBilinear op is neither a custom op nor a flex op which I believe depends on other things. Without the converter.experimental new converter True the error is still there so it definitely makes a difference but I m still not able to convert my model . TF lite has ResizeBilinear op https github.com tensorflow tensorflow blob master tensorflow lite kernels resize bilinear.cc I m guessing that the TF 2.1 doesn t contain the relevant converter changes to support this op could you try convert your model with tf nightly Thanks. Unfortunately I can t install the tf nightly here so I can t help. For me the issue was solved just by adding this found somewhere on the internet And now the model is converted. I have yet to check for correctness but at least I have a float32 tflite model. Float16 has some other problems which i ll try to investigate later. I see. tf.lite.OpsSet.SELECT TF OPS tells the converter to use the TF ResizeBilinear op instead. So it can convert. But the binary size may grow a bit. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 31040 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 31040 No a 
31070,Issue using tf.keras ModelCheckpoint when distributing under MultiWorkerMirroredStrategy, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below tf gpu 2.0.0b1 Python version 3.6.3 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 10 7.4.1 GPU model and memory 2 x GV100 32GB You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior Raises an error when using tf.keras.callbacks.ModelCheckpoint in the callbacks list when training using keras under the MultiWorkerMirroredStrategy distribution strategy on a single machine. Error is Describe the expected behavior should save a checkpoint model file and not crash Code to reproduce the issue run script called run distributed training minimal example.py distributed worker script called distributed training minimal example.py running first script will cause the issue. setting use custom check point to True in the second script will remove the error. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,I am experiencing the same issue. I am also experiencing the same issue. https github.com tensorflow tensorflow commit 6345ad553be2c23e09d7c3193533994eb522f635 This should be a fix but not included in beta1 release Correct https github.com tensorflow tensorflow commit 6345ad553be2c23e09d7c3193533994eb522f635 is the fix for this issue and should be available in the next release. Thanks for reporting the issue. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31070 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31070 No a I tried following 2 solutions 1.I applied https github.com tensorflow tensorflow commit 6345ad553be2c23e09d7c3193533994eb522f635 to my tensorflow installed code 2. I install latest nightly dev build both gave me following error seems though previous commit change data type to int64 somewhere else still expects int32 jtang7 would you mind opening an issue with the steps to repro Thanks. 
31071,Error on model fit with stateful LSTM using Dataset, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04 LTS TensorFlow installed from source or binary conda forge TensorFlow version use command below unknown 1.14.0 Python version Python 3.7.3 CUDA cuDNN version NVIDIA SMI 418.67 Driver Version 418.67 CUDA Version 10.1 GPU model and memory Quadro RTX 6000 24190MiB Exact command to reproduce Describe the problem When feeding a Dataset to Sequential.fit when a stateful LSTM layer is included a TypeError occurs TypeError DatasetV1Adapter object is not subscriptable This does not happen with a stateless LSTM Using stateful False does not produce the error . Below is a minimal example to reproduce this issue. Source code logs Output ,Issue is replicating with TF version 1.14 please find the gist of Colab https colab.sandbox.google.com drive 1N7agqrl9m2MTFK008TtJnxJA4vfBDBrF scrollTo tSmJM4LQnUr . Thanks mimxrt I could reproduce the issue with tf nightly. Here is the gist https colab.sandbox.google.com gist jvishnuvardhan a2e382b13204b6852729ac48b1fae6d1 tf 31071 nightly.ipynb . However TF2.0b1 is running without any issues. Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 7547bf0796f78ea7f8078b1b396aab3c tf20 31071.ipynb . Thanks Thank you for testing this good to know. However I am unsure if I should migrate to TF2.0b1 now given it is still a beta version. To be fair I was expecting 1.14 to be the more stable version of the two. I think the issue is fixed recently in beee660bfcc977bb65ca0c3bec4a3ea7756ee597. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31071 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31071 No a 
31091, TF 2.0 tf.image.central crop doesn t work, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below v1.12.1 7164 gf2b5825 2.0.0 dev20190726 Python version 3.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Describe the current behavior tf.image.central crop throws an error Describe the expected behavior tf.image.central crop crops image Code to reproduce the issue I ve tried two scenarios. First Second Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. First log Second log ,I have tried on colab with TF version 2.0.0 dev20190726 and was able to reproduce the issue.Please find the gist https colab.research.google.com drive 1h Hr QmSBJ445wTT 8srF O4otMOhQUS here.Thanks Wrap the op in a tf.keras.layers.Lambda layer lytkarinskiy Output This is fixed with TF 1.14. Thanks output Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31091 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31091 No a 
31100,When I used tf.keras.backend.clip in my code I got the error TypeError Using a tf.Tensor as a Python bool is not allowed,When I implemented the code below with TensorFlow 1.12.0 I got some errors. However the code can be implemented well with TensorFlow 1.11.0. I am confused by the problem. Actually I am not sure whether it is the problem of the TensorFlow version. Hope someone can give me some advice. The code The error ,I guess that maybe it is the problem of the usage of K.clip. I am not sure whether tensor can be the third parameters of the K.clip function. Actually sometimes the code can run without error but it outputs errors in majority conditions. I have tried on colab with TF version 1.12.0 and 1.11.0 beta and was able to reproduce the issue.Please find the gist https colab.research.google.com drive 1fuHtPEGXy3xJQdwGDwvON0Uoi pxEV46 here. Thanks bugzhu I was able to reproduce the issue with tf nightly. Here https colab.sandbox.google.com gist jvishnuvardhan 83a39f17991e14fc1dfb27fb7263e29d tf nightly 31100.ipynb is the gist. However it ran without an issue when TF2.0 was used. Here is the gist https colab.sandbox.google.com gist jvishnuvardhan ecedfcce76930129a03a41d889dad523 tf 20 31100.ipynb with TF2.0b1. Would you like to install TF2.0 as it has better performance than older TF1.x versions. Thanks Thank you for your comments. With TF2.0 it ran perfectly without error Maybe in my future work I will try TF2.0. It is a good idea. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31100 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31100 No a 
31109,dilated tf.keras.layers.Conv2D can t estimate output shape, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 beta1 Python version 3.6.8 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior when creating tf.keras.Model with functional api dilated convolution of tf.keras.layers can t estimate the output shape. Describe the expected behavior the dilated convolution can estimate the output shape as convolution op does. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,I have tried on colab with TF version 2.0 beta1 and was able to reproduce the issue.Please find the gist https colab.research.google.com drive 10utd0A3GuBGAvIf8RnMQVFRp2QjAbFPC here.Thanks This is fixed in latest 2.0 nightly build version 2.0.0 dev20190729 . Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31109 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31109 No a 
31137,tf.concat throws error after another call of tf.concat if values is a single Tensor or a list of length 1, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow no OS Platform and Distribution e.g. Linux Ubuntu 16.04 macOS 10.13.6 TensorFlow installed from source or binary from pip install TensorFlow version use command below v2.0.0 beta0 16 g1d91213fe7 2.0.0 beta1 Python version v3.6.7 6ec5cf24b7 Oct 20 2018 03 02 14 Describe the current behavior Assume that I have already made a previous call of tf.concat . When calling again tf.concat with a values argument which is either a list of Tensor objects of length 1 or a single Tensor object I get the following error It seems to be a naming conflict. Describe the expected behavior From tf.concat documentation https www.tensorflow.org versions r2.0 api docs python tf concat hl en values A list of Tensor objects or a single Tensor . tf.concat should not throw an error when values is a list of length 1 or a single Tensor object and no naming conflict should rise. Code to reproduce the issue which outputs If I comment both print tf.concat i j axis 1 lines then print tf.concat i axis 1 does not fail but print tf.concat i axis 1 do. If I also comment print tf.concat i axis 1 then print tf.concat i axis 1 resolves without error. Other info logs Full error log ,Added a PR 31145 for the fix. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31137 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31137 No a 
31147,Tensorflow 1.14 Keras functional API mixed with ops using placeholders throws InvalidArgumentError You must feed a value for placeholder tensor , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary Binary TensorFlow version use command below 1.14.0 Python version 3.6.8 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior Keras layers that have an input dependent on a Tensorflow placeholder will throw an InvalidArgumentError on the op creation step asking to feed a value for the placeholder. Specifically this happens during the base layer utils.create keras history inputs step in the call function on the layer where the inputs to the Keras layer are passed through a GraphExecutionFunction object made during backend.function op input . This exception is new in Tensorflow 1.14.0. Describe the expected behavior As in previous versions of Tensorflow I would not expect the InvalidArgumentError to be thrown when I am building the graph mixing Keras with Tensorflow. Code to reproduce the issue Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. , mbeissinger As mentioed in this issue https stackoverflow.com questions 50321139 keras error you must feed a value for placeholder tensor scale is a symbolic tensor. This means they have absolutely no data or value until the moment you start fitting or predicting. But in the 2nd case you have created a lambda layer which doesn t expect the values immediately You can feed in the values when you are fitting or predicting but in the 1st case its expecting the values immediately which is not possible. The example script is not running in eager execution mode and I am not expecting the symbolic tensor made by the placeholder to have or need any value at compile time. Tensorflow 1.13.2 did not throw this exception why did the behavior change in 1.14 to expect placeholder values immediately and not just at session.run Keras does not allow TensorFlow ops unless they are wrapped by a Lambda layer. mbeissinger I think there is no bug here. nairouz I have reproduced this issue. This looks like a bug as TF 1.13 https colab.sandbox.google.com gist gowthamkpr 0770971da52b163128be747f5f62c357 untitled64.ipynb doesn t throw any error but it throws an error with TF 1.14 https colab.sandbox.google.com gist gowthamkpr ed9f839bd3a574aefedcb1944cbce3fb untitled85.ipynb and TF Nightly https colab.sandbox.google.com gist gowthamkpr 238ff431df1edc6885c9188dc81295d1 untitled86.ipynb too. This is fixed with latest version of TF nightly build 1.15.0 dev20190821 . Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31147 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31147 No a 
31270,tf.data.Iterator complain Function tf data structured function wrapper xxxx is not defined. , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow YES OS Platform and Distribution e.g. Linux Ubuntu 16.04 CentOS release 6.3 Final Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below 1.10.0 Python version 2.7.14 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version CPU GPU model and memory CPU v1.10.0 0 g656e7a2b34 1.10.0 Describe the current behavior As the title said it complains I follow the sample code of api doc https www.tensorflow.org versions r1.10 api docs python tf data Iterator from structure Except that I change the tf.data.Dataset creating place. In original sample code it creates 2 datasets before run sess.run while I create 1 dataset after the other dataset has run out and it complains. If it is hard to understand I ll try to describe it after the following code section. Describe the expected behavior All is ok. Code to reproduce the issue if I change the dataset evens Dataset.from generator gen tf.int64 to the place after dataset range Dataset.from generator gen tf.int64 it also will be OK. Other info logs Explain Why I need create after sess.run because our data is generated from a generator and I want to shuffle it at every epoch end. The easiest way and most clear way is change the generator let it generates data in shuffle order at every epoch end. Why not repeat Oh it can t report the epoch end signal. Has other way YES I can keep only 1 generator and change the generator s inner state to shuffle BUT it may be tricky and cause confusion , memeda This is not an issue in Tensorflow 1.14.0. Would you like give try. Thanks Oh I ll try it sorry I am too lazy OK tf 1.14.0 really fix it with some warnings Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31270 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31270 No a 
31271, tflite micro patch Incorrect arguments to ComputePaddingHeightWidth causing wrong offsets in maxpool implementation,With a 1x4 kernel on a one dimensional tensor the max pool result was always minimum 0 in my case because no input data was selected. See padding.h TfLitePaddingValues ComputePaddingHeightWidth int stride height int stride width int dilation rate height int dilation rate width int in height int in width int filter height int filter width TfLitePadding padding int out height int out width ,It looks like this is fixed in https github.com tensorflow tensorflow commit 7645ab725f5b8ce92031e761a6fe0bbcf596b2ce. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 31271 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 31271 No a I agree that it was solved by that commit. 
31272, TF2 UnicodeDecodeError when using tf.saved model.save, System information Have I written custom code Yes. OS Platform and Distribution Ubuntu 18.04 TensorFlow installed from source or binary binary TensorFlow version 2.0.0 dev20190731 Python version 3.6.7 CUDA cuDNN version 10.1 GPU model and memory GTX 1080 Ti Describe the current behavior Code crashes with following exception This seems to be a regression I was running code I had working ago some weeks months with earlier TF2 nightlies and this appears to me as unexpected behavior. Might be introduced by https github.com tensorflow tensorflow commit 7cc180f107f142432358ac33787466de90afd776 To me protobuf object .SerializeToString .decode utf 8 seems incorrect since SerializeToString can return arbitrary binary data https developers.google.com protocol buffers docs pythontutorial parsing and serialization Note that the bytes are binary not text we only use the str type as a convenient container. decodability into UTF 8 cannot be guaranteed. This bug might have gone unnoticed as many other serializations just happen to be be UTF 8 decodable Describe the expected behavior A properly saved model. Code to reproduce the issue ,I was able to reproduce the error with TF 2.0 beta1 and nightly. This seems to have been fixed in 39bc7bcf94983261a3ee8a72802f5de056728a9c . Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31272 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31272 No a 
31291,tf.keras.optimizers.SGD with momentum does not fit when model metrics are provided, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 16.04.4 LTS Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below 1.14 Python version 3.6 CUDA cuDNN version CUDA Version 10.1 GPU model and memory 1080TI You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior momentum in tensorflow.keras.optimizers.SGD and metrics in model compilation cannot be used together when using fit generator . However each works independently. The problem does not exist when using fit Describe the expected behavior Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,Was able to reproduce the issue with Tensorflow 1.14.0 on Colab. Please take a look at gist here https colab.research.google.com drive 1NQk0 HDfUislaKeFCT77xC7KJ4KHia6h . Thanks xuevin This is not issue with Tenosrflow 2.0.0.beta1 and tf nightly 2.0 preview. You might want to give it a try instead. Thanks Thanks The problem must be limited to 1.14 then. 1.13. seems to be unaffected. I couldn t reproduce the issue with tf nightly . Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan b350996a893a58e8a54cad0f3cf1a253 tf 31291 keras opt.ipynb . Thanks Thanks jvishnuvardhan xuevin it seems to be fixed after 1.14 Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31291 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31291 No a gadagashwini jvishnuvardhan This issue happens with gpu enabled tensorflow. llan ml Based on error trace mentioned here https github.com tensorflow tensorflow issues 31291 issue 476337948 the momentum related ops looking for a specific device. So when I assign a device like below everything worked as expected. When you select gpu as with tf.device gpu 0 this will throw same error as follows. Error clearly mentions that the supported device types CPU . Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan b350996a893a58e8a54cad0f3cf1a253 tf 31291 keras opt.ipynb scrollTo Aw5V87sllwL . Thanks jvishnuvardhan Thanks for your explanation. I know there are some ops used by Momentum that do not have gpu implementations. However it is really a limitation that we cannot try the Momentum optimizer for optimizing a model with GPU. I do not know the internal implementations of different optimizers but other optimizers such as Adam and RMSprop work normally on GPUs. Sorry that I just realized that this may not be related to the topic of this issue. llan ml I think none of the optimizers would work with GPU given sparse. And this is something that we re planning to fix and should probably be done by Nov 2019 But contribution welcome if this is urgent to you. tanzhenyu Just in my cases my model can be trained with Adam but it raises the error when I declare the optimizer with optimizer tf.keras.optimizers.SGD momentum 0.9 and without any other code changed. I m using tf nightly gpu preview and follow the subclassed model style with custom training loops. If needed I can provide some tiny scripts to reproduce. llan ml Oh sorry I missed that adam is the only one that would probably work in your case because we haven t made a sparse op yet so every op it uses can be run on GPU. 
31297,tf.gradients with unconnected gradients zero returns wrong shape for unconnected resource variables, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow no OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux 5.2.1 arch1 1 ARCH x86 64 TensorFlow installed from source or binary binary TensorFlow version use command below v1.14.0 rc1 22 gaf24dc91b5 Python version 3.6.8 CUDA cuDNN version N A GPU model and memory N A Describe the current behavior Calling tf.gradients with unconnected gradients zero returns scalars for unconnected resource variables. Describe the expected behavior Calling tf.gradients with unconnected gradients zero returns appropriately shaped zero tensors for unconnected resource variables. Code to reproduce the issue ,Could able to reproduce the issue on Colab with Tensorflow 1.14.0. Please find the gist here https colab.research.google.com drive 1Kt85ANNP 7Hy1HCVcPSfgSHN0ogRuTzX . Thanks saxenasaurabh I think you improved inferred gradient shape of variables recently in another place is the same fix applicable here Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31297 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31297 No a 
31300,Problem when saving loading keras model with .tf extension and stateful ConvLSTM2D as a layer, System information WSL Win10 Ubuntu 18.04 it also happens in a real ubuntu 18 tf nightly gpu 2.0 preview 2.0.0.dev20190802 happens in cpu and gpu python3.7 Describe the current behavior I have a simple test to serialized and deserialize a model which has a stateful LSTM. The tf version returns an error the h5 version works ok. Describe the expected behavior Correct serialization and deserialization of the code in both cases. Code to reproduce the issue Working code for comparisons When not stateful the code works ok Working code 2 for comparisons When h5 works ok in both cases ,I am able to reproduce the issue on Colab with tf nightly gpu 2.0 preview 2.0.0.dev20190802. Please take a look at Colab https colab.research.google.com drive 1QbZwhKN62RicooTE2CAzoDMmuOgWCPLk . Thanks Assign to Kathy who works on save model. nguerinjr Looks like this was resolved in recent tf nightly . I was not able to reproduce the issue. Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 386c2ee4d3b2ec7268bc8f576f2e7b56 31300.ipynb . Please close the issue if it was resolved for you. Thanks Closing this as this issue has been fixed. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 31300 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 31300 No a I have noticed the same problem on stable tensorflow 2.1. Is there a regression test for this Does it pass Maybe we need to reopen this issue. I also have a similar issue on 2.2 with saving loading models with stateful GRU layers in TF format. I think the issue should be reopened. 
31318,InvalidArgumentError Cannot assign a device for operation embedding 1, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information I have written custom code as opposed to using a stock example script provided in TensorFlow OS version Fedora 29.5.1.18 also tested Ubuntu 18.10 TensorFlow installed from source or binary tensorflow tensorflow latest gpu py3 jupyter TensorFlow version use command below 1.14.0 Python version 3.6.8 CUDA cuDNN version 10.0.130 GPU model and memory GeForce RTX 2080 ti 11 Gb Describe the current behavior I m using keras. I try to to fit model that contains Embedding layer. When I call model.fit generator ... I get an error InvalidArgumentError Cannot assign a device for operation embedding embeddings Initializer random uniform sub Could not satisfy explicit device specification because the node colocation node embedding embeddings Initializer random uniform sub was colocated with a group of nodes that required incompatible device job localhost replica 0 task 0 device GPU 0 . All available devices job localhost replica 0 task 0 device CPU 0 job localhost replica 0 task 0 device XLA GPU 0 job localhost replica 0 task 0 device XLA CPU 0 job localhost replica 0 task 0 device GPU 0 . Colocation Debug Info Colocation group had the following types and supported devices Root Member assigned device name index 1 requested device name job localhost replica 0 task 0 device GPU 0 assigned device name job localhost replica 0 task 0 device GPU 0 resource device name job localhost replica 0 task 0 device GPU 0 supported device types CPU possible devices Identity GPU CPU XLA CPU XLA GPU Const GPU CPU XLA CPU XLA GPU ResourceSparseApplyRMSProp CPU RandomUniform GPU CPU XLA CPU XLA GPU ReadVariableOp GPU CPU XLA CPU XLA GPU Sub GPU CPU XLA CPU XLA GPU Add GPU CPU XLA CPU XLA GPU Mul GPU CPU XLA CPU XLA GPU VarIsInitializedOp GPU CPU XLA CPU XLA GPU VarHandleOp GPU CPU XLA CPU XLA GPU AssignVariableOp GPU CPU XLA CPU XLA GPU ResourceGather GPU CPU XLA CPU XLA GPU ResourceSparseApplyRMSProp looks strange for me. After getting this error I cannot fit new simplified model because I get this error again. I get this error even I run tensorflow.keras.backend.get value model.optimizer.lr Describe the expected behavior Model fits without any problems like the same model without Embedding layer no dest input . Code to reproduce the issue Other info logs Some times I get this error on simplified model Probably the issue occuring depends on CPU usage. I m attaching full log. gpu error.txt https github.com tensorflow tensorflow files 3464780 gpu error.txt ,I tried yo use and got new error NotFoundError Traceback most recent call last ipython input 58 d121a4b39b7f in module 3 epochs ep n 4 validation data test gen 5 validation steps X test.shape 0 batch size usr local lib python3.6 dist packages tensorflow python keras engine training.py in fit generator self generator steps per epoch epochs verbose callbacks validation data validation steps validation freq class weight max queue size workers use multiprocessing shuffle initial epoch 1431 shuffle shuffle 1432 initial epoch initial epoch 1433 steps name steps per epoch 1434 1435 def evaluate generator self usr local lib python3.6 dist packages tensorflow python keras engine training generator.py in model iteration model data steps per epoch epochs verbose callbacks validation data validation steps validation freq class weight max queue size workers use multiprocessing shuffle initial epoch mode batch size steps name kwargs 262 263 is deferred not model. is compiled 264 batch outs batch function batch data 265 if not isinstance batch outs list 266 batch outs batch outs usr local lib python3.6 dist packages tensorflow python keras engine training.py in train on batch self x y sample weight class weight reset metrics 1173 self. update sample weight modes sample weights sample weights 1174 self. make train function 1175 outputs self.train function ins pylint disable not callable 1176 1177 if reset metrics usr local lib python3.6 dist packages tensorflow python keras backend.py in call self inputs 3290 3291 fetched self. callable fn array vals 3292 run metadata self.run metadata 3293 self. call fetch callbacks fetched len self. fetches 3294 output structure nest.pack sequence as usr local lib python3.6 dist packages tensorflow python client session.py in call self args kwargs 1456 ret tf session.TF SessionRunCallable self. session. session 1457 self. handle args 1458 run metadata ptr 1459 if run metadata 1460 proto data tf session.TF GetBuffer run metadata ptr NotFoundError 2 root error s found. 0 Not found Resource localhost embedding 1 embeddings N10tensorflow3VarE does not exist. node embedding 1 embedding lookup 1 Not found Resource localhost embedding 1 embeddings N10tensorflow3VarE does not exist. node embedding 1 embedding lookup RMSprop 1 RMSprop update embedding 1 embeddings ResourceSparseApplyRMSProp 184 0 successful operations. 0 derived errors ignored. Probably the same issue https github.com fizyr keras maskrcnn issues 39 Finaly I solved my problem. I removed the definition of momentum for RMSProp. Previous run with error Current run successful Another successful run So the problem is in momentum parameter. AlinaYablokova Looks like you found workaround. Are you happy to close the issue. Thanks gadagashwini yes I have found workaround and I can continue my work now. But in my opinion there are still a number of problems 1. It s impossible to use Embedding layer and RMSProp with non zero momentum together. 2. First workaround did not work NotFoundError 2 root error s found. 0 Not found Resource localhost embedding 1 embeddings N10tensorflow3VarE does not exist. node embedding 1 embedding lookup 1 Not found Resource localhost embedding 1 embeddings N10tensorflow3VarE does not exist. node embedding 1 embedding lookup RMSprop 1 RMSprop update embedding 1 embeddings ResourceSparseApplyRMSProp 184 0 successful operations. 0 derived errors ignored. AlinaYablokova Could you please provide the complete code to reproduce the reported issue. Thanks I am able to reproduce the issue on Colab with Tensorflow 1.14.0 and TF Nightly version 1.15.0 dev20190821 . Please take a look at colab gist https colab.research.google.com drive 1P35gSdWJJ6IemiwkUqKuQa071oaJS5Xe . Thanks This is fixed with tf nightly version 2.2.0 dev20200303 . Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 31318 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 31318 No a I tried yo use and got new error NotFoundError Traceback most recent call last in 3 epochs ep n 4 validation data test gen 5 validation steps X test.shape 0 batch size usr local lib python3.6 dist packages tensorflow python keras engine training.py in fit generator self generator steps per epoch epochs verbose callbacks validation data validation steps validation freq class weight max queue size workers use multiprocessing shuffle initial epoch 1431 shuffle shuffle 1432 initial epoch initial epoch 1433 steps name steps per epoch 1434 1435 def evaluate generator self usr local lib python3.6 dist packages tensorflow python keras engine training generator.py in model iteration model data steps per epoch epochs verbose callbacks validation data validation steps validation freq class weight max queue size workers use multiprocessing shuffle initial epoch mode batch size steps name kwargs 262 263 is deferred not model. is compiled 264 batch outs batch function batch data 265 if not isinstance batch outs list 266 batch outs batch outs usr local lib python3.6 dist packages tensorflow python keras engine training.py in train on batch self x y sample weight class weight reset metrics 1173 self. update sample weight modes sample weights sample weights 1174 self. make train function 1175 outputs self.train function ins pylint disable not callable 1176 1177 if reset metrics usr local lib python3.6 dist packages tensorflow python keras backend.py in call self inputs 3290 3291 fetched self. callable fn array vals 3292 run metadata self.run metadata 3293 self. call fetch callbacks fetched len self. fetches 3294 output structure nest.pack sequence as usr local lib python3.6 dist packages tensorflow python client session.py in call self args kwargs 1456 ret tf session.TF SessionRunCallable self. session. session 1457 self. handle args 1458 run metadata ptr 1459 if run metadata 1460 proto data tf session.TF GetBuffer run metadata ptr NotFoundError 2 root error s found. 0 Not found Resource localhost embedding 1 embeddings N10tensorflow3VarE does not exist. node embedding 1 embedding lookup 1 Not found Resource localhost embedding 1 embeddings N10tensorflow3VarE does not exist. node embedding 1 embedding lookup RMSprop 1 RMSprop update embedding 1 embeddings ResourceSparseApplyRMSProp 184 0 successful operations. 0 derived errors ignored. InvalidArgumentError Cannot assign a device for operation conv2d 1 kernel IsInitialized VarIsInitializedOp node conv2d 1 kernel IsInitialized VarIsInitializedOp defined at usr local lib python3.6 dist packages tensorflow core python framework ops.py 1748 was explicitly assigned to job worker replica 0 task 0 device TPU 0 but available devices are job localhost replica 0 task 0 device CPU 0 job localhost replica 0 task 0 device XLA CPU 0 . Make sure the device specification refers to a valid device. conv2d 1 kernel IsInitialized VarIsInitializedOp AlinaYablokova Could you please provide the complete code to reproduce the reported issue. Thanks help me InvalidArgumentError Cannot assign a device for operation conv2d 1 kernel IsInitialized VarIsInitializedOp node conv2d 1 kernel IsInitialized VarIsInitializedOp defined at usr local lib python3.6 dist packages tensorflow core python framework ops.py 1748 was explicitly assigned to job worker replica 0 task 0 device TPU 0 but available devices are job localhost replica 0 task 0 device CPU 0 job localhost replica 0 task 0 device XLA CPU 0 . Make sure the device specification refers to a valid device. conv2d 1 kernel IsInitialized VarIsInitializedOp I was facing the same issue. I did following changes in my code 1. import os os.environ TF FORCE GPU ALLOW GROWTH true 2. Changed optimizer from RMRprop tp Adagrad 3. Reduced the dimention from 6181 to 3750. and the problem was resolved. Interestingly the error appeared when trying to use the optimizer tensorflow.keras.optimizers.Adam using SGD worked fine. On Apple M1 uninstalling and installing tensorflow macos tensorflow metal and tensorflow did the trick for me. 
31323,Passing a callable learning rate to Adam optimizer does not work as documented,tag bug template System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device n a TensorFlow installed from source or binary binary through pip3 TensorFlow version use command below v2.0.0 beta0 16 g1d91213fe7 2.0.0 beta1 Python version sys.version info major 3 minor 5 micro 6 releaselevel final serial 0 Bazel version if compiling from source n a GCC Compiler version if compiling from source n a CUDA cuDNN version 10.0.130 410.48 10.0 linux x64 v7.4.2.24 GPU model and memory GeForce GTX 1080 with 7598 MB memory Describe the current behavior The Adam optimizer does not seem to keep calling the supplied learning rate callable. It seems like it s being called once or a few times but then a cached value is repeatedly used in updates. Describe the expected behavior According to the documentation https www.tensorflow.org versions r2.0 api docs python tf keras optimizers Adam it should be possible to pass a callable that takes no arguments and returns the actual value to use as learning rate to tf.keras.optimizers.Adam and this can be useful for changing these values across different invocations of optimizer functions . My expectation was that the Adam optimizer would keep calling the supplied callable at each update i.e. from within apply gradients . Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. The code above produces the following output on my system As you can see tf a keeps changing at a fast pace. My expectation was that after setting the learning rate to 0.0 updates would no longer change tf a. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,Could reproduce the issue with TF Version 2.0.0 beta. Gist is attached in this file LR Issue 31323.ipynb.zip https github.com tensorflow tensorflow files 3467436 LR Issue 31323.ipynb.zip It looks like things get completely different behavior w and w o tf.function I don t think this is particularly an optimizer problem it seems that function wouldn t respect callables i.e. the simplest code to reproduce that The output to above code is After one step with learning rate 0.1... return value is 0.10000000149011612 After another step now with learning rate 0.0... return value is 0.10000000149011612 After another step now with learning rate 0.0... return value is 0.10000000149011612 After another step now with learning rate 0.0... return value is 0.10000000149011612 After another step now with learning rate 0.0... return value is 0.10000000149011612 After another step now with learning rate 0.0... return value is 0.10000000149011612 After another step now with learning rate 0.0... return value is 0.10000000149011612 After another step now with learning rate 0.0... return value is 0.10000000149011612 After another step now with learning rate 0.0... return value is 0.10000000149011612 After another step now with learning rate 0.0... return value is 0.10000000149011612 After another step now with learning rate 0.0... return value is 0.10000000149011612 is there a workaround for this I just stumbled upon the same problem.. EDIT For people with the same issue optimizer.learning rate.assign 0.0 seems to do the trick for tf.function s bjornsing I think this was resolved in tf nightly . I tried the above tanzhenyu code and the output is as follows. It returns updated learning rate as 0.0. After one step with learning rate 0.1... return value is Tensor PartitionedCall 0 shape dtype float32 After another step now with learning rate 0.0... return value is Tensor PartitionedCall 1 0 shape dtype float32 After another step now with learning rate 0.0... return value is Tensor PartitionedCall 2 0 shape dtype float32 After another step now with learning rate 0.0... return value is Tensor PartitionedCall 3 0 shape dtype float32 After another step now with learning rate 0.0... return value is Tensor PartitionedCall 4 0 shape dtype float32 After another step now with learning rate 0.0... return value is Tensor PartitionedCall 5 0 shape dtype float32 After another step now with learning rate 0.0... return value is Tensor PartitionedCall 6 0 shape dtype float32 After another step now with learning rate 0.0... return value is Tensor PartitionedCall 7 0 shape dtype float32 After another step now with learning rate 0.0... return value is Tensor PartitionedCall 8 0 shape dtype float32 After another step now with learning rate 0.0... return value is Tensor PartitionedCall 9 0 shape dtype float32 After another step now with learning rate 0.0... return value is Tensor PartitionedCall 10 0 shape dtype float32 Please close the issue if this was resolved for you. Thanks Thanks jvishnuvardhan Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 31323 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 31323 No a I don t think this is particularly an optimizer problem it seems that function wouldn t respect callables i.e. the simplest code to reproduce that I tried tanzhenyu s code with latest and and it seems the problem is still unresovled. 
31324,Passing a Variable as learning rate to Adam optimizer does not work as expected,tag bug template System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device n a TensorFlow installed from source or binary binary through pip3 TensorFlow version use command below v2.0.0 beta0 16 g1d91213fe7 2.0.0 beta1 Python version sys.version info major 3 minor 5 micro 6 releaselevel final serial 0 Bazel version if compiling from source n a GCC Compiler version if compiling from source n a CUDA cuDNN version 10.0.130 410.48 10.0 linux x64 v7.4.2.24 GPU model and memory GeForce GTX 1080 with 7598 MB memory Describe the current behavior If a tf.Variable is passed as learning rate to the Adam optimizer and the variable is later changed that does not seem to affect the optimizer. Instead the optimizer seems to cache the value of the variable at the time when the optimizer was constructed. Describe the expected behavior My expectation was that if I pass a tf.Variable as the learning rate argument to tf.keras.optimizers.Adam and later assign a new value to the variable that would affect the optimization. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. The code above produces the following output on my system As you can see tf a keeps changing at a fast pace. My expectation was that after setting the learning rate variable to 0.0 updates would no longer change tf a. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,I tried executing the code on Colab was able to reproduce the issue with Tensorflow 2.0.0.beta1. Please see the gist here https colab.research.google.com drive 1jA Y3ZOprr6i xIrTiXQWCmu3UyNKL0 . Thanks Hi This is an interesting fact to point out however I believe the proper way of setting up custom learning rates is to use an instance of a class inheriting tf.keras.optimizers.schedules.LearningRateSchedule which returns a learning rate based on the ordinal of each optimization step e.g. in your case Note however that changing the lrate property of such an instance will not affect the optimizer s learning rate either. I think this is due to its depending on a graph node being create based on a single call to the passed object rather than on making calls at each step i.e. in the formula self.lrate will be fixed to the value in place when passed to the optimizer and not checked for at each step . There probably is a workaround to this but I do not know it. Thanks and yes I filed a separate bug for the changing lrate has no effect problem 31323 . The workaround I ve found is to assign to the learning rate property on the optimizer object as in Seems to work but can t see that it s documented anywhere. Nice workaround It would indeed be worth documenting. Interesting. I searched the code a little. In both the v1 and v2 optimizers if you pass a variable as the initial learning rate it gets used as the initial value for the real learning rate variable. Optimizer v1 here https github.com tensorflow tensorflow blob a062a8c1f6a89c88a908813add05a0bfd5d523b9 tensorflow python keras optimizers.py L176 Optimizer V2 here https github.com tensorflow tensorflow blob a062a8c1f6a89c88a908813add05a0bfd5d523b9 tensorflow python keras optimizer v2 optimizer v2.py L606 This should probably warn fail or use the passed variable. tanzhenyu It looks like you wrote a lot of the optimizer v2 code. What do you think I will look into it. Something weird probably happened it should capture the variable but seem it doesn t. Meanwhile to unblock you you can do tf opt.lr 0.0 and that should work bjornsing I think this was resolved in TF2.0 . Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan dfcdb51d1f8123e38185461f56334726 tf31324.ipynb . Output from your code with TF2.0 is as follows. After setting the learning rate variable to 0.0 tf a was constant as expected. I am closing this issue as it was resolved in TF2 0 . Please feel free to open if the issue persists. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31324 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31324 No a Oh sorry to update this. Yes it is fixed before our official launch yesterday 
31331,Error freezing saved model if it contains a tf.keras.layer.BatchNormalisation layer, System information Have I written custom code Yes OS Platform and Distribution Windows 1903 TensorFlow installed from pip TensorFlow version 1.14.0 Python version 3.7.3 GPU model and memory RTX 2080 Ti Problem To make a frozen graph I first create a saved model using tf.saved model.simple save and then freeze it using tensorflow.python.tools.freeze graph.freeze graph . If a model contains some tf.keras.layers.BatchNormalisation layers freezing will fail in TF 1.14.0 with ValueError Tensor name batch normalization cond ReadVariableOp Switch 1 is invalid. TF 1.13.1 does not give an error Code to reproduce the issue Update Seems to be a problem in graph util impl.py in particular https github.com tensorflow tensorflow commit 0f486fc67070ba888204741c404a55a5f1a41fbc diff 2d2827fd48cee6884e3587c901ad6952 gargn If I change this file back to its 1.13 version there is no more error. Update 2 I put K.set learning phase 0 before creating the model then it works. I don t know what effect this has though Does it just turn off batch normalisation altogether Update 3 Final remarks Putting K.set learning phase 0 before creating the model will let it save however the Batch Normalisation layer doesn t seem to do anything updating turned off so it is not a solution. Changing graph util impl.py to its 1.13.1 version will let it save without error however there will be an error ValueError Input 0 of node batch normalization cond ReadVariableOp Switch was passed float from batch normalization gamma 0 incompatible with expected resource. when loading the frozen graph from the protobuf. The workaround is to save the model weights clear the session so that tensor names are not different because of having two graphs set learning phase to 0 recreate the model load the weights and then freeze example code in my comment below https github.com tensorflow tensorflow issues 31331 issuecomment 518655879 ,Same problem here with tensorflow gpu 1.14.0 Ubuntu 16.04 Python 3.5.2. wenshuangwang I found if I put K.set learning phase 0 before constructing the model then it works. But I don t know how this affects training It seems the accuracy is lower afterwards. Another way that works is to use normal keras instead of tensorflow.keras... hmmmm geometrikal Thank you very much it works for me as well. I think it s ok if training and freezing separately. I know the second way but I must use tf.keras to use tensorflow model optimization.sparsity. wenshuangwang Actually I think putting K.set learning phase 0 before loading the model stops the batch normalisation layer from updating. I can tell a difference I get worse accuracy same as when not using batch normalisation and the accuracy vs epochs graph looks different. wenshuangwang I finally found a workaround 1. Create the model using a function do not use K.set learning phase 0 2. Train model 3. Save weights model.save weights weights.h5 4. Clear session and set learning phase to 0 5. Recreate model and load weights 6. Freeze as before I have confirmed this works with no errors for creating the frozen model protobuf. Also there are no errors when loading the model either using tf.import graph def and it is working in my production code using tensorflow java 1.14 Here is full working example geometrikal Yes this is what I said to train and freeze the network separately. Thank you for explaining patiently. geometrikal Is this resolved by the suggestion of wenshuangwang . Can we close the issue Thanks jvishnuvardhan I think this is a bug and should be left open. The bug does not occur in normal keras and it is clunky to have to create a copy of the graph and transfer the weights across just because you have a batch normalisation layer in there. In TF 1.13.1 the models would be frozen without error but this leads to an error when loading the frozen model 3628 I tested the code in the latest tf nightly 1.15.0.dev20190807 and the model froze without issue. gargn I tested as well with 1.15.0 dev20190807 to confirm. It does save without issue but when loading the frozen graph there is a new error ValueError Node batch normalization cond ReadVariableOp Switch has an output shapes attribute inconsistent with the GraphDef for output 0 Shapes must be equal rank but are 1 and 0 Should I open a new issue for this Code for loading graph geometrikal As gargn mentioned the original issue was resolved in pip install tf nightly gpu . Please close the issue and open a new issue for the loading graph if it was not already resolved. I think loading graph was also resolved as I could not reproduce the issue. Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan b3a72cb75dd9b2ed5e05d260ad0247b0 tf 31331 freezing savedmodel.ipynb . Thanks jvishnuvardhan loading graph is not resolved the code you are using in your gist is my workaround for the problem. I have opened a new issue 31668 with a bigger scope to cover the general issue of freezing and loading models with batch normalization. This should work on the latest nightly. Please reopen if you still have issues with running the script provided above. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31331 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31331 No a import backend from tf.keras not simply keras shown below from tensorflow.keras import backend as K import backend from tf.keras not simply keras shown below from tensorflow.keras import backend as K awesome it works I noticed if my model has conditional logic the frozen model output is different from the keras model output. Any particular reason why that should happen Here is a sample Custom Layer that is part of my model 
31361,UniqueV2 reports incorrect output shape, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04.6 LTS Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary pip TensorFlow version use command below v2.0.0 beta0 16 g1d91213fe7 2.0.0 beta1 Python version 3.5.2 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version 10.0.130 GPU model and memory Tesla K40C 11441MiB Describe the current behavior When not using eager execution UniqueV2 always reports its first output to have rank 1. Describe the expected behavior UniqueV2 should report its first output to have the same rank as its input. Code to reproduce the issue The bug can be exposed by forcing non eager execution through tf.function or tf.compat.v1.disable eager execution . The former is demonstrated below This outputs but should output ,I have tried on colab with TF version 2.0 beta1 TF 2.0.0 dev20190805 and was able to reproduce the issue.Please find the gist https colab.research.google.com drive 1RpI4ba9Dg7Elu2yr4jjdOkyQoJVc6f67 here.Thanks yongtang I think this was broken since https github.com tensorflow tensorflow commit 7603480f3 when uniquev2 was added. Do you have time to work on this Tatiana can you shepherd this from the TF ops side The issue is that the shape inference function for uniquev2 ignores the fact that when axis is specified it preserves rank I think this falls into Rasmus s domain of expertise. With the latest tf nightly the result works correctly I think the issue has been resolved. As yongtang mentioned this was resolved in tf nightly . Here https colab.sandbox.google.com gist jvishnuvardhan 04693a821f4c3e3eef2a1b38088af9e5 untitled574.ipynb is the gist for your reference. I am closing the issue as it was resolved in tf nightly . Please feel free to open it if the issue persists again. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 31361 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 31361 No a 
31398,The precision difference between tensorflow and numpy for matrix multiplication, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below 1.13.1 Python version 3.6.0 Bazel version if compiling from source NA GCC Compiler version if compiling from source NA CUDA cuDNN version NA GPU model and memory NA You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior Tensorflow matrix multiplication on CPU does not reproduce same result as numpy matrix multiplication for float32. Furthermore X Y and Y.T X.T .T produces different results. Is this a normal behavior if it is then which version of the matrix product should be taken as correct output Describe the expected behavior Theoretically X Y and Y.T X.T .T should evaluate to same value and their value should be same as numpy version. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. which outputs 0.00012207031 0.0 0.00012207031 0.00012207031 Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,I was able to replicate the issue with TF verison 1.13 please find the Gist https colab.sandbox.google.com drive 1HpwjLl3nOSSFk7965 kiR22NoBsjjz scrollTo b0BhWDenuN y of collab.Thanks ozanyildiz93 I cannot reproduce the issue in tf nightly. I think this was resolved in the tf nightly . Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan ba880d7fb9ffdd6af13068a116aeed9a tf 31398.ipynb . Thanks Thank you. I will upgrade to the latest version in that case. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31398 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31398 No a 
31458,tf.metrics reset state not called when executing in graph mode, tf.keras.metrics. are reset using the method .reset states however when executed in graph mode the method isn t called and the metric isn t reset. Example to reproduce Output ,Issue replicating with TF version 2.0beta kindly find the gist https colab.sandbox.google.com drive 1qB1JOy9v5IW zbgRCiMS795e2Dmj92lc scrollTo fuCUdASHo t7 of colab. I can confirm the same bug in TF version 2.1. It behaves rather strangely the first two functions work but the last does not. This seems to be fixed in tf nightly 2.2.0a20200221 Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 31458 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 31458 No a 
31486,Shape of RaggedTensor is unknown when converting to tensor, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 macOS 10.14.6 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary From Pypa TensorFlow version use command below v1.14.0 rc1 22 gaf24dc91b5 1.14.0 Python version 3.6.4 Describe the current behavior I am using the estimator API. I have variable lenght input sentences so I want to use RaggedTensors in my tf.data.Datasets . However it seems that when converting the RaggedTensor back to tensor the shape is not evaluated correctly. Describe the expected behavior When using x.to tensor ... on the elements of the dataset the shape should be evaluated to the shape of the tensor. Code to reproduce the issue This outputs Whereas this Outputs , tmattio Can you provide a standalone code to reproduce the issue Current code throws an error. Thanks jvishnuvardhan I just copy pasted both of the code samples in a file and ran them without any issue. Are you using the same environment What is your error tmattio Sorry for the mistake. I tried with tf nightly and reported the error. Yes I can reproduce the issue with TF1 14 . Thanks It looks to me like this has already been fixed. In particular if I try your code sample in tf nightly then it outputs Though to make it work I needed to replace With By the way the PR that fixes this problem is 53fd64291fef36462293d634c84f2e338da6359d. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31486 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31486 No a 
31494,export lib.get temp export dir returns incorrect value with mixed bytes and str, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOS Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device MacBook Pro TensorFlow installed from source or binary Source TensorFlow version use command below 1.14 Python version 3.5 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior The return value of export lib.get temp export dir is mixed with string and bytes where the bytes portion is in literal form of temp b 1234567890 including the letter b and the quotes these will then become part of the directory name created. Describe the expected behavior The return value should be temp 1234567890. Code to reproduce the issue Other info logs Output of above code As you can see the b became literal. ,Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31494 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31494 No a 
31498,TF2.0beta1 Not JSON Serializable when using tf.keras.experimental.export saved model,Most code is from one of the tensorflow 2.0 beta guides Writing layers and models with TensorFlow Keras https www.tensorflow.org beta guide keras custom layers and models putting it all together an end to end example Describe the current behavior TypeError Not JSON Serializable b n x06Square x12 x06Square x1a x0fz mean Identity x07 n x01T x12 x020 x01 Describe the expected behavior Save model correctly. Code to reproduce the issue Other info logs ,The code seems working fine on with tf nightly gpu 1.15.0.dev20190809 It also seems to work with tf nightly 2.0 preview 2.0.0.dev20190811. In this version tf.keras.experimental.export saved model is deprecated. I think in earlier versions of TF the serialization of TF objects returned a binary but keras uses a .json. So there was this problem. I don t know but maybe or are the functions that caused this error in serialization. At that time I ve tried putting everything in Lambda layers to avoid the binary serialization. Even though there are some warnings about loading saving the optimizer when using this tf.keras.experimental there s no error and or warning when using the model.save ... save format tf or tf.keras.models.save model ... save format tf . Both with save format tf or save format h5 So it s a good idea to use at least this nightly version for TF and change the tf.keras.experimental to the non deprecated versions if you want to use the 2.0 version. This is fixed with latest tf 2.0 nightly build 2.0.0 dev20190812 . Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31498 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31498 No a 
31509,BaseCollectiveExecutor StartAbort Out of range warnings when fit model in graph mode TF 2.0 Nightly , System information OS Platform and Distribution Windows 10 TensorFlow installed from Binary TensorFlow version TF 2.0 Nightly GPU Preview Python version 3.6 CUDA cuDNN version 10.1 GPU model and memory 960M I have a very simple model that I have made by inheriting from tf.Keras.model which I feed with a dataset i e model MyModel ... model.compile optimizer tf.keras.optimizers.Adam learning rate 0.01 amsgrad True loss loss fn run eagerly False dataset tf.data.Dataset.from tensor slices x y dataset dataset.shuffle buffer size 10000 dataset dataset.batch batch size 1000 model.fit dataset epochs 100 verbose 0 callbacks LossAndErrorPrintingCallback If I run this using TF 2.0 beta works perfectly fine i.e with run eagerly False. If I run it using TF nightly preview with run eagerly True again fine. However if I try with run eagerly False using nightly preview I get a stream of the following warnings 2019 08 10 16 35 40.168418 W tensorflow core common runtime base collective executor.cc 216 BaseCollectiveExecutor StartAbort Out of range End of sequence node IteratorGetNext , oracle3001 In order to expedite the trouble shooting process can you please provide complete code snippet to reproduce the issue reported here.Thanks Here is a minimal model that produces the same issue when using TF 2.0 nightly but not TF 2.0 beta ... oracle3001 When tried executing the given code error NameError name generate prototypes is not defined was faced.Thanks Sorry yes it is just a function to create random points. Here is the missing functions. This is fixed with latest version of tf nightly 2.0 build 2.0.0 dev20190906 . Thanks Replace with on line 64 Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31509 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31509 No a This bug seems still present as of 2.0.0 official release as well as RC1 I tried just now with the code snippet above. I have seen a similar problem in our code. Any hint Thanks Graffa ymodak bump any update Please look into this issue. The console output becomes a complete mess Hi I am facing this issue while training tiny yolo on VOC2012 dataset using model.fit method. Note that I am using repo https github.com zzh8829 yolov3 tf2 and using TF2.0 official release. Any suggestion to fix the error Hi I also have got this problem. Model is learning normally however console output is unreadable. After every epoch there is an error with message BaseCollectiveExecutor StartAbort Out of range End of sequence . Same issue on fresh install using code found on the official documentation https www.tensorflow.org tutorials text text generation Operating System Windows 10 Python 3.7 64 Tensorflow Package tensorflow gpu 2.0.0 GPU Hardware 1080Ti CUDA Lib 10.0 cuDNN 7.6.5.32 Example Code details summary Click to expand summary details Output details summary Click to expand summary details I m having the same issue I though I was doing something wrong at first loading numpy data to tf.data but then it gave me same errors following this example https www.tensorflow.org tutorials load data numpy This bug still seems to be in the Tensorflow 2.0.0 release. I m getting exactly that error messages multiple times when running the following tutorial project https www.tensorflow.org tutorials text text classification rnn The model seems to train and accuracy is increasing so the warning doesn t seem to cause any harm besides messing up the shell. Operating System Ubuntu 18.04.3 LTS Python 3.7.5 Tensorflow Package tensorflow gpu 2.0.0 CUDA Lib 10.0.130 GPU Model and Memory GTX 1050 Ti 4GB Getting same issue would be nice to know the reason for it and how to fix it if possible. It seems fixed in tensorflow gpu 2.1.0rc1 https github.com tensorflow tensorflow releases tag v2.1.0 rc1 Operating System Ubuntu 18.04 Python 3.6.9 Tensorflow tensorflow gpu 2.1.0rc1 CUDA 10.1.243 CuDNN 7.6 GPU Quadro RTX 5000 NVidia driver 430.26 Also facing this problem Hi I am facing this issue while training tiny yolo on VOC2012 dataset using model.fit method. Note that I am using repo https github.com zzh8829 yolov3 tf2 and using TF2.0 official release. Any suggestion to fix the error Unfortunately pip3 install user tensorflow gpu 2.1.0rc1 did not fix the error The bug still seems to be in Tensorflow 2.1.0. I am now working on a different machine than the one I mentioned above. So it doesn t even seem to be specific to some setup. Different GPU CPU OS Version etc. Operating System Ubuntu 19.10 Python 3.7.5 Tensorflow Package tensorflow 2.1.0 CUDA 10.1.243 CuDNN 7.6.5 GPU Model and Memory RTX 2070 Super 8GB I m facing the same issue in Tensorflow 2.1.0. After following all the steps in official tutorial https tensorflow.google.cn tutorials load data csv I get the trained model. But the issue comes out each time when I tried to evaluate the model on test data test loss test accuracy model.evaluate test data OS Windows 10 Python 3.7.6 Tensorflow 2.1.0 Based on https github.com keras team autokeras issues 839 issuecomment 590097076 Concerning a model.fit setting validation steps in case you are using validation data or steps per epoch if your input x is a tf.data dataset solves this issue. I have not tested a model.evaluate but following the same logic I think you should set the steps parameter. Based on keras team autokeras 839 comment https github.com keras team autokeras issues 839 issuecomment 590097076 Concerning a model.fit setting validation steps in case you are using validation data or steps per epoch if your input x is a tf.data dataset solves this issue. I have not tested a model.evaluate but following the same logic I think you should set the steps parameter. Thanks for your reply that s the solution. This bug still occurs when calling model.predict on a tf.data.Dataset.from generator gen dataset in TF 2.1.0. I m getting it during the validation portion of model.fit Still present during validation indeed or just before . W tensorflow core common runtime base collective executor.cc 217 BaseCollectiveExecutor StartAbort Out of range End of sequence node IteratorGetNext 2020 05 20 10 49 02.923506 W tensorflow core common runtime base collective executor.cc 217 BaseCollectiveExecutor StartAbort Out of range End of sequence node IteratorGetNext IteratorGetNext 6 2020 05 20 10 49 17.997026 W tensorflow core common runtime base collective executor.cc 217 BaseCollectiveExecutor StartAbort Out of range End of sequence node IteratorGetNext 2020 05 20 10 49 17.997362 W tensorflow core common runtime base collective executor.cc 217 BaseCollectiveExecutor StartAbort Out of range End of sequence node IteratorGetNext IteratorGetNext 4 Same here In my case with TF 2.1 I load both train and validation dataset from a TFRecord file. E.g. tf.data.TFRecordDataset filePath . This means the tf.fit function has no chance to know the number of samples in the file. As rmarru pointed out if the params in the tf.fit function steps per epoch validation steps are set then the function knows the number of elements and no warning should be seen. If I dont set steps per epoch and validation steps after the first epoch tf.fit will remember steps per epoch but not validation steps . That means from the second epoch I will receive less warnings only for the validation. 
31519,TF 2.0 ft.GradientTape gradient second gradient None, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows Anaconda Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary TensorFlow version use command below tensorflow 2.0b1 Python version 3.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior second gradient with input is None Describe the expected behavior Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. W0811 23 13 04.473541 11804 ag logging.py 145 Entity bound method TensorFlowOpLayer. defun call of tensorflow.python.eager.function.TfMethodTarget object at 0x0000025C98A045F8 could not be transformed and will be executed as is. Please report this to the AutoGraph team. When filing the bug set the verbosity to 10 on Linux export AUTOGRAPH VERBOSITY 10 and attach the full output. Cause converting bound method TensorFlowOpLayer. defun call of tensorflow.python.eager.function.TfMethodTarget object at 0x0000025C98A045F8 AssertionError WARNING Entity bound method TensorFlowOpLayer. defun call of tensorflow.python.eager.function.TfMethodTarget object at 0x0000025C98A045F8 could not be transformed and will be executed as is. Please report this to the AutoGraph team. When filing the bug set the verbosity to 10 on Linux export AUTOGRAPH VERBOSITY 10 and attach the full output. Cause converting bound method TensorFlowOpLayer. defun call of tensorflow.python.eager.function.TfMethodTarget object at 0x0000025C98A045F8 AssertionError dy dx tf.Tensor 0.03395048 0.86635576 0.52642456 0.80100264 1.05554004 0.05975098 shape 3 2 dtype float64 dyy dx None, zwenju Hi When tried executing the given code error NameError name neutron network is not defined was faced.Thanks Please try the new code I have change neutron network to func. thanks zwenju Thank you for the code when tried executing the given code i got the output as per the screenshot output https user images.githubusercontent.com 52397990 63010108 667e2300 bea3 11e9 958e b8a9a8e843f5.png Thank you dyy dx None is actually not the expected result. Could you fix this bug Thanks. This is fixed with latest version TF 2.0 nightly Output in TF 2.0 nightly version 2.0.0 dev20190821 Great Thanks. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31519 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31519 No a 
31546,model.fit generator multithreading is broken in tf.keras, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow YES OS Platform and Distribution e.g. Linux Ubuntu 16.04 Platform independent Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device No TensorFlow installed from source or binary From pip TensorFlow version use command below 1.14.0 and 2.0 Python version 3.6.7 Summary fit generator has an option called workers setting this to 1 will use multithreading to queue up batches from a generator. It raises an exception if the generator is not thread safe. this is expected. However it does not accept thread safe generators. Describe the current behavior Calling model.fit generator on a keras model in tf 2.0 or compat.v2 using a generator object subclassed from collections.Generator raises an exception that the given generator object does not have a shape attribute. This is rooted in the calling of model iteration which then unsuccessfully attempts to find out wether the generator is in fact a generator by using inspect.isgenerator which only recognizes native python generators constructed by a function containing a yield statement however native python generators cannot be thread safe thus fit generator with workers 1 and use multiprocessing False is broken in tf.keras Describe the expected behavior In keras 2.2.4 fit generator simply calls the next gen function on the generator provided to fit generator . this is working as expected. Code to reproduce the issue Other info logs ,I have tried on colab with TF version 1.14 nightly versions 2.0.0 dev20190813 and was able to reproduce the issue.Please find the gist https colab.research.google.com drive 1yAK1iG0wdAo51uDk6YGifdV2 vpiEzqm here.Thanks I just hit this and am preparing a fix. In case you re curious the issue is that we check for generators but not iterators https github.com tensorflow tensorflow blob master tensorflow python keras utils data utils.py L99 It s slightly tricky because lots of things are iterators but we but aren t legitimate inputs to .fit generator This is now fixed. Confirmed with the latest nightly. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31546 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31546 No a robieta Just to confirm is this fixed with tf version 2.1.0 aka how do I check Thanks 
31582, TF2 Unhashable variables breaks ExponentialMovingAverage, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04 TensorFlow installed from source or binary Binary TensorFlow version use command below tf nightly 2.0 preview Describe the current behavior As described in https github.com tensorflow tensorflow commit 2e1214094b6a78ab72d39051c7fd6e86c682ddf4 diff ae1a8f7b66539f000615a4ab7e4b2151 Variables are no longer hashable in TF2. This causes the dictionary tracking of variables to break https github.com tensorflow tensorflow blob master tensorflow python training moving averages.py L371 https github.com tensorflow tensorflow blob master tensorflow python training moving averages.py L448 https github.com tensorflow tensorflow blob master tensorflow python training moving averages.py L462 Describe the expected behavior We can likely just keep the variable names as the dictionary keys. Code to reproduce the issue ,Since I added batchnorm to model MAE assign average vars broke with this error. Code to reproduce error Sean I think the bug has been fixed by dc3534c Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31582 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31582 No a I have the same error still tried fresh install of tfp 0.7 and even tfp 0.6 in my case it occurs at tfd.MultivariateNormalDiag loc scale .sample home pycharm project VAE vae tf2.py 636 compute loss latent code posterior.sample usr local lib python3.5 dist packages tensorflow probability python distributions distribution.py 840 sample return self. call sample n sample shape seed name kwargs usr local lib python3.5 dist packages tensorflow probability python distributions transformed distribution.py 391 call sample n y self.bijector.forward x bijector kwargs usr local lib python3.5 dist packages tensorflow probability python bijectors bijector.py 933 forward return self. call forward x name kwargs usr local lib python3.5 dist packages tensorflow probability python bijectors bijector.py 904 call forward mapping self. lookup x x kwargs kwargs usr local lib python3.5 dist packages tensorflow probability python bijectors bijector.py 1343 lookup mapping self. from x x .get subkey mapping .merge x x usr local lib python3.5 dist packages tensorflow probability python bijectors bijector.py 151 getitem return super WeakKeyDefaultDict self . getitem weak key usr local lib python3.5 dist packages tensorflow probability python bijectors bijector.py 181 hash return hash x usr local lib python3.5 dist packages tensorflow core python framework ops.py 713 hash raise TypeError Tensor is unhashable if Tensor equality is enabled. TypeError Tensor is unhashable if Tensor equality is enabled. Instead use tensor.experimental ref as the key. 
31636,Masked GlobalAveragePooling1D fails when size of step axis is not known statically, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac OS TensorFlow installed from source or binary pip TensorFlow version use command below 1.14.0 2.0.0b Python version 3.6.8 Code to reproduce the issue This code Fails with this exception I would expect masked global average pooling to be no different than regular global average pooling where unknown step axis is supported I think this is caused by the two lines here https github.com tensorflow tensorflow blob e19c354920c3b246dda6598229210a582caaa1a9 tensorflow python keras layers pooling.py L641 L642 Instead of finding the broadcast shape using as list it should use the shape tensor directly and tf.concat them together or whatever is an appropriate equivalent in the keras library .,Was able to replicate the issue on Colab with Tensorflow 2.0.0.beta1. Please see the gist here https colab.sandbox.google.com drive 1AijetZx2ofPuBztdlKovFiC7ZpiT2QFs scrollTo l9gPx9dqn4Uu . Thanks Had the same issue when using masking and GlobalAveragePooling1D with the Keras Sequential . For anyone else running into this it is fixed in v2.1.0 rc0 and later versions. https github.com tensorflow tensorflow blob 9837eceb39171aba9e28dc1f120f53271b6b1ef0 tensorflow python keras layers pooling.py L642 L643 mpdn As mentioned by vsimkus this issue was resolved in latest version. Please take a look at the gist https colab.sandbox.google.com gist jvishnuvardhan 42a3edc1d4fc4e771b0417e5f2b2675c tf31636.ipynb for your reference. Thanks I am closing this issue as it was resolved. Please feel free to open if the issue persists again. thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 31636 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 31636 No a Yes it is fixed in 2.1 while the issue still persists in 2.0.1. And if I upgrade to 2.1 my model cannot predict since Error when checking model input . The issue can be reproduced with tf 2.1 by https colab.research.google.com drive 1hMLd5 r82FrnFnBub B fVW78Px4KPX1 Anyone can fix it Thanks 
31638,Mask is not propagated into Sequential keras layers, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac OS TensorFlow installed from source or binary docker pip TensorFlow version use command below 1.14.0 2.0.0b Python version 3.6.8 Describe the current behavior When a masked input is fed to a tf.keras.models.Sequential the mask is ignored Describe the expected behavior I would expect the mask to be propagated as if the inner layers were not in a Sequential layer. Code to reproduce the issue This works This fails due to failed assert Other info logs I think I have narrowed this down to a cache invalidation issue in tf.keras.layers.Layer. should compute mask https github.com tensorflow tensorflow blob e19c354920c3b246dda6598229210a582caaa1a9 tensorflow python keras engine base layer.py L2055 L2059 From my debugging it seems that whenever the issue above appears should compute mask is cached as false even though evaluating the actual property expression results in true. Since this problem is in a base layer this issue might be affecting other layers as well.,I was able to replicate the issue for given code in both TF versions 1.14 and 2.0beta. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31638 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31638 No a Was able to reproduce the issue with TF v2.1 https colab.research.google.com gist amahendrakar 19fd10326257085b09f137049681b6be 2 1 template.ipynb and TF nightly https colab.research.google.com gist amahendrakar bbd1ef55fd48d1851f0b984a5513429c tf nightly.ipynb scrollTo ieAW NK5iqpf i.e. v2.2.0 dev20200327. Please find the attached gist. Thanks mpdn Closing this issue as it is fixed in TF 2.x. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 31638 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 31638 No a 
31665,tf.math ops do not work on MirroredVariables, System information Have I written custom code yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 macOS Mojave 10.14.5 TensorFlow installed from source or binary binary tf nightly TensorFlow version use command below 1.15.0 dev20190729 Python version 3.7.4 Describe the current behavior When I am using the tf.distribute.MirroredStrategy with multiple replicas tf.math ops do not work on MirroredVariables when inside a cross replica scope. Describe the expected behavior The MirroredVariable class https github.com tensorflow tensorflow blob e19c354920c3b246dda6598229210a582caaa1a9 tensorflow python distribute values.py L782 is a subclass of the DistributedDelegate class https github.com tensorflow tensorflow blob e19c354920c3b246dda6598229210a582caaa1a9 tensorflow python distribute values.py L375 which if I understand correctly means that MirroredVariables are supposed to act like regular tensors that you can perform ops on. This works fine with most standard ops like multiplication division subtraction etc. However if you try to use any tf.math ops you get TypeError Failed to convert object of type class tensorflow.python.distribute.values.Mirrored to Tensor . Code to reproduce the issue , kevin3 black When tried executing the given code I got output as per the screenshot below errror https user images.githubusercontent.com 52397990 63159607 8e9f8a80 c039 11e9 92b1 fcadadafd312.png . Thanks Sorry I forgot to comment out a line. I ve updated the post and reproduced the result on Google Colab https colab.research.google.com drive 1pLeHBPaZxC OlQw4hXfGyC4gJtZ5e5gT This issue should be fixed with commit 1348a03037e6ea79129024f318f8ae9b8f268df8 . Please reopen if the issue still persists. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 31665 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 31665 No a 
31668,Models with tf.keras.layers.BatchNormalisation layers give errors when frozen or are frozen incorrectly and cannot be loaded,This issue continues on from 31331 with a more wide ranging scope System information Have I written custom code Yes OS Platform and Distribution Windows 1903 TensorFlow installed from pip TensorFlow version 1.13.1 1.14.0 tf nightly Python version 3.7.3 GPU model and memory RTX 2080 Ti Problem Freezing a model with tf.keras.layers.BatchNormalization layers either gives an error or does not freeze correctly and gives an error when loading depending on the tensorflow version. Method Freezing Save a model with tf.keras.layers.BatchNormalization layers using tf.saved model.simple save and then freeze it using tensorflow.python.tools.freeze graph.freeze graph . Loading Load the frozen model using tf.import graph def Results 1.13.1 Freezing no error Loading ValueError Input 0 of node batch normalization cond ReadVariableOp Switch was passed float from batch normalization gamma 0 incompatible with expected resource 1.14.0 Freezing ValueError Tensor name batch normalization cond ReadVariableOp Switch 1 is invalid. Loading n a tf nightly 1.15.0 dev20190815 Freezing no error Loading Node batch normalization cond ReadVariableOp Switch 3 has an output shapes attribute inconsistent with the GraphDef for output 0 Shapes must be equal rank but are 1 and 0 Code to reproduce Colab gist is here https colab.research.google.com gist geometrikal da64b13d8a579bc46c005e981d9bc051 tf 31331 freezing savedmodel.ipynb Workaround Workaround is to save the weights clear the session tf.keras.backend.set learning phase 0 recreate the model restore the weights and then freeze. https github.com tensorflow tensorflow issues 31331 issuecomment 518655879,This should work on the latest nightly. Please reopen if you still have issues with running the script provided above. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31668 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31668 No a This is happening in TF 2.x when using hub to transfer learn. I can t clear these layers when freezing either so I can t take the model outside of TF to say Open CV DNN. 
31733,Error when subclassing for stock example Expected D2 of index to be 2 got 3 at position 1, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes. The code is derived from one of the integration test cases. OS Platform and Distribution e.g. Linux Ubuntu 16.04 Stock example in Google Colab https colab.research.google.com github tensorflow docs blob r2.0rc site en r2 tutorials keras feature columns.ipynb TensorFlow version use command below tf.version.GIT VERSION v2.0.0 beta0 16 g1d91213fe7 tf.version.VERSION 2.0.0 beta1 Python version 3.6 Describe the current behavior The following feature column creates the issue when subclassing keras.models.Model which works alright with sequential API. Describe the expected behavior Not failing with feature column.crossed column when subclassing. The integration test here https github.com tensorflow tensorflow blob r2.0 tensorflow python keras engine feature columns integration test.py L161 L183 didn t catch this problem. Code to reproduce the issue Running the following at the end of the notebook will reproduce the error. Other info logs , Guzzii Is it possible for you to try latest TF versions pip install tf nightly 2.0 preview 2.0.0.dev20190818 and let us know whether the issue persists There were lots of performance improvements in the latest versions. I am not seeing the mentioned error in nightly versions.Thanks Thanks for the reply. I am trying to see if the error still exists in the latest version. The following code block still doesn t import the tf nightly. Without the try except block it import version 1.12. Any suggestions on how to import the nightly build on colab Thanks Guzzii I tried with nightly versions 2.0.0 dev20190819 in colab and i am not seeing any error. I am able to reproduce the issue with TF 2.0.0 beta1 version. Please install pip install tf nightly 2.0 preview 2.0.0.dev20190819 and try again and let me know if the issue still persists. Thanks image https user images.githubusercontent.com 51902062 63321485 83e93c00 c33e 11e9 9332 ac3d09d4ee46.png This should be fixed with tf2 nightly. Guzzii Can you try tf2 nighly and let us know if the issue still persists. Thanks ravikyram I confirm the problem is fixed in tf nightly 2.0 preview 2.0.0.dev20190819 . I am closing the issue since the query is been resolved.Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31733 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31733 No a 
31756,tflite incorrect quantisation scale application in unit test utils, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes the bug was discovered when evaluating new quantisation modes OS Platform and Distribution e.g. Linux Ubuntu 16.04 discoverd on Linux Ubuntu 16.04 but should be applicable to any platform where the tests are run Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary source TensorFlow version use command below master branch Python version Python3.6 Bazel version if compiling from source 0.26.1 GCC Compiler version if compiling from source GCC 5.4.0 CUDA cuDNN version N A GPU model and memory N A Describe the current behavior The quantization scale in PerChannelQuantizeBias https github.com tensorflow tensorflow blob 1f61f13f8715dc26dabe46a2686216674026d812 tensorflow lite kernels test util.h L239 in tensorflow lite kernels test util.h https github.com tensorflow tensorflow blob master tensorflow lite kernels test util.h appears to be applied incorrectly. The floating point input data are multiplied by the scale value whereas they should be divided by it. The corresponding tests in conv test.cc https github.com tensorflow tensorflow blob master tensorflow lite kernels conv test.cc and depthwise conv test.cc https github.com tensorflow tensorflow blob master tensorflow lite kernels depthwise conv test.cc e.g. SimplePerChannelTest https github.com tensorflow tensorflow blob 1f61f13f8715dc26dabe46a2686216674026d812 tensorflow lite kernels conv test.cc L1343 appear to contain the wrong expected values. Describe the expected behavior The division operation should be used to convert the floating point to quantized fixed point value and the corresponding tests changes respectively. Code to reproduce the issue N A Other info logs N A ,Thanks for filing the bug. Let s continue the discussion on https github.com tensorflow tensorflow pull 31757 Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31756 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31756 No a 
31797,Deep Learning Image TensorFlow 1.14.0 m33 on Google Cloud produces wrong and non deterministic loss after backpropagation , System information Have I written custom code Yes the code is attached. OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux thomas tf 14 4.9.0 9 amd64 1 SMP Debian 4.9.168 1 deb9u4 2019 07 19 x86 64 GNU Linux Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device Tested on google cloud TensorFlow installed from source or binary Binary Deep Learning Image TensorFlow 1.14.0 m33 on Google Cloud TensorFlow version v1.14.0 0 g87989f6 1.14.0 Python version 2.7.13 3.5.3 Bazel version if compiling from source Not compiled from source GCC Compiler version if compiling from source Not compiled from source CUDA cuDNN version Not used GPU model and memory None Describe the current behavior Current behaviour on Google Cloud using Deep Learning Image TensorFlow 1.14.0 m33 is non deterministic. During multiple runs the loss after performing backpropagation and updating a variable is different for different runs and does not match the loss of the non optimized standard tensorflow installation. This behaviour exists when using python2 and when using python3. Created instance with gcloud compute instances create tf 1 14 cpu zone us west1 b image family tf 1 14 cpu image project deeplearning platform release Run 1 Run 2 Run 3 Describe the expected behavior On the the same machine using a virtualenv to force the use of non optimized tensorflow as follows Run 1 Run 2 Run 3 Code to reproduce the issue Other info logs python code.txt https github.com tensorflow tensorflow files 3520927 python code.txt tf env.txt https github.com tensorflow tensorflow files 3520903 tf env.txt ,I tried your code snippet in Google Colab using TF 1.14 and produced same expected results on 3 consecutive runs. Perhaps this is an issue with GCP instance and not with TF 1.14 version specifically. ymodak thanks for running the code in Google Colab. Given these results I assume it used the standard non mkl optimized version of TF 1.14. Do you have the possibility to test the code with the mkl optimized version of TF 1.14 and run it on a CPU That s correct I used non optimized TF version. I will add TensorFlow MKL group to get more information. Thanks We are able to reproduce your issue on 1.14. We ll keep you posted on the fix preethivenkatesh Thanks for checking this. Was it already possible to track down the reason for the non deterministic outputs and in which situations these appear This bug is also present in tensorflow 1.15 with mkl Deep Learning Image TensorFlow 1.15.0 m38 . The issue is resolved and merged into the master branch 35201 Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 31797 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 31797 No a The non deterministic and incorrect loss for the example above resurfaced in tensorflow 1.15. This happens both when installing the python3 library directly and when using docker pip3 install https storage.googleapis.com intel optimized tensorflow intel tensorflow 1.15.2 cp37 cp37m manylinux2010 x86 64.whl docker pull gcr.io deeplearning platform release tf cpu.1 15 CC https github.com tensorflow tensorflow pull 35201 thomasZen It looks like we have had perf regression on a few models and the commit has been reverted. While we try to resolve this w o impacting our other critical accelerations are you okay to use this as a patch for your build 35619 
31816,Processing batches with different sequence lengths using stacked LSTM layers, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOS 10.14.4 TensorFlow installed from source or binary Source TensorFlow version use command below 2.0.0 beta1 Python version 3.65 Describe the current behavior An exception is raised when trying to stack multiple tf.keras.layers.LSTM while the sequence length changes across batches. This behavior occurs if the tf.keras.Model is built with model subclassing. On the other hand if the model is built using the functional API everything works as intended. Describe the expected behavior Because of the identical implementations besides the difference in the way the model is built subclassing and functional API I would expect the results to be the same. In other words I am confused why an exception is raised at all if using model subclassing. Code to reproduce the issue Other info logs InvalidArgumentError Derived Operation expected a list with 58 elements but got a list with 88 elements. node gradients TensorArrayUnstack TensorListFromTensor grad TensorListStack Adam gradients 24 lstm model 22 lstm 56 StatefulPartitionedCall grad StatefulPartitionedCall Op inference keras scratch graph 75269 ,Was able to replicate the issue using Tensorflow 2.0.0.beta1 on Colab. Please see the gist here https colab.research.google.com drive 1yaYGJbGwKghWUCo R0f9icTkz3Mk8Q1C . Thanks gorjanradevski I cannot reproduce the issue with pip install tf nightly gpu 2.0 preview . I think this was resolved in pip install tf nightly gpu 2.0 preview . Please take a loot at the gist here https colab.sandbox.google.com gist jvishnuvardhan 78ed85d5cce4286d7bd8d04247f4c819 tf 31816 fit generator.ipynb . I am closing the issue. Please feel free to open if the issue persists again. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31816 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31816 No a 
31823,AutoGraph Map Infinite Loop, System information I used google colab. Here s the link to reproduce https colab.research.google.com drive 13g1AMmLsCK3dcmuUdYiDhyv9PqajFRlc Given Describe the current behavior Describe the expected behavior It should not hang. Other info logs I ve been thinking about a fix much like overriding python built in zip function https github.com tensorflow tensorflow pull 31290 . The problem is tf.data.Dataset s map only applies to one dataset https www.tensorflow.org versions r2.0 api docs python tf data Dataset map while python built in map function can accept more than 1 data. For example What s the solution Please correct me if I m wrong or I m missing something. Thank you.,Issue replicating for TF 2.0beta1. AutoGraph doesn t currently override map but the usage you describe could be added in a fashion similar to how we handle zip and enumerate . mdanatg But how to handle if we need to apply map to more than 2 datasets Unlike Dataset.zip https github.com tensorflow tensorflow blob r2.0 tensorflow python data ops dataset ops.py L708 L744 map in Datasets https github.com tensorflow tensorflow blob r2.0 tensorflow python data ops dataset ops.py L1099 L1214 can only take 1 Dataset. I think we could use a combination of Dataset.map and Dataset.zip . In other words map fn dataset1 dataset2 could translate to pseudo code the exact API calls may differ Would this work Oh yea you re right Working on it right now I ll close this issue as 32121 has been merged. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31823 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31823 No a 
31830,fit generator number of calls to the validation generator does not match validation steps,Installed pip install tf nightly 2.0 preview in a clean environment. Same bug was in tf 1.14 and 2.0 beta1 The validation generator is called more often than validation steps in tensorflow.keras.Model.fit generator . If one adds workers 0 as a call parameter everything is correct. I tracked down the issue to starting the threads parameter max queue size changes the number of calls before the calls seem to be used for validation . Reproduce the bug the output is There are 31 calls to the validation generator validation steps max queue size 1 . This might usually be no problem but it is if you have to control which validation data match which training data. I use it for transfer learning. if you uncomment workers 0 you get the expected result I tracked down the issue to starting the threads in training generator.py enqueuer.start but I am too unfamiliar with python threading to supply a patch sorry. At the moment the workaround is workers 0 ,Was able to reproduce the error. Github gist is here https colab.research.google.com gist gowthamkpr 5687a543ffcf0b2d2a72a2a731152063 untitled104.ipynb dsmic I ran the above code with TF v2.1 and got the same outputs with workers 0 commented and uncommented. Please find the gist of it here https colab.research.google.com gist amahendrakar 9ff532a0d1690d0aefd1a2fb69d9f2a9 2 1 template.ipynb . Could you please confirm if the issue is resolved Thanks Could you please confirm if the issue is resolved Thanks dsmic Any updates regarding this issue Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 31830 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 31830 No a 
31832, TF 1.14 TPU Errors while training LSTM model on Colab TPU,I worked a bit with TF2.0 but new to TF1.x working. I want o use Google TPUs and for that I am making a LSTM model in TF1.14 without eager execution. I am reusing code from one of the Tensorflow tutorials https www.tensorflow.org beta tutorials load data text split the dataset into text and train batches . Below is the code I am executing in Colab while setting the runtime environment to Python 2 and TPU . On running it in colab I am getting error KeyError u flat filenames and if I run it again without restarting runtime it throws following error InvalidArgumentError Graph is invalid contains a cycle with 1 nodes including bidirectional while Merge bidirectional while Merge 1 bidirectional while Merge 2 THANK YOU ,I have tried on colab with TF 1.14 and was able to reproduce the issue.Please find the gist https colab.research.google.com drive 15MX0aH W8nyU02743Xt7JlyfziwWhUw3 here.I tried in nightly versions and i am getting the below error ValueError Cannot use the given session to evaluate tensor the tensor s graph is different from the session s graph. Thanks Thank you for confirming the issue. ravikyram Sorry for the long delay. Please re open this issue if this is still reproducible on the latest versions of TensorFlow Cloud TPUs. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 31832 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 31832 No a 
31894,tf.keras.layers.BatchNormalization throws TypeError Incompatible types dtype resource vs. int64. Value is 0 , Throws error UPDATE this works but if you set the trainable boolean to True it throws the same error ,I could reproduce the issue with Tensorflow 1.14.0 and tf nightly. Here is the gist https colab.research.google.com drive 1CL4dYS0LLbTyoe7Dt8MQ kD2yA3o6u3T . iamnotahumanbecauseiamabot Which version of tensorflow you using. Thanks gadagashwini I am using 1.14.0 though if you don t apply TimeDistributed layer on BatchNormalisation it will work. iamnotahumanbecauseiamabot Thanks for the update. robieta I have updated the issue please see the update. I am seeing the same thing when using the MirroredStrategy. The model works fine when executing normally. We don t have TimeDistributed Layers. If you want another ticket I am happy to make one but I am probably not going to be able to generate a repro case as the model is quite large. sseveran It would be good if you create another issue with MirroredStrategy. It s even better if you can provide a simple standalone code. Thanks I m getting the same error on tensorflow 1.15 when using batch normalization inside a custom RNNCell RNN layer. The error only appears in the while loop of the rnn. It also seems like the problem was resolved in tensorflow version 2. Would be great if the fix could be ported back into version 1 if that is possible. I tried adapting control flow ops.py accordingly but ran into more errors that I don t understand akloss Closing this issue as it was resolved in TF version 2. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 31894 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 31894 No a 
31927,tf2 load model issue, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 google colab Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below tf nightly 2.0 preview 2.0.0.dev20190818 Python version 3 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Describe the current behavior When I try to load a model I obtain the following error Describe the expected behavior I would like load the saved model. Code to reproduce the issue Google Colab link https colab.research.google.com drive 1e1TPBzhSipaAGI38gZF00kZMSi5Pg8fp https colab.research.google.com drive 1e1TPBzhSipaAGI38gZF00kZMSi5Pg8fp Here a portion of source code ,Hi it is happening because you have put some part of model outside graph of your graph. entry point of you model is input so feature layer is outside . An op outside of the function building code is being passed a Graph tensor. daniele sartiano did you get a chance to look at akanyaani s comment. Thanks akanyaani thank you for your comment. I tried to edit the code in this way but I obtain the same error. Any suggestion to solve this issue Was able to reproduce the issue with Tensorflow 2.0.0.dev20190818. Please see the gist here https colab.research.google.com drive 1T4pKIz9d6zqVsoZerjaquTKVhx4 Px4P .Thanks Hello everyone gladly I found this issue. I am facing the same problem. I am using Python3 and Tensorflow 2.0.0 rc I was able to replicate the issue both with the mnist dataset and the heart dataset. I tried some model variations and I believe that the problem comes from the joint occurrence of feature columns and save model. When I get it right the above code uses the functional api for keras. With the sequential model this issue happens too. The issue is also occurring when using h5 or json for saving and then loading the model. To be precise the error for h5 is ValueError We expected a dictionary here. Instead we got tf.Tensor Placeholder 0 shape None dtype float32 If there are any information which I can provide for solving the issue please post here. I have the same error too. The issue seems to be solved for me with tensorflow 2.0 rc2 The change log states Model saving changes model.save and tf.saved model.save may now save to the TensorFlow SavedModel format. The model can be restored using tf.keras.models.load model. HDF5 files are still supported and may be used by specifying save format h5 when daniele sartiano Looks like this was resolved in tf nightly . Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan dec0d251573289e24d7e3aed4580818a untitled1.ipynb . Everything runs without any issue when I use tf nightly . Thanks I am closing the issue as it was resolved. Please feel free to open it if the issue persists again. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 31927 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 31927 No a After installing tf nightly still i am getting the same error message. I am trying to read the CSV file from hdfs using tf.data API. Following is my code sample code is referred from https www.tensorflow.org tutorials load data csv train file paths hdfs .... tr titanic train train.csv test file paths hdfs ..... tr titanic eval test.csv LABEL COLUMN survived LABELS 0 1 COLUMNS survived sex age n siblings spouses parch fare class deck embark town alone def get dataset file path kwargs dataset tf.data.experimental.make csv dataset file path batch size rowsin batch 10 Artificially small to make examples easier to show. column names COLUMNS label name LABEL COLUMN na value header False num epochs 1 shuffle False ignore errors True kwargs return dataset raw train data get dataset train file paths raw test data get dataset test file paths class PackNumericFeatures object def init self names self.names names def call self features labels numeric freatures features.pop name for name in self.names numeric features tf.cast feat tf.float32 for feat in numeric freatures numeric features tf.stack numeric features axis 1 features numeric numeric features return features labels NUMERIC FEATURES age n siblings spouses parch fare packed train data raw train data.map PackNumericFeatures NUMERIC FEATURES packed test data raw test data.map PackNumericFeatures NUMERIC FEATURES numeric column tf.feature column.numeric column numeric shape len NUMERIC FEATURES numeric columns numeric column feature layer tf.keras.layers.DenseFeatures numeric columns model tf.keras.Sequential feature layer layers.Dense 128 activation relu layers.Dense 128 activation relu layers.Dense 1 activation sigmoid model.compile loss binary crossentropy optimizer adam metrics accuracy model.fit packed train data epochs 2 print model.summary test data packed test data print evaluating model.. test loss test accuracy model.evaluate test data model.save my model.h5 print Printing new model created from saved model new model tf.keras.models.load model my model.h5 print Show the model architecture print new model.summary Please let me know if i am doing it wrong way. Hello everyone gladly I found this issue. I am facing the same problem. I am using Python3 and Tensorflow 2.0.0 rc I was able to replicate the issue both with the mnist dataset and the heart dataset. I tried some model variations and I believe that the problem comes from the joint occurrence of feature columns and save model. When I get it right the above code uses the functional api for keras. With the sequential model this issue happens too. The issue is also occurring when using h5 or json for saving and then loading the model. To be precise the error for h5 is ValueError We expected a dictionary here. Instead we got tf.Tensor Placeholder 0 shape None dtype float32 If there are any information which I can provide for solving the issue please post here. This error still persists with today s tf nightly also tried 1.15.0 and 2.0.0 . I have DenseFeatures as the first layer. I m able to evaluate predict and even save the model. But on load model I get Any suggestions Edit This only happens with a Sequential model. eliadl Please open a new issue with details related to issue and a standalone code to reproduce the issue. Thanks eliadl Have you any solutions for the problem magiclevinho I ended up using the Keras functional API https www.tensorflow.org guide keras functional rather than a Sequential model. Because that way the issue doesn t occur. eliadl Thank You for answering I don t know if the following code does similar as you said but it works for me I copied the code from an another website i don t know if I m allowed to post links the user was posting is so the credit goes for him her Egor B Eremeev from future import absolute import division print function import numpy as np import pandas as pd pip install tensorflow 2.0.0 alpha0 import tensorflow as tf from tensorflow import feature column from tensorflow import keras from tensorflow.keras import layers from sklearn.model selection import train test split URL https storage.googleapis.com applied dl heart.csv dataframe pd.read csv URL dataframe.head train test train test split dataframe test size 0.2 train val train test split train test size 0.2 print len train train examples print len val validation examples print len test test examples A utility method to create a tf.data dataset from a Pandas Dataframe def df to dataset dataframe shuffle True batch size 32 dataframe dataframe.copy labels dataframe.pop target ds tf.data.Dataset.from tensor slices dict dataframe labels if shuffle ds ds.shuffle buffer size len dataframe ds ds.batch batch size return ds batch size 5 A small batch sized is used for demonstration purposes train ds df to dataset train batch size batch size val ds df to dataset val shuffle False batch size batch size test ds df to dataset test shuffle False batch size batch size age feature column.numeric column age feature columns feature layer inputs numeric cols for header in age trestbps chol thalach oldpeak slope ca feature columns.append feature column.numeric column header feature layer inputs header tf.keras.Input shape 1 name header bucketized cols age buckets feature column.bucketized column age boundaries 18 25 30 35 40 45 50 55 60 65 feature columns.append age buckets indicator cols thal feature column.categorical column with vocabulary list thal fixed normal reversible thal one hot feature column.indicator column thal feature columns.append thal one hot feature layer inputs thal tf.keras.Input shape 1 name thal dtype tf.string embedding cols thal embedding feature column.embedding column thal dimension 8 feature columns.append thal embedding crossed cols crossed feature feature column.crossed column age buckets thal hash bucket size 1000 crossed feature feature column.indicator column crossed feature feature columns.append crossed feature batch size 32 train ds df to dataset train batch size batch size val ds df to dataset val shuffle False batch size batch size test ds df to dataset test shuffle False batch size batch size feature layer tf.keras.layers.DenseFeatures feature columns feature layer outputs feature layer feature layer inputs x layers.Dense 128 activation relu feature layer outputs x layers.Dense 64 activation relu x baggage pred layers.Dense 1 activation sigmoid x model keras.Model inputs v for v in feature layer inputs.values outputs baggage pred model.compile optimizer adam loss binary crossentropy metrics accuracy model.fit train ds magiclevinho I ended up using the Keras functional API https www.tensorflow.org guide keras functional rather than a Sequential model. Because that way the issue doesn t occur. For my purposes it does not work to use the Keras functional API isn t there another solution to this problem Having to switch to a different API will not work for every situation I also have the same issue. I tired with Tf nightly. And also tried switching to functional API. Both do not seem to work. The newly added experimental layers https www.tensorflow.org api docs python tf keras layers experimental preprocessing CategoryEncoding seems to be an attempt to solve the issue. In the old versions feature column blocks are defined outside of the model resulting in the issue. These new experimental layers are placed inside model definition. I haven t tried it yet but it looks promising. 
31957,Eager mode Accessing contents of scalars in a tf.function, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Any Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device No TensorFlow installed from source or binary binary TensorFlow version use command below 1.14.0 Python version 3.6.9 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Describe the current behavior I am trying to parallel download some images. I am using a tf.data.Dataset with the image urls as content. I want to store them in a GCS so I am using functions from the tf.io.gfile package inside a tf.function . This function will be called through tf.data.Dataset.map . When the different tf.io.gfile functions are called inside the tf.function like makedirs it raises an Error indicating that it requires a binary or unicode string as input If I try to use .numpy it is not available as expected. The result is that I cannot download the images in the GCS. Describe the expected behavior As a tensorflow package I would expect the tf.io.gfile functions to allow the use of scalar string tensors or I would expect tensorflow to provide a solution similar to the .numpy function inside tf.function for these cases. If not at least it should be a warning in the documentation that these functions cannot be used inside tf.function . Code to reproduce the issue The result in this case is Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,Was reproduced the issue with Tensorflow 1.14.0. Please see the gist here https colab.research.google.com drive 1EKMMLXpMreIz5 w3qTWKtodnbnBDydwZ . Thanks gfile is a python implementation through pywrap tensorflow not a kernel ops. So it will not work in graph mode that is why it is not working with tf.function and tf.data I believe. tf.io.read file and tf.io.write file are kernel ops so I they are compatible with tf.function and tf.data I think. yongtang Following your advice I have made another test using the tf.io.write file function The result is a If I do exactly the same but without the tf.function decorator The result is the one expected I assume I am making something wrong but sincerely I do not know what. In any case thanks for your comment. jmgc I ran into the same erorr with TF 1.14. However it looks like on TF 2.0.0RC0 it works fine runs on colab with the following jmgc I think this was resolved in tf nightly . I ran your code without any issues with pip install tf nightly . Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan ca5f8eb7db941e4bc6b69ba04ced0dfb tf 31957.ipynb . Thanks jvishnuvardhan it is correct that it works for the tf.io.read file and tf.io.write file . However it does still not work with the tf.io.gfile functions. jmgc Please check yongtang reason here https github.com tensorflow tensorflow issues 31957 issuecomment 524909757 . Thanks Automatically closing this out since I understand it to be resolved but please let me know if I m mistaken.Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31957 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31957 No a 
31962,Tensorflow 2.0 tf.function internal error, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Colab Ubuntu Windows Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below 2.0 beta 2.0 rc 2.0 nightly v1.12.1 9694 g006e2933 2.0.0 dev20190825 Python version 3.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 10 also without CUDA GPU model and memory Describe the current behavior Decorating the training loop that consumes a tf.Data.Dataset with tf.function causes an internal error in tensorflow an object is returned to Python with an error set and I m unable to understand the actual origin of the error . Not using tf.function the code works alright. Describe the expected behavior The model gradients should be calculated well regardless of being within a tf.function trace or not. Code to reproduce the issue Code is provided in the colab notebook available here https colab.research.google.com drive 1LCZqyGa8mPjBS6KXOnEH T7VOWoFpnnB Other info logs While working with a rather complex module that operates on irregular data Graphs using tf.function worsens performance in TF 2.0 due to bug 29075. While following the workaround described in that issue I stumbled upon this issue. Because of these issues TF 2.0 is not a good fit for DL on irregularly shaped data. Relevant traceback ,I was able to replicate the issue in TF version 2.0 preview. Thanks I was also able to replicate this error in the nightly 2.0 preview. This is a very strange error it seems to originate in the gradient calculation. It happens inside a Dataset.reduce but I don t know if that s the reason it crashes MainModel is fairly complex though it doesn t seem to do anything dangerous as far as I can tell. saxenasaurabh could you have a closer look at the error Perhaps we can detect its root cause and raise a more specific error Any updates on this I can confirm this only is happening if I use tf.function. I see this happening first 2020 01 01 20 59 20.099731 W tensorflow core common runtime shape refiner.cc 89 Function instantiation has undefined input shape at index 1211 in the outer inference context. 2020 01 01 20 59 20.567981 W tensorflow core common runtime shape refiner.cc 89 Function instantiation has undefined input shape at index 1211 in the outer inference context. 2020 01 01 20 59 21.052197 W tensorflow core common runtime shape refiner.cc 89 Function instantiation has undefined input shape at index 1211 in the outer inference context. 2020 01 01 20 59 21.441673 W tensorflow core common runtime shape refiner.cc 89 Function instantiation has undefined input shape at index 1211 in the outer inference context. 2020 01 01 20 59 21.757365 W tensorflow core common runtime shape refiner.cc 89 Function instantiation has undefined input shape at index 1211 in the outer inference context. My input data is a tf.data.Dataset with a padded batch. The only other thing I could think of is the Dataset supplies nested dictionaries. I m also having this error This looks like a bug related to GradientTape and Dataset . A few workarounds 1. Move the tf.function to train step so that the dataset iteration is outside the tf.function . That also seems to speed things up which might be another bug. 2. Use tf.gradients instead of GradientTape . The downside is that tf.gradients doesn t work in eager mode mdanatg For now I applied https github.com tensorflow tensorflow pull 33497 locally which fixes the problem. Currently I m already doing 1 I can try doing 2 as well. I checked the colab posted by mmv against tf nightly and it looks like 33497 resolved the issue. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 31962 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 31962 No a 
32018, TF 2.0 Unsupported op node error messages in latest tf nightly 8 27 19 , em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below tf nightly gpu 2.0 preview 2.0.0.dev20190827 Python version 3.7.4 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 10.0 7.6.2 GPU model and memory Titan Xp 12 gb You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior When running a model that previously yielded no errors I m getting errors of the form when using autograph with the tf.function decorator. Describe the expected behavior Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs I m just curious how we are supposed to interpret these logs so far it s non blocking but I m wondering if this is indicating autograph is failing to convert parts of my model. , mjlbach In order to expedite the trouble shooting process please provide a code snippet to reproduce the issue reported here. Thanks I countered the same issue. Here is my code that can be used to reproduce it. It works fine in tensorflow 2.0 without GPU but emits errors with tf nightly gpu 2.0 preview 2.0.0.dev20190827 I think the example above is sufficient I ll retest on nightly with https github.com tensorflow addons pull 459 cc alextp Hi Alexandre can you take a look at this or redirect to someone else who can Thanks. cc asimshankar I can t reproduce it on 20190830 s nightly so I think this is a bug that was fixed. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 32018 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 32018 No a 
32023,tf.keras.layers.Concatenate layer has unexpected behavior since 1.13, tf.keras.layers.Concatenate used to operate in a very straightforward way with the Keras functional API for building DenseNet esque feedforward networks. The code below works in TF 1.13 but fails in 1.14 2.0.0a0 and beyond. I also tested replacing the Concatenate layer with its functional alternative concatenate and produced the same error. The error output is Visualization of model https i.imgur.com 17UNrSk.png ,I was able to reproduce this issue in tensorflow 1.13.2 https colab.sandbox.google.com gist gowthamkpr c68210d825eda78bc557bfff218c9db8 untitled111.ipynb 1.14.0 https colab.sandbox.google.com gist gowthamkpr 08d42792fd855982f20bbfa9bf4f3b04 untitled112.ipynb scrollTo AEWA95USJilO 1.15 https colab.sandbox.google.com gist gowthamkpr 759847000ce53118d30ab366eee6211d untitled113.ipynb and 2.0 rc0 https colab.research.google.com gist gowthamkpr bb2abb3d6b2e7b546133b54c4dc52667 untitled114.ipynb Interestingly if doing it in the following way then it works. Is someone working on this noahtren I think this was resolved in tf nightly . I was not able to reproduce the issue with tf nightly . Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 747b7ec0bc45a54dfcb227dd47b7b6dd untitled114.ipynb . Thanks. Please close the issue if this was resolved for you. Thanks Yep it looks like this was fixed Thanks jvishnuvardhan Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32023 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32023 No a 
32029,tensorflow.keras.Model.compute output shape gives wrong results, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 linux Ubuntu 18.04 TensorFlow installed from source or binary conda TensorFlow version use command below tried with 1.12.0 and 1.14.0 Python version 3.6 Describe the current behavior using a keras model stored in a variable mm in tensorflow.keras I would like to calculate the output shape for a given input. This works correctly only the first time I call mm.compute output shape the subsequent results for calling the same function with different shapes are inconsistent. Using standard keras methods I get different and consistent results. An example for the problem is implemented in the tf bug.py script that you find in the zip if you call it without parameters it loads a fully convolutional model from a json file provided in the zip and does the result displaying the input and corresponding output shape on each line is I kept only the relevant lines. You see that after the first lines that are correct starting with input shape 1007 the output shape decreases and starts to produce erratic behavior while for the fully convolutional model it should increase monotonously with the input size. Describe the expected behavior Running the same script with argument keras uses the vanilla keras version 2.2.4 and in this case the output shape increases as expected Note that I can get a correct result with tf.keras as well if I clear the model. output shape cache before I compute the output shape. Running the script with argument clear uses a modified loop as follows The results are correct as expected. Looking into the function mm.compute output shape I found that compared to keras you changed the cache key generation where keras does tf.keras does It appears that the cache key in tf.keras confuses different input shapes as the same and returns wrong results from the cache. Code to reproduce the issue You find the script model and output files in the zip tf compute output shape bug.zip https github.com tensorflow tensorflow files 3548512 tf compute output shape bug.zip ,Was able to reproduce the issue. Please find the attachment of github gist here https colab.sandbox.google.com gist gowthamkpr b4519fb3fb254cd7daaecc7095070456 untitled127.ipynb . Thanks gowthamkpr Did you find the solution for this I have similar problem https github.com tensorflow tensorflow issues 33785 Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32029 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32029 No a 
32023,tf.keras.layers.Concatenate layer has unexpected behavior since 1.13, tf.keras.layers.Concatenate used to operate in a very straightforward way with the Keras functional API for building DenseNet esque feedforward networks. The code below works in TF 1.13 but fails in 1.14 2.0.0a0 and beyond. I also tested replacing the Concatenate layer with its functional alternative concatenate and produced the same error. The error output is Visualization of model https i.imgur.com 17UNrSk.png ,I was able to reproduce this issue in tensorflow 1.13.2 https colab.sandbox.google.com gist gowthamkpr c68210d825eda78bc557bfff218c9db8 untitled111.ipynb 1.14.0 https colab.sandbox.google.com gist gowthamkpr 08d42792fd855982f20bbfa9bf4f3b04 untitled112.ipynb scrollTo AEWA95USJilO 1.15 https colab.sandbox.google.com gist gowthamkpr 759847000ce53118d30ab366eee6211d untitled113.ipynb and 2.0 rc0 https colab.research.google.com gist gowthamkpr bb2abb3d6b2e7b546133b54c4dc52667 untitled114.ipynb Interestingly if doing it in the following way then it works. Is someone working on this noahtren I think this was resolved in tf nightly . I was not able to reproduce the issue with tf nightly . Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 747b7ec0bc45a54dfcb227dd47b7b6dd untitled114.ipynb . Thanks. Please close the issue if this was resolved for you. Thanks Yep it looks like this was fixed Thanks jvishnuvardhan Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32023 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32023 No a 
32029,tensorflow.keras.Model.compute output shape gives wrong results, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 linux Ubuntu 18.04 TensorFlow installed from source or binary conda TensorFlow version use command below tried with 1.12.0 and 1.14.0 Python version 3.6 Describe the current behavior using a keras model stored in a variable mm in tensorflow.keras I would like to calculate the output shape for a given input. This works correctly only the first time I call mm.compute output shape the subsequent results for calling the same function with different shapes are inconsistent. Using standard keras methods I get different and consistent results. An example for the problem is implemented in the tf bug.py script that you find in the zip if you call it without parameters it loads a fully convolutional model from a json file provided in the zip and does the result displaying the input and corresponding output shape on each line is I kept only the relevant lines. You see that after the first lines that are correct starting with input shape 1007 the output shape decreases and starts to produce erratic behavior while for the fully convolutional model it should increase monotonously with the input size. Describe the expected behavior Running the same script with argument keras uses the vanilla keras version 2.2.4 and in this case the output shape increases as expected Note that I can get a correct result with tf.keras as well if I clear the model. output shape cache before I compute the output shape. Running the script with argument clear uses a modified loop as follows The results are correct as expected. Looking into the function mm.compute output shape I found that compared to keras you changed the cache key generation where keras does tf.keras does It appears that the cache key in tf.keras confuses different input shapes as the same and returns wrong results from the cache. Code to reproduce the issue You find the script model and output files in the zip tf compute output shape bug.zip https github.com tensorflow tensorflow files 3548512 tf compute output shape bug.zip ,Was able to reproduce the issue. Please find the attachment of github gist here https colab.sandbox.google.com gist gowthamkpr b4519fb3fb254cd7daaecc7095070456 untitled127.ipynb . Thanks gowthamkpr Did you find the solution for this I have similar problem https github.com tensorflow tensorflow issues 33785 Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32029 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32029 No a 
32049,Creating a boolean constant prints a deprecation warning, System information Have I written custom code Yes OS Platform and Distribution Ubuntu 16.04 TensorFlow installed from binary TensorFlow version 2.0.0rc0 Python version 3.6 Describe the current behavior Creating a boolean constant prints a deprecation warning W0828 15 45 36.142576 139852094695168 deprecation.py 323 From lib python3.6 site packages tensorflow core python framework constant op.py 253 EagerTensorBase.cpu from tensorflow.python.framework.ops is deprecated and will be removed in a future version. Instructions for updating Use tf.identity instead. Describe the expected behavior No deprecation warning. Code to reproduce the issue ,Please find the Gist of Colab for TF 2.0rc0 https colab.sandbox.google.com gist oanush 14d18b393e1bd59d77307a28ec2b0c19 32049.ipynb .Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 32049 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 32049 No a I am still getting this warning on the current version of Tensorflow I just installed it yesterday . When will this be fixed for mainstream users Although far from satisfactory I managed to avoid the warning through tf.cast tf.zeros 10 tf.bool 
32089,TF2.0 RC tf.keras.model.Evaluate has display bug, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No. This is the code from https www.tensorflow.org beta tutorials quickstart beginner OS Platform and Distribution e.g. Linux Ubuntu 16.04 Google Colab and Ubuntu 18.04 TensorFlow version use command below pip install tensorflow 2.0.0 rc0 Python version 3.6 Describe the current behavior When I run model.evaluate with verbose True I get too many signs. I noticed this on Ubuntu 18.04 when I upgraded from TF2.0 beta to TF2.0.0 rc0. This is a mild annoyance when I am running models with soft warp enabled on the command line. Describe the expected behavior There should only be this many equal signs Not ............................................ Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs This issue may be due to the denominator of the number of evaluated samples completed. It should be 10000 10000 not 10000 1. Instead of 10000 1 ,Was able to reproduce the issue. Please find the github gist here https colab.research.google.com gist gowthamkpr a34035feed7b076e62a8fd51467394a9 untitled116.ipynb . gaborchris This was resolved in tf nightly . Please take a look at the gist https colab.sandbox.google.com gist jvishnuvardhan 9cf040a428c59f8efbd1e17f70d51a34 untitled116.ipynb . Thanks I am closing this issue as this was resolved. Please feel free to reopen if the issue persists for you. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32089 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32089 No a 
32117,tf.Keras.fit runs forever with 0 samples, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 TensorFlow installed from source or binary pip TensorFlow version use command below v2.0.0 beta1 5101 gc75bb66a99 2.0.0 rc0 Python version 3.7.4 CUDA cuDNN version n a CPU Describe the current behavior Code runs forever although nothing is to be trained no parameters no data . Describe the expected behavior Code stops pretty soon. Code to reproduce the issue Other info logs It s somewhat annoying if you set the number of training samples to 0 by mistake before starting to train you might not notice it for a while.,Was able to reproduce the issue here https colab.sandbox.google.com gist gowthamkpr 9c6f9b830da370ac1bf8774b3516b32e untitled124.ipynb Thanks for reporting bersbersbers I want to work on this issue Please assign me bersbersbers I agree with you that there is an issue with TF2.0rc0 . However it was resolved in the tf nightly 2.0 preview . Please take a look at the gist here https colab.sandbox.google.com gist jvishnuvardhan 4ee3cac706b982b4a5b80328a764e224 tf 32117.ipynb . It clearly throws ValueError Empty training data. within 2 seconds. I am closing the issue here. Please feel free to reopen if the issue persists again in the tf nightly 2.0 preview . Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 32117 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 32117 No a This problem is not fixed in 2.0.0rc1. This may be expected but I wanted to point it out. bersbersbers Thanks for finding this. I am reopening this as I could reproduce the issue with TF2.0rc1 . I closed earlier as I didn t see any issue with tf nightly 2.0 preview . Currently with TF2.0rc1 it throws a warning but runs forever as mentioned by bersbersbers Thanks imskr Please feel free to submit a PR for this issue thanks omalleyt12 Thank you I m working on it bersbersbers Could you please confirm if you are still facing any issues. As i have tried running the same code on tf nightly and TF 2.1 rc1 and it works fine please find the gist https colab.sandbox.google.com drive 1pQn6GLmHo6eyOngJKJUcGkXtX8yRRGkn scrollTo YsqkNpZ5TEdW . With tensorflow 2.1.0 I get an error as expected Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32117 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32117 No a With TF 2.3 I don t get Empty training data. any longer I now get UnboundLocalError local variable logs referenced before assignment see 38064 . 
32144,TF 2.0 metrics get mixed up when adding metrics in multiple layers, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 macOS 10.14.6 Ubuntu 18.04 TensorFlow installed from source or binary binary TensorFlow version use command below v2.0.0 beta0 16 g1d91213fe7 2.0.0 beta1 and v2.0.0 beta1 5101 gc75bb66a99 2.0.0 rc0 Python version 3.6.6 CUDA cuDNN version N A GPU model and memory N A Describe the current behavior Creating a Layer and Model calling add metric in both results in the metric values getting mixed up. Describe the expected behavior Metric values should not get mixed up Code to reproduce the issue Output Note how the values got mixed up. two should assume a value of 2.0 one a value of 1.0 three a value of 3.0. Other info logs The problem appeared in a more complex example training on GPU graph mode so although the minimal example above is using eager mode it also happens when running non eagerly. ,I have tried on colab with TF version 2.0 beta1 2.0.0 rc0 and 2.0.0.dev20190902 and was able to reproduce the issue.Please find the gist https colab.sandbox.google.com gist ravikyram 980587c5ffa5a6a8c2a049cc6dba2dce untitled145.ipynb here. Thanks peterkfm I am closing this issue as it was resolved by https github.com tensorflow tensorflow pull 32220. Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 68142e9b75eef414bf38014498fc6c85 tf 32144.ipynb . Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 32144 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 32144 No a 
32192,JSON serializable issue at RemoteMonitor,The Tensorflow implementation of RemoteMonitor callback raises the error Object of type float32 is not JSON serializable. Keras own implementation works fine with the same code. I think the relevant code difference is in the Tensorflow implementation and in the Keras implementation. ,Please provide details about what platform you are using operating system architecture . Also include your TensorFlow version. Also did you compile from source or install a binary Make sure you also include the minimal code snippet to reproduce the issue. If you are unclear what to include see the issue template displayed in the Github new issue template https github.com tensorflow tensorflow issues new choose . We ask for this in the issue submission template because it is really difficult to help without that information. Thanks My description above was a little bit short. It was meant as a short notice that the tensorflow.keras RemoteMonitor class has a bug which the Keras RemoteMonitor class does not have. Let me explain it with a little bit more detail. I have made a quick example which shows the error. This code gives a error Object of type float32 is not JSON serializable . But when delete the Tensorflow from the imports so using the keras implementation then the above code works fine. So I looked at the tensorflow.keras RemoteMonitor class https github.com tensorflow tensorflow blob r1.14 tensorflow python keras callbacks.py L1264 L1316 and compared it to the Remote Monitor class https github.com keras team keras blob master keras callbacks.py L847 from keras. As stated above the relevant difference is in the Tensorflow.keras and in the keras implementation. I have also tried to replace the lines in the Tensorflow.keras callback.py file with the lines from the keras callbacks.py file and then the code works with the Tensorflow imports. I have tried on colab with 2.0.0 rc0 and was able to reproduce the issue.Please find the gist https colab.sandbox.google.com gist ravikyram bf10dde754b4e8d73d8c021e4e014120 untitled156.ipynb here.Please let us know which TensorFlow version you are using .Thanks I have used 1.14.0 but the class RemoteMonitor is in 1.14.0 and 2.0.0 rc0 identical. I have tried on colab with 2.0.0 rc0 1.14 TF nightly versions and was able to reproduce the issue.Please find the gist here https colab.sandbox.google.com gist ravikyram 1be5308bed758dc9cedb42e2a866983b untitled156.ipynb .Thanks Hi I would like to work on fixing this. As the PR has been merged close the bug for now. Feel free to reopen it if needed. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32192 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32192 No a Is this issue resolved which build has this issue resolved 
32213, micro TFLite optimization failed with TFLite micro interpreter, System information custom code Modified TFLite micro example hello world to use MNIST. OS Platform and Distribution Linux Ubuntu 18.04 TensorFlow installed from binary TensorFlow version v1.12.1 3259 gf59745a381 2.0.0 beta0 Python version 3.6.8 Describe the current behavior Without TFL converter optimization prediction on micro interpreter works. With any TFL converter optimization prediction on micro interpreter gets wild and produce random values. For example converter.optimizations tf.lite.Optimize.DEFAULT Describe the expected behavior Regardless of TFL converter optimization prediction on micro interpreter should work. Code to reproduce the issue https github.com ehirdoy tflm helloworld tree bug NG https github.com ehirdoy tflm helloworld commit 13fd1c39e368729f574f44daf93299470ec1649d HEAD OK https github.com ehirdoy tflm helloworld commit a025d2a5f5c2487b551b6bfd8414926ebf864bfb HEAD git checkout f bug make input data.h make . hello world NG git checkout HEAD make . hello world OK Other info logs Logs are in the git commit mesage in the above OK NG commits. commit 13fd1c39e368729f574f44daf93299470ec1649d HEAD origin bug github bug Author Hiroshi Doyu hiroshi.doyu ericsson.com Date Wed Sep 4 14 14 33 2019 0300 BUG store TFLite file optimized for size Without the following converter.optimizations tf.lite.Optimize.OPTIMIZE FOR SIZE MNIST prediction works OK as seen in the previous commit log. With the above OPTIMIZE FOR SIZE inserted right before convert MNIST prediction seems to go wild with random values as seen below touch train mnist tflite.py make . hello world ...... ................ ................ ........... .... .... .... .... .... .... ... .... .... ..... .... ..... .... ..... ..... .... 0 0.096435 1 0.083848 2 0.109924 3 0.096781 4 0.103029 5 0.100698 6 0.095319 7 0.095055 8 0.114907 9 0.104004 0 0.000000 1 0.000000 2 0.000000 3 0.000000 4 0.000000 5 0.000000 6 0.000000 7 0.000000 8 1.000000 9 0.000000 , ehirdoy Is this still an issue Can you check with TF2.0 and let us know whether the issue persists with latest TF version. If this was not resolved with TF2.0 or tf nightly please update the gist https colab.sandbox.google.com gist jvishnuvardhan ea39638fcb8ad1294985631e25f4567e untitled546.ipynb and share. Thanks ehirdoy Did you had time to review my shared gist Thanks Automatically closing this out since I understand it to be resolved but please let me know if I m mistaken.Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32213 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32213 No a 
32231,Bug in categorical crossentropy loss label smoothing ,Possible bug found in categorical crossentropy s label smoothing argument in line https github.com tensorflow tensorflow blob r1.14 tensorflow python keras losses.py L940 L941 and for TF2 its in line https github.com tensorflow tensorflow blob r2.0 tensorflow python keras losses.py L960 L961 The axis specified in num class is 1 which works fine for a Rank 2 tensor but fails for higher rank tensors. def smooth labels num classes math ops.cast array ops.shape y true 1 y pred.dtype 1 instead of 1 return y true 1.0 label smoothing label smoothing num classes y true smart cond.smart cond label smoothing smooth labels lambda y true return K.categorical crossentropy y true y pred from logits from logits , praveenjune17 Sorry for the delay in response. Could you provide a simple standalone code to reproduce the issue Also provide platform details. Thanks jvishnuvardhan Raised this PR https github.com tensorflow tensorflow pull 32274 and added a Colab notebook which explains the issue. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 32231 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 32231 No a 
32250,tf.saved model.save broken for my subclassed model in tf 2.0.0rc0, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04.02 LTS Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below tf2.0.0rc0 Python version 3.6.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version N A GPU model and memory N A You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION n 4 tf.version.GIT VERSION Out 4 v2.0.0 beta1 5101 gc75bb66 In 5 tf.version.VERSION Out 5 2.0.0 rc0 Describe the current behavior I have a subclassed model that I have been saving and deploying since tf 2.0.0b0 and after upgrading to rc0 it errored out with this message. save.py 136 Skipping full serialization of object main .MyModel object at 0x7f76815add30 because an error occurred while tracing layer functions. Error message in converted code TypeError tf call takes from 2 to 3 positional arguments but 4 were given This is how the call method is defined in my class. Can someone tell me what went wrong or what I need to change to make this work Thanks David class MyModel Model def init self n layers h dim b dim activation fn kernel init batch norm super . init ... skipping ... tf.function def call self inputs training False y inputs 0 h inputs 1 n var inputs 2 x layers.concatenate y h if self.n layers 1 x self.d hidden 1 x training training if self.n layers 2 x self.d hidden 2 x training training if self.n layers 3 x self.d hidden 3 x training training if self.n layers 4 x self.d hidden 4 x training training if self.n layers 5 x self.d hidden 5 x training training scale output by 1 n var return self.d out x n var return self.d out x training training Describe the expected behavior It should just work Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. See above Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. , DavidKWH Can you please assemble a minimal reproducible code snippet to reflect this issue Thanks This has been resolved in the discuss forum. It has been reproduced and a fixed has been proposed by Kathy. Please see https groups.google.com a tensorflow.org forum topic discuss wlwLvWnD4BI This issue has been resolved in the nightly and cherrypicked to the 2.0 branch. Please let us know if it is ok to close the issue. Thanks Closing issue please feel free to reopen if you continue to see issues. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 32250 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 32250 No a 
32261,TF2.0 NonMaxSuppressionV2GPUOp segmentation fault, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below TF2.0.0 rc0 Python version Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version CUDA 10.0 GPU model and memory GeForce RTX 2080 Ti Describe the current behavior TF2.0 has a Non Max Suppression V2 op gpu implementation which seg faults when it is run. Due to this TF object detection model zoo https github.com tensorflow models blob master research object detection g3doc detection model zoo.md saved models cannot be run in TF2.0. Code to reproduce the issue gdb output ,cc samikama I think this is fixed with tf 2.0 nightly version 2.0.0 dev20190913 . Can you please confirm Thanks Output Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 32261 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 32261 No a I think this is fixed with tf 2.0 nightly version 2.0.0 dev20190913 . Can you please confirm Thanks Output Thanks It works here or install newer version 2.0 such as pip install tensorflow gpu 2.1.0 and it works for me 
32268,TF.Keras model creation results in different output node when eager is enabled or not, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Google Collab Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary No TensorFlow version use command below Google Collab default runtime Tensorflow 1.14.0 tf.keras 2.2.4 tf Python version Google Collab default runtime 3.6.8 default Jan 14 2019 11 02 34 GCC 8.0.1 20180414 experimental trunk revision 259383 Describe the current behavior building a tf.keras sequence model when eager mode is enabled vs not results in different output nodes. Describe the expected behavior Graph execution does not effect graph construction the model should be the same. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. With eager enabled Results in Without Eager Which results in a model with no output when saved to pb. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,See 32241 this may be the root of that issue vade Looks like the code is incomplete Please provide the complete code to reproduce the issue. Since i tried executing the code some of the variable such as IMG SIZE is not defined. Thanks Apologies Im happy to put together a very limited GCS demo of the issue. Hold tight. Hi. Eager Mode code in GCS https colab.research.google.com drive 16g3 5kgD5KV5 vz0DY07 bIO0miXp NP No Eager Mode code in GCS https colab.research.google.com drive 1JqmscKZTLy2GpUiTzSyOMDXepalPSZKR One line of code difference tf.enable eager execution Output with Eager Output with No Eager See the last print statement output vade Thanks for providing the GCS. I could reproduce the issue even in tf nightly . Thanks Thank you very much for looking into this. Much obliged and thank you for all of your work on TF Looking through https github.com tensorflow tensorflow issues 32241 it seems that the issue different output node was fixed but freeze graph is no longer supported. Closing this for now. If freeze graph is an issue please file another one Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32268 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32268 No a 
32285,Model unable to take input from dictionary with input layer names as key after loading it using tf.keras.models.load model, System information OS Platform and Distribution Linux Ubuntu 16.04 device TensorFlow installed from source or binary binary TensorFlow version use command below 2.0 rc Python version 3.6 My model works fine the first time when i create the architecture and train it but once i save the model using tf.keras.model.save model and then load it again using tf.keras.models.load model it fails to take input from the dictionary with input layer names as key as if the names of layers are changed. Example training mask tf.keras.layers.Input shape None None 1 name training mask target score map tf.keras.layers.Input shape None None 1 name target score map target geo map tf.keras.layers.Input shape None None 5 name target geo map These are three input layers that i am using to take input for training but once i save the model and later load it again to train it gives me the error No data provided for input 1 . Need data for each key in input 1 input 2 input 3 I am feeding data by specifying layer names inputs target score map score maps target geo map geo maps training mask training masks Therefore it works the first time when create I the architecture from code then train and save it but later when i load the model from disk it fails. Regards Shubham ,In order to expedite the trouble shooting process please provide a minimal standalone code to reproduce the issue reported here. Thanks Hey ravikyram On more trouble shooting i found out that the name of the layers are not changing something else is going on Following is the code to reproduce the issue model.fit works well but model1.fit fails with error ValueError No data provided for input 1 . Need data for each key in input 1 But if you provide the input without dictionary both model.fit and model1.fit both runs successfully. Thanks I have tried on colab with TF version 2.0 rc0 and was able to reproduce the issue.Please find the gist here https colab.sandbox.google.com gist ravikyram d76d69451de64646e2917bb0b555e56c untitled170.ipynb .Thanks I believe that this is fixed with commit https github.com tensorflow tensorflow commit 6a26d679113105820cf83a2447863a6a32488c47 diff 29e6348914dee326daf67d86aa5075db. Ran the colab thanks ravikyram with tf nightly 2.0 preview and it worked. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 32285 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 32285 No a Hello Everyone I am still facing this issue with stable release of tensorflow 2 i.e tf 2.0.0. Also I am unable to use model.summary after loading the model. It gives error You tried to call count params on Image input but the layer isn t built. You can build it manually via Image input.build batch input shape k w w Can you reopen this issue it is not yet resolved. Shubham3101 As mentioned by k w w this was already resolved in tf nightly . Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 89df5873b5a8fcf9a5214a354235525d tf 32285.ipynb . Thanks Thanks jvishnuvardhan it is working with tf nightly 2.1.0 version. 
32286,Simple model.evaluate example floods output with characters, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 Fedora 30 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary pip install tensorflow 2.0.0 rc0 TensorFlow version use command below v2.0.0 beta1 5101 gc75bb66 2.0.0 rc0 Python version Python 3.7.4 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior Running the code I get an output flooded with hundreds of thousands of characters when calling model.evaluate . Describe the expected behavior Code to reproduce the issue Other info logs Running this code in a Jupyter Notebook results in a performance penalty for the huge unnecessary output. ,Perhaps you can use semi verbose by setting to avoid repeated logging of character. GitHub Gist https colab.sandbox.google.com gist ymodak da242ce544a5102fce8eddf002b9ef10 github issue 32286.ipynb ymodak Yeah that is not a problem I can set verbose to 0 as well but I think it is a bug anyway. Note the 10000 1 in the progress bar instead of 10000 10000 . I think that is unexpected. wink same problem here https github.com tensorflow tensorflow issues 32320 issuecomment 548883188 Peque This has been resolved recently in tf nightly which is 2.1.0 dev20191108 that will be released in the future. I am closing this issue. Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 5ead8e4a4613f9bf58996093154d1290 untitled634.ipynb . Please feel free to open the issue if it persists again. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32286 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32286 No a this issue still seems to still persist with the current tf nightly has anyone found a solution warriorgiggles Can you please create a new issue with a simple standalone code to reproduce the issue Thanks 
32299,RNN stateful is incompatible with initial state, System information OS Platform and Distribution Windows 10 TensorFlow installed from source or binary binary via conda 1.14.0 and pip 2.0.0 rc0 TensorFlow version use command below 1.14.0 and 2.0.0 rc0 Python version 3.6 CUDA cuDNN version 10.0 GPU model and memory 1080 Ti Describe the current behavior When a stateful RNN is created and is given initial state it effectively resets the state to the initial state for every prediction. See the NOT EXPECTED example in the code below. Describe the expected behavior The stateful RNN should use the initial state the first time and then update the state and use it for each following time. Code to reproduce the issue ,I am able to reproduce the issue on Colab with Tensorflow 2.0.0.rc0. Please see the gist here https colab.sandbox.google.com gist gadagashwini f2f267963a9047f5804fcd0892824762 untitled130.ipynb . Thanks for reporting the issue I can reproduce it even with 1.13 which means this bug has been there for long time. Let me check what s the root cause there. Adding Francois since this probably impact the keras team keras as well. I think currently the initial state is taking priority if specified and I feel the state from previous batch should be respected if stateful True. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 32299 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 32299 No a Thanks a lot for the quick followup however I don t think the fix is correct. It seems that now the initial state is being ignored. Here s an experiment I ran after making the changes locally to recurrent.py of 2.0.0 rc0 as in https github.com tensorflow tensorflow commit dccf1c7030867865189924989c857bc1d3454216 The experiment suggests that the supplied initial state is ignored and zeros initialization the default for GRU I think is used in its place. When the model is built with a K.constant initial state of ones it should generate the same output as when ones are supplied to model.layers 1 .reset states . Also that output should be different than when the model is built with a K.constant initial state of zeros . I just edited my experiment code because I had a typo. The second version of the model was incorrectly set to ones but now I fixed it to use zeros. Now the code illustrates the issue. This is probably a better test to illustrate the issue. What we want is for the stateful True model to match the predictions of the stateful False model when an initial state is specified. This experiment creates a stateful False model1 with non zero initial state and uses it to predict 2 time steps. Then we build an equivalent stateful True model2 and see that its predictions are different when they should match model1 . Then model3 uses stateful False and no initial state and it matches the output from model2 when it shouldn t though it correctly differs from model1 . I hope that helps. Thanks for catching this. Sending fix very soon. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 32299 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 32299 No a Thanks. I think this is very close but now it seems that there is a weird corner case where using reset states to set the state to zeros has no effect. See this Thanks for pointing out the issue again. This corner case is actually hard to distinguish from the graph level. Since the initial state param is always provided in the graph there isn t an easy way to know whether the initial state or the recorded state is the latest. For now I am inclined to leave it as is since the most common use case is having a stateful RNN layer and call model.reset state . Please let me know if you have other opinions. Thanks. 
32320,keras model.evaluate progress bar WAY too long by default, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow just testing this https www.tensorflow.org tutorials keras basic classification OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 fully updated TensorFlow installed from source or binary pip3 install user tensorflow gpu 2.0.0 rc0 TensorFlow version use command below v2.0.0 beta1 5101 gc75bb66a99 2.0.0 rc0 Python version 3.7.4 CUDA cuDNN version 10.0 GPU model and memory GeForce GTX 1660 Ti 6 GB Describe the current behavior Just running a basic image classifier with Keras. Import data create model train evaluate. I run python script name.py in the Command Prompt. model.evaluate prints out an insanely long progress bar at the end. It s many MANY pages long with Command Prompt already maximized so one page is already a lot of characters . I have to scroll WAAAY UP to see the previous output. Describe the expected behavior I know I could turn off verbosity but I would expect sane defaults for the progress bars printed by TF Keras. And with verbose 1 that thing is so huge it s useless. Code to reproduce the issue ,I tried on Google colab but it is working as expected.Please see the gist here https colab.sandbox.google.com gist gadagashwini 4e8b0ca20a27ab518f6329aaf5479dd6 untitled127.ipynb . And I could replicate the issue on my system by running it on terminal. Please see the screenshot below. Screenshot from 2019 09 09 11 22 50 https user images.githubusercontent.com 48476109 64506178 3421e300 d2f4 11e9 852a 9d1a6d1e64ac.png Thnaks Same problem here. Code Output image https user images.githubusercontent.com 14865017 64515840 a7742680 d2ed 11e9 973f 488e72f1c440.png Notice how short the slider is. It is all filled with signs. I confirm I am seeing the issue too. The eval progress bar seems to be broken. Adding a batch size and a number of steps does not change the erroneous behavior model.evaluate test images test labels verbose 1 batch size 1000 steps 10 Expecting exactly 10 progress steps with an eval dataset of 10 000 elements Getting many many steps.... Same issue here. Even on the official tutorial page the progress bar is extremely long. https www.tensorflow.org tutorials keras classification url qlzh727 Could this be related to the training v2 loop Very likely. https github.com tensorflow tensorflow blob f9ad945a479caccca9002dcfe0e9623e3b753360 tensorflow python keras engine training v2.py L448 samples use sample should be samples total samples djshen Thanks for the find Agreed would you please submit a PR to fix this You can add me as a reviewer I will approve omalleyt12 I created a PR https github.com tensorflow tensorflow pull 33921 Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32320 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32320 No a same here is going to be merged in 2.1 Yes this should be fixed in 2.1 
32341,Keras model with sequence feature columns fails to convert to estimator, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Google Colab Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device No TensorFlow installed from source or binary binary TensorFlow version use command below v2.0.0 beta1 5101 gc75bb66 2.0.0 rc0 Python version 3 Bazel version if compiling from source No GCC Compiler version if compiling from source No CUDA cuDNN version Unknown from Colab GPU model and memory Unknown from Colab Describe the current behavior When i rewrote my estimator based model Dataset Feature columns to keras i was able to run train it. But when i converted it to estimator as shown in migration guide https www.tensorflow.org beta guide migration guide estimators it fails with error Describe the expected behavior Working keras model should always be convertable to estimator. Code to reproduce the issue Here is an example https colab.research.google.com drive 11pWyltRzvQPPvM2dWQ8EqxafB9ByCxNn There are 2 cases shown. First i show that keras model with sequence features is working. See model.fit generator . And then i show that conversion to estimator fails. ,The example failed with latest tf 2.0 nightly version with different stack trace as follows. ymodak it is clear from your trace that you should install estimator package before continue Same error in 2.0.0 release. P.S. Update TF version in reproduce link https colab.research.google.com drive 11pWyltRzvQPPvM2dWQ8EqxafB9ByCxNn scrollTo HH3J1Tksm3L shkarupa alex Can you please try tf nightly I think this was resolved in tf nightly as I don t see any issue as you reported. Please change first line with pip install q U tf nightly tensorflow datasets . Here is the gist https colab.sandbox.google.com gist jvishnuvardhan 361c36d2b8ad4d6cc5dd25d03bb77a97 fail to convert keras model with sequence features to estimator.ipynb with tf nightly . Thanks Please close the issue if this was resolved by tf nightly . Thanks shkarupa alex Can you please try tf nightly I think this was resolved in tf nightly as I don t see any issue as you reported. Please change first line with pip install q U tf nightly tensorflow datasets . Here is the gist https colab.sandbox.google.com gist jvishnuvardhan 361c36d2b8ad4d6cc5dd25d03bb77a97 fail to convert keras model with sequence features to estimator.ipynb with tf nightly . Thanks Please close the issue if this was resolved by tf nightly . Thanks You re right conversion error goes away. The only problem i see for now is message duplicates shkarupa alex I agree. I noticed those duplicate messages. Thanks shkarupa alex I agree. I noticed those duplicate messages. Thanks Looks like this is fixed. For the warning message part this is the output from estimator. Maybe try to set it through tf.compat.v1.logging.set verbosity Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32341 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32341 No a 
32369,tf.keras.layers.LSTM does not pass its trainable values to its cell, System information TensorFlow version 1.12.0 I believe the issue appears in 1.14.0 as well . Python version 3.5.2 When running this the last line raises the following error TypeError Input b of MatMul Op has type float32 that does not match type float64 of argument a . The LSTM constructor is not passing the dtype to the LSTMCell constructor so lstm.cell.dtype is just None. So the weights created in LSTMCell.build have their dtype default to tf.float32 which seems to cause the error. Adding lstm.cell. dtype float64 right before the last line seems to fix the issue. There may be other mismatches between LSTM and LSTMCell but the dtype mismatch is the one I ran into.,Also seems to be a very similar problem with using tf.keras.layers.LSTM 512 trainable False where the created weights end up being trainable. Am I misunderstanding the API Can you try with tf nightly pip install tf nightly and let us know whether the issue still persists. Thanks tf nightly seems to handle dtype correctly. I m guessing it s because of https github.com tensorflow tensorflow commit d90e521d71b88f469e68eb1a467606ea6d44c733 or similar. However if I use tf.keras.layers.LSTM 512 trainable False the created weights still end up being trainable. There may be other mismatches still besides trainable. I have tried on colab with TF version 1.12 1.14 nightly versions using tf.keras.layers.LSTM 512 trainable False and was able to reproduce the issue.Please find the gist here https colab.sandbox.google.com gist ravikyram 5575b47ccc0fc4a67ec99dbe4d48a4ca untitled172.ipynb .Thanks bparr I could reproduce the issue with even pip install tensorflow 1.15.0rc0 . However there is a workaround as follows tf.keras.backend.set floatx float64 workaround If you enable the above line then it will set default dtype as you wish and it doesn t throw any error. Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 1f3b64224cb322e36323f7baf0263154 tf 32369 dtype keras.ipynb . Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 32369 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 32369 No a 
32395,AssertionError Unreachable when adding or subtracting Dataset range element and tf.constant, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 Home Version 10.0.18362 Build 18362 TensorFlow installed from source or binary binary pip TensorFlow version use command below 1.14 Python version 3.6.9 Describe the current behavior I m trying to calculate the minimum index to take for extracting windows out of the dataset. I ve got a constant window size defined of which dtype defaults to int32 . After running the following code it throws Assertion Error. Code to reproduce the issue the same issue happens when I add or multiply but not divide . When dividing with this mapper I get TypeError x and y must have the same dtype got tf.int64 tf.int32 . Then I ve tried casting window size to tf.int64 window size tf.constant 3 dtype tf.int64 and that fixed the issue in all cases. Describe the expected behavior I d expect the error to be the same in case of addition subtraction multiplication as in case of division as it turns out to be the dtype issue. Other info logs ,I could reproduce the issue on Colab with TF 1.14.0. Please see the gist here https colab.sandbox.google.com gist gadagashwini 3e14ff83be3cfa578cc4762b30d30e6a untitled137.ipynb . Thanks This is fixed with TF 1.15. I tested with TF version 1.15.0 dev20190821 in google colab. The error message is more intuitive now. Output Further casting to resolves the issue as you mentioned. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 32395 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 32395 No a 
32398,ValueError from invalid weights while loading older .h5 model, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 TensorFlow installed from source or binary binary TensorFlow version 1.13.1 v1.12.1 10753 g1c2ae57 2.0.0 dev20190910 Python version 3.7.4 CUDA cuDNN version 10.1 GPU model and memory Describe the current behavior I created a tfkeras model and saved it to .h5 format using TF version 1.13.1. The model can be loaded and used for inference just fine in 1.13.1. After upgrading to TF 2.0 nightly build loading the model results in a ValueError see traceback below . I am using tf.compat.v1.disable v2 behavior in case that might make a difference. Describe the expected behavior V1 models should load correctly or present the user with a way to migrate the model to a more compatible format. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. Relevant traceback info I added some print statements to get info on the weights for this batchnorm w. renorm layer. The names for the weights in the original model are The expected weight placeholders are I m guessing the format for saving batchnorm w.renorm parameters changed at some point Is there a way to make this backwards compatible Or perhaps a way to migrate the save file , ghannum Make sure that your code fully compatible to TensorFlow 2.0 before inference. Please refer the Tensorflow website https www.tensorflow.org beta guide migration guide hl en for further guidance. Thanks As a first step to check TF2 compatibility I tried running with the new 1.15 rc0 release. The error remains as is. I don t believe this is a 2.0 specific issue. Release 1.14 works ok so the issue is somewhere between 1.14 and 1.15 2.0. I also tried to load save my model using 1.14 to effectively move the problem forward. I get the same issue trying to load it in 1.15. ghannum Will it be possible to provide the code to reproduce the issue. Thanks create model.py load model.py Here are two scripts which will reproduce the issue. The first creates a simple keras model and saves it to .h5 format. The second loads the model and prints a validation measure. If you run the first in TF1.13 the second will fail in TF1.15. This is only a problem with the BatchNormalization renorm option set to True. ghannum Thanks for the code. I tried on colab with Tf 2.0.0.rc1 I didn t receive any error Please take a look at colab gist https colab.sandbox.google.com gist gadagashwini 7a0df6ba3bf7a2f0fae872eff1c710e1 untitled162.ipynb . Thanks It looks like you ran both scripts in Tf 2.0.0.rc1. The error only occurs if the model was created in Tf 1.13. This is a backwards compatibility issue with the file format parser. That gist tool is awesome btw. ghannum I tried with Tensorflow 1.13.1 on colab but i didn t get any error. Please see the gist https colab.sandbox.google.com gist gadagashwini f050afc522840a9706dbaeb49d37e6b7 untitled163.ipynb . I executed both the create model and load model with same TF. Is there any specific reason to use two different versions of TF. Thanks Thanks for trying 1.13. The error is only when the model is created in 1.13 and loaded in 1.1.5 2.0 two different versions . The backwards compatibility is important because these models take considerable time and resources to train months in some cases . I currently have several important models in 1.13 and would like to continue using them after upgrading to 1.15. I imagine this sort of continuous compatibility is part of the 1.X design scope. Even if the file format changes there should be some migration path. In this case I ve identified the difference in the renorm parameters being stored see errors above and a summary below . Can we identify why these changes were made and how one might make them compatible Either the code can be made to recognize both formats or there can be some procedure to upgrade the file Thanks again for your help BatchNormalization weight names in 1.13 BatchNormalization weight names in 1.15 2.0 I was able to reproduce the reported behavior. Built was model using TF 1.13.1 but failed to load using TF 1.15.0 rc2. It loads successfully in TF 1.14.0 Looks like the renorm stddev weight variable was removed between 1.14 and 1.15 https github.com tensorflow tensorflow commit af15fb8624a0c0eabdd00ba1653cbbd4734c3b36 . Accidentally breaking checkpoints is extremely easy. I would recommend that in the future use the TensorFlow checkpoints by specifying save format tf which are more robust when dealing with variable changes. You can migrating the Savefile following these steps requires using different versions of TensorFlow . Using TensorFlow 1.13 or 1.14 Load the model using model tf.keras.models.load model Save the model into two separate files model.save weights model weights.ckpt save format tf The weights file will still contain the renorm stddev weight variable but will be omitted when loading. Using TensorFlow 1.15 Load the model The batch normalization layer will not have the renorm stddev weight variable but the rest of the variable values will be restored. Now save the model by calling model.save model.h5 bcoopers I m not super familiar with the BatchNormalization layer. I m a bit concerned about correctness since a variable was entirely removed. If a model were trained using the renorm stddev weight and then loaded without it would it still function correctly I have the same problem but with the 2.0 beta1 and 2.0 release tf nightly gpu.. I found a temporary hack which allowed me to load the old model and get it saved into a compatible format. I modify the file tensorflow core python keras saving hdf5 format.py Hi With the drop weight you got the same result Such as predict or resume train So far it s looking correct... ghannum Is this still an issue. Please feel free to close the issue if it is resolved Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32398 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32398 No a 
32409,keras.backend.learning phase broken when sample weights are used, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 TensorFlow installed from source or binary binary TensorFlow version use command below 1.14.0 Python version 3.7 Describe the current behavior There are multiple issues Issue 1 keras.backend.learning phase does not always return a scalar value as stated by the documentation https www.tensorflow.org api docs python tf keras backend learning phase but it returns multiple values. Issue 2 Even when though it returns multiple values the shape says it is a 0d tensor indicating it is a scalar Issue 3 Reduce does not do anything even though the tensor has multiple values. Describe the expected behavior Issue 1 Documentation is fixed to reflect the correct behavior Issue 2 The right shape is set for the tensor Issue 3 reduce any should reduce all values when no axis is given irrespective of the shape. Code to reproduce the issue Other info logs , pavanky Can you try with TF1.15.0rc0 and let us know whether the issue persists Thanks jvishnuvardhan Issue 1 no longer persists but I am not sure how I can reproduce Issue 2 and Issue 3 without this Issue 1 being broken. If we can find a way to repro the remaining 2 issues I can create a new ticket and close this one. pavanky Issue2 and 3 depends on Issue1 right If issue1 is no longer persists then Issue2 and 3 are no longer valid i guess. Please let us know what you think. You could close the issue and open a new issue when you can reproduce issue 2 and 3. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 32409 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 32409 No a 
32416, lite micro missing delete in GreedyMemoryPlanner,https github.com tensorflow tensorflow blob 8c0df1fa0b0490d8b1e54d7b019e2b2242ad6718 tensorflow lite experimental micro memory planner greedy memory planner.h L43 does not override void operator delete void p which results in link time error., csukuangfj Is this resolved or still an issue Thanks jvishnuvardhan when this pullrequest https github.com tensorflow tensorflow pull 32417 is merged this issue should be closed by GitHub automatically but more than 3 months have passed it is still not merged. csukuangfj I see the reviewer approved the PR. So it will be merged soon. Thanks jvishnuvardhan Things are not always that easy like you thought. You can see that the pullrequest has been approved for multiple times but nothing happens when the ready to pull label is added. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32416 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32416 No a 
32476,Unexpected output shape on custom keras dynamic layer, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOS Mojave 10.14.6 TensorFlow installed from source or binary pip install tensorflow 2.0.0rc0 TensorFlow version use command below 2.0.0 rc0 Python version 3.7.4 Describe the current behavior Upon attempting to create a custom dynamic keras layer keras seems to incorrectly interpret the output of compute output shape . Describe the expected behavior In the example code below model.summary outputs None 2 for the output shape. According to the docs examples I would expect that to be None 2 . When attempting to place layers after this it returns two placeholders despite the output shape only defining one. Code to reproduce the issue In my code the input layer s batch shape and the content of call are arbitrary. If I remove dynamic True then it gives the expected shape based on the contents of call . There seems to be no semantic difference in output if compute output shapes returns None 2 None 2 or None 2 Other info logs Here s what I am seeing from model.summary ,I reproduced the issue on Colab with tf 2.0.0rc0. Find a Colab gist here https colab.sandbox.google.com gist gadagashwini 56a82640d89c5c26555da2d6aa1f4a57 untitled149.ipynb . Thanks It looks like using tf.TensorShape works properly. See This outputs See updated Colab https colab.research.google.com gist porgull c93dce7d1039b3ccacc1c9c16b956fa4 untitled149.ipynb . porgull this issue is no longer seen with 2.2.0 rc0. Can you please check and close this issue if it is resolved for you Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32476 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32476 No a 
32477,BatchNormalization doesn t work in graph mode in tf2, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS MacOS TensorFlow version use command below 2.0.0 rc0 Python version 3.7 tf.keras.layers.BatchNormalization layer does not work in graph mode code to reproduce Error stack If you remove the tf.function decorator or if you pass dynamic True to the model instantialization it will work. Otherwise it fails. Note that this is specific to BatchNormalization if you replace it with any other layer it will work even other normalization layers like layer norm instancenorm ,Looks like code is incomplete. Request you to provide simple standalone code snippet to reproduce the issue in our environment.Thanks Looks like code is incomplete. Request you to provide simple standalone code snippet to reproduce the issue in our environment.Thanks Just added the import statements. Sorry for the exclusion. I have tried on colab with TF version 2.0.0 rc0 2.0.0 rc1 and was able to reproduce the issue.Please find the gist here https colab.sandbox.google.com gist ravikyram 20ab0b70683608e4d33dd82b2934835d untitled190.ipynb .Thanks bump also have this issue. code works with instance norm from TFA but fails with batch norm using tf.function and functional API... seems the issue only happens when custom layers or subclassed models use batch norm. if batch norm is used as a standalone layer it works jvishnuvardhan robieta mshlis ravikyram bionicles yeah i noticed that i shifted to use the functional api as it works there and explicitly creating the block i was using with BN but i feel its still important this gets fixed given that higher level layer abstraction and model sub classing are some of convenient interfaces of the framework Any updates news on this issue As a workaround you can set the class variable USE V2 BEHAVIOR to false. kkimdev The original code is now failing due to the assert in ops. override gradient function which comes from https github.com tensorflow tensorflow commit cafc2b641f62d00ba00c8fb7742f3c2ebc80c387. Can you take a look Will this issue be fixed in tf2.1 Will this issue be fixed in tf2.1 Just checked and this still appears in 2.1 After a little more digging you can fix this by going into the batchnormalisation constructor and replacing self. trainable var None with self. trainable var K.freezable variable trainable name self.name trainable . Not sure why but in the unaltered version this var only get created on the first call to the batch normalisation layer which I guess messes with some .fit magic. Getting the same problem. mshlis Is this still an issue I am not able to reproduce the issue with tf nightly . Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 18aab600662fbcf67620aa4844bd1abd 32477.ipynb . Thanks Please close the issue if it was already resolved for you. Thanks It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue I am closing this issue as it was resolved in recent tf nightly . Please feel free to reopen if It was not resolved for you. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32477 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32477 No a 
32487, TF 2 Multi gpu training error,I am trying to train a keras model on two k80. Have I written custom code OS Platform and Distribution e.g. Linux Ubuntu 16.04 SMP Debian 4.9.144 3.1 TensorFlow version use command below 2.0.0 rc0 Python version 3.6.9 CUDA cuDNN version 10.1 GPU model and memory Tesla K80 Here is the the keras model that I am trying to fit but I am getting the following error Everything runs fine if I exclude the with strategy.scope ,Issue replicating with TF version 2.0rc0 please find the gist https colab.research.google.com gist oanush fa05103b0c9f82e26949f3ede67ffaf5 32487.ipynb of the colab.Thanks We have an internal fix for this pending. Meanwhile adding should work. It works indeed. Thanks. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 32487 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 32487 No a I also met this problem and use above the internal fix thanks still I have few questions 1. is this a bug and will be fixed in the later version or this is the way to use it 2. what causes this 3. if i don t set this option but to do some changes to the dataset will it work as expected 
32501,Error when using stateful RNN with multiple inputs, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow no OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0rc0 Python version 3.6.8 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version 10.0.130 7.6.0 GPU model and memory GTX 980 Ti Describe the current behavior The stock example of RNNs with multiple inputs from here https www.tensorflow.org beta guide keras rnn rnns with listdict inputs or nested inputs produces an error if you set stateful True . This seems to be a problem with any multi input RNN with stateful True. Describe the expected behavior There should be no error multi input RNNs with stateful True should work the same as with stateful False other than preserving state . Code to reproduce the issue Note this code is copied from https www.tensorflow.org beta guide keras rnn rnns with listdict inputs or nested inputs with the exception that I changed the line to Other info logs ,Issue replicating for TF version 2.0rc0 and also 2.0rc1 please find the gist https colab.sandbox.google.com gist oanush 8b07187ad22ba68f2bdd7f45a058d851 32501.ipynb of the colab.Thanks This issue is present in 1.14.0 as well and probably earlier but the RNN api was different so the example doesn t run for other reasons . Thanks for reporting the issue. Let me take a look. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 32501 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 32501 No a 
32543,RNN layer does not reset dropout masks of RNNCell, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 macOS Mojave 10.14.6 TensorFlow installed from source or binary binary TensorFlow version use command below v1.12.1 9392 gf3c7314d83 1.15.0 rc0 Python version 3.7.4 Describe the current behavior The RNN layer with an RNNCell does not reset the states of dropout masks compared to the layer implementations of the cells. Thus the behavior of tf.keras.layers.GRU 10 tf.keras.layers.RNN tf.keras.layers.GRUCell 10 . This is especially problematic because the Keras RNN API tutorial https www.tensorflow.org beta guide keras rnn rnn layers and rnn cells states both approaches are mathematically equivalent. Describe the expected behavior The RNN layer should check the type of the RNNCell and if it is a subclass of DropoutRNNCellMixin reset the dropout masks after each call. By calling cell.reset recurrent dropout mask and cell.reset dropout mask . Code to reproduce the issue Partially copied from https github.com tensorflow tensorflow issues 29391 Output ,I have tried on colab with TF 1.15.0 rc0 nightly versions and was able to reproduce the issue.Please find the gist here https colab.sandbox.google.com gist ravikyram d29fc400d2933fbf3efedfe1311f9a83 untitled186.ipynb .Thanks Thanks for reporting the issue let me fix it. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 32543 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 32543 No a 
32553,FutureWarning in TensorFlow 1.14 after NumPy update.,System information OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 TensorFlow installed from source or binary binaries from pip TensorFlow version use command below 1.14 Python version 3.6.1 CUDA cuDNN version 10.0 with any 7.5.0 GPU model and memory Geforce RTX 2060 After updating the NumPy to the latest version 1.17.2 mkl I receive the following warnings in TensorFlow 1.14 , kiflowb777 In order to expedite the trouble shooting process please provide a code snippet to reproduce the issue reported here. Thanks I believe this is closed by 3402d711846 kiflowb777 Associated PR has been merged. Let us know if the problem still persists. Thanks Closing this issue since the associated PR has been merged. Feel free to reopen if the problem still persists. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32553 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32553 No a 
32570,Assertion error when using mask with unrolled stacked LSTM, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device n a TensorFlow installed from source or binary source TensorFlow version use command below 1.14 Python version 3.6 Bazel version if compiling from source n a GCC Compiler version if compiling from source n a CUDA cuDNN version n a GPU model and memory no GPU 32 GB ram You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior I receive an assertion error when creating a forward pass for an unrolled multi layer LSTM while using a mask. Describe the expected behavior No assertion error or at least a better explanation as to the cause. Code to reproduce the issue Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. , neonrights I tried in colab with TF version 1.14 and 1.15.0 rc0 and i am getting the below error .AttributeError module tensorflow.python.keras.api. v1.keras has no attribute layer . I am attaching the gist https colab.sandbox.google.com gist ravikyram 9603c7f3d9814aa73513ef4db56f94b0 untitled193.ipynb for your reference. Thanks Sorry I have a typo in my code try this instead I have tried on colab with TF version 1.14 1.15.0 rc0 and was able to reproduce the issue.Please find the gist here https colab.sandbox.google.com gist ravikyram e202ba52a64cfd2a3fb1f51bd50f9b67 untitled200.ipynb .Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 32570 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 32570 No a 
32586,RNN does not forward the training flag to StackedRNNCells, System information Have I written custom code Yes OS Platform and Distribution Ubuntu 16.04 TensorFlow installed from binary TensorFlow version 2.0.0rc0 Python version 3.6.6 Describe the current behavior When using tf.keras.layers.StackedRNNCells with tf.keras.layers.RNN the RNN layer does not forward the training flag to the cell. This is because the RNN code checks that cell explictly defines the training flag as argument which tf.keras.layers.StackedRNNCells does not. https github.com tensorflow tensorflow blob r2.0 tensorflow python keras layers recurrent.py L709 L710 Describe the expected behavior The training flag should be passed to tf.keras.layers.StackedRNNCells and to each stacked cell. Code to reproduce the issue The code below should not raise the AssertionError ,Was able to reproduce this issue. Please find the attachment of github gist here https colab.sandbox.google.com gist gowthamkpr 6e1f7a532de56ec863606e7f1657f1c3 untitled144.ipynb . Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 32586 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 32586 No a 
32608,Failed to convert an RNN built with tf.keras by TFLiteConverter, System information OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 7 TensorFlow installed from source or binary binary TensorFlow version or github SHA if from source tensorflow gpu 2.0.0rc1 Provide the text output from tflite convert Also please include a link to a GraphDef or the model if possible. Code which would reproduce the error log Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,Hi We are working on a new converter that can convert keras lstm rnn into tflite. Please stay tuned bbb00437 Can you check the with the new converter with an experimental flag cvt.experimental new converter True . I ran your code with the experimental flag and works without any issue. Please take a look at the gist here https colab.sandbox.google.com gist jvishnuvardhan 82af131c5bb275fbe683458510ca833a untitled635.ipynb . I am closing the issue as it was resolved with the new converter. Please feel free to reopen the issue if it persists again. thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32608 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32608 No a 
32612,Keras can not load custom Loss functions., em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes. OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary pip install TensorFlow version use command below 2.0.0 beta1 Python version 3.6 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version Cuda 10.0 GPU model and memory RTX 2080 Titan You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior If overloading the loss function in tf.keras.losses.Loss the created model can be compiled trained and saved but not loaded. Describe the expected behavior Custom loss function should be loadable. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Output Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,I have tried on colab with TF version 2.0 beta1 2.0.0 rc0 2.0.0 rc1 and was able to reproduce the issue.Please find the gist here https colab.sandbox.google.com gist ravikyram cae08a626511eaab7fa6b4eda8f80405 untitled199.ipynb .Thanks I also tested 2.0.0 rc1 and the issue persists. ravikyram If there is a label for 2.0.0 rc1 can we also add it The same problem also exists for classes implementing tf.keras.metrics.Metric . somedadaism Can you save the model before compiling with MyCustomLoss function. Then load the saved model and compile with MyCustomLoss function as shown below. Thanks This does only work for this simple toy example. But if I want to fit the model and then save and the reload it will not work. I know how to workaround this thing myself but the intended way is not working for me. The same problem also exists for classes implementing tf.keras.metrics.Metric. somedadaism Custom functions are not compatible. model.save before model.compile works for small toy example and big model also. When you load the saved model load the model using tf.keras.models.load model and then compile . Please try this even for the above example or any other big model and let us know if you have any issue there. If you don t have any custom function then follow define model then compile model then save the model and then reload the model . Thanks By simple toy example I meant my code was so minimal that your workaround would work for it but not for the standard usecase. It is not related to the model size. The difference is that usually you also fit the model. I had updated the code in my last post to show how save before compile usually does not make sense. Can you have a second look at my last code snippet I think Custom Loss Functions and Metrics should be supported. Can you fix this so keras works as expected somedadaism I haven t tried customloss as class object but I tried it as function like In this way you can save the model as usual create model compile fit save and load the model using load model with custom objects like new model tf.keras.models.load model . model.h5 custom objects customLoss1 customLoss1 customLoss2 customLoss2 Please check entire gist here https colab.sandbox.google.com gist jvishnuvardhan 04028d08660378ee7c2cef2b9c3bb2e9 custom metric.ipynb . Thanks Hello thank you for your effort. I know also how to workaround for the loss function. My favorite workaround for now is to use load model path compile False . If it was only the loss functions this whole issue could be solved by just using normal functions as you propose. The fundamental problem is that the subclasses of tf.keras.metrics.Metric does not work either for presumably the same reason. Subclasses of this class can not be replaced by simple functions because metrics can be stateful. If I implement my own stateful metrics in the style of for an example tf.keras.metrics.mIoU there is no alternative to this but if I save the model I can not load it anymore or if I use my workaround I loose the optimizer state. For this reason I think that a fix is necessary for the issue although there are workarounds that work in many cases. k w w Can you give me a rough estimation for the timeframe to expect for the fix of the issue codebase I will deliver to a client depends on it. k w w jvishnuvardhan Are there any updates on the issue k w w The solution proposed by thierryherrmann here https github.com tensorflow tensorflow pull 33229 is good but only if the inputs to the loss object are not tensors. Otherwise a serialization exception is generated. We could try to convert the tensor into a numpy array inside get config as it is done by the LossWrapperFunction class. Unfortunately that does not work either. It seems Keras is not well integrated with eager execution. Side note having access to the LossWrapperFunction class would lead to much cleaner solutions . I have created a gist here https colab.research.google.com gist humcasma d7d00d6d31c4cca623c1140f050e8f94 problem loading custom loss.ipynb with different attempts to solve the problem. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32612 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32612 No a 
32622, TF 2.0 tf.assign fails with int32 tensors using a multi device distribution strategy, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below 2.0rc0 Python version 3.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 10 7 GPU model and memory 2 GPUs Describe the current behavior If you create a distribution strategy like MultiWorkerMirroredDistributionStrategy then create a tf.int32 variable var then call var.assign some int32 tensor to assign TF will crash. It crashes in values.py line 1220 So what s happening here appears to be that TF tries to divide the int32 tensor by a float with no cast and blows up. Describe the expected behavior It seems like being able to assign integers to tensors while using distributed training is something that should be possible given the goal of abstracting away distribution as much as possible. I know the distribution strategies may be kind of in flux right now and I m mainly just posting to make more knowledgeable people away of the issue I m just going to make all my tensors floats in the meantime . I don t have the context to know how this should be approached anyway. Cast everything to floats Make it so that the tensors don t always have to be divided Divide integer tensors by assigning everything to one worker and giving zeros to everyone else Who knows. ,I believe this has been fixed by in one way perhaps not the best https github.com tensorflow tensorflow commit 438fc52e81ead89799067a2b961f6990d9b4f5f6 diff 580627c9b7904095019167ef005a72de. Please re open if not and thank you for filing Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 32622 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 32622 No a 
32662,Attention layer not serializable because it takes init args but doesn t implement get config, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOS 10.14.5 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below v2.0.0 rc0 101 gd2d2566eef 2.0.0 rc1 Python version Python 3.6.8 Anaconda Inc. Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Describe the current behavior Trying to save a model with a tensorflow.keras.layers.Attention layer throws because it doesn t implement get config . NotImplementedError Layers with arguments in init must override get config . Describe the expected behavior Model.save should save the model. Code to reproduce the issue a Input shape None 10 attn Attention a a model Model a attn model np.zeros 50 10 dtype np.float32 model.save my model.h5 Other info logs miniconda3 envs tf20 lib python3.6 site packages tensorflow core python keras engine base layer.py in get config self 571 or that get config has been overridden 572 if len extra args 1 and hasattr self.get config is default 573 raise NotImplementedError Layers with arguments in init must 574 override get config . 575 TODO reedwm Handle serializing self. dtype policy. NotImplementedError Layers with arguments in init must override get config .,Was able to reproduce the issue. Please find the github gist here https colab.sandbox.google.com gist gowthamkpr d79de99944192c0f8195b975718809b5 untitled147.ipynb . In the mean while I was able to save the weights and load them. You can find it in the gist above. The issue was recently fixed in https github.com tensorflow tensorflow commit 498e815097e74aff7fefdbbae69ba9daf6e9c023 but we haven t cherrypick it into 2.0 rc yet. I verified the code against nightly and it works. Could you try to save it as tf format as a workaround Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 32662 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 32662 No a same problem in tensorflow 2.0 right now. Actually I don t know what triggers this issue. 
32672,A keras model containing a tf.tile op layer with a tensor in the multiples arg fails when saving to hdf5, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOS 10.14.5 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below v2.0.0 rc0 101 gd2d2566eef 2.0.0 rc1 Python version Python 3.6.8 Anaconda Inc. Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior A keras model containing a tf.tile op layer with a tensor in the multiples arg throws an exception when saving to hdf5. I m using tf.tile because RepeatVector n doesn t accept a tensor for n. The goal is to stack a 2d feature batch so it can be concatenated to a variable length 3d batch of sequence features. Describe the expected behavior Model.save should save the model. Code to reproduce the issue import numpy as np import tensorflow as tf from tensorflow.keras.layers import Input from tensorflow.keras import Model a Input shape 10 out tf.tile a 1 tf.shape a 0 model Model a out x np.zeros 50 10 dtype np.float32 print model x .numpy model.save my model.h5 Other info logs ValueError Traceback most recent call last ipython input 2 9b1429243599 in module 11 print model x .numpy 12 13 model.save model dir my model.h5 miniconda3 envs tf20 lib python3.6 site packages tensorflow core python keras engine network.py in save self filepath overwrite include optimizer save format signatures options 1187 1188 saving.save model self filepath overwrite include optimizer save format 1189 signatures options 1190 1191 def save weights self filepath overwrite True save format None miniconda3 envs tf20 lib python3.6 site packages tensorflow core python keras saving save.py in save model model filepath overwrite include optimizer save format signatures options 110 or using save weights . 111 hdf5 format.save model to hdf5 112 model filepath overwrite include optimizer 113 else 114 saved model save.save model filepath overwrite include optimizer miniconda3 envs tf20 lib python3.6 site packages tensorflow core python keras saving hdf5 format.py in save model to hdf5 model filepath overwrite include optimizer 107 model weights group f.create group model weights 108 model layers model.layers 109 save weights to hdf5 group model weights group model layers 110 111 TODO b 128683857 Add integration tests between tf.keras and external miniconda3 envs tf20 lib python3.6 site packages tensorflow core python keras saving hdf5 format.py in save weights to hdf5 group f layers 623 624 for layer in layers 625 g f.create group layer.name 626 weights legacy weights layer 627 weight values K.batch get value weights miniconda3 envs tf20 lib python3.6 site packages h5py hl group.py in create group self name track order 66 name lcpl self. e name lcpl True 67 gcpl Group. gcpl crt order if track order else None 68 gid h5g.create self.id name lcpl lcpl gcpl gcpl 69 return Group gid 70 h5py objects.pyx in h5py. objects.with phil.wrapper h5py objects.pyx in h5py. objects.with phil.wrapper h5py h5g.pyx in h5py.h5g.create ValueError Unable to create group name already exists ,I was able to replicate the issue with given code for TF 2.0rc1 please find the gist https colab.sandbox.google.com gist oanush 8ee3a61919408dadf12499d2d95792ca 32672.ipynb of colab.Thanks currivan I Don t see any issue if you remove the .h5 from the model.save . I changed only last line from your code as follows. Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 9348e139288b969517ed527334e5fb57 32672.ipynb . Thanks I am closing this issue as it was resolved. Please feel free to open it if the issue persists again. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 32672 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 32672 No a This should not have been closed. Throwing an exception here is a bug and the solution is unacceptable. Saving this type of model with h5 takes a second and the proposed way it takes over a minute. Loading is also unacceptably slow. currivan In case If i need to use an op like tf.tile as in your case I will call it with a lambda layer. So the code is as follows Please let me know what you think Please check the gist https colab.sandbox.google.com gist jvishnuvardhan 56a985172d3b0702b1eb0b0b2d263761 tf32672.ipynb . Thanks jvishnuvardhan thanks I was also able to create a custom layer. In general I feel if a model can be executed it should be savable by all standard methods without throwing an opaque exception. Someone should still fix the bug with saving op layers. ValueError Unable to create group name already exists isn t the right way to handle this even if it s unsupported. currivan I agree. May be we need to update the error description. Please note that fixing this is not simple. k w w Could you please take a look at this issue I believe that this is the same issue as 12195. When adding a tf.tile operation the keras model gets a tf op layer Title multiples layer added before a tf op layer Tile layer which I believe is what causes the problem. ELind77 12195 seems totally unrelated to this. Did you mean to link to a different issue currivan One problem with using Lambda is that your layer won t have a proper shape defined in case following layers need that. currivan I think this was resolved recently in tf nightly . I ran it with tf nightly without any issue. Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 15f06ff402a678a77d0c47592a290701 32672.ipynb . Thanks I am closing this issue as it was resolved. Please feel free to reopen. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32672 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32672 No a I have a similar problem. I m not using tf.tile I have a custom layer which uses tf.concat tf.map fn and tf.cast. The latest nightly tf nightly 2.2.0.dev20200226 seems to crash on my machine so I m not able to test it fully. The error message is the same I found this issue by googling it . jsilter Can you please create new issue with a standalone code to reproduce the issue Thanks This issue is happening for me with tf.clip by value . I haven t tested the nightly build. 
32680,autograph False not respected when calling get concrete function , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 reproduced on Ubuntu 18.04 and MacOS 10.14 TensorFlow installed from source or binary binary TensorFlow version use command below 1.14.0 v1.14.0 rc1 22 gaf24dc91b5 Python version 2.7.16 Describe the current behavior When passing in tf.function ... autograph False console output indicates that autograph still attempts to autograph the function. Describe the expected behavior That the function is not going to be autographed. Code to reproduce the issue This encountered when passing in a function that autograph fails on. For example a python function that is actually a wrapper around an R function. This example is in R ,I just checked against TensorFlow version 1.15.0rc1 and this issue is fixed there. Opening back up because this is not fixed in TensorFlow version 2.0.0rc1 t kalinowski I could reproduce the issue with Tf 2.0.0.rc1. Can you try with latest Tf 2.0 nightly version. Thanks Thanks This appears to be fixed in tf 2.0 nightly preview tf version 2.0.0 dev20190920 and in tf version 1.15.0 rc1 However this is currently broken in the tf v1 nightly taken from pip install tf nightly tf version 1.15.0 dev20190821 TF 1.15 rc1 is the latest TF 1.X release so we can expect the fix to be included in final TF 1.15 version as well. TF nightly version was built on 08 21 2019 where as TF 1.15rc1 version was released on 09 16 2019. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 32680 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 32680 No a 
32702,Keras casts targets to incorrect dtype, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0rc1 Python version 3.6.8 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior Keras tries to cast targets to the dtypes of the model outputs. Currently it assumes that every model output has a corresponding target so when doing this casting it just matches outputs and targets up one to one. But in the case where some outputs are not part of the loss function i.e. they were missing from the loss dictionary passed to compile this may match outputs to the wrong targets. Then it casts targets to the wrong dtype causing errors. Describe the expected behavior Keras should match up targets with the correct output when casting according to the loss dictionary defined in compile . If a model output is not part of the loss function then it should be ignored when casting targets. Code to reproduce the issue This results in the error ,Issue replicating for TF 2.0rc1 please find the gist https colab.sandbox.google.com gist oanush b796d39a08fb5c287034ece6853834d3 32702.ipynb of colab.Thanks Is this issue still open drasmuss can you please check this issue with tf nightly . When I ran your code with tf nightly i see the following output. Please close the issue if this issue was resolved for you. Thanks Yep looks like this is fixed in tf nightly . Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32702 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32702 No a 
32737,tf.keras.layers.Input has undefined shape when setting sparse True making it impossible to use in a Model, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04 TensorFlow installed from source or binary binary TensorFlow version use command below 1.15.0rc1 Python version 3.5.2 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 10.1 GPU model and memory Nvidia GeForce GTX 1050 4GB Describe the current behavior The tensor created by tensorflow.keras.layers.Input specified shape sparse True has shape None None len specified shape and cannot be used as input to e.g. Dense layers. Describe the expected behavior The tensor created by tensorflow.keras.layers.Input specified shape sparse True has shape None specified shape and can be used as input to e.g. Dense layers. This is the default behaviour in Keras. Code to reproduce the issue Other info logs Stack trace ,Could reproduce the issue on colab with TF 1.15.1.rc1.Please take a look at gist here https colab.sandbox.google.com gist gadagashwini 8ed685bf1786cd6fe8f392a285fdd2c6 untitled162.ipynb . Thanks danielegrattarola This is due to sparse tensors. Please read this https github.com tensorflow tensorflow issues 23748 issuecomment 443890338 for more details on the source of the issue. There is not an easy fix for this. A workaround is i Input shape 1 sparse True batch size 10 Thanks jvishnuvardhan I see but why did it work in previous versions of Keras and TensorFlow then Also defining a batch size is only a workaround for very simple use cases not a stable solution at all. It feels like this makes using sparse tensors practically impossible in most situation. Is there anything that I or anybody else can do to restore this functionality in Tensorflow danielegrattarola I think omalleyt12 provided couple of other ideas creating subclass layer or Lambda layer to convert your sparse inputs to dense in response to the other similar issue https github.com tensorflow tensorflow issues 23748 issuecomment 443890338 . Please feel free to follow that link and start contribute through PRs. Thanks I ve fixed this in 74195b50fb5e1f22eb95ffd1646b4b0ceca2ea9b 23748 Closing this issue as it was resolved already. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32737 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32737 No a I am on tf. version 2.5.0 rc3 and while Model.predict now works fine with sparse data Model.fit does not If I switch my imports from tf.keras to just import from keras Dense models work just fine. Why does the keras package support training with sparse input but tf.keras does not zachmayer Shoot when I fixed it for predict I added tests for predict assuming that there were tests for fit somewhere ... I ll take a look it may b e too late to patch for tf2.5... Can you post the fill trace back and or the some simple code to reproduce this I tried to reproduce this but did not get that error model.fit worked fine for me . I need more details to make any progress. Is it possible you re passing a sparse tensor to a layer that doesn t support it Here is a reproducible example. Some notes 1. If you are using sparse data you must call x.sort indices before fitting a keras model. It sort of feels like maybe keras should do that for you 2. My specific error was caused by calling model.fit on a pandas.Series but the error message did not make this at all clear I suggest that maybe keras either a. raise an exception on pandas series input or b. convert the pandas series to a numpy array. I fixed my error by converting the pandas series to a numpy array. Here is the code And then you simply change model.fit x y to model.fit x y.values and the code works. I am on tensorflow version 2.5.0 rc3 The traceback when the fit fails is below. What s interesting is that the pandas series isn t part of the traceback. Converting numpy to pandas somehow changes how the sparse matrix is indexed MarkDaoust given my code snippet are you able to reproduce MarkDaoust ping Thanks for the info and the ping . I haven t worked out the details but it looks like the difference in behavior stems from this line in the data adapter code https github.com tensorflow tensorflow blob r2.5 tensorflow python keras engine data adapter.py L988 When you pass the pd.Series the result is GenericArrayLikeDataAdapter fails but when you pass the values as a np.array the result is a CompositeTensorDataAdapter passes . That decision seems to be driven by the can handle method here https github.com tensorflow tensorflow blob 15d5b930d7e6e4463e275cfcff22042105148e3f tensorflow python keras engine data adapter.py L569 . Maybe that line should be checking the get tensor types https github.com tensorflow tensorflow blob 15d5b930d7e6e4463e275cfcff22042105148e3f tensorflow python keras engine data adapter.py L569 list of classes. But IDK. 
32748,AttributeError when passing tf.Variable into tf.function , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device no TensorFlow installed from source or binary source TensorFlow version use command below v2.0.0 rc2 Python version 3.7.2 Bazel version if compiling from source 0.25.2 GCC Compiler version if compiling from source 5.4.0 20160609 CUDA cuDNN version None GPU model and memory None Describe the current behavior Using this code piece tensorflow throws an error AttributeError NoneType object has no attribute shape Describe the expected behavior No error occured. Code to reproduce the issue Provided above Other info logs None, dovahcrow Workaround can be change any one of tf.Variable to tf.constant or both to tf.constant . Let us know this helps. Thanks gadagashwini Thanks I currently solve this by using tf.identity to convert the Variable to a Tensor . But still I wonder is this a bug or feature dovahcrow I could reproduce the issue with TF 2.0.0.rc2. Please see the gist here https colab.sandbox.google.com gist gadagashwini 4960b9907cbf83daae6a643136653125 untitled164.ipynb . Thanks dovahcrow tf.function on any defined function allows it to retrace the function when necessary to generate the correct graphs. I think your intention is not to control and trace the graph. In that case you could use the following code Please check the resource here https www.tensorflow.org beta tutorials eager tf function when to retrace for more details on the retrace. Please close the issue if this was resolved by the modification. Thanks jvishnuvardhan Hi I don t think this is because of the misuse of the API. First by changing the code piece to Tensorflow will raise the error I reported. In this case Tensorflow is disallowing my redefining the variable which is totally a legal operation in Python. This is very unexpected and super hard to debug when happened in a large codebase in my case I spent 5 hours to reduce thousands lines of code to such a simple case. Second as per the document said For all other Python types the keys are based on the object id so that methods are traced independently for each instance of a class. In the future TensorFlow may add more sophisticated caching for Python objects that can be safely converted to tensors. I get this as the output as well as the error This indicates tf.function somehow didn t retrace the graph even if the id s of the objects are different. Thank you guys for the notes. I spend two days to realize that when using tf.function causes the error and when removing tf.function everything works fine. Then I came up with search phrase of AttributeError NoneType object has no attribute shape when using tf.function and I found this post. I fixed the issue by using tf.constant when feeding training set to the function. Sorry for the late update. This was fixed in 4a5b3dbee4e2e5b60f58fe61530b73664e36d87e and should land in 2.1. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32748 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32748 No a 
32755,Bincount Op test test negative fails with TF 2.0, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary Binary TensorFlow version use command below 2.0.0 rc1 Python version 3.6 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version 10.0 GPU model and memory RTX 2080 Ti 11G Describe the current behavior The test negative test in tensorflow python kernel tests bincount op test.py fails as the bincount call with negative values does not throw an InvalidArgumentError. This behavior might be the result of the op being called on the GPU as only the CPU call is expected to throw the error as per the comment here https github.com tensorflow tensorflow blob r2.0 tensorflow python kernel tests bincount op test.py L107. Setting CUDA VISIBLE DEVICES to be empty forces the op to run on CPU and the test passes the invalid argument error is successfully thrown but passing use gpu False as an option to the session wrapper does not have this effect. Describe the expected behavior The test negative test should pass as the call to bincount with a negative input value is expected to throw an InvalidArgumentError. Code to reproduce the issue Run the python test tensorflow python kernel tests bincount op test.py . , MattConley Will it be possible to provide the sample standalone code to replicate the issue. Thanks gadagashwini Unfortunately the minimal repro I have at the moment is the bincount op test itself specifically test negative https github.com tensorflow tensorflow blob master tensorflow python kernel tests bincount op test.py L106 L110 . The issue might be with the test framework itself as internal session wrappers are used even with TF 2.0. Also setting CUDA VISIBLE DEVICES to be empty forces the op to run on CPU and the test passes the invalid argument error is successfully thrown but passing use gpu False as an option to the session wrapper does not have this effect. I was able to reproduce the error reported in TF 2.0 nightly version reedwm can you take a look at this Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32755 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32755 No a 
32770,Asset file not exported in the SavedModel when using tf.lookup.StaticVocabularyTable, System information Have I written custom code Yes OS Platform and Distribution Ubuntu 16.04 TensorFlow installed from binary TensorFlow version 2.0.0rc2 Python version 3.6.6 Describe the current behavior When using tf.lookup.StaticVocabularyTable the asset file is not exported in the SavedModel assets directory. However it is correctly saved when using tf.lookup.StaticHashTable . Describe the expected behavior The vocabulary file should be saved in the assets directory of the SavedModel. Code to reproduce the issue Other info logs The code above raises an AssertionError as the assets directory is empty.,Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32770 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32770 No a 
32772,Training parameter to model passed as None in 1.14.0, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary pip TensorFlow version use command below v1.14.0 0 g87989f6959 1.14.0 Python version 3.6.8 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version 10.1 7.5 GPU model and memory GTX 1080Ti Describe the current behavior Training parameter to model passed as None Describe the expected behavior Training parameter should be True in training and False in inference Code to reproduce the issue Other info logs Outputs , stefanpantic Is it possible for you to try latest TF versions pip install tensorflow gpu 1.15.0 rc0 and let us know whether the issue persists There were lots of performance improvements in the latest versions. Thanks This gets passed as the training parameter instead of None in the 1.15 release candidate. An aditional question I have is is it necessary to pass the training parameter to the Dropout BatchNormalization layers or can they infer the appropriate value based on K.learning phase stefanpantic Thanks for the issue This is fixed in the latest tf nightly pip install U tf nightly Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32772 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32772 No a 
32786,Keras Layer.compute output shape calls build with wrong input shape, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0rc2 Python version 3.6.8 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior When using Layer.compute output shape or Layer.compute output signature on a Layer with a build function the build input shape ... argument is always set to None. Describe the expected behavior The input shape should be set to the shape passed to compute output shape . Code to reproduce the issue ,I have tried on colab with TF version 2.0.0 rc2 2.0.0 dev20190924 and was able to reproduce the issue.Please find the gist here https colab.sandbox.google.com gist ravikyram 4071357c68ee4ac7295198754d9489a0 untitled219.ipynb . Thanks issue still exist in nightly and in tensorflow 2.1 https colab.sandbox.google.com gist Saduf2019 8f56913030e3bf31cd3344a9076d6b2d 32786.ipynb drasmuss I updated your code to build the layer correctly. With that update code is not throwing any error. Please check the gist here https colab.research.google.com gist jvishnuvardhan 00a883bdcb7bcd40f9f1007176d99df4 32786.ipynb . Please close the issue if this was resolved for you. Thanks I don t think that solves the issue it just avoids it by explicitly calling build ahead of time with the correct shape . The problem that is being highlighted in this issue is that when compute output shape calls build automatically it calls it with the wrong shape. It says right in the docstring of compute output shape that it shouldn t be necessary to manually call build so I don t think this is expected behaviour Computes the output shape of the layer. If the layer has not been built this method will call build on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. https www.tensorflow.org api docs python tf keras layers Layer compute output shape Re assigning to Yanhui Hi drasmuss there is a fix just submitted and you may verify it with tf nightly later. Close this for now and feel free to reopen it if needed. Thanks. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32786 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32786 No a 
32801,UpSampling2D doesn t support bfloat16, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow no OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary TensorFlow version use command below nightly Python version 3.6.8 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior TypeError Value passed to parameter images has DataType bfloat16 not in list of allowed values int8 uint8 int16 uint16 int32 int64 float16 float32 float64 Describe the expected behavior support bfloat16 Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. , fsx950223 I could reproduce the issue with TF1.15.0 . Here is the gist https colab.sandbox.google.com gist jvishnuvardhan 371cabae8a54797e13b887103170f6c6 untitled629.ipynb with TF1.15.0 . However your code works without an issue using TF2.0 . Please take a look at the gist https colab.sandbox.google.com gist jvishnuvardhan 701fd556c91f231b7cc2316fe53d62d4 untitled628.ipynb . I think there may not be any updates to TF1.15.0 unless the issue is related to security. Are you willing to upgrade to TF2.0 Thanks fsx950223 I could reproduce the issue with TF1.15.0 . Here is the gist https colab.sandbox.google.com gist jvishnuvardhan 371cabae8a54797e13b887103170f6c6 untitled629.ipynb with TF1.15.0 . However your code works without an issue using TF2.0 . Please take a look at the gist https colab.sandbox.google.com gist jvishnuvardhan 701fd556c91f231b7cc2316fe53d62d4 untitled628.ipynb . I think there may not be any updates to TF1.15.0 unless the issue is related to security. Are you willing to upgrade to TF2.0 Thanks Yes Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32801 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32801 No a I m getting this same issue in TF2.1 line 10 in module x tf.keras.layers.UpSampling2D x File C Users mdlambe1 AppData Local Continuum anaconda3 envs tf2 lib site packages tensorflow core python keras engine base layer.py line 773 in call outputs call fn cast inputs args kwargs File C Users mdlambe1 AppData Local Continuum anaconda3 envs tf2 lib site packages tensorflow core python keras layers convolutional.py line 2004 in call interpolation self.interpolation File C Users mdlambe1 AppData Local Continuum anaconda3 envs tf2 lib site packages tensorflow core python keras backend.py line 2782 in resize images x new shape method image ops.ResizeMethod.NEAREST NEIGHBOR File C Users mdlambe1 AppData Local Continuum anaconda3 envs tf2 lib site packages tensorflow core python ops image ops impl.py line 1357 in resize images v2 skip resize if same False File C Users mdlambe1 AppData Local Continuum anaconda3 envs tf2 lib site packages tensorflow core python ops image ops impl.py line 1133 in resize images common images resizer fn images size File C Users mdlambe1 AppData Local Continuum anaconda3 envs tf2 lib site packages tensorflow core python ops image ops impl.py line 1337 in resize fn images t new size half pixel centers True File C Users mdlambe1 AppData Local Continuum anaconda3 envs tf2 lib site packages tensorflow core python ops gen image ops.py line 3419 in resize nearest neighbor name name File C Users mdlambe1 AppData Local Continuum anaconda3 envs tf2 lib site packages tensorflow core python framework op def library.py line 576 in apply op helper param name input name File C Users mdlambe1 AppData Local Continuum anaconda3 envs tf2 lib site packages tensorflow core python framework op def library.py line 61 in SatisfiesTypeConstraint .join dtypes.as dtype x .name for x in allowed list TypeError Value passed to parameter images has DataType bfloat16 not in list of allowed values int8 uint8 int16 uint16 int32 int64 float16 float32 float64 LambertMark Please create a new issue with details and a simple standalone code to reproduce the issue. Thanks 
32819,Shared and mutated state between training and validation callbacks, System information Have I written custom code Yes OS Platform and Distribution Datalab Mobile device NA TensorFlow installed from Binary TensorFlow version 1.15.0rc1 Python version 3.5.6 Bazel version NA GCC Compiler version NA CUDA cuDNN version NA GPU model and memory NA Describe the current behavior When invoking keras.Model.fit with two data.Dataset s one is for training and one is validation so that the training one is infinite with epochs and steps per epoch provided to fit and the validation one is finite without any additional parameters to fit the progress bar shows incorrect number of steps after the first validation. In particular the number of steps per epoch gets set to the number of steps in the validation set. Describe the expected behavior The prescribed steps per epoch remains the same in the progress bar for all epochs. Code to reproduce the issue See below. Other info logs I believe the problem is due to this modification when the input is exhausted https github.com tensorflow tensorflow blob r1.15 tensorflow python keras engine training v2.py L136 The number of steps gets overwritten. However self.params is shared across both the training and validation callbacks which essentially misleads the progress bar for training. The state share is potentially due to the following line https github.com tensorflow tensorflow blob r1.15 tensorflow python keras engine training v2.py L347 In other words validation callbacks is based on training callbacks . configure callbacks should probably take callbacks instead., IvanUkhov In order to expedite the trouble shooting process please provide a minimal standalone code to reproduce the issue reported here. Thanks ravikyram it is good that you asked for an example. It turned out it was only about those datasets whose size could not be known in advance. I had to use a generator to reproduce. I have tried on colab with TF version 1.15.0 rc0 1.15.0 rc1 and was able to reproduce the issue.Please find the gist here https colab.sandbox.google.com gist ravikyram 608e6fcff91f3b80d8025f7f9c4876f6 untitled231.ipynb . Thanks I have an idea of how it could be fixed and I will try to explore it in 32847. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32819 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32819 No a 
32849,TFlite conversion of tf.keras model fails, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device No TensorFlow installed from source or binary pip install TensorFlow version use command below v2.0.0 beta1 0 g8e423e3d56 2.0.0 beta1 Python version 3.6.8 Bazel version if compiling from source GCC Compiler version if compiling from source 7.4.0 CUDA cuDNN version No GPU model and memory Nvidia titan Xp Describe the current behavior The converter fails to convert the model I m able to use the model on a python based inference engine. I m trying to just compress the model to deploy it on smaller setup and consume via a c c wrapper.,any update on this Were you able to fix this issue codeyman I don t see any issue with tf lite conversion of keras model in TF2.0. Here https colab.sandbox.google.com gist jvishnuvardhan 6ff9f60fcc2245727ef380070e0951d1 tf32849.ipynb is the gist. I am closing this issue. Please feel free to open it if the issue persists with TF2.0. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 32849 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 32849 No a Could you please use the model from the summary that I provided I understand that it works on the sample code. Can you please try with the same layers codeyman Can you please create the model and follow the steps I mentioned in the code I shared. If there is any issue please create a colab gist and share it here. Thanks jvishnuvardhan Please look at my bug creation code. It is exactly the same import tensorflow as tf model tf.keras.models.load model keras model.h5 model.summary converter.convert This throws the error codeyman Could you share a standalone code to reproduce the issue I cannot run your code as I don t have keras model.h5 and also you didn t instantiate converter . Please provide entire standalone code to proceed further in resolving the issue. Thanks jvishnuvardhan Please see the standalone code Here https colab.research.google.com drive 1F1F3wp0QFOs7UKVk6J7t42UdU2ccurfW . jvishnuvardhan Can you please reopen this issue codeyman I could reproduce the issue. I think this is more related to embedding . Were you able to train this model without any issues Thanks jvishnuvardhan Yeah no issues while training. Note that is the standard malconv model https arxiv.org pdf 1710.09435.pdf Any news on this We are running into the same issue with a network using an embedding layer. We encountered a very similar issue in our model and it is consistent in tf2.0 official release but it looks it is gone in the latest TF nightly build. Closing this issue because the code snippet in https github.com tensorflow tensorflow issues 32849 issuecomment 539646714 works on the nightly pip install tf nightly . codeyman Please request to reopen if you experience additional problems. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32849 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32849 No a gargn Could you please provide the bug that fixes this issue I have to compile tensorflow on my setup and I d like to just cherry pick just this changeset over the tf 2.0 release. Closing this issue because the code snippet in 32849 comment https github.com tensorflow tensorflow issues 32849 issuecomment 539646714 works on the nightly pip install tf nightly . codeyman Please request to reopen if you experience additional problems. what if I need Tensorflow 1.11.0 for example I cannot just pip install tf nightly because my customer s code is not compatible with tf 2.x. I am getting this exception in 2.3.0. There seems to be a workaround see https github.com usimarit TiramisuASR blob main tiramisu asr models layers embedding.py but I am not a big fan of having my own Embedding layer implementation in my code. Is there another way Any exact solutions Meet the same issue for LSTM. 
32869,sparse categorical crossentropy does not use underlying logits of tf.keras.layers.Softmax, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac OS TensorFlow installed from source or binary pip TensorFlow version use command below 2.0.0 rc0 Python version 3.6 Describe the current behavior As per the comment in sparse categorical crossentropy the function will use the underlying logits for better numerical stability if the last op is a softmax https github.com tensorflow tensorflow blob 1ae01a07f5e4b2911218d5fc3419b3eaed48b75b tensorflow python keras backend.py L4522 L4533 But since keras adds an identity op to the output of every layer the last node of a layer will always be Identity so the condition in the above if statement will never fail for keras models and the underlying logits are never used https github.com tensorflow tensorflow blob b0aa37c3fdff00b5c69bce11c716c3e56f656dd4 tensorflow python keras engine base layer.py L851 L853 Using non keras layers like tf.nn.softmax will work however. Describe the expected behavior Instead sparse categorical crossentropy should check the first non Identity node when determining whether the last node is a softmax node. I.e. it should first walk the chain of identity nodes. Code to reproduce the issue , This seems to have an absolutely HUGE impact on my models. I really recommend anyone using tf.keras.layers.Softmax to use tf.nn.softmax instead until this issue is fixed. Edit do be warned that using tf.nn.softmax directly will not propagate any masks you might have. You can hack masks back like this Edit Do not do the above. It seems that this puts tensorflow with a weird state wrt. propagating masks where the mask is propagated during graph construction but not during model fitting. This means that you might see a improvement in your models because it tries to predict masked outputs instead of real outputs Tried replicating with given code for TF 2.0rc2 the same error was received. Kindly find the gist https colab.sandbox.google.com gist oanush 78bd4585819aa58fbc13155a1ab3871d 32869.ipynb of colab.Thanks mpdn I think this was resolved in tf nightly . Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan d30cfe2f0b6a5c3f6b72227378fbdad9 32869.ipynb . I am closing this issue as it was resolved. Please feel free to reopen if the issue persists again. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32869 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32869 No a 
32870,No gradient defined for operation ExtractVolumePatches ,I saw it already been solved for images but it still hasn t been solved for 3D volumes. The ExtractImagePatches solution https github.com tensorflow tensorflow issues 2921 I m using tf 1.12 and get the following message LookupError No gradient defined for operation gpu 0 local z 3d EXTRACT LOCAL Z op type ExtractVolumePatches , dahliau Can you please provide standalone code to reproduce the error mentioned here Thanks I ve checked it again now in a simple standalone code and it had gradients. I ll try figure out later the problem in my original code. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 32870 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 32870 No a Have you solved this issue I also have this problem. Thanks It look like it works on google colab tf 1.14 . I guess it depends on the tf version. I still haven t solve it in my work tf 1.12 . You can check the stand alone code I ve uploaded it works on tf 1.14. Thanks Get Outlook for iOS https aka.ms o0ukef From dahliau notifications github.com Sent Friday November 8 2019 9 43 15 AM To tensorflow tensorflow tensorflow noreply.github.com Cc Chen Xiaoyang xychen email.unc.edu Comment comment noreply.github.com Subject Re tensorflow tensorflow No gradient defined for operation ExtractVolumePatches 32870 It look like it works on google colab tf 1.14 . I guess it depends on the tf version. I still haven t solve it in my work tf 1.12 . You can check the stand alone code I ve uploaded it works on tf 1.14. You are receiving this because you commented. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 32870 email source notifications email token AHVAPZZJLSLNDCJNERERQFDQSV3IHA5CNFSM4I3ED2LKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEDSJLMQ issuecomment 551851442 or unsubscribe https github.com notifications unsubscribe auth AHVAPZ4IC5WI45FREYGRXFLQSV3IHANCNFSM4I3ED2LA . 
32880,deprecation warning when training a simple embedding with custom function, Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 macOS 10.13.6 TensorFlow installed from source or binary pip install tensorflow 2.0.0 beta1 TensorFlow version use command below v2.0.0 beta1 5101 gc75bb66a99 2.0.0 rc0 Python version v3.6.7 6ec5cf24b7 Oct 20 2018 03 02 14 Describe the current behavior The following code trains a simple embedding on temporal indices Z to match a given time series X and I get the following warning about the use of a deprecated method This reminds me about an other issue https github.com tensorflow tensorflow issues 29881 I opened with a lot of deprecation warnings arising when I use the API. This issue is closed but the warnings are still here and in fact the release of the rc0 has brought a new one see my last comment there . ,Was able to reproduce the issue. The github gist is here https colab.sandbox.google.com gist gowthamkpr 3b8b4b959ee44861ce8c20c3ff399559 untitled150.ipynb The warning is not here in tf2.0.0 so I can close this issue. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 32880 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 32880 No a 
32944,Incorrect behaviour creating constants from complex arrays of length 64, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow custom code OS Platform and Distribution e.g. Linux Ubuntu 16.04 macOS 10.14.6 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below v1.14.0 rc1 22 gaf24dc91b5 1.14.0 Python version 3.7.3 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory Intel Iris Plus Graphics 640 1536 MB Describe the current behavior The following code produces an array of zeros after running several times for me it seems to reliably be on the fourth run Describe the expected behavior That code should produce an array 0j 1j 2j ... no matter how many times it s run. Code to reproduce the issue Also see Colab notebook here https colab.research.google.com drive 1Ci bqoMpgK4JfCt0s2eUFjHR0vgOo PF Other info logs I have no idea what s happening but some observations only occurs for arrays 64 in length also occurs if we construct the list manually instead of with numpy e.g. session.run tf.constant 0 1j 63 doesn t occur if 1j is replaced by 1 1j doesn t occur if the constant is created first then multiplied by 1j e.g. session.run 1j tf.constant np.arange 64 .astype np.complex128 is fine ,I could reproduce this issue with Tf 1.15.0rc1. Please see the gist here https colab.sandbox.google.com gist gadagashwini ee529a96c94b3e9a5aa5b22303ceeae1 untitled173.ipynb scrollTo CQh69Vy8KtmP . Thanks I could reproduce the issue with TF1.15.0rc2 also. However TF2.0 works as expected. Please check the gist https colab.sandbox.google.com gist jvishnuvardhan 96c7fa6462f94cb421a9c2fa2684d9f1 untitled173.ipynb with TF2.0 . Thanks I could reproduce with TF2.0 when disabling eager execution gist here https colab.research.google.com drive 1hgzD W PfSNwCGOrNnhMk89u12gy3ISP . charmasaur I agree. This works well upto 63 as in np.arange 63 . I could reproduce the issue in 1.15.0rc3. However I am not sure why you want to initialize and print a tensor inside a for loop. When I change your code to the following everything worked as expected. Thanks We will dig deep into the issue why it is throwing error when we use np.arange 63 . Thanks I get the same type of issue when building a larger graph normally i.e. not building or running in a for loop I was just using the loop to give a minimal example showing the issue. Thanks for investigating I d be very interested to know what s going on here. I have identified the bug in constant tensor compression and a fix should be submitted shortly. charmasaur I cannot reproduce the issue when I ran your code in tf nightly . Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan a0be9e53419c37cf63d4d090aededd23 untitled1.ipynb . Thanks This issue was fixed by rmlarsen fix. It is available in tf nightly . I am closing the issue as it was resolved. Please feel to open the issue if the issue persists again. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32944 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32944 No a Yep that looks to be fixed thanks Any chance the fix could be linked here I d be keen to see what was going wrong. 
32960,Keras Nadam optmizer generates error when using MirroredStrategy, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18 TensorFlow installed from source or binary Tensorflow 2 TensorFlow version use command below Tensorflow 2.0.0 Python version Python 3.7 CUDA cuDNN version 10 GPU model and memory 2 x Nvidia 1080 TI Describe the current behavior The training crashes with an error ValueError You must specify an aggregation method to update a MirroredVariable in Replica Context. If the model is compiled with the optimizer Nadam tf.keras.optimizers.Nadam along with a MirroredStrategy. Describe the expected behavior Expect to be able to train with any optimizer from Keras options. ,In order to expedite the trouble shooting process please provide a minimal standalone code to reproduce the issue reported here. Thanks import tensorflow datasets as tfds import tensorflow as tf tfds.disable progress bar import os os.environ CUDA VISIBLE DEVICES 1 datasets info tfds.load name mnist with info True as supervised True mnist train mnist test datasets train datasets test mirrored strategy tf.distribute.MirroredStrategy print Number of devices .format mirrored strategy.num replicas in sync You can also do info.splits.total num examples to get the total number of examples in the dataset. num train examples info.splits train .num examples num test examples info.splits test .num examples BUFFER SIZE 100 BATCH SIZE PER REPLICA 64 BATCH SIZE BATCH SIZE PER REPLICA mirrored strategy.num replicas in sync def scale image label image tf.cast image tf.float32 image 255 return image label train dataset mnist train.map scale .cache .shuffle BUFFER SIZE .batch BATCH SIZE eval dataset mnist test.map scale .batch BATCH SIZE with mirrored strategy.scope model tf.keras.Sequential tf.keras.layers.Conv2D 32 3 activation relu input shape 28 28 1 tf.keras.layers.MaxPooling2D tf.keras.layers.Flatten tf.keras.layers.Dense 64 activation relu tf.keras.layers.Dense 10 activation softmax model.compile loss sparse categorical crossentropy optimizer tf.keras.optimizers.Nadam metrics accuracy model.summary model.fit train dataset epochs 5 I have tried on colab with TF version 2.0.0 rc2 and was able to reproduce the issue.Please find the gist here https colab.sandbox.google.com gist ravikyram 7dbb1f1dd1ecc4a0fba0cc6792bdfb98 untitled247.ipynb . However i am able to reproduce the issue with optimizer tf.keras.optimizers.Nadam and not seeing any issue if we use optimizer tf.keras.optimizers.Adam .Thanks Hi this issue if fixed here https github.com tensorflow tensorflow commit 1a1c0b6980f1648fe674de3e3b471e78a59143e1 which unfortunately hasn t made its way into the latest release. It should be there in the next release and already in the nightlies. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32960 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32960 No a 
33035,tf.saved model.save unable to save model with sparse input., em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu Colab Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below fails on 1.14.0 1.15.0rc2 2.0.0 Python version python 3 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Describe the current behavior See this gist for reproduction https colab.research.google.com gist yzhuang 3d4c511b247988fdafbd083785d624c1 model saving fails with sparse input tensor.ipynb If a model has sparse input they seem to be converted to dense tensors when saved model.save is called. This causes model saving to fail. Describe the expected behavior If a model works during model.fit I expect the same model to also work for saved model.save . Code to reproduce the issue Colab gist https colab.research.google.com gist yzhuang 3d4c511b247988fdafbd083785d624c1 model saving fails with sparse input tensor.ipynb Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. , yzhuang Thank you for reporting the issue error also replicating for TF version 2.0rc2 https colab.sandbox.google.com gist oanush d8dc1f89e88530a4ab3c83cdf81adc15 model saving fails with sparse input tensor.ipynb .Thanks yzhuang The error is because of the below line of code. sparse tensor tf.sparse.SparseTensor indices sparse tensor.indices values sparse tensor.values dense shape 2 2 . It works fine if you replace that line with below line sparse tensor tf.sparse.SparseTensor indices tf.constant 0 0 1 1 dtype tf.int64 values tf.constant 1.0 1.0 dtype tf.float32 dense shape 2 2 . Hi rmothukuru I am reproducing this bug specifically During tf.saved model.save while calling a model s call method all SparseTensor inputs are converted to dense tensors. That s why the above code fails. While the suggested one line change fixes the crash on that line it no longer uses the input to the call method on that line and hence no longer reproduce this bug on that line. I updated the repro colab to simply remove that line and it ll crash later. See https colab.research.google.com gist yzhuang 3d4c511b247988fdafbd083785d624c1 model saving fails with sparse input tensor.ipynb Let me describe the problem in another way focus on the input to the call method sparse tensor . Its type is a tf.SparseTensor during model.fit but its type is a dense Tensor during tf.saved model.save . Could reproduce this bug with TF Version 2.0. Here is the Gist https colab.sandbox.google.com gist rmothukuru 807a0c0d20281b818a2abeafeeae7a6b model saving fails with sparse input tensor.ipynb . Is there a workaround to still save a trained model My project absolutely needs sparse input tensors and if I am not able to save them that will be a major blocker. Thanks Hi guys is there any update on this This is fixed with latest tf nightly version 2.2.0 dev20200302 . Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33035 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33035 No a 
33045,TPU support in tensorflow 2.0 release,From the following link https www.tensorflow.org guide distributed training I understand that TPU training is supported in tensorflow 2.0. I followed the snippet code provided in the same page cluster resolver tf.distribute.cluster resolver.TPUClusterResolver tpu tpu address tf.config.experimental connect to cluster cluster resolver tf.tpu.experimental.initialize tpu system cluster resolver tpu strategy tf.distribute.experimental.TPUStrategy cluster resolver And I got the following error InvalidArgumentError Unable to find a context id matching the specified one 7989870214237460624 . Perhaps the worker was restarted or the context was GC d Additional GRPC error information created 1570180842.964900283 description Error received from peer file external grpc src core lib surface call.cc file line 1039 grpc message Unable to find a context id matching the specified one 7989870214237460624 . Perhaps the worker was restarted or the context was GC d grpc status 3 Same error was thrown when I didn t provide the tpu address as it is stated in the above link. The TPUClusterResolver instance helps locate the TPUs. In Colab you don t need to specify any arguments to it. The test was done in google colab and I selected the TPU accelerator. I installed tensorflow gpu with pip install tensorflow gpu. If TPU is not yet supported in tf 2.0 when it is planed to be added , georgealexandruvlad Please provide the complete code to reproduce the issue reported issue here. Thanks gadagashwini The code can be found at the following link https colab.research.google.com drive 1CfY bW sRjZbvkQDjjl1CMiIzN6bJhQn Basically I just tried to initialize the TPU. Full error message georgealexandruvlad Thanks for testing TF2.0 TPUs. This seems to be a known issue as https github.com huan tensorflow handbook tpu issues 1 which the colab kernel on tpu worker is out dated Team is working on the fix. Thanks jvishnuvardhan Thank you. Is there any way to follow the progress on the matter or do you happen to know when the issue might be solved. Thanks jvishnuvardhan I checked it today and it looks like the issue has been solved. Thanks. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33045 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33045 No a Uhh I don t it s fixed yet. Initializing works but not fitting. First off you installed 1.15.0 rc3 not tensorflow 2.0. Second you need to fit under tpu strategy.scope to train it under the TPU Do that and you get an error. Bottom line TPU is not ready for Tensorflow 2.0 yet I am wondering if there s still issues with TPU support in tf 2.0. I have a model that runs just fine in GPU but I am getting an Error at self.optimizer.apply gradients zip gradients trainable vars . Link https github.com tensorflow tensorflow issues 34635 issuecomment 650667602 
33143,Tensorflow Keras not allowing different size X and y for fit generator, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 Professional Edition TensorFlow installed from source or binary binary installed using conda TensorFlow version use command below unknown 1.14.0 Python version 3.7.3 CUDA cuDNN version 10.0 7.6 GPU model and memory T1000 4GB VRAM Describe the current behavior When having model with two inputs with shapes e.g. 486 3673 486 and one output 87 1 after calling model.fit generator the training crashes Describe the expected behavior Training should proceed without problems the data is valid. When trained during eager execution using the model is trained without problems and I expect using model.fit generator would allow this behaviour too. Code to reproduce the issue model is built using Other info logs When using segmented aggregations naturally the size of X and Y is different but after aggregation dimensions fit but keras model training don t allow this. Using those aggregations is motivated by https arxiv.org pdf 1609.07257.pdf paper I am trying to use to some problem. Keras repo allows turning those checks of in this PR https github.com keras team keras pull 11548 but it is not incorporated into tensorflow version of keras., racinmat Can you please provide a github gist or a colab notebook to reproduce the issue. Thanks Of course the gist is here https gist.github.com racinmat a8eb2f727fcc4bd24745042eee5cd8d6 I am getting an error racinmat Please find my github gist here https colab.sandbox.google.com gist gowthamkpr 361962dd71b59b1bf19b62c0a53bb414 untitled210.ipynb . Oh my bad fixed now in the github gist and it produces the abovementioned error in the colab. I am still facing the same error racinmat Please share your github gist. I updated the github gist not the colab. Which error are you getting with the most recent version of the gist Was able to reproduce it. Please find the gist here https colab.sandbox.google.com gist gowthamkpr f0f721735d1855b103d171de87643f7f untitled210.ipynb . racinmat Thanks for the issue This is fixed in the latest tf nightly pip install U tf nightly Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33143 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33143 No a Hey omalleyt12 I m having the same issue here with TF 2.5. Is there any estimate for when this is going to be in stable 
33175,tf1.12 keras input shape does not match the correct data feed, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 ubuntu16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary TensorFlow version use command below 1.12 Python version 3.6.9 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 9.0.176 GPU model and memory You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior Raise ValueError Error when checking input expected input tensor to have 1 dimensions but got array with shape 1 1 Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. In TF2.0 the code listed above can be run correctly. So I think it is a bug in tf1.12. How to fix in tf1.12 Line r input keras.layers.Input shape dtype tf.float32 name input tensor can be replaced to r input keras.layers.Input shape 1 dtype tf.float32 name input tensor to fix this problem. , This is a bug in Tensorflow 1.12 but has been fixed in later versions. Please find the github gist here https colab.sandbox.google.com gist gowthamkpr 24c4c51235dbc238dc0314fd950d3d1e untitled172.ipynb . 
33215, TF2 Tensor object has no attribute keras history , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOS TensorFlow installed from source or binary source TensorFlow version use command below 2.0 Python version 3.7 Problem in TF2.0 use Keras to save model.h5 then load model.h5. I have saved model.h5 from official.nlp.bert models.py by use model.save model.h5 . everything is ok but when I load the model.h5 there have some problems. emmmmm how to solve this problem I am not sure this is BUG but in this link someone say this maybe is a bug. https github.com tensorflow models issues 7643 Traceback Traceback most recent call last File Users lollipop Documents tf2g false news test.py line 30 in model tf.keras.models.load model . my model1.h5 custom objects BertModel bert modeling.BertModel File Users lollipop .conda envs tf2 lib python3.7 site packages tensorflow core python keras saving save.py line 146 in load model return hdf5 format.load model from hdf5 filepath custom objects compile File Users lollipop .conda envs tf2 lib python3.7 site packages tensorflow core python keras saving hdf5 format.py line 168 in load model from hdf5 custom objects custom objects File Users lollipop .conda envs tf2 lib python3.7 site packages tensorflow core python keras saving model config.py line 55 in model from config return deserialize config custom objects custom objects File Users lollipop .conda envs tf2 lib python3.7 site packages tensorflow core python keras layers serialization.py line 102 in deserialize printable module name layer File Users lollipop .conda envs tf2 lib python3.7 site packages tensorflow core python keras utils generic utils.py line 191 in deserialize keras object list custom objects.items File Users lollipop .conda envs tf2 lib python3.7 site packages tensorflow core python keras engine network.py line 906 in from config config custom objects File Users lollipop .conda envs tf2 lib python3.7 site packages tensorflow core python keras engine network.py line 1852 in reconstruct from config process node layer node data File Users lollipop .conda envs tf2 lib python3.7 site packages tensorflow core python keras engine network.py line 1802 in process node output index nest.flatten output tensors 0 . keras history.node index AttributeError Tensor object has no attribute keras history Process finished with exit code 1,Did you try this comment https github.com tensorflow models issues 7643 issuecomment 540684664 Let me know if it solves the issue. Thanks Hi even if tf format can be a workaround this seems to be an issue that TF team need to take a look. gowthamkpr could we find an owner gowthamkpr saberkun yes tf.saved model.save successed save and load. but I think it is too complex. I hope tf.keras.models.load model can works. code SmileTM saberkun Can you please try using tf nightly and let me know if the issue still persists. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33215 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33215 No a 
33216, TF 2.0.0 Training keras Model on tf.data.Dataset causes small bug in logging, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 Python version 3.6.x CUDA cuDNN version 10.0 7.6.1 GPU model and memory Describe the current behavior When fitting .fit a keras Model on a tf.data.Dataset the dataset size is not inferred. Because of this when setting verbose 1 during the first epoch the log becomes current step Unknown . Also the following is thrown though it does not cause crashing Describe the expected behavior I would expect to see the number of samples batches etc. Code to reproduce the issue I created a small Colab notebook to demonstrate the issue https colab.research.google.com drive 1 S787cE6BWhXJ 0BeAb6EGq4GaXAFwmu I recommend downloading the .py file and running it in command line so colab logging doesn t interfere because after the epoch is done the correct batch number is found. The problem is during the epoch. Other info logs I found that the dataset size inferring is actually run but the returned value is not stored or used anywhere it is only to throw a warning if the initialization made by the user is faulty in some way. I am referring to this line https github.com tensorflow tensorflow blob r2.0 tensorflow python keras engine training v2.py L247 https github.com tensorflow tensorflow blob r2.0 tensorflow python keras engine training v2.py L247 The above problem would be eliminated with something like this or the like , angeliand I am able to successfully execute Colab notebook provided by you with a warning message.Please find the gist here https colab.sandbox.google.com gist ravikyram d23afdb38149eeb3fc179a6e30d20721 untitled254.ipynb .Please let me know is this still an issue .Thanks As I have said the problem is with what gets logged during the epoch. Not after. In your notebook you did not interrupt the training to view the printed log during the epoch. If interrupted the described behaviour can still be seen. Also I recommend running the script in command line or anywhere where the output is printed line by line where the problem can be viewed without stopping the training. angeliand I tried running in command line and i am able to execute the .py file successfully. Please find the log file in the attachment.Is this the expected output text.txt.tar.gz https github.com tensorflow tensorflow files 3716797 text.txt.tar.gz .I could reproduce the issue in colab when i interrupted during training.Thanks As I have stated in the original post the bug does not cause crashing but it is still inconvenient and can be avoided easily with a minor fix I have also offered a solution . If logging is line by line so when it doesn t refresh eg. in PyCharm or when logging to file the problem can be viewed easier. It appears during the first epoch and ceases when that is done. Probably because after the first epoch we know the number of steps. This is why training has to be interrupted in colab to view the issue or you can check the runtime logs in colab . Also the IteratorGetNext warning appears at the end of the first epoch before validating steps. This is because the training loop does not know the number of steps to take times to query the dataset and the dataset runs out. All in all this is not huge but still not what would be the expected behaviour. I will provide a log file from PyCharm in a few hours. PyCharm output is line by line so the problem can be seen better. Here is the output without the proposed solution keras bug op.txt https github.com tensorflow tensorflow files 3717600 keras bug op.txt And here is the correct expected output after using the proposed solution keras bug sol.txt https github.com tensorflow tensorflow files 3717605 keras bug sol.txt Hope this helps. angeliand Are you willing to contribute through PR to update relevant codes Thanks Sure. I m busy in the next few days but I will do it after that. It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33216 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33216 No a 
33245,Cannot use dict base datasets with keras.Model.fit., System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 Python version 3.6.4 CUDA cuDNN version 10.1 GPU model and memory 7.6.2 In order to make both dataset and keras model have good structures I create a dataset and a vanilla model like this. When I use the dict based dataset from tfds with keras.Model.fit the first call will cause expection as I checked the code and found that when dict is passed iterating through zip targets outputs will just get the keys of the dict so the string keys have no dtyle. https github.com tensorflow tensorflow blob master tensorflow python keras engine training utils.py L1246 image https user images.githubusercontent.com 11533479 66652073 59916c00 ec67 11e9 8974 eda50bf98e18.png So how can I use dict based dataset and model with keras.Model.fit , npuichigo Will it be possible to provide the standalone code to replicate the reported issue. Thanks npuichigo Any update on code. Thanks The problem is that if our model and dataset return dictionaries how can we compile and fit the model. any updates I am facing the same issue when I am retrieving data from TFRecords . Assigning to Tom who will be working on this as part of ideal fit compile change. For a quick workaround note the Keras expects Datasets passed to fit to be of the form features labels or features labels sample weights Here s an example of grabbing MNIST from tensorflow datasets and formatting it in the way Keras expects omalleyt12 That s true. But sometimes we have complicated models with multiple inputs and outputs so it s more elegant if we can use dict based model and dataset. npuichigo For those Models each of features and labels can be a dictionary omalleyt12 So it looks like my example. But it seems that keras restrict the input and output names to something like input 1 input 2 output 1 and so on. Can you give a correct example If I use tf.feature column.categorical column with vocabulary list I was able to specify the column name in keras. But I faced the issue when I called model to estimator API to convert back to tensorflow model as the converted model was expecting inputs like input1 input2 ... as you mentioned same problem I faced same issue. My workaround is not using the subclass way. When I change to tf.keras.Model inputs ... outputs ... the problem solved. Or before model.complie add two lines inputs tf.keras.layer.Input shape ... model inputs you need to run the model once. model.build doen t help. In fact I found there s quite some limitation on subclass model and there s no good guidline about how to avoid these limitations.Hope tensorflow can improve the usability of subclass model. I really prefer subclass model than sequence model becuase that s the way how we can easily debug the model with eager. Thanks for the issue all This is now fixed in the latest tf nightlym you can use any arbitrary nested structure of data with subclassed Models. Code below works note I had to add a call to dataset.batch this is needed to create batches of data when using from tensor slices Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33245 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33245 No a omalleyt12 When eager execution is disabled the code snippet you posted above still fails under version 2.2.0 dev20200501 with the same error AttributeError str object has no attribute dtype . jgmakin Disabling eager execution is a v1 compatibility feature in general we re only making critical security or regression fixes in the v1 compatibility code for tf.keras. I d recommend migrating to the TF2 style Ah ok thanks. I tend to disable eager execution under the assumption that this improves performance but perhaps that isn t the case anymore jgmakin When using Model.fit everything runs inside a graph anyway a tf.function graph so performance should be good For more check out this guide https www.tensorflow.org guide function Hello is there any way to do this too for validation data I can t get it work for validation after every epoch end 
33247,Saving GRU with dropout to SavedModel fails, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Colab Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary Pip TensorFlow version use command below v2.0.0 rc2 26 g64c3d38 2.0.0 Python version python 3.6.8 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Describe the current behavior When Model containing GRU layer dropout is set and activation relu the model is not savable. Error Attempted to save a function b inference GRU layer call fn 8041 which references a symbolic Tensor Tensor dropout mul 1 0 shape None 3 dtype float32 that is not a simple constant. This is not supported. Describe the expected behavior Model gets saved. Code to reproduce the issue Based on https www.tensorflow.org guide keras save and serialize ,Issue replicating for TF 2.0 kindly find the gist https colab.sandbox.google.com gist oanush 4a4201abcbe95c5eb2e19bc5d3f5bde1 33247.ipynb of colab.Thanks fhausmann The error disappears if dropout 0 or if layers.Dense instead of layers.GRU . The link https www.tensorflow.org guide keras save and serialize provided by you doesn t demonstrate on how to Save the Model with layers.GRU . Can you please refer Image Captioning Tutorial https www.tensorflow.org tutorials text image captioning Text Generation Tutorial https www.tensorflow.org tutorials text text generation and NMT With Attention Tutorial https www.tensorflow.org tutorials text nmt with attention where usage of layers.GRU is demonstrated and let us know if it helps. Thanks rmothukuru Thanks for your answer. The error also disappers if activation is not set and yes the link is maybe a bit missleading since the example there is for Dense layer only. However I thought this was a good minimal working example also with GRU. I had a look into your links but unfortunately non of them demonstrate how to save a model with GRU layer in the SavedModel format. I tried for the Text Generation Tutorial added model.save model save format tf after training but it also failed here with another error AssertionError Tried to export a function which references untracked object Tensor StatefulPartitionedCall args 2 0 shape dtype resource .TensorFlow objects e.g. tf.Variable captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly. Seems that GRU models does not allow saving in SavedModel format at the moment Maybe related to 33355 fhausmann Model is Saved Successfully if you replace model.save model save format tf with model.save model.h5 . Here is the Gist https colab.sandbox.google.com gist rmothukuru 72984b3a326970c85edb8f03312cb199 33247.ipynb . Yes this probably solves the issue for most people but its a different format binary HDF5 vs. SavedModel. rmothukuru is the SavedModel format deprecated fhausmann In your case you can either use keras.models.save model model model filepath model or model.save model . Here is the Gist https colab.sandbox.google.com gist rmothukuru 01572d4bff7572acba853ef91ebe2efb 33247.ipynb . Please let me know if your issue is resolved. Thanks There is an issue probably with the argument save format tf because the code model.save model save format tf is resulting in the Error mentioned by you. rmothukuru I tried both on your gist and both give me the error. Only the model.h5 works in my hands I also encountered the same problem. Have you solved it tf.keras.experimental.export saved model model modelfolder and tf.keras.experimental.load from saved model modelfolder seems to be a workaround however they are deprecated. To add more information Model is Saved without any Error if we build the Model using Dense or using LSTM and Dense . But if we build the Model with GRU as one of the Layers it is resulting in either the Error Error Attempted to save a function b inference GRU layer call fn 8041 which references a symbolic Tensor Tensor dropout mul 1 0 shape None 3 dtype float32 that is not a simple constant. This is not supported. or the Error AssertionError Tried to export a function which references untracked object Tensor StatefulPartitionedCall args 2 0 shape dtype resource .TensorFlow objects e.g. tf.Variable captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly. Here is Gist of Error1 https colab.sandbox.google.com gist rmothukuru 00386fc1a493b344282b1b8aa97e20e7 33247.ipynb and Gist Of Error2 https colab.sandbox.google.com gist rmothukuru 78f245f68f3729eb1c8f0da1e0df93a1 33355.ipynb . The issue 33355 is similar. See the above link. I don t think LSTM works as well tf.keras.experimental.export saved model still works fhausmann This was resolved in tf nightly . Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 28eb34025ba304b45f2a8bb74c2cd2a4 untitled774.ipynb . As this was resolved I am closing this issue. Please feel free to reopen if the issue persists later. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33247 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33247 No a Does anyone have the commit ID which resolved this issue I don t want to run a nightly release and need to backport the fix into a stable version. phemmer New stable version TF2.2 will be release soon. Please check the recent announcements here https groups.google.com a tensorflow.org forum forum build . Thanks Can we not just have the commit ID Yes 2.2 may be close but it s still a few weeks off I d imagine. And in addition I m using IBM s Large Model Support patch https github.com IBM tensorflow large model support which will likely take an additional several weeks to be updated to 2.2. 
33253,sqlite dataset fails to raise a StopIteration incorrect result, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 TensorFlow installed from source or binary issue is reproducible with both TensorFlow version use command below Confirmed on 2.0 and 1.14 Python version python2 Describe the current behavior After creating a tf.data.experimental.SqlDataset and performing a map and batch operaion the dataset fails to raise a StopIteration after going through the entire database and begins to repeat recycle values incorrectly. Describe the expected behavior The dataset stops after returning all the records in the sqlite database. Code to reproduce the issue image https user images.githubusercontent.com 8462255 66663537 ecef8080 ec18 11e9 84fd afd02c8a66ab.png Other info logs This has been reproduced on tensorflow versions 2.0 1.14 and 1.15 ,Added PR 33271 for the fix. Thank you for the quick fix Any chance this could make it into 1.15 too t kalinowski We will have to wait for the PR to be merged into master. Once it is in the master it might be cherry picked into release 1.15 or 2.0. Though given 1.15 s release schedule I would not count on this fix being picked up in 1.15. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33253 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33253 No a 
33258,keras.layers.LSTM does not work with model.evaluate after training, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Debian 9.11 GCE DeepLearning image Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below v2.0.0 rc2 26 g64c3d38 2.0.0 Python version 3.7.3 anaconda3 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version V10.0.130 7.6.4.38 GPU model and memory V100 16GB Describe the current behavior model.evaluate raises this error after training Describe the expected behavior model.evaluate should not raise this error after training. Code to reproduce the issue Code and log https gist.github.com matthew z e5848d545b60792dd84bfb9470ea541f I tested with this machine on GCE n1 standard 4 1 x NVIDIA Tesla V100 image c1 deeplearning common cu100 20191003 Google Deep Learning VM common dl gpu installed anaconda3 CUDNN 7.6.4.38 got some errors with the original 7.4 CUDNN in the image tensorflow gpu 2.0 Other info logs The problem may not happen with CPU.,I have tried on colab with TF version 2.0 and was able to reproduce the issue.Please find the gist here https colab.sandbox.google.com gist ravikyram 039e4a49146a41cc220c28b3e2b7cacb untitled266.ipynb .Thanks I can reproduce this issue. Somehow I don t think its a LSTM issue since the error log didn t indicate anything about LSTM. Also when I change the LSTM layer to SimpleRNN the issue still persists. The other experiment I did was change experimental run tf function False in model.compile which will let model run in func graph mode and the issue goes away. So I think there is some issue for the training function in v2. For now you can set the experimental run tf function False to walk around the issue. We will dig deep to find the root cause. Thanks. Just tested tested same code again with the latest tf nigthly and it runs fine now. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33258 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33258 No a 
33261,Can t save a Model with a TimeDistributed layer wrapping another Model, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution Linux Ubuntu 18.04.2 TensorFlow installed from source or binary binary TensorFlow version 2.0.0 Python version 3.7.3 CUDA cuDNN version CUDA 10.1 cuDNN 7.5.1 GPU model and memory TITAN X Describe the current behavior I get a ValueError when trying to save a tf.keras.Model with a tf.keras.layers.TimeDistributed layer wrapping another tf.keras.Model that has convolutional layers. I am using tf.keras.Model.save with the default save format SavedModel . See below for examples. There is no error when saving with save format h5 . Describe the expected behavior Successfully saving a SavedModel with the tf.keras.layers.TimeDistributed layer. Code to reproduce the issue 1. Wrapping a 1 layer convolutional NN with tf.keras.layers.TimeDistributed Error 2. Wrapping a pre trained tf.keras.applications model closer to my actual use case Error 3. Saving as an HDF5 file instead This works without errors. 4. Saving to the SavedModel format works with just dense layers This works without errors. ,I have tried on colab with TF version 2.0 and was able to reproduce the issue.Please find the gist here https colab.sandbox.google.com gist ravikyram 00b7541a4eb2b6492c984d4201acce7f untitled265.ipynb .Thanks Is there any update I noticed in 33094 that there was a fix for saving TimeDistributed layers in the nightly release. I tried with tf nightly 2.1.0 dev20191113 but I m getting the same error. Any update Same errors with tensorflow 2.1.0 rc0 . By the way there seems to be no problem saving a TimeDistributed layer wrapping a convolutional layer instead of a Model which has a convolutional layer in it. For example this works Whereas this doesn t I also encountered this bug how can I solve it 1 Also running into this k w w this is a pretty major limitation at the moment. Any idea on what needs to be changed to fix it There appears to be a problem with saving layer masks using SavedModel. looking into this k w w any update on this issue Bump Appears to be fixed by 9f2aa61811b29e700b8325bb57b1f4b0093c1d4d Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33261 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33261 No a Hi there I am using time distributed layer to apply another model on each frame and processing the final features by conv 3D. the model is trained properly. but when I want to load the model and use it for test I face different issues first the model cant be loaded because of the nested model second the model acc on the same val dataset is not reproducible marziehoghbaie Please create a new issue with a simple standalone code to reproduce the error. Thanks 
33297,Surprising random seed behavior when using tf.function, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOSX 10.14.6 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below v2.0.0 rc2 26 g64c3d382ca 2.0.0 Python version 3.7.4 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior Random seeds work in surprising ways in TF 2.0 when using tf.function . In particular the value of the global random seed is only taken into account when a function is traced not when it is called. This is surprising and different from TF 1.x behavior. Describe the expected behavior I expect the value of the global random seed to be taken into account every time a pseudo random number is generated. Code to reproduce the issue The output value is Notice that we get the same sequence of random numbers every time ignoring the value of the global random seed. The only value that matters is the first one when the function gets traced . More code and examples of surprising behavior in this colab https colab.research.google.com drive 1C3LZkt5hfO6T2Uo2xaYVG8hiNQL8c3xu . Other info logs Specifically I would expect the output to look the same as when the function is not decorated with tf.function Note that the second sequence is different as expected in fact the pseudo random numbers should be identical whether the function is decorated or not but that s a nice to have ., ageron Can you please refer the issue 33034 and let us know if it helps. Thanks Hi rmothukuru I looked at issue 33034 before filing this issue I think it s a different problem. I am not using per operation seeds in this issue. Could reproduce the issue with TF Version 2.0. Here is the Gist https colab.sandbox.google.com gist rmothukuru faae8f0424cd0bcfcee29854140cab8c random seeds in tf 2 0.ipynb . alextp wangpengmit FYI Unfortunately this is a known issue of tf.random.seed . We ll fix the documentation to help avoid the issue and I ll post an example for how the original example can be updated so that it consistently works in tf.function as well.. Update here is a method that currently works in TF2 and which will give you the expected results. Unfortunately tf.set seed and tf.random.uniform and similar ops are currently broken although they will probably be either fixed or removed in the future Note currently calling the function with a new generator will causing it to be retraced that is will be slow the first time you call it this will be fixed soon. yashk2810 it would be nice to make sure this snippet or something similar is visible in one of our tutorials or perhaps the docs for tf.random Its fixed now https www.tensorflow.org api docs python tf random set seed https www.tensorflow.org api docs python tf random uniform We also link off to the notebooks where the particular symbol is being used. See this for example https www.tensorflow.org api docs python tf random uniform used in the guide Thanks mdanatg and yashk2810 the documentation is clearer now and it s nice to have a workaround. I really like the tf.random.experimental.Generator solution and in fact I think I ll stop using tf.random.set seed altogether as well as op level seeds. Their behavior is way too complicated and error prone IMHO. Ideally a function s behavior should not change when you decorate it with tf.function and things should be as simple and intuitive as possible. In NumPy there is a single global random number generator RNG and all random ops use it by default. If users need other RNGs they just create them. Works fine. Here s a solution that works fine and which I find very simple and intuitive Everything works exactly like you would expect and it gives the exact same result whether the rng function is decorated with tf.function or not. The only problem is that reset from seed does not support None . That s really something that should be fixed IMHO it should generate a random state like in from non deterministic state using non deterministic ints . Next if I want to use a specific RNG I can do so easily Again this is simple and behaves exactly like you would expect and just the same way with or without decorating the function with tf.function . It s a pity the reset method explodes if I don t pass it a state argument. I would expect it to revert to its initial state instead. It should just save its initial state upon creation and revert to that state when reset is called without any argument. Wdyt Thank you for the thorough suggestion Yes it s absolutely crucial that code works the same way with or without tf.function anything that doesn t is a bug. I ll let Peng weigh in on a solution he s the RNG expert. Personally I like your proposal because it s clean and minimizes the symbols we d need to export Generator is the sole RNG the easy tf.random.uniform functions just offer access to the default generator and we can omit tf.stateless.stateless uniform . One thing I m not sure about is how to add this to TF 2.0 while keeping backward compatibility. I see a few options 1. Keep the existing tf.random and add tf.random v2 in parallel as I did. But it feels wrong to add yet a v2 just a few weeks after 2.0 is released which had finally removed all the v2s that had accumulated in TF1. 2. Add tf.random.enable simple behavior to make the new behavior opt in. Not very elegant shouldn t the simple behavior be the default Perhaps make it the default behavior in TF 2.1 3. Or just update tf.random to use the new behavior and consider that we re just fixing a bug. Since the output differs when you decorate the function with tf.function it is a bug. Since TF 2 was just released there s probably not much production code out there anyway and the impact is limited it will just change the random sequences. But in that case the recently added documentation should be updated quickly. Hi ageron your code is roughly what we have in mind for the future tf.random.uniform and alike if we choose to keep them. Generator.reset from non deterministic state seems like a useful feature which I ll consider. I m hesitant to make Generator save the initial state to support an argument less reset since for basic facilities such Generator we need to be careful about memory usage. After all the user can retrieve and store the initial state herself and use it to reset the generator later. Hi wangpengmit Thanks for your feedback. Just to be clear are you planning to go for option 3 in my previous comment In other words will the new simpler behavior replace the old one or will they live side by side if so how Also I d like to suggest providing a set seed method in the Generator class that calls reset from seed if a seed is provided or reset from non deterministic state if not. IMHO it would be more convenient more intuitive and less verbose. Regarding the initial state I see your point it makes sense. Perhaps the initial state could be saved by default when creating the Generator since most people won t create millions of Generator s RAM usage should be fine and have an option in the constructor such as save initial state True so that people who need the extra RAM can set this to False if they want Or perhaps set it to False by default and make the reset method print our a clear error message if the user doesn t provide a state You need to set save initial state True when constructing the Generator if you want to be able to call reset without any argument . Not sure about this. ageron Your not me We are pretty sure the old behavior is wrong and will go away. We haven t decided whether to remove symbols such as tf.random.uniform and only provide tf.random.Generator or to give tf.random.uniform the new behavior. Thank you for the other suggestions We ll consider them. Thanks for your feedback. Please don t remove tf.random.uniform and other random functions there s really no need to break everyone s existing code IMHO. Most people won t even notice the change. I think this modification is mostly an implementation detail and a bug fix it doesn t justify changing the API much. Just adding a warning in the release notes explaining what s changed and the motivation would be sufficient. Moreover I would recommend making the default Generator available directly through the API. Something like tf.random.default generator or tf.random.get default generator for example in case someone needs to pass a Generator to a function and they want to pass the default one. Thanks for your work it s a really nice improvement We already exported tf.random.experimental.get global generator https github.com tensorflow tensorflow blob ba96c40cb452c68ba08eb30f4e34795f4774cb92 tensorflow python ops stateful random ops.py L682 . Is it the same as the tf.random.get default generator you have in mind Ah yes that s exactly what I had in mind thanks wangpengmit . Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33297 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33297 No a Hey wangpengmit When do you expect tf.random.experimental.get set global generator functions to be moved to tf.random and when will tf.random.uniform and other random functions move to using the global generator by default We don t have a timeline yet. I understand. In the meantime don t you want to leave this issue open The Generator class is a great workaround but as long as the default random functions don t use it they re still kind of broken IMHO. Sure I ve reopened the issue. Hi ageron there is a discussion about RNG migration plan https groups.google.com a tensorflow.org d topic developers WLBbnQMB2Rw discussion . We welcome your comments Thanks wangpengmit It looks good to me. I added a note about making it possible to revert to the old behavior if needed but I understand if that s hard to do. I m closing this bug because after many discussions we ve decided that we will leave the old RNGs unchanged and discourage and eventually deprecate them in favor of the new RNGs and stateless RNGs. The main reasons include 1 we can t find a way to fully reproduce the old RNGs behavior using the new mechanism and 2 we can t afford to change the old RNGs behavior which will break a lot of users code. BTW there is a new guide for the recommended ways to generate random numbers in TF2 https www.tensorflow.org guide random numbers Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33297 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33297 No a 
33339,embedding column converts from variable length input feature does not work with Distributed Keras MultiWorkerMirroredStrategy, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow YES OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOS Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below v2.0.0 rc2 26 g64c3d382ca 2.0.0 Python version 2.7.13 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version CPU only GPU model and memory CPU only Describe the current behavior embedding column converts from variable length input feature does not work with Distributed Keras MultiWorkerMirroredStrategy . IF With local training non distributed everything is fine . Convert variable length input feature to indicator column everything is ok. embedding column converts from fixed length input feature works pretty good. Maybe this problem is caused by SparseTensor Code to reproduce the issue Other info logs This problem has been bothering me for a week any help will be appreciated, yuefengz I had the same error have you solved . cdj0311 No I have not found a solution yet. I am still waiting for a response. I can only use the old distributed strategy Estimator ParameterServerStrategy at present . The issue should have been fixed in tf nightly. In your original example you will need to disable dataset autosharding see https github.com tensorflow tensorflow issues 35878 as the number of files is less than the number of workers. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33339 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33339 No a The issue should have been fixed in tf nightly. In your original example you will need to disable dataset autosharding see 35878 as the number of files is less than the number of workers. ckkuang Thanks for the reply I have tried using TF2.1 and turned off auto shard but the problem still exists. code based on TF 2.1.0 ERROR LOG The fix is currently in tf nightly and will be included in future tf 2.2. You may try it using tf nightly. 
33365,No float64 support with batch normalization in Tensorflow 2.0 ,Stock Ubuntu 19.04 with Cuda 10.0 Tensorflow 2.0.0 installed via pip3 Python 3.7.3 GTX1060. I have a float64 valued dataset with a simple conv2d network that includes tf.keras.layers.BatchNormalization which is where the error is being thrown I think. The first set of issues After setting tf.keras.backend.set floatx float64 next set of errors So is there perhaps another hopefully drop in method of batch normalization that supports float64 I don t want to go hacking at allowed list and all that., tb438 In order to expedite the trouble shooting process please provide a minimal standalone code to reproduce the issue reported here. Thanks tb438 Please let us know any update on this issue. Thanks Yes give me another day and I ll upload sanitized code. Hi. I have a similar problem. I used this keras code https github.com keras team keras blob master examples cifar10 resnet.py. And it works fine with TF2. But if I set K.set floatx float64 It writed Value passed to parameter x has DataType float64 not in list of allowed values float16 bfloat16 float32 Are there working examples where batch normalization works with float64 in TF 2 Hi Ravikyram Here is the code and exception Python 3.7.5rc1 default Oct 8 2019 16 47 45 GCC 9.2.1 20191008 on linux Type help copyright credits or license for more information. import numpy as np import tensorflow as tf print tf. version 2.0.0 tf.keras.backend.set floatx float64 inputs tf.keras.layers.Input shape 28 28 1 x tf.keras.layers.Conv2D filters 32 kernel size 3 strides 2 padding same inputs ... x tf.keras.layers.LeakyReLU 0.2 x x tf.keras.layers.Conv2D filters 64 kernel size 3 strides 2 padding same x x tf.keras.layers.BatchNormalization x Traceback most recent call last File stdin line 1 in module File usr local lib python3.7 dist packages tensorflow core python keras engine base layer.py line 842 in call outputs call fn cast inputs args kwargs File usr local lib python3.7 dist packages tensorflow core python keras layers normalization.py line 659 in call outputs self. fused batch norm inputs training training File usr local lib python3.7 dist packages tensorflow core python keras layers normalization.py line 517 in fused batch norm training fused batch norm training fused batch norm inference File usr local lib python3.7 dist packages tensorflow core python keras utils tf utils.py line 59 in smart cond pred true fn true fn false fn false fn name name File usr local lib python3.7 dist packages tensorflow core python framework smart cond.py line 59 in smart cond name name File usr local lib python3.7 dist packages tensorflow core python util deprecation.py line 507 in new func return func args kwargs File usr local lib python3.7 dist packages tensorflow core python ops control flow ops.py line 1174 in cond return cond v2.cond v2 pred true fn false fn name File usr local lib python3.7 dist packages tensorflow core python ops cond v2.py line 84 in cond v2 op return value pred File usr local lib python3.7 dist packages tensorflow core python framework func graph.py line 915 in func graph from py func func outputs python func func args func kwargs File usr local lib python3.7 dist packages tensorflow core python keras layers normalization.py line 503 in fused batch norm training data format self. data format File usr local lib python3.7 dist packages tensorflow core python ops nn impl.py line 1509 in fused batch norm name name File usr local lib python3.7 dist packages tensorflow core python ops gen nn ops.py line 4620 in fused batch norm v3 name name File usr local lib python3.7 dist packages tensorflow core python framework op def library.py line 631 in apply op helper param name input name File usr local lib python3.7 dist packages tensorflow core python framework op def library.py line 60 in SatisfiesTypeConstraint .join dtypes.as dtype x .name for x in allowed list TypeError Value passed to parameter x has DataType float64 not in list of allowed values float16 bfloat16 float32 I have tried on colab with TF version 2.0 2.1.0 dev20191029 and was able to reproduce the issue.Please find the gist here https colab.sandbox.google.com gist ravikyram e6adf13651041e582a832373063de997 untitled319.ipynb .Thanks Hi Ravikyram Thank you for the prompt response. Using your gist above on Colab I am still getting the same error 2.0.0 And with 2.1.0 I ve updated my development workstation to 2.1.0 as well same issue Can you try running the same code with GPU enabled Colab Perhaps this would only fixed for the CPU branch and not tensorflow gpu 2.1.0 Thanks again for your help. ARE YOU GOOGLE MOTHERFUCKERS GOING TO FIX THIS FLOAT64 ISSUE OR NOT. WHY IS FLOAT64 A SECOND CLASS CITIZEN WHEN IT COMES TO TENSORFLOW 2.0 LEMME TELL YOU WHAT THAT LITTLE FLOAT32 SOMETHING OR THE OTHER AINT COMPARE WITH US IN OUR FLOAT64 VALUED DATASETS AND THE GOOGLE IS STRAIGHT UP DISCRIMINATING WITHOUT FLOAT64 AT THIS POINT LOOK AGAIN YOUR REDUCED PRECISION INT8 AND INT16 AINT MEAN NOTHING TO US DO YOU UNDERSTAND GOOGLE YOUR GOOGLE TPU CAIN T TRAIN SO IMA GO WITH CUDA128 JUST AS SOON AS YOU BRING MY FLOAT64 BATCH NORMALIZATION BABY BACK Hello Is there a solution for this i am getting it when i call the model.predict on boolean data. it also causes a segmentation fault. i am using tf2 s keras Hi all supporting float64 would be really cool indeed TrailBlazerAI maybe consider a change of tone when asking for other people to help you Hello Is there a solution for this i am getting it when i call the model.predict on boolean data. it also causes a segmentation fault. i am using tf2 s keras I changed the data to np.float32 and it worked well with tf2 maybe it may help someone still having this issue. Thanks Hello Can anyone provide info on the matter Is it gonna be fixed If no why Is there a timeline Are there any blockers Many thanks Julien Are there any updates to this I m facing the same problem and I m having to set BatchNorm s dtype to float32 for now but it d be nice to be able to use float64 This appears to be an issue with fused batch norm in particular and if you disable the fused kernel the code above does not raise an error reedwm qlzh727 this is an unfortunate hard edge for BatchNorm. Can we either add fp64 support to the fused kernel or not default to fusing for unsupported datatypes I ll start by not defaulting to fused for unsupported datatypes. We can add fp64 support to the fused kernel later. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33365 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33365 No a 
33380,get config missing from AdditiveAttention, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 colab Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary pip TensorFlow version use command below 2.0.0 Python version Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Describe the current behavior Model containing AdditiveAttention cannot be saved due to missing get config Describe the expected behavior AdditiveAttention has get config and can be saved Code to reproduce the issue Other info logs Code based on https www.tensorflow.org api docs python tf keras layers AdditiveAttention ,I could reproduce the issue with tf 2.0.0. Please take a look at colab gist https colab.sandbox.google.com gist gadagashwini 6ab8b86252382e26b668b41815b19305 untitled197.ipynb . Thanks I m having a similar issue with tf.keras.layers.Attention with TensorFlow 2.0.0 Error I didn t want to start a new issue but this is also preventing me from saving models with the HDF5 format. My temporary workaround is something like the following Note that if I omit custom objects when loading the model I get the error Related 32662 Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33380 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33380 No a 
33383,Desynchronized zipped datasets when using tf.data.experimental.ignore errors, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device na TensorFlow installed from source or binary binary TensorFlow version use command below v2.0.0 rc2 26 g64c3d38 2.0.0 Python version Python 3.7.4 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version cuda 10.0 GPU model and memory P100 16GB You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior When an error is caught using the tf.data.experimental.ignore errors on a zipped dataset only the faulty dataset drops an element. The datasets are therefore desynchronized. Describe the expected behavior Datasets should stay synchronized by dropping an element from both datasets. Code to reproduce the issue Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,I could reproduce the issue with Tf 2.0.0. Please see the gist here https colab.sandbox.google.com gist gadagashwini 48e54114ef599b79cd9d09e4a7fb9736 untitled201.ipynb . Thanks This is working as intended. The good dataset dropped 0.0 while the bad dataset dropped 1 0.0 . scharron What output would you expect for your example jsimsa Oops sorry I went too fast I fixed the code to reproduce if zipping good then bad it works but the reverse does not I think the issue is that in case of zip when error or end of sequence encountered the remaining components are not flushed out . So out of sync happens when ignore errors is in play. Created a PR 33887 think this could fix the issue. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33383 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33383 No a Thanks Wouldn t it be possible and useful to also issue a warning if the tf.data.experimental.ignore errors is applied to any Dataset used as input to tf.data.Dataset.zip . This could also result in datasets desynchronization if any of the zipped datasets drops any item and in my opinion shouldn t be the behaviour of a zip function. 
33394,AttributeError list object has no attribute op when calling SparseCategoricalCrossentropy, Describe the current behavior When running the following code in the documentation https www.tensorflow.org api docs python tf keras losses SparseCategoricalCrossentropy of SparseCategoricalCrossentropy I obtained the following error AttributeError Traceback most recent call last ipython input 2 e7331c659215 in module 3 loss cce 4 0 1 2 5 .9 .05 .05 .5 .89 .6 .05 .01 .94 6 print Loss loss.numpy Loss 0.3239 3 frames usr local lib python3.6 dist packages tensorflow core python keras backend.py in sparse categorical crossentropy target output from logits axis 4397 if not from logits 4398 if isinstance output ops.EagerTensor variables module.Variable or 4399 output.op.type Softmax 4400 epsilon constant to tensor epsilon output.dtype.base dtype 4401 output clip ops.clip by value output epsilon 1 epsilon AttributeError list object has no attribute op ,Added a PR 33406 for the fix. netw0rkf10w The above PR will fix the issue for you. In the mean time you can provide converted tensors of y pred and y true as follows. Thanks loss cce tf.convert to tensor 0 1 2 tf.convert to tensor .9 .05 .05 .5 .89 .6 .05 .01 .94 jvishnuvardhan Thanks a lot Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33394 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33394 No a Still doesn t work. Can t reproduce the example from documentation. AttributeError Traceback most recent call last ipython input 21 8338a7552986 in module 1 sce y true y pred opt local Library Frameworks Python.framework Versions 3.6 lib python3.6 site packages tensorflow core python keras losses.py in call self y true y pred sample weight 124 y true y pred sample weight 125 with K.name scope scope name or self. class . name graph ctx 126 losses self.call y true y pred 127 return losses utils.compute weighted loss 128 losses sample weight reduction self. get reduction opt local Library Frameworks Python.framework Versions 3.6 lib python3.6 site packages tensorflow core python keras losses.py in call self y true y pred 219 y pred y true tf losses util.squeeze or expand dimensions 220 y pred y true 221 return self.fn y true y pred self. fn kwargs 222 223 def get config self opt local Library Frameworks Python.framework Versions 3.6 lib python3.6 site packages tensorflow core python keras losses.py in sparse categorical crossentropy y true y pred from logits axis 976 def sparse categorical crossentropy y true y pred from logits False axis 1 977 return K.sparse categorical crossentropy 978 y true y pred from logits from logits axis axis 979 980 opt local Library Frameworks Python.framework Versions 3.6 lib python3.6 site packages tensorflow core python keras backend.py in sparse categorical crossentropy target output from logits axis 4528 if not from logits 4529 if isinstance output ops.EagerTensor variables module.Variable or 4530 output.op.type Softmax 4531 epsilon constant to tensor epsilon output.dtype.base dtype 4532 output clip ops.clip by value output epsilon 1 epsilon AttributeError list object has no attribute op ilyarudyak Can you please create a new issue and provide a standalone code to reproduce the issue Please ping me in that issue. Thanks I m getting AttributeError list object has no attribute op when I try to get the output node name of a loaded model as follows My Tensorflow version is 2.5.0 
33397,LEAKY RELU not supported in INT8 quantization, System information OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 TensorFlow installed from source or binary binary TensorFlow version or github SHA if from source 2.0 Provide the text output from tflite convert Also please include a link to a GraphDef or the model if possible. Model definition Conversion options Any other info logs I saw there were some PR about the implementation of Leaky ReLU in tflite and quantization. But I still cannot get it to work. Ref PR https github.com tensorflow tensorflow pull 27061 Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,BTW I am not using tflite convert.py I am using TFLiteConverter as shown in the code snippet above. I am not sure if I am looking at the right place. But the error might be thrown from here https github.com tensorflow tensorflow blob 265126ca309d9e9502ab072f033308192d1367b0 tensorflow lite tools optimize quantize model.cc L590 which calls operator property.cc link https github.com tensorflow tensorflow blob d08bf910b3ea5d8895a07553379436ad543f2da7 tensorflow lite tools optimize operator property.cc where LEAKY RELU and RESIZE NEAREST NEIGHBOUR are not defined. Fixed in https github.com tensorflow tensorflow pull 36876 Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33397 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33397 No a Hi wuhy08 After adding Leaky Relu into operator property I can generate the INT8 model which include Leaky Relu . But the int8 implementation of Leaky Relu is still missing right When I called invoked tflite model it will return fail. Did you meet the same problem Error detail ValueError Didn t find op for builtin opcode LEAKY RELU version 2 Registration failed. Thanks Chunjue Hi cjtang There was a PR about Leaky RELU INT8 a while ago https github.com tensorflow tensorflow pull 27061 . I believe that part of code was never touched since nobody successfully quantized a Leaky RELU op. Let me investigate a little. It looks the error comes from this line https github.com tensorflow tensorflow blob 3a8c575c4efe57560f36670ddc028f6fc889060a tensorflow lite core api op resolver.cc L41 Will continue to investigate. A PR is created for this issue https github.com tensorflow tensorflow pull 37279. Will wait for approval. Great I also added my version of INT8 Leaky RELU implementation but the result is always not good. Now I know that is because model doesn t restrict the same scale for input and output. Thank you for pointing that out. Let s wait for approval. cjtang The PR has just been merged. Expect a few days before it is in tf nightly. Thanks. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33397 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33397 No a 
33403,max pool with argmax can gives different results on GPU than on CPU, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow YES OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOS Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary source TensorFlow version use command below 1.13.1 Python version 3.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 10.1 GPU model and memory GPU 1080 Ti Describe the current behavior When running the test in the code below one of the tests fail. Somehow the argmax result of max pool with argmax is different when running with self.session use gpu True . Interestingly the failing test behaves as if it was ignoring the batch index in the argmax. Describe the expected behavior All 3 tests should pass. Code to reproduce the issue import tensorflow as tf import numpy as np class TestMaxPool tf.test.TestCase def setUp self tf.reset default graph def test self sess Image with 2 channels H 4 W 4 image t tf.constant 1.0 12.0 3.0 14.0 5.0 16.0 7.0 18.0 11.0 2.0 13.0 4.0 15.0 6.0 17.0 8.0 38.0 37.0 36.0 35.0 34.0 33.0 32.0 31.0 28.0 27.0 26.0 25.0 24.0 23.0 22.0 21.0 input t tf.stack image t image t pool indices tf.nn.max pool with argmax input t ksize 1 2 2 1 strides 1 2 2 1 padding SAME pool indices out sess.run pool indices expected image indices np.array 10 3 14 7 16 17 20 21 self.assertAllEqual pool indices out np.stack expected image indices expected image indices 32 def test with normal session self with tf.Session as sess self. test sess def test with self session and gpu self with self.session use gpu True as sess self. test sess def test with self session and cpu self with self.session use gpu False as sess self. test sess Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. , jnd77 Thanks for reporting the issue. Please provide the complete code to reproduce the reported issue. Thanks gadagashwini Thanks for taking a look. The complete code can be found in my original message. Only 2 out of 3 tests are passing ... jnd77 I tried replicating the reported issue. Please take a look at the gist https colab.sandbox.google.com gist gadagashwini 696ae88444b31f485f5a8a97c723bfb5 untitled210.ipynb . And provide the more information to reproduce the issue. Thanks gadagashwini . Here is the gist https colab.research.google.com gist jnd77 e7193cb2eb57c23a35e460c6538751ff untitled210.ipynb reproducing the issue It looks like the argmax returned by the CPU op follows the documentation and is equal to b height y width x channels c While the one returned by the GPU op ignores the batch y width x channels c Am very unclear why test with normal session is passing as I thought it would be sent to the GPU. jnd77 Is this still an issue Can you please try with TF1.15 and let us know whether the issue persists. If possible test it in tf nightly or TF2.1 . Thanks jvishnuvardhan I did a check with TF1.15. The function has this new argument include batch in index Both CPU and GPU versions give the same correct result whether include batch in index is True or False. And documentation has been updated as well. All good on my side Thanks a lot. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33403 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33403 No a 
33411,Saving model containing sequence numeric column fails, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOS Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary Binary TensorFlow version use command below v1.12.1 15925 g2e1e8ecea2 2.1.0 dev20191015 Python version 3.6.8 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Describe the current behavior Saving a keras model containing a sequence numeric column feature column results in the error TypeError Input must be a SparseTensor. However saving a model using a sequence categorical column with together with indicator column or embedding column works as expected. For example the following works as expected Describe the expected behavior It should be possible to save a model containing all types of sequence feature columns. Code to reproduce the issue Other info logs Full traceback , emla2805 Looks like code is incomplete. Can you please help us with reproducible code .It will be easy for localizing the issue faster.Thanks ravikyram Running the code in Code to reproduce the issue reproduces the issue for me. Maybe it wasn t clear from the original report that tensorflow had been installed through pip install tf nightly e.g. 1. Create new Python 3.6 virtualenv. 2. Run 3. Run the example code in the original report ravikyram Same here I don t see how the code is incomplete could you please elaborate emla2805 I am able to reproduce the issue with tf.feature column.sequence numeric column But you said saving a model using a sequence categorical column with together with indicator column or embedding column works as expected is where i am facing the issue. Thanks ravikyram Ah got it. I have updated the original description with an example of working code using two feature columns one indicator column wrapping a sequence categorical column with vocabulary list column and one embedding column wrapping a sequence categorical column with hash bucket column. I have tried on colab with TF version 2.1.0 dev20191015 2.1.0 dev20191022 and was able to reproduce the issue.Please find the gist here https colab.sandbox.google.com gist ravikyram d563809afcdf9878e04257ba037ffb9b untitled289.ipynb .Thanks This is fixed with the latest tf nightly. colab gist https colab.sandbox.google.com gist goldiegadde 20e648fa5e3e9cc7f8457ca251287530 untitled289.ipynb Marking this as closed. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33411 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33411 No a 
33425,Tensorflow eager execution not working with tf.math.unsorted segment max Gradient output is null, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 Professional Edition Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary installed using conda TensorFlow version use command below unknown 1.14.0 Python version 3.7.3 CUDA cuDNN version 10.0 7.6 GPU model and memory T1000 4GB VRAM Describe the current behavior When using tf.math.unsorted segment max with Tensorflow eager execution and Gradient Tape the source code see below produces following error Operations tf.math.segment max tf.math.segment mean and tf.math.unsorted segment mean are working ok though. I need the unsorted version because in more complex code bases I a using several segmented aggregations and concatenating them so I need to have fixed sizes. Describe the expected behavior It should work without throwing error. Code to reproduce the issue The code is here https gist.github.com racinmat 9a95cac7db36d5f0b6b33e9c35678ca2 Other info logs Exception thrown is mentioned above., racinmat I tried reproducing the issue with TF 1.14 on colab. However i am seeing the different error.Please find the gist here https colab.sandbox.google.com gist ravikyram 2f6897da2b30cbaba0bdd43ec9a84aa3 untitled279.ipynb . Thanks Yes my bad it is fixed now in the gist and produces the abovementioned error. Here is the google colab notebook with fixed code https colab.research.google.com gist racinmat 057bb526253484884f3f484f62cb1f0a untitled279.ipynb I have tried on colab with TF 1.14 1.15.0 rc3 and was able to reproduce the issue Please find the gist here https colab.sandbox.google.com gist ravikyram c69ad2af68dbb3ff0747b0d4ed0e7df4 untitled279.ipynb scrollTo YVFBL130Y2w7 . Thanks Just a small question should this still have the awaiting response label Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33425 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33425 No a I see the issue is fixed in master will there be a 1.X version with the fix released sometimes I m seeing this same problem in TensorFlow 2.1. My model uses tf.math.unsorted segment max . When I call tape.gradient in eager mode I get the error Which versions is this supposed to be fixed in The issue is still not resolved in v1.15.2 ravikyram why has it been closed The bug is still there. Also in TensorFlow 2.1 it appears this is no longer restricted to eager mode. Even if I wrap the calculation in tf.function it still fails. Can someone reopen this so it will get fixed Actually I cannot reproduce this against nightly TF so I think it has been fixed since. Sorry for the noise. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33425 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33425 No a ravikyram This problem still happened in tensorflow gpu 2.0.0. But it s ok in tensorflow gpu 2.1.0. 
33438,UpSample2D INT8 quantization not supported, System information OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 TensorFlow installed from source or binary pip TensorFlow version or github SHA if from source 2.0 Provide the text output from tflite convert TFLiteConverter Also please include a link to a GraphDef or the model if possible. source code Any other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,see comment https github.com tensorflow tensorflow issues 33397 issuecomment 542880236 I submitted a PR to address the issue https github.com tensorflow tensorflow pull 33492 . Will close this one for now. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33438 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33438 No a 
33459,resnet50 imagenet weights are differents in tf.keras vs keras, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow no OS Platform and Distribution e.g. Linux Ubuntu 16.04 win10 pycharm same issue from colab Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary pycharm TensorFlow version use command below 2.0 and keras 2.3.0 same issue with tf 1.15 and keras 2.2.5 in google colab Python version 3.7 Describe the current behavior The weights of the kernel in layers 13 14 are different in tf.keras and keras the weights of the layer 13 in tf.keras are the ones of the layers 14 in keras and the weights of the layer 14 in tf.keras are the ones of the layer 13 in keras with tf.keras layer 13 conv2 block1 0 conv 0.00460704 0.0613995 0.04595907 ... 0.12616335 0.00781816 0.03271283 ... 0.00736084 0.00832207 0.00591875 ... 0.16227128 0.00581011 0.01718325 layer 14 conv2 block1 3 conv 0.00412396 0.01779881 0.01002417 ... 0.0397268 0.01897338 0.00012411 ... 0.01601992 0.00197976 0.01605847 ... 0.06464136 0.0353195 0.02405972 with keras layer 13 res2a branch2c 0.00412396 0.01779881 0.01002417 ... 0.0397268 0.01897338 0.00012411 ... 0.01601992 0.00197976 0.01605847 ... 0.06464136 0.0353195 0.02405972 layer 14 res2a branch1 0.00460704 0.0613995 0.04595907 ... 0.12616335 0.00781816 0.03271283 ... 0.00736084 0.00832207 0.00591875 ... 0.16227128 0.00581011 0.01718325 this is with include top True but include top False gives the same thing Describe the expected behavior I would expect the same weights not sure which weights are the right ones Code to reproduce the issue from tensorflow.python.keras.applications import ResNet50 from keras.applications import ResNet50 image size 224 model ResNet50 input shape image size image size 3 include top True weights imagenet print model.layers 13 .name weights model.layers 13 .get weights 0 print weights print model.layers 14 .name weights model.layers 14 .get weights 0 print weights Other info logs The difference is coming from the different path used to load the h5 file in tf.keras https github.com keras team keras applications releases download resnet resnet50 weights tf dim ordering tf kernels.h5 in keras https github.com fchollet deep learning models releases download v0.2 resnet50 weights tf dim ordering tf kernels.h5 ,I could reproduce the issue with Tf 2.0.0 keras 2.3.0 and TF 1.15.0 keras 2.2.5. Please take a look at the gist of tf.keras here https colab.sandbox.google.com gist gadagashwini 903f26e451e5b6d40ac5f99991716a25 untitled206.ipynb and keras with tensorflow backend here https colab.sandbox.google.com gist gadagashwini 99de5876f3f6ff5937bb29c6ab1185e7 untitled207.ipynb . Thanks gadagashwini This does not answer anything. The issue still remains. Or is it that they are downloading two versions of resnet v1 and v2 Adding Francois for this. When did we update the weights last time Any updates on this please It seems that several blocks are affected. For example in the conv5 block the weights of the following layers are switched conv5 block1 3 bn and conv5 block1 0 bn conv5 block1 3 conv and conv5 block1 0 conv . This affects quite significantly the accuracy of transfer learning on at least semantic segmentation in my case . I guess 35336 is a similar issue. In my testing code which loading weights from tf.keras the accuracy for pre trained weights are under 70 . CNOCycle Nice. I guess we now have enough evidence that the pre trained weights are broken. I think tf.keras.application is exporting the latest implementation of resnet50 while keras.application is exporting the old version of implementation. They are structured differently eg the layer may appear in different order but functionality wise they are the same. I think the root cause is for keras.application not exporting the latest version is in https github.com keras team keras applications blob master keras applications init .py. Basically both resnet50.py and resnet.py will export Resnet50 and the former one is exported by contains the legacy implementation. Checked with fchollet offline for this issue. I think the keras team keras application was exporting the old model. The PR should fix the issue but since the keras application is not going to make any new release I would suggest you to use the version in tf.keras.application which should be latest and correct. Btw the keras team keras application is still correct it is just structured differently compare to the latest implementation. You can still use it for transfer learning. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33459 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33459 No a Not sure I fully understood the root cause but I m happy there s some progress on this. The difference between tf.keras and keras does not seem to be an issue as they used two different implementations. But is it sure that the weights of tf.keras ResNet50 are good As CNOCycle pointed out in 35336 the best accuracy he she could get on ImageNet validation set is 45.7 for ResNet50 which is far from the reported https keras.io applications 74.9 . CNOCycle Would it be possible for you to try applying the above PR and performing your experiment again I do the test again in docker image tensorflow tensorflow 2.1.0 gpu py3 and comment out the line as qlzh727 suggestion usr local lib python3.6 dist packages keras applications init .py 65L The following is the output of testing code which is shown in 35336 However I still get similar results. I hope that someone would check correctness of my code. CNOCycle note that the downloading url is from https github.com keras team keras applications releases which should be same as the one in tf.keras.application. This somehow indicating that your transfer learning model might not be constructed correctly. The easiest way to verify whether the original resnet model is correct is to run some eval with the eval data which I am quite sure we did before we release the model. CNOCycle note that the downloading url is from https github.com keras team keras applications releases which should be same as the one in tf.keras.application. This somehow indicating that your transfer learning model might not be constructed correctly. Right. I have just tried again with tf nightly and indeed the URLs have changed Downloading data from https storage.googleapis.com tensorflow keras applications resnet resnet50 weights tf dim ordering tf kernels notop.h5 One can check this in my Colab notebook https colab.research.google.com drive 13zpJSsJ0GmXluiVNAF4DLSqW3Ao84G5Z . CNOCycle Could you please try again with tf nightly Thanks a lot. I checked the weight from storage.googleapis.com and keras applications releases . Both files hash are same. What I do is not transfer learning. I m trying to reproduce to accuracy which publish in many papers. From my experience my implement of training ImageNet from scratch by TF and pytorch. For pytorch version its top 1 accuracy is about 74 . For TF version its top 1 accuracy is about 68 . For TF version with pre trained weights its top 1 accuracy is under 50 Hmm that s bad news... The easiest way to verify whether the original resnet model is correct is to run some eval with the eval data which I am quite sure we did before we release the model. This is exactly what CNOCycle did. qlzh727 Would you mind sharing your eval code so that we can check the weights again on our side Hmm... I think this issue is rather critical... Imagine getting 2 3 lower accuracy than PyTorch just because of bad conversion of pretrained models That d be a pity... Hello everyone I have just tried to use pre trained ResNet50 with tf.keras.applications.ResNet50 args . The thing is happening for me is that after downloading weights nothing else happens even after a couple of hours it does not go any further Any idea what may be the casue of this Any solution Many thanks for your help in advance. I use as follow ENCODER BASE tf.keras.applications.ResNet50 include top False input shape None None 3 weights imagenet nick nikzad At least show us your code to see what you would expect to see after loading the model. netw0rkf10w Thanks for the reply. I just expect using the output of some blocks as below ENCODER BASE tf.keras.applications.ResNet50 include top False input shape None None 3 weights imagenet for layer in ENCODER BASE.layers layer.traiable True f0 ENCODER BASE.get layer conv1 relu .output 43358 f1 ENCODER BASE.get layer conv2 block2 out .output f2 ENCODER BASE.get layer conv3 block4 out .output f3 ENCODER BASE.get layer conv4 block6 out .output f4 ENCODER BASE.get layer conv5 block3 out .output nick nikzad Can you reproduce that using Colab https stackoverflow.com questions 67365237 imagenet pretrained resnet50 backbones are different between pytorch and tensorf And given my ongoing experiments transfer learning doesn t work as well in TensorFlow o 
33469,Simultaneous fetching of collective ops and global step triggers Skipping rendezvous re initialization , System information Have I written custom code Yes OS Platform and Distribution Linux CentOS 7.6.1810 Mobile device if the issue happens on mobile device No TensorFlow installed from Binary TensorFlow version v1.13.1 0 g6612da8951 Python version 3.6.8 Bazel version None GCC Compiler version None CUDA cuDNN version None GPU model and memory None Describe the current behavior collective ops.all reduce and global step can be fetched successfully when the operations are carried out separately by a session. Nevertheless the calling will trigger an info or occasionally fail when the two tensors are fetched simultaneously in a session run. regularly raise info occasionally raise failure Full logs are attached below. Describe the expected behavior Value is fetched without warnings. Code to reproduce the issue Other info logs Full log with regular info of Skipping rendezvous re initialization processes exit successfully Full log with occasionally warning BaseCollectiveExecutor StartAbort Aborted processes hang A related issue can be found 33321.,I hope this issue is clear enough to be supported. slightly smiling face ravikyram I believe the skipping rendezvous re initialization message was benign and has been fixed in bcb615c42a6215037ed7f1c81316bc0960e76269. Can you try to rerun your program with a nightly build to confirm cc haoyuz qqfish It works perfectly. Thanks very much 1 Maybe we should migrate to the compat.v1 module in tf 2.0. whhu Please let me know if we can close this issue since it looks to be fixed. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33469 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33469 No a 
33471,TF 2.0 Keras add loss function seems to make trainable weight not in model but added in add loss also being updated during training, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04.2 LTS Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary pip TensorFlow version use command below v2.0.0 rc2 26 g64c3d38 2.0.0 v1.14.0 rc1 22 gaf24dc9 1.14.0 Python version 3.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Describe the current behavior In TF 2.0 if output of another model with trainable weight is included in add loss function all weight from that model will be updated during training. I also couldn t find any documentation regarding this change. So I assume it is unintended. Describe the expected behavior It should be like in TF 1.14. The gradient of additional model in add loss should be back propagated because the model to be trained may be used as input for additional model. But additional model should not be updated. Code to reproduce the issue Other info logs If you run the code above in TF 1.14 it will show 0 because model2 remains and should remain the same after training of model1. But if you run it in TF 2.0 it will show 100 as model2 is updated as well for some reason during model1 training. ,Yes in TF 1.14 it shows 0 but in TF 1.15rc3 and TF 2.0 its showing 100. I think this is clearly a bug Here are my github gists Tf 1.14 https colab.sandbox.google.com gist gowthamkpr 1fd2d4e745b093af63325bfbd7dbefb7 untitled198.ipynb TF 1.15rc3 https colab.sandbox.google.com gist gowthamkpr edf243d59c16cbe5709754d8d346636e untitled199.ipynb and TF2.0 https colab.sandbox.google.com gist gowthamkpr 0fefab16eefe321e944f7fe528376765 untitled197.ipynb As a followup it seems add loss function incorrectly add any trainable variables in the model linked within custom loss function into the main model being trained. You should be able to see the difference by printing out model1 s trainable variable in TF 1.14 and TF 1.15. https github.com matterport Mask RCNN issues 1889 issuecomment 566217231 As mentioned in the post above. The issue can be resolved by converting passed function into a lambda function without input parameters. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33471 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33471 No a the title of this issue gave me a headache 
33484,Issue with tf.keras.mixed precision.experimental.LossScaleOptimizer, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary source TensorFlow version use command below 2.0 Python version 3.6.8 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 10.0 7.5 GPU model and memory Titian RTX 24gb You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior When attempting to use tf.keras.mixed precision.experimental.LossScaleOptimizer it fails to cast a matmul to float16 Describe the expected behavior Matmul should cast to float16 Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. import tensorflow as tf import numpy as np from tensorflow.keras.datasets import mnist gpus tf.config.experimental.list physical devices GPU if gpus try for gpu in gpus tf.config.experimental.set memory growth gpu True logical gpus tf.config.experimental.list logical devices GPU print len gpus Physical GPUS len logical gpus Logical GPUS except RuntimeError as e print e tf.keras.backend.set floatx float16 tf.keras.backend.set epsilon 1e 4 model tf.keras.models.Sequential model.add tf.keras.layers.Conv2D 32 3 3 activation relu input shape 28 28 1 model.add tf.keras.layers.MaxPooling2D 2 2 padding same model.add tf.keras.layers.Conv2D 64 3 3 activation relu padding same model.add tf.keras.layers.Flatten model.add tf.keras.layers.Dense 64 activation relu model.add tf.keras.layers.Dense 10 activation softmax model.summary train images train labels test images test labels mnist.load data train images train images.reshape 60000 28 28 1 train images train images.astype np.float16 255 test images test images.reshape 10000 28 28 1 test images test images.astype np.float16 255 train labels tf.keras.utils.to categorical train labels dtype float16 test labels tf.keras.utils.to categorical test labels dtype float16 opt tf.keras.optimizers.RMSprop opt tf.keras.mixed precision.experimental.LossScaleOptimizer opt dynamic model.compile optimizer opt loss categorical crossentropy metrics accuracy model.fit tf.dtypes.cast train images tf.float16 tf.dtypes.cast train labels tf.float16 epochs 50 batch size 64 steps per epoch 200 test loss test acc model.evaluate test images test labels test acc Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,I could reproduce the issue with Tf 2.0. Please take a look at the colab gist https colab.sandbox.google.com gist gadagashwini 7909678539c398aa1f4b8fafa334807f untitled208.ipynb . Thanks mattbernardini The tf.matmul op does not perform automatic type conversions so both of its inputs must have the same element type. Try converting all the inputs to float32 and it should work fine. Please find my github gist here https colab.sandbox.google.com gist gowthamkpr 2ab5ef2ed2af387abc2e60016a3936bc untitled208.ipynb . For more context you can go through the following issue https stackoverflow.com questions 36210887 how to fix matmul op has type float64 that does not match type float32 typeerror . Thanks Right that works for float32 but does not work for float16 which was the original issue. reedwm Can you PTAL Thanks gowthamkpr I m running into the same issue trying to run float16 training. Not sure why you re casting to float32 in the .fit method Thanks I encounter the same problem with float16 training. So far I have figured out that the issue lies in the LossScaleOptimizer because the issue disappears when you remove it but then obviously losses are too tiny to be effective . To be more specific the error is raised when the LossScaleOptimizer calls its get scaled loss function which simply multiplies via python operator the loss with the loss scale which is an instance of tf.train.experimental.LossScale . See the code here. https github.com tensorflow tensorflow blob r2.0 tensorflow python keras mixed precision experimental loss scale optimizer.py L149 The error says that the y input of the multiplication is float32 and the x input is float16. Since the multiplication in get scaled loss is done like this loss loss scale this means that loss scale is a float32. A few lines above we see that loss scale self. loss scale and self. loss scale is an instance of tf.train.experimental.LossScale . Now if we look into the source code of tf.train.experimental.LossScale here https github.com tensorflow tensorflow blob r2.0 tensorflow python training experimental loss scale.py we see that the call method of e.g. the DynamicLossScale which is the one mattbernardini uses returns ops.convert to tensor self. current loss scale . If we then check the initialization of self. current loss scale we see that it is initialized as float32 So this is why the TypeError is thrown when using LossScaleOptimizer in float16 training. I am not exactly sure why LossScales were implemented to only return float32 values. There are also no clear online guides on how to do float16 training in TF 2.0 so it would be nice if someone who knows the details of how Tensorflow implements the optimization process could provide information on proper float16 training because evidently neither tf.keras.backend.set floatx float16 nor tf.keras.mixed precision.experimental.set policy mixed float16 seem to work out of the box. verrannt good investigation In TF 2.1 this will be fixed. Alternatively you can use the nightly builds with pip install tf nightly gpu which has the fix now. This has been fixed by casting the float32 loss scale to the dtype of the loss. For reference to do float16 training typically referred to as mixed precision training you always want to do the following You do not want to set floatx to float16 with the following unless you also set the policy to mixed float16 The reason you want to set the policy instead of floatx is that the policy will keep variables in float32 which is necessary for numeric stability. As for an online guide to do float16 training I agree we need one and we will have one in the future. For now the Policy API documentation https www.tensorflow.org api docs python tf keras mixed precision experimental Policy has a brief guide on how to do mixed precision training Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33484 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33484 No a 
33496, tensorflow python kernel tests decode raw op test fails with Assertion error, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 19.04 s390x Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device NA TensorFlow installed from source or binary Source TensorFlow version use command below 2.0.0 Python version 2.7.16 Bazel version if compiling from source 0.26.1 GCC Compiler version if compiling from source gcc Ubuntu 8.3.0 6ubuntu1 8.3.0 CUDA cuDNN version NA GPU model and memory NA Describe the current behavior The above test fails with the output array mismatch in testToComplex64 https github.com tensorflow tensorflow blob v2.0.0 tensorflow python kernel tests decode raw op test.py L105 and testToComplex128 https github.com tensorflow tensorflow blob v2.0.0 tensorflow python kernel tests decode raw op test.py L118 . The cause of failure seems to be in the byte swapping code to solve endianness problem here https github.com tensorflow tensorflow blob v2.0.0 tensorflow core kernels decode raw op.cc L88 . The code doesn t seem to work as intentioned for complex data . Describe the expected behavior The test should pass on s390x. Code to reproduce the issue Other info logs ,Further Analysis shows that the real and imaginary components in the output are getting swapped with each other causing the array mismatch on Big Endian. jiefangxuanyan I could see PR https github.com tensorflow tensorflow pull 9876 for fixing endianness problem in the decode raw op functionality. However the tests for complex data types have been recently added and need more changes for incorporating this particular case. Could you please have a look abhay1722 In the process of reproducing the error I have cloned the Branch corresponding to TF Version 2.0 and ran the command bazel test usr local google home mothukuru tensorflow 2.0 tensorflow python kernel tests decode raw op test but it resulted in the below error Can you please help us reproduce the error. Thanks abhay1722 In the process of reproducing the error I have cloned the Branch corresponding to TF Version 2.0 and ran the command bazel test usr local google home mothukuru tensorflow 2.0 tensorflow python kernel tests decode raw op test but it resulted in the below error Can you please help us reproduce the error. Thanks rmothukuru I suggest that you run the test from the location where you have cloned the Tensorflow repo. For instance If I clone the repo https github.com tensorflow tensorflow.git in the home test folder then I must do the following Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33496 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33496 No a 
33503,Model.fit displays wrong information on progress bars for attrs classes, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes a minimal self contained python script demonstrating the bug OS Platform and Distribution e.g. Linux Ubuntu 16.04 OSX TensorFlow installed from source or binary Installed from anaconda TensorFlow version use command below 1.14 Python version 3.6.9 Describe the current behavior Wrong progress bar information 1000 12 overly long progress bar Describe the expected behavior Correct progress bar information 100 100 progress bar reasonably short Code to reproduce the issue Other info logs The bug is caused by in module tensorflow.python.keras.engine.training generator . get num samples or steps should always return steps per epoch True for a Sequence irrespective of the attrs.Attributes of the class implementing the Sequence interface. The statement if data utils.is generator or sequence x in module tensorflow.python.keras.engine.training is already ensuring that the passed object is a Sequence and therefore the call to function get num samples or steps seems superfluous. ,Could replicate the error with TF Version 1.14 and 1.15 and by importing from tensorflow.python.keras and tensorflow.keras as well. Here is the Gist https colab.sandbox.google.com gist rmothukuru c2fde49c1f29ff4dff930baf03f9ad81 33503 progressbar.ipynb . Thanks trendelkampschroer Thanks for the issue This is fixed in the latest tf nightly pip install tf nightly Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33503 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33503 No a 
33550,tf.keras.Model.fit ignores class weight when passed tf.data.Dataset, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOS 10.14.6 or Colab TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 1.15.0 rc3 Python version 3.7 CUDA cuDNN version GPU model and memory None or Colab You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior tf.keras.Model.fit ignores class weight when passed a tf.data.Dataset . When passed an instance of tf.data.Dataset and a class weight dictionary with nonsensical label keys it runs without exception whereas it correctly raises ValueError when passed a pair of np.ndarray . Describe the expected behavior tf.keras.Model.fit should apply class weight when passed a tf.data.Dataset . It should raise an exception for incorrect class weight keys. Code to reproduce the issue Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,I could reproduce the issue on colab with TF 2.0.0. Please see the gist here https colab.sandbox.google.com gist gadagashwini 12fbb6df6ff8c99dce74b1464b725141 untitled213.ipynb . Thanks Could it be that the call to model. standardize user data in training v2.py https github.com tensorflow tensorflow blob 8b324344adacdfd37587d1fa591aab0e6a23a6ac tensorflow python keras engine training v2.py L641 should also be passed extract tensors from dataset True p.s. Or could it be that calling train on batch by running strategy.experimental run v2 on per replica function in https github.com tensorflow tensorflow blob 8b324344adacdfd37587d1fa591aab0e6a23a6ac tensorflow python keras engine training v2 utils.py L72 is failing to pass the class weight to train on batch Unless there is a partial binding of class weight kwarg to train on batch somewhere but I don t see it. . Thank you. We are having a similar problem. It would be nice if we could use class weights in model.fit instead of relying on adding a sample weight to the tf.data.Dataset x y weight . It is very dangerous for the training behavior to completely change in terms of applying vs. ignoring without any warning the class weight kwarg simply by going from model.fit x y class weight class weight ... to model.fit tf.data.Dataset.from tensor slices x y .batch ... class weight class weight ... . Consider for example the Classification on imbalanced data https www.tensorflow.org tutorials structured data imbalanced data tutorial that specifically demos class weight . By simply replacing train features test features with tf.data.Dataset.from tensor slices train features test features .batch BATCH SIZE and similar for validation data and also removing the batch size kwarg in model.fit the tutorial example fails to apply class weight again without any warning. Has this problem been resolved in tf2.1 mmilosav Thanks for the issue This has been fixed in the latest tf nightly pip install U tf nightly Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33550 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33550 No a 
33560,BatchMatmul on GPU Wrong Result, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Linux Ubuntu 18.04 TensorFlow installed from source or binary pip install tensorflow gpu TensorFlow version use command below 2.0 Python version 3.7.3 CUDA cuDNN version GPU model and memory Quadro P4000 8GB Describe the current behavior batchmatmul on gpu gives wrong result. Describe the expected behavior batchmatmul should give correct result. Code to reproduce the issue Other info logs python diff.py tf.Tensor 0. 0. 0. 0. 0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ... 0. 0. 0. 0. 0. 0. 0. 0. 0. ... 2.7437444 2.3577256 3.3067198 1.0874022 0.7325933 2.4926019 2.3672104 1.6497741 2.1795664 ... 1.4023315 0.68545413 1.9975882 0.44164103 1.6766946 0.6477224 2.5612988 0.7603723 4.286979 0.9761815 1.5332515 0.12404668 1.871354 0.73664904 1.0545558 1.2774239 1.6571116 2.36473 ... 2.4847538 1.5389507 1.5169847 2.2117858 3.3593345 0.15468699 0.82464755 2.0175495 0.11506134 2.8697317 0.54362947 0.10824746 1.6531861 4.2324843 0.97695297 4.3966837 3.0250916 1.8032213 ... 4.5854487 2.0374475 1.1027236 2.7206469 0.864337 1.3582373 0.98220587 0.53226703 4.277097 shape 1 480 640 3 dtype float32 , kor01 Thanks for reporting this issue I tried replicating the issue on colab with TF 2.0 it throws assertion error. Since the diff is 0 assert not throws exception. Let me know if i misunderstood your issue. Please take a look at the gist https colab.sandbox.google.com gist gadagashwini 24dfd6669d3be4e3a40c45e561195a11 untitled217.ipynb . Thanks kor01 Thanks for reporting this issue I tried replicating the issue on colab with TF 2.0 it throws assertion error. Since the diff is 0 assert not throws exception. Let me know if i misunderstood your issue. Please take a look at the gist https colab.sandbox.google.com gist gadagashwini 24dfd6669d3be4e3a40c45e561195a11 untitled217.ipynb . Thanks I run the script and replicate the error on my laptop Quadro P4000 8GB and my desktop computer TITAN V Titan X Maxwell . However I could not replicate on K80 device in colab cloud. I have python 3.7 on my two computers whereas colab has python 3.6. It could be device or python version dependent. The problem is also replicated in https github.com tensorflow tensorflow issues 31166 the problem characteristics are the same with mine. A portion of computation is done correctly another portion is left with zeros. This is fixed in latest tf nightly version 2.1.0 dev20200110 which uses cuda 10.1. tested in google colab Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33560 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33560 No a 
33565,Keras Model cannot be saved as .h5 when tf.keras.backend.clip is used, System information Tensorflow 2.0.0 Python 3.7.4 Current behavior I m using the tf.keras.backend.clip function in a model and cannot save it because of a naming problem Two layers have the same name see tf op layer clib by value 16 in the ModelToDot output below . This causes a problem when exporting it to hdf5. Couldn t find a workaround or way to rename one of the two layers. Code to reproduce the issue bug https user images.githubusercontent.com 47213738 67195858 0606e700 f3fa 11e9 8ad4 80d6f425fe63.PNG Other info logs , oholimoli Tried to reproduce your issue in google colab using the lastest version of Tensorflow i.e. 2.1.0 dev20191021 but was not able to reproduce it. Please find the gist here https colab.sandbox.google.com gist gowthamkpr 5cf063d9e3509684b288f1839b1252e4 untitled203.ipynb scrollTo b9F2THOpllp3 . If you try model.save model.h5 instead you will get the same error. oholimoli This is indeed a bug in Tensorflow 2.0. In tensorflow 2.0 you can change .h5 to .tf and everything should be saved. There is also a workaround to save the model in.h5 format and its mentioned in this comment https github.com keras team keras issues 12195 issuecomment 544416524 . This is fixed with latest tf nightly 2.2.0 dev20200129 . Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33565 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33565 No a 
33572, tflite Support INT8 quantization for PACK with TFLITE BUILTINS INT8 OpsSet, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04 TensorFlow installed from source or binary binary TensorFlow version use command below 1.14 Python version 3.6 Describe the current behavior Similar to the UNPACK node issue in https github.com tensorflow tensorflow issues 31902 the new TFLiteConverter post training quantisation flow as described in https www.tensorflow.org lite performance post training quantization full integer quantization of weights and activations does not support quantization of PACK STACK operation when only integer operations are requested in the output model. When such conversion is attempted the following error is reported RuntimeError Quantization not yet supported for op PACK Code to reproduce the issue For example the script below produces errors as follows Both kTfLiteUInt8 and kTfLiteInt8 version of the PACK operator is already implemented in TFLite see pack.cc https github.com tensorflow tensorflow blob 186e794d71c17b52deb52ace151ec5add8525f2c tensorflow lite kernels pack.cc so it should be straightforward to support PACK as well in the TFLite Converter.,Any updates Hi I have a CL in progress to fix this. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33572 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33572 No a Hey I m facing the same issue when converting ssd mobilenet v1 fpn coco see model zoo https github.com tensorflow models blob master research object detection g3doc tf1 detection zoo.md to quantized tflite. Simple conversion to float32 tflite works just fine. With TF 2 I can t even convert the model to tflite proper. Using tensorflow gpu version 1.5.3 on google Colab Ubuntu 18.04. Any help would be much appreciated pray Conversion script looks like this 
33588,tflite interpreter makes huge difference in output for tf.reduce mean when it is quantized, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution Linux Ubuntu 16.04 TensorFlow installed from source TensorFlow version use command below v2.0.0 0 g64c3d38 2.0.0 v2.0.0 rc2 26 g64c3d38 2.0.0 Python version 2.7.12 Bazel version if compiling from source 0.24.1 GCC Compiler version if compiling from source gcc Ubuntu 5.4.0 6ubuntu1 16.04.11 5.4.0 20160609 CUDA cuDNN version 10.0 7.0 GPU model and memory GeForce GTX TITAN Describe the current behavior When I quantize the graph with tf.reduce mean it makes huge difference in outputs compared original tf function and tflite model not quantized . Describe the expected behavior the quantized model gives similar output values with the original tf graph. With quantization True The output of the tflite model seems too different with that of the original tf function. When quantization False the their outputs looks the same. It does not seem normal that the quantization gives too much differences in output values. Code to reproduce the issue Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,I modified lite kernels reduce.cc and obtained following result that seems correct. tflite interpreter runs following function that is not correct interpretation of tf.reduce mean . https github.com tensorflow tensorflow blob 64c3d382cadf7bbe8e7e99884bede8284ff67f56 tensorflow lite kernels reduce.cc L323 This code block should be changed as to prevent wrong operation. Secondly it should be dealt with elsewhere. It seems that the code contains suitable macro TF LITE MEAN . https github.com tensorflow tensorflow blob 64c3d382cadf7bbe8e7e99884bede8284ff67f56 tensorflow lite kernels reduce.cc L332 I don t know why the authors did not call this already but I can use this to cover the case. I added following code into the switch statement borrowing the kTfLiteUInt8 case. Then the interpreter seems work. It seems that similar correction is already implemented in the master branch. Maybe it works fine in the tf nightly version however the v2.0.0 version needs this modification to run the test code correctly. Fix is in tf nightly. It ll appear in the next tf.2.x release soon. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33588 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33588 No a 
33593,distributed training with MirroredStrategy crashes when input shapes are variable, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary pip binary TensorFlow version use command below tf2.0.0 Python version 3.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version cuda 10.0 cudnn 7.6.4 GPU model and memory rtx titan 24gb You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior Tensorflow raise error ValueError When input signature is provided all inputs to the Python function must be convertible to tensors when checking input signature of a tf.function if input shape is variable in a distributed training environment. The training works without any error when I train it with single GPU or input with fixed shape or I delete a path to cuda from my environment path. Describe the expected behavior Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,when I change inputs to be a fixed shape with strategy tf.distribute.MirroredStrategy x tf.random.uniform 800 50 maxval 50 dtype tf.int64 y tf.random.uniform 800 50 maxval 50 dtype tf.int64 dataset tf.data.Dataset.from tensor slices x y batchfier dataset.batch 4 it works perfectly In addition I tried setting experimental relax shapes to true but it didn t work. Not passing input signature argument which means just decorating codes only with tf.function did not raise error but it significantly slowed down the training speed. This issue has now been fixed and an example has been provided in 29911 for how to use the element spec property of distributed datasets iterators to specify the input signature. Please reopen this issue as needed. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33593 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33593 No a 
33623,tf.strings.split bug, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below 1.15.0 Python version 3.6.8 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior AttributeError tensorflow.python.framework.ops.EagerTensor object has no attribute to sparse Describe the expected behavior The op should return SparseTensor or RaggedTensor Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. colab https colab.research.google.com drive 1PrFuL7hC25yRGmFfbwwRK1if9N8d1M9A ,You need brackets for the input. Try this You need brackets for the input. Try this tf.strings.split a b result type RaggedTensor works and returns a Tensor.In fact it should return a RaggedTensor Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33623 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33623 No a 
33641,Output names lost when loading Keras model in SavedModel format, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux 5.0.0 25 generic x86 64 with Ubuntu 18.04 bionic TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 Python version 3.7.4 Describe the current behavior When loading a Keras model saved in the SavedModel format the output names are lost. Losing the output names causes loading to fail if using dictionaries for configuring loss or metrics. Describe the expected behavior Output names to be restored when loading the model and dictionaries for losses to be working when loading the model. Code to reproduce the issue output names is not restored and outputs are given new auto generated names Model fails to load when using dictionaries for losses ,Issue is replicating with TF 2.0.0. Please take a look at the gist https colab.sandbox.google.com gist gadagashwini 529b06c9cac349064ad90177da793da9 untitled223.ipynb . Thanks method fails with precondition error in TF 2.0 nightly version 2.1.0 dev20191024 This is fixed with latest tf version 2.1.0. rc0. Thanks Confirming thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33641 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33641 No a 
33646,ValueError Unknown metric function CustomMetric using custom metrics when loading tf saved model type with tf.keras.models.load model , em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04 TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 Python version 3.7 Describe the current behavior ValueError Unknown metric function CustomMetric occurs when trying to load a tf saved model using tf.keras.models.load model with a custom metric. If you look at the code for load model it is clear the load model currently ignores the custom objects dict for the tf saved model format. Describe the expected behavior load model loads the custom metric successfully either just implicitly or through the custom objects dict. Code to reproduce the issue Other info logs ,I have tried on colab with TF version 2.0 and was able to reproduce the issue.Please find the gist here https colab.sandbox.google.com gist ravikyram 1c9190b3588867fa05dd7103b212bccb untitled296.ipynb . Thanks AndersonHappens I think there is an issue with saving a model in .tf version when the model has custom metrics. I have saved the model in .h5 format and everything works as expected. Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 5cd85f60f1d6e975ce65675227422b3e untitled296.ipynb . Thanks Please close the issue if it was resolved for you. Thanks jvishnuvardhan While it does work in the h5 format if I have saved a model to the tf format I cannot load the model to resave it to the h5 format later since I can t load the model in the first place so ultimately this is still an issue that needs to be addressed. Here is a workaround for the meantime same issue here when you save the model in tf format you can t re load the model with custom objects this should be fixed. Here is a workaround for the meantime not working at keras 2.3.1 tf 2.0.0 jvishnuvardhan While it does work in the h5 format if I have saved a model to the tf format I cannot load the model to resave it to the h5 format later since I can t load the model in the first place so ultimately this is still an issue that needs to be addressed. AndersonHappens Can you please check with the tf nightly . I saved model in tf format then loaded model and saved in h5 format without any issues. Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 9f3c879cdb48821e6b7554e4fe700e0b untitled702.ipynb . Please let us know what you think. Thanks. JustinhoCHN can you please try tf nightly . If you still have an issue please open a new issue with a standalone code to reproduce the error. I am closing this issue as it was resolved in recent tf nightly . Please feel free to open if the issue persists again. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33646 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33646 No a jvishnuvardhan This issue should not be closed. The loading as in your gist works but once you use the model e.g. to further train it you will get an error that the custom object is unkown. Here is a new workaround not sure what changed that the old one does not work anymore j o d o Can you try adding one more line as follows and train the model loaded my new model saved in h5 . loaded my new model saved in h5.compile loss sparse categorical crossentropy optimizer tf.keras.optimizers.Adam lr .001 metrics CustomMetric Thanks The models saved in h5 format seem to work fine the issue is about models saved with SavedModel format as explained here https www.tensorflow.org guide saved model What is working is setting the compile flag to False and then compiling it on its own e.g. The point is 1 The default way of loading models fails if there are custom objects involved. 2 By compiling yourself you are setting up a new optimizer instead of loading the previously trained models optimizer weights. Moreover I already submited a PR that would fix this https github.com tensorflow tensorflow pull 34048. But it seems nobody bothers about it j o d o Can you please check using model.save after compile and the use keras.models.load model to load the model. I tried it without any issue. Here https colab.sandbox.google.com gist jvishnuvardhan 0f8c9294cfd1e8ae945f9edae36831ad untitled702.ipynb is the gist. Please run it with tf nightly . Thanks I am closing this issue as it was resolved. Please feel free to reopen if the issue didn t resolve for you. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33646 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33646 No a jvishnuvardhan tf nightly works but doesn t run on the GPU. Ps. regular tensorflow does run on GPU as expected. Also isn t nightly an unstable build Is there a stable solution to the problem Or when is the regular tensorflow expected to be fixed rodrigoruiz Can you please open a new issue with details and a simple standalone code to reproduce the issue Currently TF2.2.0rc2 is the latest release candidate. I expect there will be TF2.2 stable version will be released in the near future. Thanks jvishnuvardhan I think i figured it out tf nightly does not run on GPU tf nightly gpu does... It s just that this is not specified in the docs. I ll just wait for the stable version I guess. Just tried this on 2.2.0. While it doesn t run into error it seems to load an empty model. I m using Feature Column API. timatim Please create a new issue with a simple standalone to reproduce the issue. Thanks I have this problem loading an .h5 model on TF 2.3.0. I am using tensorflow v 2.3 in R saving and loading the model with save model tf load model tf and I get the same error because of my custom metric balanced accuracy. I can t compile it afterwards because I am running a grid search for the optimizer learning rate so it wont be practical. my issue was resolved by adding my custom metric in the custom objects load model tf path custom objects list CustomLayer CustomLayer 
33648,Can save but not load custom metrics with a variable named weights in the tf saved model format, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04 TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 Python version 3.7 Describe the current behavior AttributeError occurs when trying to load a tf saved model using tf.keras.models.load model with a custom metric with a variable named weights . Describe the expected behavior Either an error gets thrown during assignment or saving that you are not allowed to save a variable with the name weights or no attribute error occurs and load model loads the metric successfully. Code to reproduce the issue Other info logs ,Issue replicating for the given code TF 2.0 please find the gist https colab.sandbox.google.com gist oanush a6c1d66f14b79a9f45a39412ecc52c1c 33648.ipynb of colab.Thanks AndersonHappens I agree. May be it is better to add some checks before saving the model but it is not as simple as it looks. However the error clearly describe what needs to changed. Here https colab.sandbox.google.com gist jvishnuvardhan e3ac0c1f35bb4c9cf73afaedde6d0975 33648.ipynb is a gist for your reference. Thanks The issue is that the custom objects are not passed to the compile in case of save fromt tf . I belive this PR should fix this issue https github.com tensorflow tensorflow pull 34048 This issue has been fixed in the latest nightly please see the gist here https colab.sandbox.google.com gist goldiegadde 7ed551183f76afd08d3dc36bb7ad6ce0 33648.ipynb there is one small change to the code load models line should be changed to new model keras.models.load model model custom objects CustomMetric CustomMetric AndersonHappens marking this as fixed please re open if you run into any further issues. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33648 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33648 No a 
33660,tf.test.compute gradient v2 expects wrong empty gradient shape, System Information Have I written custom code YES see below OS Platform and Distribution Ubuntu 18.04.3 LTS inside tensorflow tensorflow 2.0.0 gpu Mobile device N A TensorFlow installed from binary inside tensorflow tensorflow 2.0.0 gpu TensorFlow version 2.0.0 Python version 2.7.15 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version 10.0 GPU model and memory TITANV 12GB Current Behavior When the gradient tensors are totally empty i.e. all their dimensions are zero and more than one input is considered at the same time the shape of an expected gradient tensor does not match the actual shape produced by the op s gradient function. This case is not currently tested in tensorflow python ops gradient checker v2 test.py see testEmptyMatMul . UPDATE I also tried the configuration in testEmptyMatMul see additional code below and found that this also tickles the apparent bug. Surprisingly tensorflow python gradient checker v2 test passes when running on tag v2.0.0 I don t currently understand why. I have tested with two different ops each with two inputs both fail . cases where not all of the gradient tensors are empty as in testEmptyMatMul . tf.compat.v1.test.compute gradient with more than one empty input does not exhibit this behavior so it was probably not present in tensorflow python ops gradient checker.py but was introduced in tensorflow python ops gradient checker v2.py . See code at the end in the TensorFlow Version 1 API section that demonstrates this. I have not tested using ops with more than two inputs Expected Behavior These exceptions should not be produced when using built in ops. Either the ops gradient functions are producing the wrong shaped tensors which would be a bug or more likely the code in gradient checker v2.py is expecting the wrong shape. Code to reproduce the issue The following is essentially the same as the code in testEmptyMatMul in tensorflow python ops gradient checker v2 test.py Other info logs Output from the above code Work Around compute gradient can be called multiple times once for each input to get the analytical and numerical jacobians for each input separately. As follows TensorFlow Version 1 API The following code does not throw an exception demonstrating that this problem does not exist with tf.compat.v1.test.compute gradient . ,Issue replicating for TF 2.0 kindly find the gist https colab.sandbox.google.com gist oanush 839b2999d53195a5dd8c98f22c31ab50 33660.ipynb of the colab.Thanks I just added an update to the description I also tried the configuration in testEmptyMatMul see additional code and found that this also tickles the apparent bug. This suggests that tensorflow python ops gradient checker v2 test.py is not being run as part of the CI. I also added some code to the end of the description that demonstrates that tf.compat.v1.test.compute gradient does not exhibit this behavior. Surprisingly tensorflow python gradient checker v2 test passes when running on tag v2.0.0 I don t currently understand why. duncanriach I tried with Tf 2.1 its working without any error. Please find the gist here https colab.sandbox.google.com gist gadagashwini 0c11957c6fca4f94461cd57b63040389 untitled456.ipynb and please close if issue is resolved. Thanks Thank you gadagashwini. I can confirm that this issue is resolved in TensorFlow version 2.1.0. Closing. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33660 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33660 No a 
33686,TF 2.0 tf.keras.models.load model won t load models compiled with a loss dictionary, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary docker image tensorflow tensorflow 2.0.0 gpu py3 TensorFlow version use command below v2.0.0 rc2 26 g64c3d38 2.0.0 Python version 3.6 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version 10 GPU model and memory N A Describe the current behavior Attempting to load a keras model which has been compiled with a loss dictionary fails throwing the following error ValueError Unknown entries in loss dictionary MyLoss . Only expected following keys dense 1 Describe the expected behavior The model should load. Code to reproduce the issue EDIT refer to latest gist below Other info logs N A ,I have tried on colab with TF version 2.0 2.1.0 dev20191024 and was able to reproduce the issue.Please find the gist here https colab.sandbox.google.com gist ravikyram 33341889953f2e4a695282ac2f2eb3a2 untitled298.ipynb . Thanks optiluca Did you define any custom loss function If you are trying to define custom loss please follow this link https stackoverflow.com questions 43818584 custom loss function in keras or similar kinds of resources to write the custom loss function. Thanks I did not. I think the example code is pretty clear There s nothing custom about it. optiluca I modified couple of things in the code and it works without any issues. Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 6b67e588cad1c274afe2d5ccca391ec8 untitled600.ipynb . Please close this issue if this was resolved. Thanks The changes to the code now simply mean that it doesn t have a loss dictionary so indeed it works. The bug I reported is that it s impossible to load a model which has a loss dictionary defined. I have a model with multiple outputs each with its own loss function that breaks because of this bug. Simply removing the loss dictionary isn t an option I reduced it down to the test code I posted above for ease of reproducibility and to isolate the issue. I ve played around with the gist I think I d made a mistake in my example script not assigning the name of the last layer to match the name of the key in the loss dictionary . I ve made a new repro script here https colab.research.google.com gist optiluca 83eeca1cd238ed119d6a6185bd45c635 untitled298.ipynb the issue still stands. Could it be that the real issue is that the layer names aren t persisting across a save load meaning that the dictionary key then turns out to be wrong I say this because if I name my output output 1 then it works. Hi is there any update on this issue The latest gist is here https colab.research.google.com gist optiluca 83eeca1cd238ed119d6a6185bd45c635 untitled298.ipynb optiluca Looks like this was resolved in tf nightly . I cannot reproduce the issue with tf nightly . Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 19db72f446e25f5192d2df18b09bbc8c untitled298.ipynb . Thanks Please close the issue if it was resolved for you. Thanks Looks ok now thanks When can I expect this to be released officially Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33686 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33686 No a optiluca TF Team is working hard on releasing the TF2.1. As of now I don t know exact release date. Thanks 
33709,K.batch dot work not right when dims 3,In TF 2.0 the K.batch dot output dim is wrong import tensorflow.keras.backend as K import tensorflow as tf a K.ones 7 4 2 b K.ones 7 2 5 c K.batch dot a b print c.shape a1 K.ones 8 7 4 2 b1 K.ones 8 7 2 5 c1 K.batch dot a1 b1 print K.batch dot a1 b1 c1.shape c2 tf.matmul a1 b1 print tf.matmul a1 b1 c2.shape output 7 4 5 K.batch dot a1 b1 8 7 4 7 5 tf.matmul a1 b1 8 7 4 5 ,Was able to reploduce the issue. Heres my github gist https colab.sandbox.google.com gist gowthamkpr 8b961afedcdb49de048431b503b7ae48 untitled211.ipynb . This is fixed with latest tf nightly version 2.2.0 dev20200218 . Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33709 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33709 No a 
33724,Infinite loop with generators wrapping a dataset in tf.function, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below v2.0.0 rc2 26 g64c3d38 2.0.0 Python version 3.7.4 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 10.0 7.6.3 GPU model and memory TITAN Xp 12196MiB You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior My use case was to use tqdm to track progress on a training loop over a tf.data.Dataset However when the function train one epoch is wrapped in a tf.function the AutoGraph is stuck in an infinite loop. Describe the expected behavior The expected behavior would be that using tf.function results in the same behavior than the eager mode. The current issue is that AutoGraph doesn t recognize tqdm dataset as a tf.data.Dataset which is normal . However iterating infinitely over the dataset in AutoGraph is weird and shouldn t happen. Maybe it should give an exception. Maybe the easiest fix would be to prevent dataset. iter being called inside tf.function if it is not in a loop. So for x in dataset would be fine but for x in tqdm dataset wouldn t. Code to reproduce the issue The minimal Iterable class can be replaced by tqdm from tqdm import tqdm and this should yield the same results. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. Without the tf.function the wrapped dataset Iterable dataset is iterated over in eager mode With the tf.function the AutoGraph mode doesn t recognize the wrapped Iterable dataset as a tf.data.Dataset so it tries to iterate over it in python to trace the graph. However it looks like the Iterable dataset . iter infinitely yields a next element named IteratorGetNext . This results in an infinite iteration over the dataset ,Indeed AutoGraph does not recognize Iterator or the generators executed by tqdm . I ll explain in more detail below but in general the objects are not recognized because they are not subclasses of a dataset or a dataset iterator. AutoGraph only transforms objects of types it recognizes everything else is executed in Python at function tracing time. For this reason when running the code above it s as if you ran it without AutoGraph try executing tf.function autograph False it should behave identically . To better understand this have a look at the dispatch logic for for loops https github.com tensorflow tensorflow blob master tensorflow python autograph operators control flow.py L331 . Since Iterator is not an instance of any of the classes listed there AutoGraph reverts to running the for loop in Python. In Python it will cycle infinitely because the Tensor objects that the hidden Dataset iterator returns evaluate to True. next iter Dataset never returns StopIteration in graph mode it can t because it doesn t know how much data there is at graph construction. You might wonder why AutoGraph doesn t detect that the for loop inside Iterator which does use a Dataset. The answer is because AutoGraph doesn t touch generators however the warning it generates is silenced by default https github.com tensorflow tensorflow blob master tensorflow python autograph core unsupported features checker.py L40 . I think we ll need to make that warning more obvious. Now AutoGraph could detect that a for loop is trying to iterate over a generator and disallow it in graph mode. However that could break legitimate uses of generators e.g. such as those used by Keras but again it looks like a warning would be useful. Separately from all that tqdm itself contains console output logic and other operations that are highly dependent on the Python runtime and which are not very compatible with TF graphs. So it would be tricky to run in graph mode regardless though it could be possible to create a more graph friendly version . When running training loops inside tf.function you can currently only use summaries and tf.print and tensorboard to monitor the training progress. I think adding a graph compatible tqdm one that uses TF Iterators and tf.print would be a useful API although we probably won t have the chance to build one soon it would be a welcome contribution though . There is now a verification in autograph that outputs a warning in situations such as this. We could add a verification that looks specifically for tqdm but that won t work work the custom generator in the OP so it would be of limited help. Here is an alternative that might be useful for adding a progress bar to a dataset it modifies a dataset to print tqdm messages whenever it is being iterated using tf.py function Another similar alternative is to use Dataset.from generator and supply it with a custom generator that itself includes tqdm. This will work whenever you are constructing the dataset from a Python object it will not work when using built in datasets like TFRecordDataset though Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33724 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33724 No a 
33767,AutoGraph Error transforming entity AssertionError Bad argument number for Name 3 expecting 4, System information OS Platform and Distribution Arch Linux 5.3.7 arch1 1 ARCH TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 Kerasversion use command below 2.2.4 tf Python version 3.7.4 CUDA cuDNN version CUDA 10.1.243 cuDNN 7.6.2.24 GPU model and memory 2x GTX 1080 Ti 11GB various errors warnings as Entity function Function. initialize uninitialized variables. locals .initialize variables at 0x7f0f2846c320 could not be transformed and will be executed as is 2019 10 27 22 54 04 017 INFO Error transforming entity function Function. initialize uninitialized variables. locals .initialize variables at 0x7f0f2846c320 Traceback most recent call last File usr lib python3.7 site packages tensorflow core python autograph impl api.py line 506 in converted call converted f conversion.convert target entity program ctx File usr lib python3.7 site packages tensorflow core python autograph impl conversion.py line 322 in convert free nonglobal var names File usr lib python3.7 site packages tensorflow core python autograph impl conversion.py line 240 in convert with cache entity program ctx File usr lib python3.7 site packages tensorflow core python autograph impl conversion.py line 469 in convert entity to ast nodes name entity info convert func to ast o program ctx File usr lib python3.7 site packages tensorflow core python autograph impl conversion.py line 669 in convert func to ast node node to graph node context File usr lib python3.7 site packages tensorflow core python autograph impl conversion.py line 698 in node to graph node converter.standard analysis node context is initial True File usr lib python3.7 site packages tensorflow core python autograph core converter.py line 383 in standard analysis node qual names.resolve node File usr lib python3.7 site packages tensorflow core python autograph pyct qual names.py line 254 in resolve return QnResolver .visit node File usr lib python3.7 ast.py line 262 in visit return visitor node File usr lib python3.7 ast.py line 317 in generic visit value self.visit value File usr lib python3.7 ast.py line 262 in visit return visitor node File usr lib python3.7 ast.py line 317 in generic visit value self.visit value File usr lib python3.7 ast.py line 262 in visit return visitor node File usr lib python3.7 ast.py line 326 in generic visit new node self.visit old value File usr lib python3.7 ast.py line 262 in visit return visitor node File usr lib python3.7 ast.py line 317 in generic visit value self.visit value File usr lib python3.7 ast.py line 262 in visit return visitor node File usr lib python3.7 site packages tensorflow core python autograph pyct qual names.py line 236 in visit Subscript if isinstance s.value gast.Num AttributeError module gast has no attribute Num WARNING tensorflow Entity function Function. initialize uninitialized variables. locals .initialize variables at 0x7f0f2846c320 could not be transformed and will be executed as is. Please report this to the AutoGraph team. When filing the bug set the verbosity to 10 on Linux export AUTOGRAPH VERBOSITY 10 and attach the full output. Cause module gast has no attribute Num 2019 10 27 22 54 04 017 WARNING Entity function Function. initialize uninitialized variables. locals .initialize variables at 0x7f0f2846c320 could not be transformed and will be executed as is. Please report this to the AutoGraph team. When filing the bug set the verbosity to 10 on Linux export AUTOGRAPH VERBOSITY 10 and attach the full output. Cause module gast has no attribute Num 2019 10 27 22 54 06.670890 W tensorflow core grappler optimizers implementation selector.cc 310 Skipping optimization due to error while loading function libraries Invalid argument Functions inference backward standard gru 10450 11011 and inference backward cudnn gru with fallback 9300 9441 specialized for StatefulPartitionedCall 1 at inference distributed function 12398 both implement gru ee50c0e8 e326 45b7 b98e 88e06e2f6f01 but their signatures do not match. 2019 10 27 22 54 01 401 INFO Error transforming entity bound method Output.call of model.Output object at 0x7f0f3c96aa90 Traceback most recent call last File usr lib python3.7 site packages tensorflow core python autograph impl api.py line 506 in converted call converted f conversion.convert target entity program ctx File usr lib python3.7 site packages tensorflow core python autograph impl conversion.py line 322 in convert free nonglobal var names File usr lib python3.7 site packages tensorflow core python autograph impl conversion.py line 240 in convert with cache entity program ctx File usr lib python3.7 site packages tensorflow core python autograph impl conversion.py line 471 in convert entity to ast nodes name entity info convert func to ast o program ctx File usr lib python3.7 site packages tensorflow core python autograph impl conversion.py line 669 in convert func to ast node node to graph node context File usr lib python3.7 site packages tensorflow core python autograph impl conversion.py line 699 in node to graph node converter.apply node context function scopes File usr lib python3.7 site packages tensorflow core python autograph core converter.py line 409 in apply node converter module.transform node context File usr lib python3.7 site packages tensorflow core python autograph converters function scopes.py line 120 in transform return FunctionBodyTransformer ctx .visit node File usr lib python3.7 site packages tensorflow core python autograph core converter.py line 346 in visit return super Base self .visit node File usr lib python3.7 site packages tensorflow core python autograph pyct transformer.py line 480 in visit result super Base self .visit node File usr lib python3.7 ast.py line 262 in visit return visitor node File usr lib python3.7 site packages tensorflow core python autograph converters function scopes.py line 87 in visit FunctionDef node self.generic visit node File usr lib python3.7 ast.py line 317 in generic visit value self.visit value File usr lib python3.7 site packages tensorflow core python autograph core converter.py line 346 in visit return super Base self .visit node File usr lib python3.7 site packages tensorflow core python autograph pyct transformer.py line 480 in visit result super Base self .visit node File usr lib python3.7 ast.py line 262 in visit return visitor node File usr lib python3.7 site packages tensorflow core python autograph converters function scopes.py line 44 in visit Return value node.value File usr lib python3.7 site packages tensorflow core python autograph pyct templates.py line 261 in replace replacements k convert to ast replacements k File usr lib python3.7 site packages tensorflow core python autograph pyct templates.py line 223 in convert to ast return gast.Name id n ctx None annotation None File usr lib python3.7 site packages gast gast.py line 19 in create node format Name nbparam len Fields AssertionError Bad argument number for Name 3 expecting 4 WARNING tensorflow Entity bound method Output.call of model.Output object at 0x7f0f3c96aa90 could not be transformed and will be executed as is. Please report this to the AutoGraph team. When filing the bug set the verbosity to 10 on Linux export AUTOGRAPH VERBOSITY 10 and attach the full output. Cause Bad argument number for Name 3 expecting 4 2019 10 27 22 54 01 401 WARNING Entity bound method Output.call of model.Output object at 0x7f0f3c96aa90 could not be transformed and will be executed as is. Please report this to the AutoGraph team. When filing the bug set the verbosity to 10 on Linux export AUTOGRAPH VERBOSITY 10 and attach the full output. Cause Bad argument number for Name 3 expecting 4 WARNING tensorflow From usr lib python3.7 site packages tensorflow core python keras backend.py 5783 sparse to dense from tensorflow.python.ops.sparse ops is deprecated and will be removed in a future version, olk Can you share a simple and standalone code to reproduce the issue reported Thanks oanush it s not so easy to produce standalone code I could give you the link to the git repo contains Makefiles to generate the data and to train the model alternatively I could post the model code. It is the same code that causes issue 33809 MirroredStrategy compared to OneDeviceStrategy slower and much weaker learning maybe both issues are related. Model training stripped Hi this issue should go away if you downgrade gast to 0.2.2 pip install gast 0.2.2 . The dependencies in TF 2.0 pull the latest version of gast which breaks compatibility at 0.3. This should be fixed in 2.1 which pins to this older version. Please reopen the issue if downgrading didn t work. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33767 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33767 No a Thanks a lot Hi this issue should go away if you downgrade gast to 0.2.2 pip install gast 0.2.2 . The dependencies in TF 2.0 pull the latest version of gast which breaks compatibility at 0.3. This should be fixed in 2.1 which pins to this older version. Please reopen the issue if downgrading didn t work. 
33776,LSTMCell name is ignored in trainable variables when wrapped in keras.layers.RNN,I created a model with several LSTMCell cells wrapped in keras.layers.RNN. When I print trainable variables cell names are ignored which results into duplicate variable names several same recurrent kernel etc which confuses Tensorboard for example. Collab notebook to reproduce https colab.research.google.com drive 11W6ntLFGj4gqh5sS09CeVgb0LMNb Fiu Environment tensorflow 2 release,Issue replicating for the given code TF 2.0. I would like to work on this one. qlzh727 Thanks for reporting this issue. I think this is a historical issue since the we directly invoke cell.call method rather than cell. call . This cause the name scope in the call to be ignored which result into the variable name doesn t have the prefix. The fix might need some special care since it affects the existing saved model since the variable name changed as well as the ops under the cell they will now have a prefix of the cell s name . For anyone who want to try a fix I would suggest to make sure the save model and checkpoint is explicitly verified. qlzh727 thank you Maybe you have an idea of a simple workaround not sure if calling call explicitly for each cell would help until the proper fix is done qlzh727 How do you recommend to manually set the LSTM weights before the first call of .call The Layer.set weights method will say that the layer was expecting 0 weights but was provided with a list of N weights. The weights of the layer is initialized in build so you have to build the layer first with proper input shape so that it can init the weights with proper shape. then set weights should work fine. I tried to build each cell but the names are still duplicated Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33776 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33776 No a 
33799,TF 2.0.0 Python 3.8 TypeError logger find caller takes from 0 to 1 positional arguments but 2 were given, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow See script from Tensorflow training session and uploaded file below. Nb There is no error with TF2.0.0 and python 3.6 or 3.7. The error occurs with TF2.0.0 and python 3.8. OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 TensorFlow installed from source or binary source TensorFlow version use command below 2.0.0 Python version 3.8 Bazel version if compiling from source 0.26.1 GCC Compiler version if compiling from source 7.4.0 CUDA cuDNN version CUDA 10 cuDNN 7.6.4 GPU model and memory NVidia RTX 2080 TI and 2080 MaxQ Describe the current behavior After running the code below with the attached file you get the following error AssertionError Traceback most recent call last tf38 lib python3.8 site packages tensorflow core python autograph impl api.py in converted call f args kwargs caller fn scope options 525 options options autograph module tf inspect.getmodule converted call 526 converted f conversion.convert target entity program ctx 527 tf38 lib python3.8 site packages tensorflow core python autograph impl conversion.py in convert entity program ctx 324 325 converted entity info convert with cache entity program ctx 326 free nonglobal var names tf38 lib python3.8 site packages tensorflow core python autograph impl conversion.py in convert with cache entity program ctx free nonglobal var names 238 239 nodes converted name entity info convert entity to ast 240 entity program ctx tf38 lib python3.8 site packages tensorflow core python autograph impl conversion.py in convert entity to ast o program ctx 474 elif tf inspect.ismethod o 475 nodes name entity info convert func to ast o program ctx 476 elif hasattr o class tf38 lib python3.8 site packages tensorflow core python autograph impl conversion.py in convert func to ast f program ctx do rename 672 context converter.EntityContext namer entity info program ctx new name 673 node node to graph node context 674 tf38 lib python3.8 site packages tensorflow core python autograph impl conversion.py in node to graph node context 702 node converter.standard analysis node context is initial True 703 node converter.apply node context function scopes 704 node converter.apply node context arg defaults tf38 lib python3.8 site packages tensorflow core python autograph core converter.py in apply node context converter module 408 node standard analysis node context 409 node converter module.transform node context 410 return node tf38 lib python3.8 site packages tensorflow core python autograph converters function scopes.py in transform node ctx 119 def transform node ctx 120 return FunctionBodyTransformer ctx .visit node tf38 lib python3.8 site packages tensorflow core python autograph core converter.py in visit self node 345 try 346 return super Base self .visit node 347 finally tf38 lib python3.8 site packages tensorflow core python autograph pyct transformer.py in visit self node 479 if not anno.hasanno node anno.Basic.SKIP PROCESSING 480 result super Base self .visit node 481 self.ctx.current origin parent origin usr local lib python3.8 ast.py in visit self node 359 visitor getattr self method self.generic visit 360 return visitor node 361 tf38 lib python3.8 site packages tensorflow core python autograph converters function scopes.py in visit FunctionDef self node 101 102 wrapped body templates.replace 103 template tf38 lib python3.8 site packages tensorflow core python autograph pyct templates.py in replace template replacements 268 for node in nodes 269 node ReplaceTransformer replacements .visit node 270 if isinstance node list tuple usr local lib python3.8 ast.py in visit self node 359 visitor getattr self method self.generic visit 360 return visitor node 361 usr local lib python3.8 ast.py in generic visit self node 435 if isinstance value AST 436 value self.visit value 437 if value is None usr local lib python3.8 ast.py in visit self node 359 visitor getattr self method self.generic visit 360 return visitor node 361 usr local lib python3.8 ast.py in generic visit self node 444 elif isinstance old value AST 445 new node self.visit old value 446 if new node is None usr local lib python3.8 ast.py in visit self node 359 visitor getattr self method self.generic visit 360 return visitor node 361 usr local lib python3.8 ast.py in generic visit self node 435 if isinstance value AST 436 value self.visit value 437 if value is None usr local lib python3.8 ast.py in visit self node 359 visitor getattr self method self.generic visit 360 return visitor node 361 tf38 lib python3.8 site packages tensorflow core python autograph pyct templates.py in visit Name self node 199 200 new nodes self. prepare replacement node node.id 201 tf38 lib python3.8 site packages tensorflow core python autograph pyct templates.py in prepare replacement self replaced key 138 139 new nodes ast util.copy clean repl preserve annos self.preserved annos 140 if isinstance new nodes gast.AST tf38 lib python3.8 site packages tensorflow core python autograph pyct ast util.py in copy clean node preserve annos 75 76 return CleanCopier preserve annos .copy node 77 tf38 lib python3.8 site packages tensorflow core python autograph pyct ast util.py in copy self node 53 if not f.startswith and hasattr node f 54 new fields f self.copy getattr node f 55 new node type node new fields tf38 lib python3.8 site packages tensorflow core python autograph pyct ast util.py in copy self node 40 if isinstance node list 41 return self.copy n for n in node 42 elif isinstance node tuple tf38 lib python3.8 site packages tensorflow core python autograph pyct ast util.py in listcomp .0 40 if isinstance node list 41 return self.copy n for n in node 42 elif isinstance node tuple tf38 lib python3.8 site packages tensorflow core python autograph pyct ast util.py in copy self node 54 new fields f self.copy getattr node f 55 new node type node new fields 56 tf38 lib python3.8 site packages gast gast.py in create node self args kwargs 9 nbparam len args len kwargs 10 assert nbparam in 0 len Fields 11 Bad argument number for expecting . AssertionError Bad argument number for keyword 1 expecting 2 During handling of the above exception another exception occurred TypeError Traceback most recent call last ipython input 10 8b26b7af23a7 in module 1 tf model.fit Xs train 0 1 y train.reshape 1 1 tf38 lib python3.8 site packages tensorflow core python eager def function.py in call self args kwds 566 xla context.Exit 567 else 568 result self. call args kwds 569 570 if tracing count self. get tracing count tf38 lib python3.8 site packages tensorflow core python eager def function.py in call self args kwds 613 This is the first call of call so we have to initialize. 614 initializers 615 self. initialize args kwds add initializers to initializers 616 finally 617 At this point we know that the initialization is complete or less tf38 lib python3.8 site packages tensorflow core python eager def function.py in initialize self args kwds add initializers to 494 self. graph deleter FunctionDeleter self. lifted initializer graph 495 self. concrete stateful fn 496 self. stateful fn. get concrete function internal garbage collected pylint disable protected access 497 args kwds 498 tf38 lib python3.8 site packages tensorflow core python eager function.py in get concrete function internal garbage collected self args kwargs 2363 args kwargs None None 2364 with self. lock 2365 graph function self. maybe define function args kwargs 2366 return graph function 2367 tf38 lib python3.8 site packages tensorflow core python eager function.py in maybe define function self args kwargs 2671 2672 self. function cache.missed.add call context key 2673 graph function self. create graph function args kwargs 2674 self. function cache.primary cache key graph function 2675 return graph function args kwargs tf38 lib python3.8 site packages tensorflow core python eager function.py in create graph function self args kwargs override flat arg shapes 2551 arg names base arg names missing arg names 2552 graph function ConcreteFunction 2553 func graph module.func graph from py func 2554 self. name 2555 self. python function tf38 lib python3.8 site packages tensorflow core python framework func graph.py in func graph from py func name python func args kwargs signature func graph autograph autograph options add control dependencies arg names op return value collections capture by value override flat arg shapes 956 converted func 957 958 func outputs python func func args func kwargs 959 960 invariant func outputs contains only Tensors CompositeTensors tf38 lib python3.8 site packages tensorflow core python eager def function.py in wrapped fn args kwds 437 wrapped allows AutoGraph to swap in a converted function. We give 438 the function a weak reference to itself to avoid a reference cycle. 439 return weak wrapped fn . wrapped args kwds 440 weak wrapped fn weakref.ref wrapped fn 441 tf38 lib python3.8 site packages tensorflow core python eager function.py in bound method wrapper args kwargs 3179 However the replacer is still responsible for attaching self properly. 3180 TODO mdan Is it possible to do it here instead 3181 return wrapped fn args kwargs 3182 weak bound method wrapper weakref.ref bound method wrapper 3183 tf38 lib python3.8 site packages tensorflow core python framework func graph.py in wrapper args kwargs 935 TODO mdan Push this block higher in tf.function s call stack. 936 try 937 return autograph.converted call 938 original func 939 args tf38 lib python3.8 site packages tensorflow core python autograph impl api.py in converted call f args kwargs caller fn scope options 552 Cause s target entity e 553 else 554 logging.warn 555 AutoGraph could not transform s and will run it as is. n 556 Please report this to the TensorFlow team. When filing the bug set tf38 lib python3.8 site packages tensorflow core python autograph utils ag logging.py in warn msg args kwargs 144 145 def warn msg args kwargs 146 logging.warn msg args kwargs 147 if echo log to stdout 148 output to stdout WARNING msg args kwargs tf38 lib python3.8 site packages tensorflow core python platform tf logging.py in warn msg args kwargs 159 tf export v1 logging.warn 160 def warn msg args kwargs 161 get logger .warning msg args kwargs 162 163 usr local lib python3.8 logging init .py in warning self msg args kwargs 1444 1445 if self.isEnabledFor WARNING 1446 self. log WARNING msg args kwargs 1447 1448 def warn self msg args kwargs usr local lib python3.8 logging init .py in log self level msg args exc info extra stack info stacklevel 1563 IronPython can use logging. 1564 try 1565 fn lno func sinfo self.findCaller stack info stacklevel 1566 except ValueError pragma no cover 1567 fn lno func unknown file 0 unknown function TypeError logger find caller takes from 0 to 1 positional arguments but 2 were given Describe the expected behavior There should be no error. It works fine with TF2.0.0 and Python 3.6 or Python 3.7. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. import tensorflow as tf import numpy as np import gzip import json from sklearn.model selection import ShuffleSplit with gzip.open small data cal house.json.gz r as fin housing json.load fin for train test in ShuffleSplit 1 0.2 random state 42 .split housing data X train np.array housing data train .astype np.float32 y train np.array housing target train .astype np.float32 X test np.array housing data test .astype np.float32 y test np.array housing target test .astype np.float32 X mean X train.mean axis 0 X std X train.std axis 0 Xs train X train X mean X std Xs test X test X mean X std class LinearRegressionTF def init self eta .1 self.W tf.Variable 0. self.b tf.Variable 0. self.opt tf.keras.optimizers.SGD learning rate eta def loss self X y return func False def loss return tf.reduce mean tf.square X self.W self.b y if not return func return loss return loss tf.function def fit self X y steps 1 for in range steps self.opt.minimize self.loss X y return func True self.W self.b tf model LinearRegressionTF tf model.fit Xs train 0 1 y train.reshape 1 1 cal house.json.gz https github.com tensorflow tensorflow files 3780890 cal house.json.gz Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. Nil, dbonner I tried to reproduce the issue. However i am seeing the different error. AttributeError LinearRegressionTF object has no attribute fit .Please help me with the reproducible code . It helps in localizing the issue faster. ravikyram I m sorry the code is not properly indented in a number of places when it appears in github. I can t seem to edit it to get it to show properly. Please find an attached file moved to next post with the properly indented code that reproduces this error on my system when running in one cell in Jupyter Notebook. Apologies .... See the next post for the correct file. ravikyram I ve finally got this right. Sorry to mess you around with this. Github markdown removed the underscores on the init part of the LinearRegressionTF class when I pasted it in. This got transferred through to the code file. The correct code is attached. It runs fine in Python 3.7 but errors in Python 3.8. I have also removed the reference to the subdirectory small data so you can run the code with the file cal house.json.gz in the current working directory. code py38 tf2 error.txt https github.com tensorflow tensorflow files 3786825 code py38 tf2 error.txt cal house.json.gz https github.com tensorflow tensorflow files 3786826 cal house.json.gz Hi ymodak Have you had a chance to test the python 3.8 error I reported Issue 33799 . All the best Dan I am able to reproduce the issue with the following command on python 3.8 master build Haven t figure out the reason yet. dbonner ymodak Added a PR 33953 for the fix. Please take a look. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33799 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33799 No a I recieved this error also when using tensorflow 1.13.2 python 3.8 I am considering opening a separate issue. 
33811,Got Data adapters should be mutually exclusive for handling inputs. Found multiple adapters to handle error when calling model.fit with ImageDataGenerator, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 aarch64 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary source TensorFlow version use command below 2.0.0 Python version 3.6.8 Bazel version if compiling from source 0.29.0 GCC Compiler version if compiling from source 7.4 CUDA cuDNN version N A GPU model and memory N A Describe the current behavior When fitting a model with ImageDataGenerator it raises this error Data adapters should be mutually exclusive for handling inputs. Found multiple adapters GeneratorDataAdapter KerasSequenceAdapter to handle . Describe the expected behavior 1. Log warning message if multiple data adapters found instead of raising an error 2. Use the first available data adapter Code to reproduce the issue Please refer to link below https colab.research.google.com github tensorflow examples blob master courses udacity intro to tensorflow for deep learning l05c02 dogs vs cats with augmentation.ipynb I connected to my local jupyter instance with Colab UI. To avoid this issue I ll have to manually exclude KerasSequenceAdapter before calling model.fit Other info logs N A. ,Humm seems that the Image Iterator class implements the interface for both generator and keras sequence object. Let me take a closer look to fix the issue. Hey I think we did some recent update for data adapter which might fix this issue. Could u have a try with latest nightly https github.com tensorflow tensorflow commit ac20030c96d37e980333b604402ef6dba48ef5e2 Sure I will thanks btw qlzh727 The fix works Thank you Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33811 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33811 No a qlzh727 I m having this same problem under TF 2.2 TF2.3 and tf nightly as of yesterday . The problem seems to be identical in description to that of gekowa. Any advice Here is my system info... System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below 2.2.0 also tried 2.3 and tf nightly Python version 3.6.9 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 10.1 7.6.5 GPU model and memory RTX 2080 Super 8 GB 
33825,Using metric SparseTopKCategoricalAccuracy on an RNN results in rank mismatch, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes. The code is given below to reproduce the issue. OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04 TensorFlow installed from source or binary Binary using pip TensorFlow version v2.0.0 rc2 26 g64c3d38 2.0.0 Python version 3.6 CUDA cuDNN version 10.1 7.6.2 GPU model and memory GeForce GTX 1070 8GB Describe the current behavior When I compile an RNN model with the metric SparseTopKCategoricalAccuracy the following error results. No error occurs if I use SparseCategoricalAccuracy instead. Describe the expected behavior I expect no error. I suppose SparseTopKCategoricalAccuracy can be used in exactly the same way as SparseCategoricalAccuracy . Code to reproduce the issue Other info logs I use this custom metric to get around the problem ,I have tried on colab with TF version 2.0 2.1.0 dev20191029 and was able to reproduce the issue.Please find the gist here https colab.sandbox.google.com gist ravikyram 33f418bcd6dfd460f37114fa31dd99ad untitled315.ipynb . Thanks I have submitted a fix for this now please try it out and let us know if things look good. Thank you nickoala Looks like the fix by pavithrasv resolved the issue. I ran your code with tf nightly and I cannot reproduce the issue. Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 7a0d7006ed71f30a7f65a97e754d5a36 untitled315.ipynb . Thanks I am closing this issue as it was resolved. Please feel free to reopen if the issue persists again. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33825 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33825 No a jvishnuvardhan and pavithrasv thank you very much. I am satisfied with the fix. Thanks for the work. 
33870, tf nightly unable to load saved functional model, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 macos mohave Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary pip install tf nightly TensorFlow version use command below tf nightly 2.1.0.dev20191029 Python version 3.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Describe the current behavior Loading functional model should pass in latest version of tf2. Everything works great in tensorflow 2.0.0. Describe the expected behavior Loading saved functional model raises exception. Code to reproduce the issue Other info logs , loads a model saved via See https www.tensorflow.org api docs python tf keras models load model In your code snippet you may try since you are saving your model with Following snippet executes successfully Thank you ymodak yes your example is working. However tf.keras.models.load model and tf.keras.models.save model should support according to documentation both formats SavedModel TF and H5 file Keras . This is passing However this fails as it is working with saved model format I m using a ModelCheckpoint callback to save intermediate steps as saved model.pb . With tf 2.0 it works but with the nightly 2.1.0 dev20191104 I also get the same loading error. If I replace keras.load model with tf.saved model.load I get This is also related to tensorflow 2.1.0rc. Thanks for reporting this Cospel. Confirming that this is a bug I am in the middle of redoing SavedModel loading behavior which will address this issue. Thank you k w w for awesome work This is passing in the tf nightly right now. This is strange it worked for several days in tf nightly however in tf2.1.0 and tf nightly from today it is not working again. This is fixed with latest tf nightly version 2.2.0 dev20200121 . Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33870 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33870 No a I save the model using the below on Google Colab cp callback tf.keras.callbacks.ModelCheckpoint filepath checkpoint path verbose 1 save weights only False period 5 training model.save checkpoint path.format epoch 0 Then i load the model and put it inside a class and get the summary and it works model tf.keras.models.load model 15867364372951858 cp 0005.ckpt class dic model def init self model dictionary self.model model self.dic dictionary mymodel dic model model features list mymodel.model.summary but loading the model using the same tf.keras.models.load model and same model gets the below error TypeError The added layer must be an instance of class Layer. Found tensorflow.python.saved model.load.Loader. recreate base user object. locals . UserObject object at 0x0000026E77B2C788 QuantumNinja92 When loading checkpoints you should call model.load weights instead of tf.keras.models.load model . QuantumNinja92 When loading checkpoints you should call model.load weights instead of tf.keras.models.load model . It only worked after using the Nightly build 
33884,GRU layer fails on single GPU when using MirroredStrategy, System information Have I written custom code yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 windows and linux TensorFlow installed from binary TensorFlow version 2.0 Python version 3.7 Describe the current behavior Using single GPU and tf.keras.layers.GRU layer and model.fit with MirroredStrategy fails on error Invalid argument var and delta do not have the same shape tf.keras.layers.LSTM works fine on one GPU. On machine with more GPU the GRU layers works fine. I know it does not make much sense to run Mirrored Strategy on only one GPU but it is good for testing your code before uploading it to multi gpu machine. It works in TF 1.5. Code to reproduce the issue On Google Colab use GPU runtime and run this code Other info logs ,I could reproduce the issue with pip install tf nightly gpu . Here https colab.sandbox.google.com gist jvishnuvardhan 4331c4a13bf6983eb0359d81cec85f19 untitled617.ipynb is the gist. Thanks Thanks for reporting the issue. We also found a similar issue internally it is caused by the graph rewrite between cpu kernel and gpu kernel. For now if you want to walk around the issue with some performance loss you can add gru.could use cudnn False to disable the cudnn path which will allow the model to run but just not use the fused cudnn GRU kernel. We will fix the graph rewrite in future release. reedwm for visibility. Thanks for the workaround. Question Considering the fact that it does work on multiple gpu does it mean that the cuDNN path is not used when more than one gpu is used In the colab TF 2.0 is still being used instead of the nightly despite installing tf nightly gpu. I am not familiar with colab so I am not sure why this is happening. The colab prints the version is 2.0.0 but the nightly version will look similar to to 2.1.0 dev20191023. I confirmed the issue does not occur in the TF nightlies by running outside colab. The fix will be in TF 2.1 so you might want to just wait until that is out. Closing this issue since this is fixed in the nightlies. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33884 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33884 No a Considering the fact that it does work on multiple gpu does it mean that the cuDNN path is not used when more than one gpu is used I am not sure but since this will be fixed in 2.1 I do not think it s worth investigating. 
33919,parallel for No converter defined for SpaceToBatchND,Computation of jacobian with does not work for with because there is no converter implemented for tf version 2.1.0 dev20191030 , sipposip I tried reproducing the issue using colab however i am seeing the below error. UnrecognizedFlagError Unknown command line flag f .Please find the gist here https colab.sandbox.google.com gist ravikyram 66529b5afdb793dc273f35c1a47d4ae4 untitled325.ipynb . Is this the expected behavior .Thanks The expected behaviour is to get the same result as when not using pfor which gives a tensor of shape filled with random numbers since the network in this minimal example has random weights I have tried on colab with TF version 2.1.0 dev20191103 and was able to reproduce the issue.Please find the gist here https colab.sandbox.google.com gist ravikyram 1cf3040cd25084f0414ff1385820fe58 untitled325.ipynb . Thanks There is now a converter for SpaceToBatchND. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33919 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33919 No a 
33939,Laconic error when validation steps ommitted for model.fit in TF 2.0.0, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary Colab TensorFlow version use command below v2.0.0 rc2 26 g64c3d38 2.0.0 Python version 3.6.8 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory TPU selected Colab Describe the current behavior When running the script below on TF 1.15 it runs well throughout. Running in TF 2.0 it crashes on the validation stage of the first epoch. if validation steps in model.fit is included it does run throughout. however if it s omitted which is natural to assume as the dataset itself is not repeated a strange error message TypeError function object is not subscriptable is given. Describe the expected behavior Either TF 2 should behave similarly to TF 1 ie use up the entire validation set. Or report a more informative error regarding validation steps requirement similar to the error if steps per epoch are omitted for the training data . Code to reproduce the issue this produces this output after going through the training fine it crashes on the validation set Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,We should minimally be able to improve the error message here. yhliang2018 will take a look but if you are interested in contributing a fix please let us know. MikeOfZen This issue should have been fixed. Can you verify it with tf nightly If the error is gone feel free to close the issue. MikeOfZen As mentioned by yhliang2018 this was resolved TF2.2.0 rc2 . Please check the gist here https colab.research.google.com gist jvishnuvardhan bbdb01789e1f322d5ddf16f329109309 untitled49.ipynb . Thanks Can you please verify and close the issue Thanks Closing this issue as this is fixed in the latest upcoming release as well as fix is available in tf nightly. MikeOfZen Please reopen if you are still running into issues. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33939 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33939 No a 
33947,NotImplementedError Layers with arguments in init must override get config ,Related to 32662 System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Google Compute Engine GPU running Intel R Xeon R CPU 2.30GHz Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 Python version 3.6 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Describe the current behavior Model is unable to be saved using model.save Describe the expected behavior Model is saved successfully. Code to reproduce the issue Code to reproduce this issue can be found on the related Stackoverflow page https stackoverflow.com q 58678836 12315223 Another good reproduction of this since I m using Attention layers is found here https github.com tensorflow tensorflow issues 33380 issuecomment 542931948 Other info logs log.txt https github.com tensorflow tensorflow files 3801439 log.txt ,This issue has been resolved thanks to a hacky solution . Solution https stackoverflow.com a 58680354 12315223 I ll leave this issue open in case a better solution is provided on Stackoverflow or here. Feel free to close it here if no solution better than mine is available. Closing this issue since its resolved. Feel free to reopen if necessary. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33947 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33947 No a 
33985,Unable to get layer by name on custom layer and lambda layer, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 macos mohave Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary pip install tensorflow 2.0.0 TensorFlow version use command below pip install tensorflow 2.0.0 Python version python3.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Describe the current behavior get layer raises exception Describe the expected behavior Should pass and not raise exception. ValueError No such layer embeddings Code to reproduce the issue import tensorflow as tf Other info logs ,I have tried on colab with TF version 2.0 2.1.0 dev20191103 and was able to reproduce the issue.Please find the gist here https colab.sandbox.google.com gist ravikyram 604379b8a672a26e23f2cf0f05343ef5 untitled329.ipynb . Thanks Ok I think the problem is that y L2NormalizeLayer ... is not specified in the outputs of tf.keras.Model. Once I changed code to model2 tf.keras.Model inputs inputs outputs outputs y everything pass. I don t know if this is intentional behavior if so then you can close it now. Cospel This is intended as the argument x is passed to both Dense and L2NormalizeLayer . So there are two outputs in this functional model. I am closing this issue as it is resolved. Feel free to open a new issue if you encounter any other bugs performance issues. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33985 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33985 No a 
33997, TF 2.0 Unknown graph error when using tf.function decorator with pre trained models, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 rc0 Python version 3.6.9 CUDA cuDNN version 10.0 GPU model and memory GTX 1050 Ti Describe the current behavior If I remove the tf.function decorator the error disappears. Facing this error when I created the loss function where feature extraction with the pre trained VGG16 is a step in the pipeline. ValueError Unknown graph. Aborting. Code to reproduce the issue Other info logs ,A temporary solution was suggested here https stackoverflow.com questions 56615565 evaluating tf model inside a tf op throws error 58705303 58705303 I am able to reproduce the issue with Tf 2.0. Please see the gist here https colab.sandbox.google.com gist gadagashwini c63657b9aa1f9133b430742a689e58fd untitled236.ipynb . Thanks I think the temporary solution makes sense but fchollet would know for sure. It does not solve the issue but I found a workaround to this problem. It worked with the tf.function decorator applying all the layers one by one replacing the extract feat method by It looks like Model.predict is not compatible with tf.function. Note that the error is different in tf nightly and I confirmed that it s not caused by autograph by modifying the gist above As shown in this comment https github.com tensorflow tensorflow issues 33997 issuecomment 586279455 calling the model layer s call method can work under a tf.function but we do not expect calling the model s predict method inside a function to work necessarily. Inside of predict we wrap the calling of the model itself in a function but predict takes care of some higher level processing that doesn t guarantee operation inside of functions. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33997 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33997 No a 
34015,Identity initializer not working as expected in Layer class add weight method, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below v1.15.0 rc3 22 g590d6ee 1.15.0 Python version 3.6.9 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version CUDA 10.0 cuDNN 7.6.4 GPU model and memory Tesla K80 You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior If trying to use the identity initializer in the build method of a custom Layer passing the shape as a tuple of Dimension s fails e.g. fails with TypeError num rows and num columns must be positive integer values. . However exactly the same code where we replace identity by ones works perfectly. Failure happens because tf.eye doesn t accept Dimension objects as inputs only integers e.g. works Describe the expected behavior Behavior for different initializers should be consistent. tf.initializers.identity should accept a tuple of Dimension objects for shape as other initializers do. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,I have tried on colab with TF version 1.15 and i am not seeing any error message. Please find the gist here https colab.sandbox.google.com gist ravikyram 75eadc7e4d604f1462811cf67e109b1c untitled332.ipynb . Thanks ravikyram sorry just realized the example to reproduce had ones initializer instead of identity . Just edited the description and tested on your colab notebook. As shown in the gist https colab.research.google.com gist theoallard 251081388e066f5babd4d59de134eeb6 untitled332.ipynb I confirm this fails This works with tf nightly version 2.2.0 dev20200302 . Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34015 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34015 No a 
34020,Checkpoint.restore doesn t restore Dataset iterator state when Dataset contains shuffle , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device n a TensorFlow installed from source or binary binary TensorFlow version use command below v2.0.0 rc2 26 g64c3d38 2.0.0 Python version 3.5.2 Bazel version if compiling from source n a GCC Compiler version if compiling from source n a CUDA cuDNN version not installed GPU model and memory no GPU Describe the current behavior The code at the end results in the following output. Describe the expected behavior After restoring from the checkpoint I expect the iterator to return the same elements as it did after saving the checkpoint. I.e. Code to reproduce the issue FWIW I get the expected result if I remove .shuffle ... . Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,Issue replicating for TF 2.0 kindly find the gist https colab.sandbox.google.com gist oanush 571af19a2605e0b63938207f55803361 untitled26.ipynb of colab.Thanks kaisuke Checkpoint.save and Checkpoint.restore write and read object based checkpoints in contrast to TensorFlow 1.x s tf.compat.v1.train.Saver which writes and reads variable.name based checkpoints. Object based checkpointing saves a graph of dependencies between Python objects Layers Optimizers Variables etc. with named edges and this graph is used to match variables when restoring a checkpoint. It can be more robust to changes in the Python program and helps to support restore on create for variables. This is the reason why you are observing changes when you are using .shuffle method. For more information you can go through the following link https www.tensorflow.org api docs python tf train Checkpoint . Thanks Thanks gowthamkpr for the explanation but I couldn t see the logical connection between your paragraph 1 2... In the TensorFlow C codebase I see support for saving restoring iterators for a shuffling Dataset IteratorStateReader IteratorStateWriter etc. in https github.com tensorflow tensorflow blob 4fae137 tensorflow core kernels data shuffle dataset op.cc . So I thought that iterators qualify as types that contain trackable state as mentioned in the tf.train.Checkpoint documentation https www.tensorflow.org api docs python tf train Checkpoint . jsimsa It looks like you ve been working on tf.data including shuffle dataset op.cc . May I ask what your view on this is aaudiber could you please take a look thanks I was able to reproduce the issue. It appears that some state is not properly reset when restoring an active shuffle iterator from a checkpoint. I have a fix in flight. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34020 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34020 No a 
34021,Unable to save TensorFlow Keras LSTM model to SavedModel format, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary pip installed TensorFlow version use command below v2.0.0 rc2 26 g64c3d382ca 2.0.0 Python version 3.7.1 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior Unable to save TensorFlow Keras LSTM model to SavedModel format for exporting to Google Cloud and Google AI platform. Error message ValueError Attempted to save a function b inference lstm 2 layer call fn 36083 which references a symbolic Tensor Tensor dropout mul 1 0 shape None 1280 dtype float32 that is not a simple constant. This is not supported. Describe the expected behavior LSTM model would be saved in the SavedModel format to be exported into a Google Cloud bucket to work with Google s AI platform. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. import tensorflow as tf import os import cv2 import numpy as np import matplotlib.pyplot as plt import tqdm import datetime from sklearn.preprocessing import LabelBinarizer model tf.keras.Sequential tf.keras.layers.Masking mask value 0. tf.keras.layers.LSTM 512 dropout 0.5 recurrent dropout 0.5 tf.keras.layers.Dense 256 activation relu tf.keras.layers.Dropout 0.5 tf.keras.layers.Dense len LABELS activation softmax model.compile loss categorical crossentropy optimizer rmsprop metrics accuracy top k categorical accuracy test file C ... testlist01.txt train file C ... trainlist01.txt with open test file as f test list row.strip for row in list f with open train file as f train list row.strip for row in list f train list row.split 0 for row in train list def make generator file list def generator np.random.shuffle file list for path in file list full path os.path.join BASE PATH path .replace .avi .npy label os.path.basename os.path.dirname path features np.load full path padded sequence np.zeros SEQUENCE LENGTH 1280 padded sequence 0 len features np.array features transformed label encoder.transform label yield padded sequence transformed label 0 return generator train dataset tf.data.Dataset.from generator make generator train list output types tf.float32 tf.int16 output shapes SEQUENCE LENGTH 1280 len LABELS train dataset train dataset.batch 16 .prefetch tf.data.experimental.AUTOTUNE valid dataset tf.data.Dataset.from generator make generator test list output types tf.float32 tf.int16 output shapes SEQUENCE LENGTH 1280 len LABELS valid dataset valid dataset.batch 16 .prefetch tf.data.experimental.AUTOTUNE model.fit train dataset epochs 17 validation data valid dataset BASE DIRECTORY C ... saved model LSTM 1 tf.saved model.save model BASE DIRECTORY Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. , tmartin293 Request you to share the supporting files and code with proper indentation to reproduce the issue in our environment. Thanks Project code based on previous LSTM project for activity recognition using UCF101 dataset https github.com PacktPublishing Hands On Computer Vision with TensorFlow 2 blob master Chapter08 ch8 nb1 action recognition.ipynb Dataset https www.crcv.ucf.edu data UCF101.php Code tmartin293 Please post this question in stack overflow as this question is not related to bug performance build install docs or feature request. Thanks This does seem like a bug I tried a different model that has tf.keras.layers.LSTM using tf.saved model.save method without a generator and I didn t run into any error. Heres my gist https colab.sandbox.google.com gist gowthamkpr b1082d7f664edeca3929108d56792f79 untitled4.ipynb tmartin293 Can you please check if the error still exists with tf nightly Thanks tmartin293 Can you please try with recent tf nightly and let us know whether it was resolved for you Thanks It has been 15 days with no activity and the awaiting response label was assigned. Is this still an issue Closing due to lack of recent activity. Similar issue https github.com tensorflow tensorflow issues 34644 was resolved by recent tf nightly . Please update the issue when new information becomes available and we will reopen the issue. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34021 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34021 No a It occurs when dropout parameter is used on the LSTM. Still occuring in TF 2.1 wtesler Can you please open a new issue with a standalone code to reproduce the error. Thanks 
34025,Understanding warning 5 out of the last 5 calls to function XXX triggered tf.function retracing , System information Have I written custom code Yes below. Linux Ubuntu 16.04 TensorFlow 2.1.0 dev20191103 binary install. Python 3.6 CUDA 10.0 cnDNN 7.6.4 4 NVIDIA TITAN X 12GB I defined a very simple training script with a custom loss function and .fit as below. The loss fn is very simple and I think every time it takes tensors of the same shape and type . But I m getting the following warning message . Interesting is that I m getting the message only when training with multiple GPUs . Is it a bug Is it harmful Is it really affecting the computational cost Warning message Code ,I get the same error if trying to predict in a for loop due to variable length inputs . I have to call model.predict np.expand dims xi axis 0 on each sample individually or tensorflow will attempt to concatenate predictions and fail. This is probably calling something several times when it shouldn t. I have the same issue when training my model. Any updates on this same issue same here only with multi gpu setup singel gpu has no warnings I think the warning is red herring. The function is traced on every GPU so presumably if one has 5 or more than 5 GPUs on the machine the warning will be printed out. Perhaps try adding a print statement inside the tf.function https www.tensorflow.org tutorials customization performance and see how many times the function has actually been traced As long as it not traced per step it should be fine. If you have less than 5 GPUs though it might be a bug. When calling a simple RNN with input sequences of various lengths it seems that the model gets traced once for each sequence length as you can see in this Colab https colab.research.google.com drive 1Aj51rVjkpUesVz3dSTvih7kW9CT QjRA . Here s the code I get the dreaded warning Perhaps there s a way to force the model to use a single graph instead of tracing a new one for each input shape I think the warning is red herring. The function is traced on every GPU I think we had the same idea exactly as I stated here 33649 Hi bela127 and ckkuang In my code example I m not using any GPU at all. Just calling an RNN with input sequences of varying lengths. UPDATE is it possible its only a problem with the keras.layers.SimpleRNN 10 return sequences True input shape None 4 Im not sure but return sequences TRUE when im correct will return the hole RNN Sequence not just the last element so for different sized inputs you will get different sized outputs. Dens needs a fixed sized input. So it will be retraced every time. when you try to use the same dense layer for every single output you cant do this with Sequential because it would be a parallel operation. If this is the case i my have a solution for you... OLD yes i see ageron had the same problem with Keras functional api it seams the functional keras api has problems with variable length inputs different than the batchdim a workaround is using the layer class api and subclass Keras Layer. here you can update the call method with tf.function annotation with relax shape tf.function experimental relax shapes True or explicit self.call tf.function self.call input signature tf.TensorSpec None None None 3 dtype tf.float32 tf.TensorSpec None dtype tf.float32 tf.TensorSpec None 15 3 dtype tf.float32 tf.TensorSpec dtype tf.float32 tf.TensorSpec dtype tf.float32 in init or build But i agree the functional api should although support this. I am also getting this error on a classical functional model without tf.function and variable length inputs in a for loop using predict . This happens only in tf 2.2.0rc2 and as soon as I switch back to tf 2.1 the problem disappears. I am also facing the same issue with Tensorflow 2.2.0 rc2 while training with variable lengths in a loop. ywpkwon I tried the code you originally linked in a colab with tf nightly gpu and I don t see the warnings anymore. Please re open if you still see the warning. ageron bela127 others Please file a separate ticket for the RNN issue or other issues without multi GPU. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34025 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34025 No a guptapriya Here is a new code snippet you can use to reproduce the issue in tf 2.2.0rc2 I am using Python 3.6.8 on Ubuntu 16.04 without GPU and tf installed from pip. This got me the following warnings Do you want me to file a separate ticket I think it looks like the same issue. zaccharieramzi yes please file a separate ticket as this is a Keras issue without distribution strategy. The error message might be the same but the root causes are different. I have same issue with my model trained with MirroredStrategy . Using TF2.1 with four TitanXp SangwonSUH are you using variable inputs see https github.com tensorflow tensorflow issues 38561 SangwonSUH are you using variable inputs see 38561 Yes I do I will fix the input layer of my model and try prediction again. Thank you I have same issue with my model trained with MirroredStrategy . Using TF2.1 with four TitanXp I have a very similar issue using MirroredStrategy WARNING tensorflow 11 out of the last 11 calls to function Model.make predict function. locals .predict function at 0x7f9dc8698598 triggered tf.function retracing. However I placed a print statement inside the tf.function distributed training step and it appears to only be executed twice matching the number of GPUs I am distributing onto. It may be an erroneous message which is annoying as it spams the output. I do notice that mirroredstrategy is significantly 10x slower than using a single GPU which I did not expect. you have to place the print statement in make predict function not in distributed training step for debugging the statement should be placed in the exact function that is mentioned in the message WARNING tensorflow 11 out of the last 11 calls to function Model.make predict function. locals .predict function at 0x7f9dc8698598 triggered tf.function retracing. I guess it relay is traced every time if it does not stop appearing in the output after a few runs that is why your training is so slow. you have to place the print statement in make predict function not in distributed training step for debugging the statement should be placed in the exact function that is mentioned in the message WARNING tensorflow 11 out of the last 11 calls to function Model.make predict function. locals .predict function at 0x7f9dc8698598 triggered tf.function retracing. I guess it relay is traced every time if it does not stop appearing in the output after a few runs that is why your training is so slow. I cannot place a print function inside make predict function as that isn t anything I ve written it s tensorflow code. Also note it is predict function that is causing the issue. This implies that it s the input causing the retracing. However the error also only appears when distributing not when running on one GPU. tf.functions can only handle a pre defined input shape if the shape changes or if diferent python objects get passed tensorflow automagically rebuilds the function. so i would suggest you check and print all the input parameter that get passed to Model.make predict function. locals .predict function if a input is no tensor or if the shape changes you have found the problem tf.functions can only handle a pre defined input shape if the shape changes or if diferent python objects get passed tensorflow automagically rebuilds the function. so i would suggest you check and print all the input parameter that get passed to Model.make predict function. locals .predict function if a input is no tensor or if the shape changes you have found the problem The inputs to my training function that calls strategy.run print out to However I don t think I have any control over this as these come directly from a distributed tf.dataset. If we print out the input for the actual distributed training function we see as expected the training loop code could be helpful but i think this is not a bug and could be a question for stack overflow. one way or the other could you post a minimal example of your training loop code that reproduces your issue so one can check if its a bug or a implementation error on your side the training loop code could be helpful but i think this is not a bug and could be a question for stack overflow. one way or the other could you post a minimal example of your training loop code that reproduces your issue so one can check if its a bug or a implementation error on your side This is all I m really doing My trainop is standard calls model inputs and calculates a simple loss. The only strange thing I may be doing is generating some noise inside the training op using tf.random.normal and calling model2 noise but I fail to see how a tensorflow op could cause this issue like i said earlier the input to or what ever happens in your Model.make predict function. locals .predict function courses the retracing. with only the code you have provided a reproduction of the issue is not possible. the relevant information is inside Model.make predict function. locals .predict function and the inputs to this function. this is where the issue originates are you using any python code in your model or do you produce differently shaped outputs inside the model what model are you using one thing im missing in your training loop is the with strategy block do you have this in your code your model should be created inside this scope and the loop should run in the scope the rest of your training loop seams to be fine. me too.. how to solve this problem I have the same problem too... model.predict on batch input gives the warning as WARNING tensorflow 8 out of the last 8 calls to function Model.make predict function. locals .predict function at 0x7f8fe3fdc940 triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also tf.function has experimental relax shapes True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https www.tensorflow.org tutorials customization performance python or tensor args and https www.tensorflow.org api docs python tf function for more details. How to solve this problem I was getting this same warning message. Using a LSTM model with variable sequence lengths. Upgrading from TF 2.2 to TF 2.3 seems to have fixed it. Am using only 1 gpu and still am facing this retracing issue. After trying out verious fixes to this problem by searching over the internet nothing worked. Including tf nightly gpu since its first of all not detecting my gpu. So I have just downgraded from tf 2.3.0 to tf 2.2.1 and it has fixed this issue for now. 
34029,error on saving RNN layer with recurrent dropout parameter as saved model , save this model as saved model on tf nightly 2.1.0 dev20191104 raise bug as after trying to change some parameter i got the conclusion that this issue happens because of recurrent dropout parameter. Any suggestion , deaputri Looks like the code provided is in complete Can you share a simple and standalone code to reproduce the issue Thanks deaputri Hi the provided code is not enough to replicate. can you please provide complete code to reproduce Thanks deaputri When tried running the given code for tf nightly i got the following error. Kindly find the gist https colab.sandbox.google.com gist oanush 8eda00767d8043f8415d7911c6eee738 34029.ipynb of colab.Thanks same error saving any recurrent model with dropout or recurrent dropout will raise a error. BolunHan i can save the model just using dropout. would u try oanush i m using tf nightly 2.1.0 dev20191104 by using pip install tf nightly 2.1.0 dev20191104 deaputri I just figure out that if you call model.save some path not ended with.h5.model adding dropout or recurrent dropout will cause a error. however if your given file path ended with .h5 everything works just fine. BolunHan actually i want to save the model as SavedModel. As the tensorflow documentation on here https www.tensorflow.org guide saved model the code is tf.saved model.save pretrained model tmp mobilenet 1 . I have no problem on saving as the .model of .h5 even with tensorflow only not using tf nightly deaputri thx. I will give it a try. I was using tf.keras to build my recurrent network model tf.keras.Model inputs inputs outputs outputs model.save given file path save model using tf.keras will cause the problem. I didn t test tf.saved model.save method BolunHan There are lots model type we can use to save. This tensor doc https www.tensorflow.org tutorials keras save and load may help you. BolunHan Did deaputri comment help you solve the problem Closing this issue as it has been inactive for more than 3 weeks. Please add additional comments and we can open this issue again. Thanks Can we open this back up I m having this same issue and I m running on a relatively new tensorflow nightly deploy. https github.com tensorflow tensorflow issues 33247 same error use dropout recurrent dropout in GRU when complete train and use tf.saved model.save will raise an error. I try to save model directly without train that will be ok. NLP ZY Just make sure the save path of your model ended with .h5 which solved my own problem. Thanks but I should save in tf format to use tf serving. deaputri This was resolved in tf nightly . Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 172a07741aa76b82fa1a128c08e33f5a untitled775.ipynb . I am closing this issue as it was resolved. Please feel free to reopen if the issue persists again. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34029 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34029 No a 
34038,TF 2.0 distribution strategy throws invalid argument error, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow somewhat OS Platform and Distribution e.g. Linux Ubuntu 16.04 DGX Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device n z TensorFlow installed from source or binary docker image TensorFlow version use command below 2.0 Python version 3.6 Bazel version if compiling from source n a GCC Compiler version if compiling from source n a CUDA cuDNN version 10.1 GPU model and memory Telsa V100 SXM2 You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior From the docs https www.tensorflow.org guide gpu with tfdistributestrategy I adapted this I hope correctly for multiple gpus which prints 8 Physical GPUs 3 Logical GPUs as expected Then calling just this line throws Describe the expected behavior It just works as in the docs Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. See above. Docker image tensorflow tensorflow 2.0.0 gpu py3 jupyter with nvidia docker Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,Hi guys I have the same issue only using I just verified and I believe this is resolved in the nightly and the fix should be available in 2.1.0. Please let me know if you are able to still reproduce with the nightly or the 2.1.0 rc1 release. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34038 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34038 No a 
34039,tf.data.Dataset fixed size batching with subsequent map under tf.distribute.MirroredStrategy leads to a crash, System information The same environment as in https github.com tensorflow tensorflow issues 33531 Code to reproduce the issue It took me a few weeks of debugging to reproduce IMPORTANT I DO NOT THINK IT WILL REPRODUCE IN COLAB YOU NEED AT LEAST 2 GPUS. As you see I am feeding a tf.data.Dataset pipeline to a Keras model under tf.distribute.MirroredStrategy . In my case there are 4 GPUs. Here is the log which indicates a crash details summary Full log summary pre 2019 11 06 11 09 37.077575 I tensorflow core common runtime gpu gpu device.cc 1746 Adding visible gpu devices 0 1 2 3 2019 11 06 11 09 37.077858 I tensorflow core common runtime gpu gpu device.cc 1159 Device interconnect StreamExecutorwith strength 1 edge matrix 2019 11 06 11 09 37.077880 I tensorflow core common runtime gpu gpu device.cc 1165 0 1 2 3 2019 11 06 11 09 37.077894 I tensorflow core common runtime gpu gpu device.cc 1178 0 N Y N N 2019 11 06 11 09 37.077904 I tensorflow core common runtime gpu gpu device.cc 1178 1 Y N N N 2019 11 06 11 09 37.077914 I tensorflow core common runtime gpu gpu device.cc 1178 2 N N N Y 2019 11 06 11 09 37.077923 I tensorflow core common runtime gpu gpu device.cc 1178 3 N N Y N 2019 11 06 11 09 37.084775 I tensorflow core common runtime gpu gpu device.cc 1304 Created TensorFlow device device GPU 0 with 10470 MB memory physical GPU device 0 name GeForce GTX 1080 Ti pci bus id 0000 02 00.0 compute capability 6.1 2019 11 06 11 09 37.086075 I tensorflow core common runtime gpu gpu device.cc 1304 Created TensorFlow device device GPU 1 with 10470 MB memory physical GPU device 1 name GeForce GTX 1080 Ti pci bus id 0000 03 00.0 compute capability 6.1 2019 11 06 11 09 37.087140 I tensorflow core common runtime gpu gpu device.cc 1304 Created TensorFlow device device GPU 2 with 10470 MB memory physical GPU device 2 name GeForce GTX 1080 Ti pci bus id 0000 82 00.0 compute capability 6.1 2019 11 06 11 09 37.088126 I tensorflow core common runtime gpu gpu device.cc 1304 Created TensorFlow device device GPU 3 with 10470 MB memory physical GPU device 3 name GeForce GTX 1080 Ti pci bus id 0000 83 00.0 compute capability 6.1 Model densenet121 Layer type Output Shape Param Connected to input 1 InputLayer 3 372 558 3 0 zero padding2d ZeroPadding2D 3 378 564 3 0 input 1 0 0 conv1 conv Conv2D 3 186 279 64 9408 zero padding2d 0 0 conv1 bn BatchNormalization 3 186 279 64 256 conv1 conv 0 0 conv1 relu Activation 3 186 279 64 0 conv1 bn 0 0 zero padding2d 1 ZeroPadding2D 3 188 281 64 0 conv1 relu 0 0 pool1 MaxPooling2D 3 93 140 64 0 zero padding2d 1 0 0 conv2 block1 0 bn BatchNormali 3 93 140 64 256 pool1 0 0 conv2 block1 0 relu Activation 3 93 140 64 0 conv2 block1 0 bn 0 0 conv2 block1 1 conv Conv2D 3 93 140 128 8192 conv2 block1 0 relu 0 0 conv2 block1 1 bn BatchNormali 3 93 140 128 512 conv2 block1 1 conv 0 0 conv2 block1 1 relu Activation 3 93 140 128 0 conv2 block1 1 bn 0 0 conv2 block1 2 conv Conv2D 3 93 140 32 36864 conv2 block1 1 relu 0 0 conv2 block1 concat Concatenat 3 93 140 96 0 pool1 0 0 conv2 block1 2 conv 0 0 conv2 block2 0 bn BatchNormali 3 93 140 96 384 conv2 block1 concat 0 0 conv2 block2 0 relu Activation 3 93 140 96 0 conv2 block2 0 bn 0 0 conv2 block2 1 conv Conv2D 3 93 140 128 12288 conv2 block2 0 relu 0 0 conv2 block2 1 bn BatchNormali 3 93 140 128 512 conv2 block2 1 conv 0 0 conv2 block2 1 relu Activation 3 93 140 128 0 conv2 block2 1 bn 0 0 conv2 block2 2 conv Conv2D 3 93 140 32 36864 conv2 block2 1 relu 0 0 conv2 block2 concat Concatenat 3 93 140 128 0 conv2 block1 concat 0 0 conv2 block2 2 conv 0 0 conv2 block3 0 bn BatchNormali 3 93 140 128 512 conv2 block2 concat 0 0 conv2 block3 0 relu Activation 3 93 140 128 0 conv2 block3 0 bn 0 0 conv2 block3 1 conv Conv2D 3 93 140 128 16384 conv2 block3 0 relu 0 0 conv2 block3 1 bn BatchNormali 3 93 140 128 512 conv2 block3 1 conv 0 0 conv2 block3 1 relu Activation 3 93 140 128 0 conv2 block3 1 bn 0 0 conv2 block3 2 conv Conv2D 3 93 140 32 36864 conv2 block3 1 relu 0 0 conv2 block3 concat Concatenat 3 93 140 160 0 conv2 block2 concat 0 0 conv2 block3 2 conv 0 0 conv2 block4 0 bn BatchNormali 3 93 140 160 640 conv2 block3 concat 0 0 conv2 block4 0 relu Activation 3 93 140 160 0 conv2 block4 0 bn 0 0 conv2 block4 1 conv Conv2D 3 93 140 128 20480 conv2 block4 0 relu 0 0 conv2 block4 1 bn BatchNormali 3 93 140 128 512 conv2 block4 1 conv 0 0 conv2 block4 1 relu Activation 3 93 140 128 0 conv2 block4 1 bn 0 0 conv2 block4 2 conv Conv2D 3 93 140 32 36864 conv2 block4 1 relu 0 0 conv2 block4 concat Concatenat 3 93 140 192 0 conv2 block3 concat 0 0 conv2 block4 2 conv 0 0 conv2 block5 0 bn BatchNormali 3 93 140 192 768 conv2 block4 concat 0 0 conv2 block5 0 relu Activation 3 93 140 192 0 conv2 block5 0 bn 0 0 conv2 block5 1 conv Conv2D 3 93 140 128 24576 conv2 block5 0 relu 0 0 conv2 block5 1 bn BatchNormali 3 93 140 128 512 conv2 block5 1 conv 0 0 conv2 block5 1 relu Activation 3 93 140 128 0 conv2 block5 1 bn 0 0 conv2 block5 2 conv Conv2D 3 93 140 32 36864 conv2 block5 1 relu 0 0 conv2 block5 concat Concatenat 3 93 140 224 0 conv2 block4 concat 0 0 conv2 block5 2 conv 0 0 conv2 block6 0 bn BatchNormali 3 93 140 224 896 conv2 block5 concat 0 0 conv2 block6 0 relu Activation 3 93 140 224 0 conv2 block6 0 bn 0 0 conv2 block6 1 conv Conv2D 3 93 140 128 28672 conv2 block6 0 relu 0 0 conv2 block6 1 bn BatchNormali 3 93 140 128 512 conv2 block6 1 conv 0 0 conv2 block6 1 relu Activation 3 93 140 128 0 conv2 block6 1 bn 0 0 conv2 block6 2 conv Conv2D 3 93 140 32 36864 conv2 block6 1 relu 0 0 conv2 block6 concat Concatenat 3 93 140 256 0 conv2 block5 concat 0 0 conv2 block6 2 conv 0 0 pool2 bn BatchNormalization 3 93 140 256 1024 conv2 block6 concat 0 0 pool2 relu Activation 3 93 140 256 0 pool2 bn 0 0 pool2 conv Conv2D 3 93 140 128 32768 pool2 relu 0 0 pool2 pool AveragePooling2D 3 46 70 128 0 pool2 conv 0 0 conv3 block1 0 bn BatchNormali 3 46 70 128 512 pool2 pool 0 0 conv3 block1 0 relu Activation 3 46 70 128 0 conv3 block1 0 bn 0 0 conv3 block1 1 conv Conv2D 3 46 70 128 16384 conv3 block1 0 relu 0 0 conv3 block1 1 bn BatchNormali 3 46 70 128 512 conv3 block1 1 conv 0 0 conv3 block1 1 relu Activation 3 46 70 128 0 conv3 block1 1 bn 0 0 conv3 block1 2 conv Conv2D 3 46 70 32 36864 conv3 block1 1 relu 0 0 conv3 block1 concat Concatenat 3 46 70 160 0 pool2 pool 0 0 conv3 block1 2 conv 0 0 conv3 block2 0 bn BatchNormali 3 46 70 160 640 conv3 block1 concat 0 0 conv3 block2 0 relu Activation 3 46 70 160 0 conv3 block2 0 bn 0 0 conv3 block2 1 conv Conv2D 3 46 70 128 20480 conv3 block2 0 relu 0 0 conv3 block2 1 bn BatchNormali 3 46 70 128 512 conv3 block2 1 conv 0 0 conv3 block2 1 relu Activation 3 46 70 128 0 conv3 block2 1 bn 0 0 conv3 block2 2 conv Conv2D 3 46 70 32 36864 conv3 block2 1 relu 0 0 conv3 block2 concat Concatenat 3 46 70 192 0 conv3 block1 concat 0 0 conv3 block2 2 conv 0 0 conv3 block3 0 bn BatchNormali 3 46 70 192 768 conv3 block2 concat 0 0 conv3 block3 0 relu Activation 3 46 70 192 0 conv3 block3 0 bn 0 0 conv3 block3 1 conv Conv2D 3 46 70 128 24576 conv3 block3 0 relu 0 0 conv3 block3 1 bn BatchNormali 3 46 70 128 512 conv3 block3 1 conv 0 0 conv3 block3 1 relu Activation 3 46 70 128 0 conv3 block3 1 bn 0 0 conv3 block3 2 conv Conv2D 3 46 70 32 36864 conv3 block3 1 relu 0 0 conv3 block3 concat Concatenat 3 46 70 224 0 conv3 block2 concat 0 0 conv3 block3 2 conv 0 0 conv3 block4 0 bn BatchNormali 3 46 70 224 896 conv3 block3 concat 0 0 conv3 block4 0 relu Activation 3 46 70 224 0 conv3 block4 0 bn 0 0 conv3 block4 1 conv Conv2D 3 46 70 128 28672 conv3 block4 0 relu 0 0 conv3 block4 1 bn BatchNormali 3 46 70 128 512 conv3 block4 1 conv 0 0 conv3 block4 1 relu Activation 3 46 70 128 0 conv3 block4 1 bn 0 0 conv3 block4 2 conv Conv2D 3 46 70 32 36864 conv3 block4 1 relu 0 0 conv3 block4 concat Concatenat 3 46 70 256 0 conv3 block3 concat 0 0 conv3 block4 2 conv 0 0 conv3 block5 0 bn BatchNormali 3 46 70 256 1024 conv3 block4 concat 0 0 conv3 block5 0 relu Activation 3 46 70 256 0 conv3 block5 0 bn 0 0 conv3 block5 1 conv Conv2D 3 46 70 128 32768 conv3 block5 0 relu 0 0 conv3 block5 1 bn BatchNormali 3 46 70 128 512 conv3 block5 1 conv 0 0 conv3 block5 1 relu Activation 3 46 70 128 0 conv3 block5 1 bn 0 0 conv3 block5 2 conv Conv2D 3 46 70 32 36864 conv3 block5 1 relu 0 0 conv3 block5 concat Concatenat 3 46 70 288 0 conv3 block4 concat 0 0 conv3 block5 2 conv 0 0 conv3 block6 0 bn BatchNormali 3 46 70 288 1152 conv3 block5 concat 0 0 conv3 block6 0 relu Activation 3 46 70 288 0 conv3 block6 0 bn 0 0 conv3 block6 1 conv Conv2D 3 46 70 128 36864 conv3 block6 0 relu 0 0 conv3 block6 1 bn BatchNormali 3 46 70 128 512 conv3 block6 1 conv 0 0 conv3 block6 1 relu Activation 3 46 70 128 0 conv3 block6 1 bn 0 0 conv3 block6 2 conv Conv2D 3 46 70 32 36864 conv3 block6 1 relu 0 0 conv3 block6 concat Concatenat 3 46 70 320 0 conv3 block5 concat 0 0 conv3 block6 2 conv 0 0 conv3 block7 0 bn BatchNormali 3 46 70 320 1280 conv3 block6 concat 0 0 conv3 block7 0 relu Activation 3 46 70 320 0 conv3 block7 0 bn 0 0 conv3 block7 1 conv Conv2D 3 46 70 128 40960 conv3 block7 0 relu 0 0 conv3 block7 1 bn BatchNormali 3 46 70 128 512 conv3 block7 1 conv 0 0 conv3 block7 1 relu Activation 3 46 70 128 0 conv3 block7 1 bn 0 0 conv3 block7 2 conv Conv2D 3 46 70 32 36864 conv3 block7 1 relu 0 0 conv3 block7 concat Concatenat 3 46 70 352 0 conv3 block6 concat 0 0 conv3 block7 2 conv 0 0 conv3 block8 0 bn BatchNormali 3 46 70 352 1408 conv3 block7 concat 0 0 conv3 block8 0 relu Activation 3 46 70 352 0 conv3 block8 0 bn 0 0 conv3 block8 1 conv Conv2D 3 46 70 128 45056 conv3 block8 0 relu 0 0 conv3 block8 1 bn BatchNormali 3 46 70 128 512 conv3 block8 1 conv 0 0 conv3 block8 1 relu Activation 3 46 70 128 0 conv3 block8 1 bn 0 0 conv3 block8 2 conv Conv2D 3 46 70 32 36864 conv3 block8 1 relu 0 0 conv3 block8 concat Concatenat 3 46 70 384 0 conv3 block7 concat 0 0 conv3 block8 2 conv 0 0 conv3 block9 0 bn BatchNormali 3 46 70 384 1536 conv3 block8 concat 0 0 conv3 block9 0 relu Activation 3 46 70 384 0 conv3 block9 0 bn 0 0 conv3 block9 1 conv Conv2D 3 46 70 128 49152 conv3 block9 0 relu 0 0 conv3 block9 1 bn BatchNormali 3 46 70 128 512 conv3 block9 1 conv 0 0 conv3 block9 1 relu Activation 3 46 70 128 0 conv3 block9 1 bn 0 0 conv3 block9 2 conv Conv2D 3 46 70 32 36864 conv3 block9 1 relu 0 0 conv3 block9 concat Concatenat 3 46 70 416 0 conv3 block8 concat 0 0 conv3 block9 2 conv 0 0 conv3 block10 0 bn BatchNormal 3 46 70 416 1664 conv3 block9 concat 0 0 conv3 block10 0 relu Activatio 3 46 70 416 0 conv3 block10 0 bn 0 0 conv3 block10 1 conv Conv2D 3 46 70 128 53248 conv3 block10 0 relu 0 0 conv3 block10 1 bn BatchNormal 3 46 70 128 512 conv3 block10 1 conv 0 0 conv3 block10 1 relu Activatio 3 46 70 128 0 conv3 block10 1 bn 0 0 conv3 block10 2 conv Conv2D 3 46 70 32 36864 conv3 block10 1 relu 0 0 conv3 block10 concat Concatena 3 46 70 448 0 conv3 block9 concat 0 0 conv3 block10 2 conv 0 0 conv3 block11 0 bn BatchNormal 3 46 70 448 1792 conv3 block10 concat 0 0 conv3 block11 0 relu Activatio 3 46 70 448 0 conv3 block11 0 bn 0 0 conv3 block11 1 conv Conv2D 3 46 70 128 57344 conv3 block11 0 relu 0 0 conv3 block11 1 bn BatchNormal 3 46 70 128 512 conv3 block11 1 conv 0 0 conv3 block11 1 relu Activatio 3 46 70 128 0 conv3 block11 1 bn 0 0 conv3 block11 2 conv Conv2D 3 46 70 32 36864 conv3 block11 1 relu 0 0 conv3 block11 concat Concatena 3 46 70 480 0 conv3 block10 concat 0 0 conv3 block11 2 conv 0 0 conv3 block12 0 bn BatchNormal 3 46 70 480 1920 conv3 block11 concat 0 0 conv3 block12 0 relu Activatio 3 46 70 480 0 conv3 block12 0 bn 0 0 conv3 block12 1 conv Conv2D 3 46 70 128 61440 conv3 block12 0 relu 0 0 conv3 block12 1 bn BatchNormal 3 46 70 128 512 conv3 block12 1 conv 0 0 conv3 block12 1 relu Activatio 3 46 70 128 0 conv3 block12 1 bn 0 0 conv3 block12 2 conv Conv2D 3 46 70 32 36864 conv3 block12 1 relu 0 0 conv3 block12 concat Concatena 3 46 70 512 0 conv3 block11 concat 0 0 conv3 block12 2 conv 0 0 pool3 bn BatchNormalization 3 46 70 512 2048 conv3 block12 concat 0 0 pool3 relu Activation 3 46 70 512 0 pool3 bn 0 0 pool3 conv Conv2D 3 46 70 256 131072 pool3 relu 0 0 pool3 pool AveragePooling2D 3 23 35 256 0 pool3 conv 0 0 conv4 block1 0 bn BatchNormali 3 23 35 256 1024 pool3 pool 0 0 conv4 block1 0 relu Activation 3 23 35 256 0 conv4 block1 0 bn 0 0 conv4 block1 1 conv Conv2D 3 23 35 128 32768 conv4 block1 0 relu 0 0 conv4 block1 1 bn BatchNormali 3 23 35 128 512 conv4 block1 1 conv 0 0 conv4 block1 1 relu Activation 3 23 35 128 0 conv4 block1 1 bn 0 0 conv4 block1 2 conv Conv2D 3 23 35 32 36864 conv4 block1 1 relu 0 0 conv4 block1 concat Concatenat 3 23 35 288 0 pool3 pool 0 0 conv4 block1 2 conv 0 0 conv4 block2 0 bn BatchNormali 3 23 35 288 1152 conv4 block1 concat 0 0 conv4 block2 0 relu Activation 3 23 35 288 0 conv4 block2 0 bn 0 0 conv4 block2 1 conv Conv2D 3 23 35 128 36864 conv4 block2 0 relu 0 0 conv4 block2 1 bn BatchNormali 3 23 35 128 512 conv4 block2 1 conv 0 0 conv4 block2 1 relu Activation 3 23 35 128 0 conv4 block2 1 bn 0 0 conv4 block2 2 conv Conv2D 3 23 35 32 36864 conv4 block2 1 relu 0 0 conv4 block2 concat Concatenat 3 23 35 320 0 conv4 block1 concat 0 0 conv4 block2 2 conv 0 0 conv4 block3 0 bn BatchNormali 3 23 35 320 1280 conv4 block2 concat 0 0 conv4 block3 0 relu Activation 3 23 35 320 0 conv4 block3 0 bn 0 0 conv4 block3 1 conv Conv2D 3 23 35 128 40960 conv4 block3 0 relu 0 0 conv4 block3 1 bn BatchNormali 3 23 35 128 512 conv4 block3 1 conv 0 0 conv4 block3 1 relu Activation 3 23 35 128 0 conv4 block3 1 bn 0 0 conv4 block3 2 conv Conv2D 3 23 35 32 36864 conv4 block3 1 relu 0 0 conv4 block3 concat Concatenat 3 23 35 352 0 conv4 block2 concat 0 0 conv4 block3 2 conv 0 0 conv4 block4 0 bn BatchNormali 3 23 35 352 1408 conv4 block3 concat 0 0 conv4 block4 0 relu Activation 3 23 35 352 0 conv4 block4 0 bn 0 0 conv4 block4 1 conv Conv2D 3 23 35 128 45056 conv4 block4 0 relu 0 0 conv4 block4 1 bn BatchNormali 3 23 35 128 512 conv4 block4 1 conv 0 0 conv4 block4 1 relu Activation 3 23 35 128 0 conv4 block4 1 bn 0 0 conv4 block4 2 conv Conv2D 3 23 35 32 36864 conv4 block4 1 relu 0 0 conv4 block4 concat Concatenat 3 23 35 384 0 conv4 block3 concat 0 0 conv4 block4 2 conv 0 0 conv4 block5 0 bn BatchNormali 3 23 35 384 1536 conv4 block4 concat 0 0 conv4 block5 0 relu Activation 3 23 35 384 0 conv4 block5 0 bn 0 0 conv4 block5 1 conv Conv2D 3 23 35 128 49152 conv4 block5 0 relu 0 0 conv4 block5 1 bn BatchNormali 3 23 35 128 512 conv4 block5 1 conv 0 0 conv4 block5 1 relu Activation 3 23 35 128 0 conv4 block5 1 bn 0 0 conv4 block5 2 conv Conv2D 3 23 35 32 36864 conv4 block5 1 relu 0 0 conv4 block5 concat Concatenat 3 23 35 416 0 conv4 block4 concat 0 0 conv4 block5 2 conv 0 0 conv4 block6 0 bn BatchNormali 3 23 35 416 1664 conv4 block5 concat 0 0 conv4 block6 0 relu Activation 3 23 35 416 0 conv4 block6 0 bn 0 0 conv4 block6 1 conv Conv2D 3 23 35 128 53248 conv4 block6 0 relu 0 0 conv4 block6 1 bn BatchNormali 3 23 35 128 512 conv4 block6 1 conv 0 0 conv4 block6 1 relu Activation 3 23 35 128 0 conv4 block6 1 bn 0 0 conv4 block6 2 conv Conv2D 3 23 35 32 36864 conv4 block6 1 relu 0 0 conv4 block6 concat Concatenat 3 23 35 448 0 conv4 block5 concat 0 0 conv4 block6 2 conv 0 0 conv4 block7 0 bn BatchNormali 3 23 35 448 1792 conv4 block6 concat 0 0 conv4 block7 0 relu Activation 3 23 35 448 0 conv4 block7 0 bn 0 0 conv4 block7 1 conv Conv2D 3 23 35 128 57344 conv4 block7 0 relu 0 0 conv4 block7 1 bn BatchNormali 3 23 35 128 512 conv4 block7 1 conv 0 0 conv4 block7 1 relu Activation 3 23 35 128 0 conv4 block7 1 bn 0 0 conv4 block7 2 conv Conv2D 3 23 35 32 36864 conv4 block7 1 relu 0 0 conv4 block7 concat Concatenat 3 23 35 480 0 conv4 block6 concat 0 0 conv4 block7 2 conv 0 0 conv4 block8 0 bn BatchNormali 3 23 35 480 1920 conv4 block7 concat 0 0 conv4 block8 0 relu Activation 3 23 35 480 0 conv4 block8 0 bn 0 0 conv4 block8 1 conv Conv2D 3 23 35 128 61440 conv4 block8 0 relu 0 0 conv4 block8 1 bn BatchNormali 3 23 35 128 512 conv4 block8 1 conv 0 0 conv4 block8 1 relu Activation 3 23 35 128 0 conv4 block8 1 bn 0 0 conv4 block8 2 conv Conv2D 3 23 35 32 36864 conv4 block8 1 relu 0 0 conv4 block8 concat Concatenat 3 23 35 512 0 conv4 block7 concat 0 0 conv4 block8 2 conv 0 0 conv4 block9 0 bn BatchNormali 3 23 35 512 2048 conv4 block8 concat 0 0 conv4 block9 0 relu Activation 3 23 35 512 0 conv4 block9 0 bn 0 0 conv4 block9 1 conv Conv2D 3 23 35 128 65536 conv4 block9 0 relu 0 0 conv4 block9 1 bn BatchNormali 3 23 35 128 512 conv4 block9 1 conv 0 0 conv4 block9 1 relu Activation 3 23 35 128 0 conv4 block9 1 bn 0 0 conv4 block9 2 conv Conv2D 3 23 35 32 36864 conv4 block9 1 relu 0 0 conv4 block9 concat Concatenat 3 23 35 544 0 conv4 block8 concat 0 0 conv4 block9 2 conv 0 0 conv4 block10 0 bn BatchNormal 3 23 35 544 2176 conv4 block9 concat 0 0 conv4 block10 0 relu Activatio 3 23 35 544 0 conv4 block10 0 bn 0 0 conv4 block10 1 conv Conv2D 3 23 35 128 69632 conv4 block10 0 relu 0 0 conv4 block10 1 bn BatchNormal 3 23 35 128 512 conv4 block10 1 conv 0 0 conv4 block10 1 relu Activatio 3 23 35 128 0 conv4 block10 1 bn 0 0 conv4 block10 2 conv Conv2D 3 23 35 32 36864 conv4 block10 1 relu 0 0 conv4 block10 concat Concatena 3 23 35 576 0 conv4 block9 concat 0 0 conv4 block10 2 conv 0 0 conv4 block11 0 bn BatchNormal 3 23 35 576 2304 conv4 block10 concat 0 0 conv4 block11 0 relu Activatio 3 23 35 576 0 conv4 block11 0 bn 0 0 conv4 block11 1 conv Conv2D 3 23 35 128 73728 conv4 block11 0 relu 0 0 conv4 block11 1 bn BatchNormal 3 23 35 128 512 conv4 block11 1 conv 0 0 conv4 block11 1 relu Activatio 3 23 35 128 0 conv4 block11 1 bn 0 0 conv4 block11 2 conv Conv2D 3 23 35 32 36864 conv4 block11 1 relu 0 0 conv4 block11 concat Concatena 3 23 35 608 0 conv4 block10 concat 0 0 conv4 block11 2 conv 0 0 conv4 block12 0 bn BatchNormal 3 23 35 608 2432 conv4 block11 concat 0 0 conv4 block12 0 relu Activatio 3 23 35 608 0 conv4 block12 0 bn 0 0 conv4 block12 1 conv Conv2D 3 23 35 128 77824 conv4 block12 0 relu 0 0 conv4 block12 1 bn BatchNormal 3 23 35 128 512 conv4 block12 1 conv 0 0 conv4 block12 1 relu Activatio 3 23 35 128 0 conv4 block12 1 bn 0 0 conv4 block12 2 conv Conv2D 3 23 35 32 36864 conv4 block12 1 relu 0 0 conv4 block12 concat Concatena 3 23 35 640 0 conv4 block11 concat 0 0 conv4 block12 2 conv 0 0 conv4 block13 0 bn BatchNormal 3 23 35 640 2560 conv4 block12 concat 0 0 conv4 block13 0 relu Activatio 3 23 35 640 0 conv4 block13 0 bn 0 0 conv4 block13 1 conv Conv2D 3 23 35 128 81920 conv4 block13 0 relu 0 0 conv4 block13 1 bn BatchNormal 3 23 35 128 512 conv4 block13 1 conv 0 0 conv4 block13 1 relu Activatio 3 23 35 128 0 conv4 block13 1 bn 0 0 conv4 block13 2 conv Conv2D 3 23 35 32 36864 conv4 block13 1 relu 0 0 conv4 block13 concat Concatena 3 23 35 672 0 conv4 block12 concat 0 0 conv4 block13 2 conv 0 0 conv4 block14 0 bn BatchNormal 3 23 35 672 2688 conv4 block13 concat 0 0 conv4 block14 0 relu Activatio 3 23 35 672 0 conv4 block14 0 bn 0 0 conv4 block14 1 conv Conv2D 3 23 35 128 86016 conv4 block14 0 relu 0 0 conv4 block14 1 bn BatchNormal 3 23 35 128 512 conv4 block14 1 conv 0 0 conv4 block14 1 relu Activatio 3 23 35 128 0 conv4 block14 1 bn 0 0 conv4 block14 2 conv Conv2D 3 23 35 32 36864 conv4 block14 1 relu 0 0 conv4 block14 concat Concatena 3 23 35 704 0 conv4 block13 concat 0 0 conv4 block14 2 conv 0 0 conv4 block15 0 bn BatchNormal 3 23 35 704 2816 conv4 block14 concat 0 0 conv4 block15 0 relu Activatio 3 23 35 704 0 conv4 block15 0 bn 0 0 conv4 block15 1 conv Conv2D 3 23 35 128 90112 conv4 block15 0 relu 0 0 conv4 block15 1 bn BatchNormal 3 23 35 128 512 conv4 block15 1 conv 0 0 conv4 block15 1 relu Activatio 3 23 35 128 0 conv4 block15 1 bn 0 0 conv4 block15 2 conv Conv2D 3 23 35 32 36864 conv4 block15 1 relu 0 0 conv4 block15 concat Concatena 3 23 35 736 0 conv4 block14 concat 0 0 conv4 block15 2 conv 0 0 conv4 block16 0 bn BatchNormal 3 23 35 736 2944 conv4 block15 concat 0 0 conv4 block16 0 relu Activatio 3 23 35 736 0 conv4 block16 0 bn 0 0 conv4 block16 1 conv Conv2D 3 23 35 128 94208 conv4 block16 0 relu 0 0 conv4 block16 1 bn BatchNormal 3 23 35 128 512 conv4 block16 1 conv 0 0 conv4 block16 1 relu Activatio 3 23 35 128 0 conv4 block16 1 bn 0 0 conv4 block16 2 conv Conv2D 3 23 35 32 36864 conv4 block16 1 relu 0 0 conv4 block16 concat Concatena 3 23 35 768 0 conv4 block15 concat 0 0 conv4 block16 2 conv 0 0 conv4 block17 0 bn BatchNormal 3 23 35 768 3072 conv4 block16 concat 0 0 conv4 block17 0 relu Activatio 3 23 35 768 0 conv4 block17 0 bn 0 0 conv4 block17 1 conv Conv2D 3 23 35 128 98304 conv4 block17 0 relu 0 0 conv4 block17 1 bn BatchNormal 3 23 35 128 512 conv4 block17 1 conv 0 0 conv4 block17 1 relu Activatio 3 23 35 128 0 conv4 block17 1 bn 0 0 conv4 block17 2 conv Conv2D 3 23 35 32 36864 conv4 block17 1 relu 0 0 conv4 block17 concat Concatena 3 23 35 800 0 conv4 block16 concat 0 0 conv4 block17 2 conv 0 0 conv4 block18 0 bn BatchNormal 3 23 35 800 3200 conv4 block17 concat 0 0 conv4 block18 0 relu Activatio 3 23 35 800 0 conv4 block18 0 bn 0 0 conv4 block18 1 conv Conv2D 3 23 35 128 102400 conv4 block18 0 relu 0 0 conv4 block18 1 bn BatchNormal 3 23 35 128 512 conv4 block18 1 conv 0 0 conv4 block18 1 relu Activatio 3 23 35 128 0 conv4 block18 1 bn 0 0 conv4 block18 2 conv Conv2D 3 23 35 32 36864 conv4 block18 1 relu 0 0 conv4 block18 concat Concatena 3 23 35 832 0 conv4 block17 concat 0 0 conv4 block18 2 conv 0 0 conv4 block19 0 bn BatchNormal 3 23 35 832 3328 conv4 block18 concat 0 0 conv4 block19 0 relu Activatio 3 23 35 832 0 conv4 block19 0 bn 0 0 conv4 block19 1 conv Conv2D 3 23 35 128 106496 conv4 block19 0 relu 0 0 conv4 block19 1 bn BatchNormal 3 23 35 128 512 conv4 block19 1 conv 0 0 conv4 block19 1 relu Activatio 3 23 35 128 0 conv4 block19 1 bn 0 0 conv4 block19 2 conv Conv2D 3 23 35 32 36864 conv4 block19 1 relu 0 0 conv4 block19 concat Concatena 3 23 35 864 0 conv4 block18 concat 0 0 conv4 block19 2 conv 0 0 conv4 block20 0 bn BatchNormal 3 23 35 864 3456 conv4 block19 concat 0 0 conv4 block20 0 relu Activatio 3 23 35 864 0 conv4 block20 0 bn 0 0 conv4 block20 1 conv Conv2D 3 23 35 128 110592 conv4 block20 0 relu 0 0 conv4 block20 1 bn BatchNormal 3 23 35 128 512 conv4 block20 1 conv 0 0 conv4 block20 1 relu Activatio 3 23 35 128 0 conv4 block20 1 bn 0 0 conv4 block20 2 conv Conv2D 3 23 35 32 36864 conv4 block20 1 relu 0 0 conv4 block20 concat Concatena 3 23 35 896 0 conv4 block19 concat 0 0 conv4 block20 2 conv 0 0 conv4 block21 0 bn BatchNormal 3 23 35 896 3584 conv4 block20 concat 0 0 conv4 block21 0 relu Activatio 3 23 35 896 0 conv4 block21 0 bn 0 0 conv4 block21 1 conv Conv2D 3 23 35 128 114688 conv4 block21 0 relu 0 0 conv4 block21 1 bn BatchNormal 3 23 35 128 512 conv4 block21 1 conv 0 0 conv4 block21 1 relu Activatio 3 23 35 128 0 conv4 block21 1 bn 0 0 conv4 block21 2 conv Conv2D 3 23 35 32 36864 conv4 block21 1 relu 0 0 conv4 block21 concat Concatena 3 23 35 928 0 conv4 block20 concat 0 0 conv4 block21 2 conv 0 0 conv4 block22 0 bn BatchNormal 3 23 35 928 3712 conv4 block21 concat 0 0 conv4 block22 0 relu Activatio 3 23 35 928 0 conv4 block22 0 bn 0 0 conv4 block22 1 conv Conv2D 3 23 35 128 118784 conv4 block22 0 relu 0 0 conv4 block22 1 bn BatchNormal 3 23 35 128 512 conv4 block22 1 conv 0 0 conv4 block22 1 relu Activatio 3 23 35 128 0 conv4 block22 1 bn 0 0 conv4 block22 2 conv Conv2D 3 23 35 32 36864 conv4 block22 1 relu 0 0 conv4 block22 concat Concatena 3 23 35 960 0 conv4 block21 concat 0 0 conv4 block22 2 conv 0 0 conv4 block23 0 bn BatchNormal 3 23 35 960 3840 conv4 block22 concat 0 0 conv4 block23 0 relu Activatio 3 23 35 960 0 conv4 block23 0 bn 0 0 conv4 block23 1 conv Conv2D 3 23 35 128 122880 conv4 block23 0 relu 0 0 conv4 block23 1 bn BatchNormal 3 23 35 128 512 conv4 block23 1 conv 0 0 conv4 block23 1 relu Activatio 3 23 35 128 0 conv4 block23 1 bn 0 0 conv4 block23 2 conv Conv2D 3 23 35 32 36864 conv4 block23 1 relu 0 0 conv4 block23 concat Concatena 3 23 35 992 0 conv4 block22 concat 0 0 conv4 block23 2 conv 0 0 conv4 block24 0 bn BatchNormal 3 23 35 992 3968 conv4 block23 concat 0 0 conv4 block24 0 relu Activatio 3 23 35 992 0 conv4 block24 0 bn 0 0 conv4 block24 1 conv Conv2D 3 23 35 128 126976 conv4 block24 0 relu 0 0 conv4 block24 1 bn BatchNormal 3 23 35 128 512 conv4 block24 1 conv 0 0 conv4 block24 1 relu Activatio 3 23 35 128 0 conv4 block24 1 bn 0 0 conv4 block24 2 conv Conv2D 3 23 35 32 36864 conv4 block24 1 relu 0 0 conv4 block24 concat Concatena 3 23 35 1024 0 conv4 block23 concat 0 0 conv4 block24 2 conv 0 0 pool4 bn BatchNormalization 3 23 35 1024 4096 conv4 block24 concat 0 0 pool4 relu Activation 3 23 35 1024 0 pool4 bn 0 0 pool4 conv Conv2D 3 23 35 512 524288 pool4 relu 0 0 pool4 pool AveragePooling2D 3 11 17 512 0 pool4 conv 0 0 conv5 block1 0 bn BatchNormali 3 11 17 512 2048 pool4 pool 0 0 conv5 block1 0 relu Activation 3 11 17 512 0 conv5 block1 0 bn 0 0 conv5 block1 1 conv Conv2D 3 11 17 128 65536 conv5 block1 0 relu 0 0 conv5 block1 1 bn BatchNormali 3 11 17 128 512 conv5 block1 1 conv 0 0 conv5 block1 1 relu Activation 3 11 17 128 0 conv5 block1 1 bn 0 0 conv5 block1 2 conv Conv2D 3 11 17 32 36864 conv5 block1 1 relu 0 0 conv5 block1 concat Concatenat 3 11 17 544 0 pool4 pool 0 0 conv5 block1 2 conv 0 0 conv5 block2 0 bn BatchNormali 3 11 17 544 2176 conv5 block1 concat 0 0 conv5 block2 0 relu Activation 3 11 17 544 0 conv5 block2 0 bn 0 0 conv5 block2 1 conv Conv2D 3 11 17 128 69632 conv5 block2 0 relu 0 0 conv5 block2 1 bn BatchNormali 3 11 17 128 512 conv5 block2 1 conv 0 0 conv5 block2 1 relu Activation 3 11 17 128 0 conv5 block2 1 bn 0 0 conv5 block2 2 conv Conv2D 3 11 17 32 36864 conv5 block2 1 relu 0 0 conv5 block2 concat Concatenat 3 11 17 576 0 conv5 block1 concat 0 0 conv5 block2 2 conv 0 0 conv5 block3 0 bn BatchNormali 3 11 17 576 2304 conv5 block2 concat 0 0 conv5 block3 0 relu Activation 3 11 17 576 0 conv5 block3 0 bn 0 0 conv5 block3 1 conv Conv2D 3 11 17 128 73728 conv5 block3 0 relu 0 0 conv5 block3 1 bn BatchNormali 3 11 17 128 512 conv5 block3 1 conv 0 0 conv5 block3 1 relu Activation 3 11 17 128 0 conv5 block3 1 bn 0 0 conv5 block3 2 conv Conv2D 3 11 17 32 36864 conv5 block3 1 relu 0 0 conv5 block3 concat Concatenat 3 11 17 608 0 conv5 block2 concat 0 0 conv5 block3 2 conv 0 0 conv5 block4 0 bn BatchNormali 3 11 17 608 2432 conv5 block3 concat 0 0 conv5 block4 0 relu Activation 3 11 17 608 0 conv5 block4 0 bn 0 0 conv5 block4 1 conv Conv2D 3 11 17 128 77824 conv5 block4 0 relu 0 0 conv5 block4 1 bn BatchNormali 3 11 17 128 512 conv5 block4 1 conv 0 0 conv5 block4 1 relu Activation 3 11 17 128 0 conv5 block4 1 bn 0 0 conv5 block4 2 conv Conv2D 3 11 17 32 36864 conv5 block4 1 relu 0 0 conv5 block4 concat Concatenat 3 11 17 640 0 conv5 block3 concat 0 0 conv5 block4 2 conv 0 0 conv5 block5 0 bn BatchNormali 3 11 17 640 2560 conv5 block4 concat 0 0 conv5 block5 0 relu Activation 3 11 17 640 0 conv5 block5 0 bn 0 0 conv5 block5 1 conv Conv2D 3 11 17 128 81920 conv5 block5 0 relu 0 0 conv5 block5 1 bn BatchNormali 3 11 17 128 512 conv5 block5 1 conv 0 0 conv5 block5 1 relu Activation 3 11 17 128 0 conv5 block5 1 bn 0 0 conv5 block5 2 conv Conv2D 3 11 17 32 36864 conv5 block5 1 relu 0 0 conv5 block5 concat Concatenat 3 11 17 672 0 conv5 block4 concat 0 0 conv5 block5 2 conv 0 0 conv5 block6 0 bn BatchNormali 3 11 17 672 2688 conv5 block5 concat 0 0 conv5 block6 0 relu Activation 3 11 17 672 0 conv5 block6 0 bn 0 0 conv5 block6 1 conv Conv2D 3 11 17 128 86016 conv5 block6 0 relu 0 0 conv5 block6 1 bn BatchNormali 3 11 17 128 512 conv5 block6 1 conv 0 0 conv5 block6 1 relu Activation 3 11 17 128 0 conv5 block6 1 bn 0 0 conv5 block6 2 conv Conv2D 3 11 17 32 36864 conv5 block6 1 relu 0 0 conv5 block6 concat Concatenat 3 11 17 704 0 conv5 block5 concat 0 0 conv5 block6 2 conv 0 0 conv5 block7 0 bn BatchNormali 3 11 17 704 2816 conv5 block6 concat 0 0 conv5 block7 0 relu Activation 3 11 17 704 0 conv5 block7 0 bn 0 0 conv5 block7 1 conv Conv2D 3 11 17 128 90112 conv5 block7 0 relu 0 0 conv5 block7 1 bn BatchNormali 3 11 17 128 512 conv5 block7 1 conv 0 0 conv5 block7 1 relu Activation 3 11 17 128 0 conv5 block7 1 bn 0 0 conv5 block7 2 conv Conv2D 3 11 17 32 36864 conv5 block7 1 relu 0 0 conv5 block7 concat Concatenat 3 11 17 736 0 conv5 block6 concat 0 0 conv5 block7 2 conv 0 0 conv5 block8 0 bn BatchNormali 3 11 17 736 2944 conv5 block7 concat 0 0 conv5 block8 0 relu Activation 3 11 17 736 0 conv5 block8 0 bn 0 0 conv5 block8 1 conv Conv2D 3 11 17 128 94208 conv5 block8 0 relu 0 0 conv5 block8 1 bn BatchNormali 3 11 17 128 512 conv5 block8 1 conv 0 0 conv5 block8 1 relu Activation 3 11 17 128 0 conv5 block8 1 bn 0 0 conv5 block8 2 conv Conv2D 3 11 17 32 36864 conv5 block8 1 relu 0 0 conv5 block8 concat Concatenat 3 11 17 768 0 conv5 block7 concat 0 0 conv5 block8 2 conv 0 0 conv5 block9 0 bn BatchNormali 3 11 17 768 3072 conv5 block8 concat 0 0 conv5 block9 0 relu Activation 3 11 17 768 0 conv5 block9 0 bn 0 0 conv5 block9 1 conv Conv2D 3 11 17 128 98304 conv5 block9 0 relu 0 0 conv5 block9 1 bn BatchNormali 3 11 17 128 512 conv5 block9 1 conv 0 0 conv5 block9 1 relu Activation 3 11 17 128 0 conv5 block9 1 bn 0 0 conv5 block9 2 conv Conv2D 3 11 17 32 36864 conv5 block9 1 relu 0 0 conv5 block9 concat Concatenat 3 11 17 800 0 conv5 block8 concat 0 0 conv5 block9 2 conv 0 0 conv5 block10 0 bn BatchNormal 3 11 17 800 3200 conv5 block9 concat 0 0 conv5 block10 0 relu Activatio 3 11 17 800 0 conv5 block10 0 bn 0 0 conv5 block10 1 conv Conv2D 3 11 17 128 102400 conv5 block10 0 relu 0 0 conv5 block10 1 bn BatchNormal 3 11 17 128 512 conv5 block10 1 conv 0 0 conv5 block10 1 relu Activatio 3 11 17 128 0 conv5 block10 1 bn 0 0 conv5 block10 2 conv Conv2D 3 11 17 32 36864 conv5 block10 1 relu 0 0 conv5 block10 concat Concatena 3 11 17 832 0 conv5 block9 concat 0 0 conv5 block10 2 conv 0 0 conv5 block11 0 bn BatchNormal 3 11 17 832 3328 conv5 block10 concat 0 0 conv5 block11 0 relu Activatio 3 11 17 832 0 conv5 block11 0 bn 0 0 conv5 block11 1 conv Conv2D 3 11 17 128 106496 conv5 block11 0 relu 0 0 conv5 block11 1 bn BatchNormal 3 11 17 128 512 conv5 block11 1 conv 0 0 conv5 block11 1 relu Activatio 3 11 17 128 0 conv5 block11 1 bn 0 0 conv5 block11 2 conv Conv2D 3 11 17 32 36864 conv5 block11 1 relu 0 0 conv5 block11 concat Concatena 3 11 17 864 0 conv5 block10 concat 0 0 conv5 block11 2 conv 0 0 conv5 block12 0 bn BatchNormal 3 11 17 864 3456 conv5 block11 concat 0 0 conv5 block12 0 relu Activatio 3 11 17 864 0 conv5 block12 0 bn 0 0 conv5 block12 1 conv Conv2D 3 11 17 128 110592 conv5 block12 0 relu 0 0 conv5 block12 1 bn BatchNormal 3 11 17 128 512 conv5 block12 1 conv 0 0 conv5 block12 1 relu Activatio 3 11 17 128 0 conv5 block12 1 bn 0 0 conv5 block12 2 conv Conv2D 3 11 17 32 36864 conv5 block12 1 relu 0 0 conv5 block12 concat Concatena 3 11 17 896 0 conv5 block11 concat 0 0 conv5 block12 2 conv 0 0 conv5 block13 0 bn BatchNormal 3 11 17 896 3584 conv5 block12 concat 0 0 conv5 block13 0 relu Activatio 3 11 17 896 0 conv5 block13 0 bn 0 0 conv5 block13 1 conv Conv2D 3 11 17 128 114688 conv5 block13 0 relu 0 0 conv5 block13 1 bn BatchNormal 3 11 17 128 512 conv5 block13 1 conv 0 0 conv5 block13 1 relu Activatio 3 11 17 128 0 conv5 block13 1 bn 0 0 conv5 block13 2 conv Conv2D 3 11 17 32 36864 conv5 block13 1 relu 0 0 conv5 block13 concat Concatena 3 11 17 928 0 conv5 block12 concat 0 0 conv5 block13 2 conv 0 0 conv5 block14 0 bn BatchNormal 3 11 17 928 3712 conv5 block13 concat 0 0 conv5 block14 0 relu Activatio 3 11 17 928 0 conv5 block14 0 bn 0 0 conv5 block14 1 conv Conv2D 3 11 17 128 118784 conv5 block14 0 relu 0 0 conv5 block14 1 bn BatchNormal 3 11 17 128 512 conv5 block14 1 conv 0 0 conv5 block14 1 relu Activatio 3 11 17 128 0 conv5 block14 1 bn 0 0 conv5 block14 2 conv Conv2D 3 11 17 32 36864 conv5 block14 1 relu 0 0 conv5 block14 concat Concatena 3 11 17 960 0 conv5 block13 concat 0 0 conv5 block14 2 conv 0 0 conv5 block15 0 bn BatchNormal 3 11 17 960 3840 conv5 block14 concat 0 0 conv5 block15 0 relu Activatio 3 11 17 960 0 conv5 block15 0 bn 0 0 conv5 block15 1 conv Conv2D 3 11 17 128 122880 conv5 block15 0 relu 0 0 conv5 block15 1 bn BatchNormal 3 11 17 128 512 conv5 block15 1 conv 0 0 conv5 block15 1 relu Activatio 3 11 17 128 0 conv5 block15 1 bn 0 0 conv5 block15 2 conv Conv2D 3 11 17 32 36864 conv5 block15 1 relu 0 0 conv5 block15 concat Concatena 3 11 17 992 0 conv5 block14 concat 0 0 conv5 block15 2 conv 0 0 conv5 block16 0 bn BatchNormal 3 11 17 992 3968 conv5 block15 concat 0 0 conv5 block16 0 relu Activatio 3 11 17 992 0 conv5 block16 0 bn 0 0 conv5 block16 1 conv Conv2D 3 11 17 128 126976 conv5 block16 0 relu 0 0 conv5 block16 1 bn BatchNormal 3 11 17 128 512 conv5 block16 1 conv 0 0 conv5 block16 1 relu Activatio 3 11 17 128 0 conv5 block16 1 bn 0 0 conv5 block16 2 conv Conv2D 3 11 17 32 36864 conv5 block16,
1 relu 0 0 conv5 block16 concat Concatena 3 11 17 1024 0 conv5 block15 concat 0 0 conv5 block16 2 conv 0 0 bn BatchNormalization 3 11 17 1024 4096 conv5 block16 concat 0 0 relu Activation 3 11 17 1024 0 bn 0 0 avg pool GlobalAveragePooling2 3 1024 0 relu 0 0 fc1000 Dense 3 10 10250 avg pool 0 0 Total params 7 047 754 Trainable params 6 964 106 Non trainable params 83 648 Train for 100 steps validate for 10 steps 2019 11 06 11 12 15.235702 I tensorflow stream executor platform default dso loader.cc 44 Successfully opened dynamiclibrary libcublas.so.10.0 shape TensorShape 12 372 558 3 12 372 558 3 2019 11 06 11 12 15.481528 W tensorflow core framework op kernel.cc 1622 OP REQUIRES failed at strided slice op.cc 108 Invalid argument slice index 10 of dimension 0 out of bounds. shape TensorShape 12 372 558 3 12 372 558 3 2019 11 06 11 12 15.485747 W tensorflow core framework op kernel.cc 1622 OP REQUIRES failed at strided slice op.cc 108 Invalid argument slice index 10 of dimension 0 out of bounds. shape TensorShape 12 372 558 3 12 372 558 3 2019 11 06 11 12 15.488817 W tensorflow core framework op kernel.cc 1622 OP REQUIRES failed at strided slice op.cc 108 Invalid argument slice index 10 of dimension 0 out of bounds. 2019 11 06 11 12 15.489183 W tensorflow core common runtime base collective executor.cc 216 BaseCollectiveExecutor StartAbort Invalid argument function node inference Dataset map batch print 35 slice index 10 of dimension 0 out of bounds. node strided slice MultiDeviceIteratorGetNextFromShard RemoteCall IteratorGetNext 2 Identity 4 188 2019 11 06 11 12 15.489398 W tensorflow core common runtime base collective executor.cc 216 BaseCollectiveExecutor StartAbort Invalid argument function node inference Dataset map batch print 35 slice index 10 of dimension 0 out of bounds. node strided slice MultiDeviceIteratorGetNextFromShard RemoteCall IteratorGetNext 2 shape TensorShape 12 372 558 3 12 372 558 3 2019 11 06 11 12 15.494247 W tensorflow core framework op kernel.cc 1622 OP REQUIRES failed at strided slice op.cc 108 Invalid argument slice index 10 of dimension 0 out of bounds. 2019 11 06 11 12 15.887854 W tensorflow core common runtime base collective executor.cc 216 BaseCollectiveExecutor StartAbort Invalid argument function node inference Dataset map batch print 35 slice index 10 of dimension 0 out of bounds. node strided slice MultiDeviceIteratorGetNextFromShard RemoteCall IteratorGetNext 2 replica 2 metrics accuracy AssignAddVariableOp 1 39 1 100 .............................. ETA 3 57 11Traceback most recent call last File user vmarkovtsev images efficientoffice efficientoffice shape bug.py line 45 in module sys.exit main File user vmarkovtsev images efficientoffice efficientoffice shape bug.py line 41 in main model.fit ds train validation data ds val epochs 1 steps per epoch 100 File usr local lib python3.6 dist packages tensorflow core python keras engine training.py line 728 in fit use multiprocessing use multiprocessing File usr local lib python3.6 dist packages tensorflow core python keras engine training v2.py line 324 in fit total epochs epochs File usr local lib python3.6 dist packages tensorflow core python keras engine training v2.py line 123 in run one epoch batch outs execution function iterator File usr local lib python3.6 dist packages tensorflow core python keras engine training v2 utils.py line 86 in execution function distributed function input fn File usr local lib python3.6 dist packages tensorflow core python eager def function.py line 457 in call result self. call args kwds File usr local lib python3.6 dist packages tensorflow core python eager def function.py line 520 in call return self. stateless fn args kwds File usr local lib python3.6 dist packages tensorflow core python eager function.py line 1823 in call return graph function. filtered call args kwargs pylint disable protected access File usr local lib python3.6 dist packages tensorflow core python eager function.py line 1141 in filtered call self.captured inputs File usr local lib python3.6 dist packages tensorflow core python eager function.py line 1224 in call flat ctx args cancellation manager cancellation manager File usr local lib python3.6 dist packages tensorflow core python eager function.py line 511 in call ctx ctx File usr local lib python3.6 dist packages tensorflow core python eager execute.py line 67 in quick execute six.raise from core. status to exception e.code message None File string line 3 in raise from tensorflow.python.framework.errors impl.InvalidArgumentError 4 root error s found. 0 Invalid argument slice index 10 of dimension 0 out of bounds. node strided slice defined at local lib python3.6 dist packages tensorflow core python framework ops.py 1751 MultiDeviceIteratorGetNextFromShard RemoteCall IteratorGetNext 2 Identity 4 188 1 Invalid argument slice index 10 of dimension 0 out of bounds. node strided slice defined at local lib python3.6 dist packages tensorflow core python framework ops.py 1751 MultiDeviceIteratorGetNextFromShard RemoteCall IteratorGetNext 2 2 Cancelled 3 Cancelled 0 successful operations. 1 derived errors ignored. Op inference distributed function 166689 Function call stack distributed function distributed function distributed function distributed function distributed function distributed function pre details This is how the log ends the crash This is why the bug is so spicy both the static and dynamic shapes are 12 but if you try to access an element under index 3 3 12 4 you crash. I am really interested in why. If you remove drop remainder True the code works.,This has to do with how tf.data rebatches datasets for distribution strategy. We recently changed the implementation of rebatching such that this will no longer happen the fix will be available in TF 2.1. Let us know if this is still an issue. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34039 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34039 No a ,,
34043,Unsupported numpy type NPY LONGLONG, System information Linux Mint 19 Dell XPS 7590 laptop Python 3.7.4 Anaconda Tensorflow version v2.0.0 rc2 26 g64c3d38 2.0.0 pip tensorflow gpu with GPU disabled Numpy version 1.17.3 Describe the current behavior Tensorflow 2.0 raises a ValueError if a Numpy array of type np.longlong is converted to a tf.Tensor. Describe the expected behavior A np.array dtype np.longlong should automatically convert to tf.Tensor dtype np.int64 as was the case in Tensorflow 1.15. Code to reproduce the issue , chrism0dwk Thanks for reporting this issue. Issue is replicating on colab with Tf 2.0 and it is working as expected in Tf 1.15. Please take a look at gist https colab.sandbox.google.com gist gadagashwini c645c20cf82e371fbfd71690808dc9b8 untitled242.ipynb . Thanks I tried with tf nightly tf nightly 2.1.0.dev20191106 and it looks to be fine. Think this issue has been resolved. Glad to know it s in the pipeline for the next release. Thanks all. Closing this issue since its resolved with tf nightly. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34043 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34043 No a I am facing with this issue. What is the solution Using tf nighty 
34055,model.reset states does not work for bidirectional RNNs in tf.keras., System information Have I written custom code as opposed to using a stock example script provided in TensorFlow YES . OS Platform and Distribution e.g. Linux Ubuntu 16.04 macOS 10.14.6 TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 Python version 3.7.4 GPU model and memory none MacBook Pro Core i5 Iris Graphics 6100 1.5 GB Describe the current behavior State handling in RNNs with a Bidirectional wrapper has changed in tf.keras from keras with TF 1.x. In the old keras with TF 1.x using stateful True in a bidi RNN had no effect i.e. all bidi RNN models behaved as if stateful False . Therefore model.reset states did not do anything. In the new tf.keras stateful True in a bidi RNN does have an effect the fwd RNN is stateful and the bwd RNN is stateful. This is a good change IMO even though stateful bidi RNNs are unusual this is the best way to implement. However in tf.keras the model.reset states does not do anything for bidi RNN models SimpleRNN GRU LSTM . Describe the expected behavior For the minimal example script provided below here is the output The results after the STATE RESET should be the same as the first set of results i.e. the last third set of results should produce the same result for the stateful and non stateful models same as the first set of results . Code to reproduce the issue ,Note this is an issue with tf.keras vs. keras not TF 1.x vs TF 2.0 I could replicate issue on colab with Tf 2.0. Please take a look at gist https colab.sandbox.google.com gist gadagashwini 0d4771e1d00b215c8f06577915bf7fef untitled246.ipynb . Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34055 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34055 No a 
34068,TF2 keras.models.load model fails with custom metrics both h5 and tf format , Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 macOS 10.13.6 TensorFlow installed from source or binary pip install tensorflow 2.0.0 beta1 TensorFlow version use command below v2.0.0 rc2 26 g64c3d382ca 2.0.0 Python version v3.6.7 6ec5cf24b7 Oct 20 2018 03 02 14 Describe the current behavior I have a custom metric in my model and using tf.keras.models.load model with compile True after saving it results in an error in almost all cases whereas I use the custom objects argument according to the documentation https www.tensorflow.org versions r2.0 api docs python tf keras models load model . I tried to pass my custom metric with two strategies by passing a custom function custom accuracy to the tf.keras.Model.compile method or by subclassing the MeanMetricWrapper class and giving an instance of my subclass named CustomAccuracy to tf.keras.Model.compile . I also tried the two different saving format available h5 and tf. Here are my results with tf format with custom function fail with ValueError message Unknown metric function custom accuracy with subclassed metric fail with ValueError message Unknown metric function CustomAccuracy with h5 format with custom function success with subclassed metric fail with TypeError message must be str not ABCMeta Note that given the complete error logs see below the error with h5 format and subclassed metric is in fact the same as the error with the tf format. The TypeError occurs when the code tries to raise the ValueError . Describe the expected behavior This should not fail in any case except if I am using the custom objects argument wrong. The documentation could be a little expanded on that matter by the way. Code to reproduce the issue Other info logs The logs are the same in the 3 error cases to get them with the code above just add raise at the end of the except blocks , durandg12 I tried reproducing the code in colab using TF 2.0 beta1 TF 2.0 and i am seeing different error messages. Kindly provide minimal stand alone reproducible code it helps us in localizing the issue faster.Please find the gist here. https colab.sandbox.google.com gist ravikyram 1780a9fe7e8d0e39cf52ba6e973d22e6 untitled342.ipynb Thanks ravikyram I have looked at your gist. The error messages in your gist for tf2.0.0 are exactly the same as mine. There is also a deprecation warning that I have too but that I hadn t copied in my first message because it didn t seem relevant. For tf2.0.0 beta1 the error message is effectively different but it comes from the compile method because I call it without an optimizer argument. I have added optimizer adam in my compile call and now the output is the same for 2.0.0 and 2.0.0 beta1. So the code now looks like this I think that my code was already minimal as it just 1. created the simplest custom accuracy possible 2. created the simplest MLP possible 3. compiled the MLP with the custom accuracy 4. saved the MLP 5. loaded the MLP I don t know how I can make it simpler. Except if you want the same piece of code but without the print calls and without the try and except blocks. In this case here it is but you have to manually comment or uncomment some parts if you want to observe all four cases. Ironically adding an optimizer for tf2.0.0 beta1 makes the code less minimal. durandg12 Thanks for the detailed explanation. After adding optimizer adam in compile call i am able to reproduce the same error message in both TF 2.0.0 and 2.0.0 beta1. Please find the gist here https colab.sandbox.google.com gist ravikyram 17024d04ca0bc9a7e28cec4244715c7a untitled355.ipynb .Thanks durandg12 Thanks for the detailed report. As of now there is no solution available. There are some workarounds suggested here https github.com tensorflow tensorflow issues 32612 . There is a PR https github.com tensorflow tensorflow pull 33229 to resolve an issue similar to this issue. Please follow the PR and test it once it is approved and released in tf nightly . Thanks I have reviewed the issue you linked. It seems to be the same problem indeed. I had also found the workaround of loading without compile but as somedadaism said this post https github.com tensorflow tensorflow issues 32612 issuecomment 534248672 it is not satisfying. So right now the best workaround is to use a custom function and pass it to the compile method and not subclassing MeanMetricWrapper . But this only worked with h5 format and not tf format for which I don t find a satisfying workaround. jvishnuvardhan I did not try the PR yet I am not sure how to do it. Am I supposed to create a new virualenv and install tf nightly in it How do I know when the PR is approved and released durandg12 If you have a solution to an issue in Tensorflow you can raise PR by going here https github.com tensorflow tensorflow pulls . You can edit related TF source code with your solution test it locally then checkit into PR. Tensorflow Team will review it and responds. If everything is looking good then it will be approved and then merged into TF source code. This https www.tensorflow.org community contribute is a very good resource to start contributing. Hope this helps. Thanks jvishnuvardhan my question was more focused on your last sentence as I know what is a PR in general. My question is how do I do this Please follow the PR and test it once it is approved and released in tf nightly I see that the PR is actually awaiting review so it is not approved yet. Once it is approved what steps do I need to follow Can you confirm that I just have to set a new virtual env up run pip install tf nightly and then try my example code Once it is approved you don t need to do anything. After approval it will be merged into tf nightly . During the approval process Reviewer will guide you through the steps if any required. Thanks durandg12 As of now https github.com tensorflow tensorflow pull 33229 was approved but not merged. Once it is merged you can use tf nightly to test it. Thanks durandg12 Can you try tf nightly tomorrow as the related PR merged. Please let us know whether it solved your issue or not. Thanks durandg12 Looks like load model is working for both the cases when the model is saved in h5 format. Here https colab.sandbox.google.com gist jvishnuvardhan 95b9d359762b766dba337e0cfa0c248b untitled355.ipynb is the gist. Thanks Both the cases are still failing when the model was saved in tf format. Thanks I have seen your gist and after installing tf nightly I have been able to replicate it on my laptop thank you. The only small difference I see is that locally I have an additional warning WARNING Logging before flag parsing goes to stderr. This is fixed latest tf nightly version 2.2.0 dev20200123 . Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34068 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34068 No a I have tested and the issue is indeed fixed. But we shall note that the tf mode still raises a warning Hello Was this ever solved for saving loading custom metrics in SavedModel format opposed to .h5 I had the same issue only my error was Unable to restore custom object of type tf keras metric currently. Please make sure that the layer implements get config and from config when saving. In addition please use the custom objects arg when calling load model . I had subclassed MeanMetricWrapper so it couldn t possibly have been a lack of implementing get config and from config and I had already made up the custom objects dict which had Everything was referenced correctly in the main script model would run manually and through hyperparameter searches but I kept getting this error whenever I tried loading the saved TF model. I then switched to saving loading an H5 model instead and got an error stating that MeanAbsoluteScaledErrorMetric wasn t included in custom objects . So I updated it to ...and then it worked. I then switched back to the TF model and it kept working. So in the end I suppose somewhere in the loader it s not respecting the key value relationship in custom objects and only looking for the class name in the keys. 
34070,Keras named inputs lost with SavedModel format, System information Have I written custom code yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Google Colab TF 2.0 Describe the current behavior If I define a Keras model with named inputs Input name x embedding I can use a Dict as input with the original model or after saving to h5 but not with the new SavedModel format The name of my input layer is changed from x embedding to input 1 . Describe the expected behavior The model loaded from SavedModel should keep the named inputs. Code to reproduce the issue https colab.research.google.com drive 19ICXHeL4tzAN9z1LCi 3Su8l5X2 NhxM Other info logs ValueError No data provided for input 1 . Need data for each key in input 1 ,Still an issue with tf nightly 2.1.0.dev20191126 louisjc This is fixed with the latest tf nightly 1 22 here is an updated gist https colab.research.google.com drive 19ICXHeL4tzAN9z1LCi 3Su8l5X2 NhxM usp sharing Great thanks goldiegadde Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34070 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34070 No a 
34073,Problem with fit generator when using a generator for training and a fixed set for validation., System information I have written custom code. Code is available here as a Google Colab notebook https drive.google.com open id 1xp5LES0HiEEPWozrD4HR5DPPa dce1mH OS Platform and Distribution e.g. Linux Ubuntu 16.04 The code was run in Google Colab with the version of all packages defined by Google Colab at this date 07 Nov 2019 . TensorFlow version 2.0.0 Python version 3.6.7 Describe the current behavior The provided code originally written by Xifeng Guo for Keras with Tensorflow 1.2 as backend was updated to run in Tensorflow 2.0. The code builds a Capsule Network as defined by Hinton s paper Dynamic Routing Between Capsules . It is trained tested with MNIST dataset. The code gives two options either training with model.fit it works perfectly or with model.fit generator where a generator makes only shifting augmentation the problem is here . This part of the code remained as it was originally written by Mr. Guo as I think it does not need to be updated see below where the part of model.fit was commented out . Note that in the original implementation of CapsNets there is an encoder x is input y is output and a decoder y is input x is output thus the input of the network is x x train y train and the output is y y train x train . Training without data augmentation model.fit x train y train y train x train batch size args.batch size epochs args.epochs validation data x test y test y test x test callbacks log tb checkpoint lr decay Begin Training with data augmentation def train generator x y batch size shift fraction 0. train datagen ImageDataGenerator width shift range shift fraction height shift range shift fraction generator train datagen.flow x y batch size batch size while True x batch y batch generator.next yield x batch y batch y batch x batch Training with data augmentation. If shift fraction 0 then no augmentation. model.fit generator generator train generator x train y train args.batch size args.shift fraction steps per epoch 5 int y train.shape 0 args.batch size epochs args.epochs validation data x test y test y test x test callbacks log tb checkpoint lr decay End Training with data augmentation If fit generator is used with the indicated generator there is a problem when the function ends generating training samples and starts using the validation set to test the model giving this error ValueError could not broadcast input array from shape 100 28 28 1 into shape 100 Remember that the MNIST dataset contains gray scale images of 28x28 pixels and the batch size here is 100 images. Interestingly if I change the validation data line from ... validation data x test y test y test x test ... to a generator with no augmentation... validation data train generator x test y test args.batch size ... it works perfectly. And again if I do not use the fit generator but the model.fit it also works fine. Describe the expected behavior As described fit generator should be able to deal with validation data correctly. Code to reproduce the issue The issue can be easily reproducible in the following notebook just run all the cells . I have reduced the steps per epoch 5 to go fast to the point where the code breaks. https drive.google.com open id 1xp5LES0HiEEPWozrD4HR5DPPa dce1mH,Issue replicating for the given code in TF 2.0.Thanks I think this is now obsolete. As of https github.com tensorflow tensorflow commit ac20030c96d37e980333b604402ef6dba48ef5e2 fit generator just calls fit which supports generators by turning them into Datasets . Previously fit generator was using a separate somewhat antiquated code path. With the consolidation both endpoints are now using the more modern codepath and when I test your colab with the most recent tf nightly it runs without issue. I m going to close this as fixed feel free to re open if you still encounter issues. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34073 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34073 No a Thanks You are right the nightly version works perfectly. I did not occur to me to try tf nightly before submitting the issue. 
34158,Loss and metrics differ with masking or sample weights,Update on Jan 8 2021 I updated the title from Metrics incorrect for RNN with mask as I discovered more information that widens the scope of this issue. See comment on that date. System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device n a TensorFlow installed from source or binary binary conda TensorFlow version use command below 2.0.0 Python version 3.6.8 Bazel version if compiling from source n a GCC Compiler version if compiling from source n a CUDA cuDNN version 10.0.130 7.6.0 GPU model and memory 1080 Ti Describe the current behavior I am training an RNN GRU where my varying length sequences are right padded with 0s and a mask is applied. Many sequences are more than half 0s padding . I compile the model with a loss of mean squared error and a metric of mean squared error but the output is different when the mask is in effect. Or equivalently Example output note the different values for loss vs. mean squared error for both training and validation When I disable the masking I get the following output Without the mask the values for loss and mean squared error agree. For the validation set the values are not really improving and the value of 6.7e 06 seems to be what you get when you evaluate on the 0s that would otherwise be ignored by the masking. Comparing the values between the runs suggests that the mean squared error calculations are not using the mask when it is in effect but the loss calculations do use the mask. We d expect lower values when we correctly ignore irrelevant time steps. Describe the expected behavior The values for loss and mean squared error should agree and both use the masking. Code to reproduce the issue I don t have full code and data to share since my model and data are proprietary. Other info logs I can t think of any relevant logs. , seandaug Please provide the complete code to reproduce the reported issue. Thanks Here s a small example that illustrates the issue. Example output when mask is used note that loss and mean squared error differ Example output when no mask is used note that loss and mean squared error match I could replicate the issue with Tf 2.0. Please take a look at the gist here https colab.sandbox.google.com gist gadagashwini 740c0fd036e17fe505996957b79d5cbb untitled255.ipynb . Thanks Adding pavithrasv who is the owner of metric and loss. I think the loss will take into account of the masks and exclude the masked value but I don t think metric will do that which is why you see the value difference here. I will let pavithrasv to confirm. seandaug I think this was resolved in the tf nightly . I am not able to reproduce the error with tf nightly . Please take a look at the gist here https colab.sandbox.google.com gist jvishnuvardhan 6e844af327510d58c27d59f78593825d untitled255.ipynb . Thanks I am closing this issue as this was resolved. Please feel free to reopen if I am mistaken. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34158 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34158 No a jvishnuvardhan The gist you posted using tf nightly does not appear to work anymore. The issue seems to have reappeared. It appears that the difference is in how the mean is computed over the values for a loss vs. a metric . Consider this very simple example that just outputs the input after applying a mask This generates the following output Here we have a sequence of length 3 the second value of which is masked out. Therefore the MAE should be 0.5 because the total absolute error is 1.0 and it is divided among 2 entries after applying the mask . However as a loss the MAE is reported as 0.3333 instead of 0.5. We get similar intuitively unexpected results when using sample weight as well no masking involved . Consider this simple example that just outputs the input and allows sample weights to be specified This generates the following output Here we have two samples each with a loss of 1.0 but due to the provided sample weights one of them should be ignored. Thus we would expect to see an MAE of 1.0 which is correctly reported using the metric . However the loss does not match intuition because it is dividing by the batch size not the total sample weight. I suspect these observations are because of the use of reduction losses utils.ReductionV2.AUTO in tf.keras.losses.Loss that relies on losses utils.ReductionV2.SUM OVER BATCH SIZE . Then the denominator of the MAE calculation is the batch size including sequence length that does not consider the mask nor the sample weights. It appears that for the metric calculation MAE is computed using Mean in tensorflow.python.keras.metrics that specifies reduction metrics utils.Reduction.WEIGHTED MEAN . However there s no analogous WEIGHTED MEAN reduction option for loss functions. So to me the big question here is Why do loss functions use SUM OVER BATCH SIZE that ignores masking and sample weights I agree with seandaug. Loss and metrics differ if I enable masking on an LSTM model. I am trying to find a way to solve this problem as well. Hi. I ve encountered this problem as well. Is there any update on this issue Recepcan Taoup See https github.com tensorflow tensorflow issues 34491 issuecomment 869086784 for a workaround. 
34199,Named dictionary outputs in tf.keras.Model do not work, System information Have I written custom code Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 any TensorFlow installed from source or binary source TensorFlow version use command below 2.0.0 Python version 3.6.5 Describe the current behavior Using a custom model with named outputs does not work in TF2. Describe the expected behavior While using tuples for multi output model works fine using a dictionary fails. Dict inputs and outputs seem to be allowed in the code and in the tf.keras documentation however the functionality seem not to be functional yet. Code to reproduce the issue https colab.research.google.com gist kpe 501901b5197675818a2e8a0e0bc8f3a6 keras named output dict.ipynb The output from the above code is , kpe I tried reproducing the issue in colab with TF 2.0 . I am seeing the below error message. AttributeError str object has no attribute dtype . Is this the expected behavior . Thanks ravikyram I update the example and added a colab link for easier reproducibility. I have tried on colab with TF version 2.0 2.1.0 dev20191119 and was able to reproduce the issue.Please find the gist here https colab.sandbox.google.com gist ravikyram d767a6d43b7280b9b2dbdff119b6afe1 keras named output dict.ipynb . Thanks I would also like to add that we could have resolved this issue with a simple workaround by simply modifying the model.output names and model. output layers in tensorflow 1.15 and below e.g Now when we do that on tensorflow 2.0 after changing the output layers the model adds the nested layers into itself and creates extra layers on top of the original model. Any news regarding the bug Thanks for the issue Support for arbitrary nested structures including dicts is available in the latest tf nightly pip install U tf nightly . I think there are some issues in the code provided regarding the loss used I don t think the Model is outputting the shape of data that SparseCategoricalAccuracy expects but confirmed that prediction is working as expected. Also see this bug https github.com tensorflow tensorflow issues 33245 Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34199 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34199 No a 
34210,TFLiteConverter GetOpWithOutput error, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Google Colab TensorFlow installed from source or binary pip install tf nightly TensorFlow version use command below 2.1.0 dev20191111 Python version 3.6.8 Describe the current behavior I m trying to convert a BERT keras model imported from the transformers library. Using the convert method from the TFLiteConverter the following GetOpWithOutput error happens no matter the values used passed to allow custom ops or target spec.supported ops tensorflow lite toco tooling util.cc 935 Check failed GetOpWithOutput model output array Specified output array Identity is not produced by any op in this graph Describe the expected behavior The TFLiteConverter should convert the model and return the tflite version Code to reproduce the issue https colab.research.google.com drive 16kWs2ji8xrgjSuLftXknDqEh2VixDR4R Other info logs The same bert large uncased whole word masking finetuned squad model imported from the transformers library works perfectly when used directly on a QA task without TFLite conversion. Also the conversion of the DistilBERT model from the same library and using the same steps works correctly with converter.target spec.supported ops tf.lite.OpsSet.SELECT TF OPS .,Hi It s possible that the model doesn t have an output node named Identity . Can you use netron https lutzroeder.github.io netron to visualize this graph and see what s the output I m guessing this is just a tensor naming issue if not I can take a further look Thank you for your answer. I tried to visualize the graph using Netron without success either with a h5 or SavedModel version. However as you can see in this cell of the notebook https colab.research.google.com drive 16kWs2ji8xrgjSuLftXknDqEh2VixDR4R scrollTo 94UoamBP17VO line 4 uniqifier 1 it seems there are two outputs Identity and Identity 1 . Also visible in this Tensorboard graph img width 751 alt Screen Shot 2019 11 13 at 2 34 35 PM src https user images.githubusercontent.com 5020707 68797525 c1e6bb00 0622 11ea 99ef b5c1e8454ccc.png Does it answer your question haozha111 That seems like a bug in our old converter. We have developed a new MLIR based converter you can try it out by downloading the tf nightly pip package and then set converter.experimental new converter True. Could you try this out and see if the problem is resolved I get this error when trying with converter.experimental new converter True whatever the value of other options target spec.supported ops and allow custom ops But it seems there is an input 1 in the model as visible in my previous TensorBoard graph and in this cell of the notebook https colab.research.google.com drive 16kWs2ji8xrgjSuLftXknDqEh2VixDR4R scrollTo 94UoamBP17VO line 4 uniqifier 1 . I could reproduce the issue now. It seems like a bug in our converter will take a look and try to fix it soon. I changed transformers 2 in the pip command. https colab.research.google.com drive 1MrPEJWqReZGKWBHAkoYYCrDiDTccmLUb Does this help I tried again with the latest TF nightly version and it works now using the experimental converter and partial TF operators Specifying transformers 2 doesn t affect the result since it defaults to the last version available which is 2.3.0 ucalyptus. Closing this thanks to the TF team for all the continuous improvements Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34210 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34210 No a Using these options did great Thanks converter.experimental new converter True converter.allow custom ops True 
34211,No gradient defined error when training Sequential model restored from SavedModel with tf.keras.models.load model , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 macOS 10.15.1 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device No TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 Python version 3.7.3 CUDA cuDNN version none GPU model and memory none MacBook pro iCore i7 16 GB Describe the current behavior When training a Sequential model Embedding LSTM Dense layer restored from SavedModel with load model model tf.keras.models.load model imodel saved I am getting the following error LookupError No gradient defined for operation while op type While . I am able to train the same model if the model was previously saved in h5 format tf.keras.models.save model model imdb model include optimizer True save format h5 . However it should work with save format tf as well. It works for other models e.g. convolutional model but not for this specific model Describe the expected behavior We should be able to train the model when restored from SavedModel. Code to reproduce the issue First run this script to define and save the Sequential model Then run the following script to restore the model and train it on the IMDB dataset Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. forward function backwards function self.forward backward len doutputs File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager function.py line 582 in forward backward forward backward self. construct forward backward num doutputs File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager function.py line 629 in construct forward backward func graph backwards graph File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python framework func graph.py line 915 in func graph from py func func outputs python func func args func kwargs File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager function.py line 619 in backprop function src graph self. func graph File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python ops gradients util.py line 679 in GradientsHelper lambda grad fn op out grads File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python ops gradients util.py line 350 in MaybeCompile return grad fn Exit early File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python ops gradients util.py line 679 in lambda lambda grad fn op out grads File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager function.py line 715 in registered grad fn return self. rewrite forward and call backward op doutputs File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager function.py line 661 in rewrite forward and call backward forward function backwards function self.forward backward len doutputs File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager function.py line 582 in forward backward forward backward self. construct forward backward num doutputs File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager function.py line 629 in construct forward backward func graph backwards graph File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python framework func graph.py line 915 in func graph from py func func outputs python func func args func kwargs File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager function.py line 619 in backprop function src graph self. func graph File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python ops gradients util.py line 637 in GradientsHelper op.name op.type LookupError No gradient defined for operation while op type While yanndupis yanns MBP Documents dropoutlabs project cowbay example medical symptoms model training 1 yanndupis yanns MBP Documents dropoutlabs project cowbay example medical symptoms model training 1 clear yanndupis yanns MBP Documents dropoutlabs project cowbay example medical symptoms model training 1 python train model.py 2019 11 12 17 36 48.074011 I tensorflow core platform cpu feature guard.cc 142 Your CPU supports instructions that this TensorFlow binary was not compiled to use AVX2 FMA 2019 11 12 17 36 48.086305 I tensorflow compiler xla service service.cc 168 XLA service 0x7ff129c9cae0 executing computations on platform Host. Devices 2019 11 12 17 36 48.086321 I tensorflow compiler xla service service.cc 175 StreamExecutor device 0 Host Default Version Train on 25000 samples 32 25000 .............................. ETA 1 28Traceback most recent call last File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python framework ops.py line 2383 in get attr c api.TF OperationGetAttrValueProto self. c op name buf tensorflow.python.framework.errors impl.InvalidArgumentError Operation StatefulPartitionedCall has no attr named XlaCompile . During handling of the above exception another exception occurred Traceback most recent call last File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python ops gradients util.py line 345 in MaybeCompile xla compile op.get attr XlaCompile File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python framework ops.py line 2387 in get attr raise ValueError str e ValueError Operation StatefulPartitionedCall has no attr named XlaCompile . During handling of the above exception another exception occurred Traceback most recent call last File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python framework ops.py line 2383 in get attr c api.TF OperationGetAttrValueProto self. c op name buf tensorflow.python.framework.errors impl.InvalidArgumentError Operation StatefulPartitionedCall has no attr named XlaCompile . During handling of the above exception another exception occurred Traceback most recent call last File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python ops gradients util.py line 345 in MaybeCompile xla compile op.get attr XlaCompile File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python framework ops.py line 2387 in get attr raise ValueError str e ValueError Operation StatefulPartitionedCall has no attr named XlaCompile . During handling of the above exception another exception occurred Traceback most recent call last File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python ops gradients util.py line 621 in GradientsHelper grad fn ops.get gradient function op File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python framework ops.py line 2541 in get gradient function return gradient registry.lookup op type File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python framework registry.py line 97 in lookup s registry has no entry for s self. name name LookupError gradient registry has no entry for While During handling of the above exception another exception occurred Traceback most recent call last File train model.py line 11 in module model.fit x train y train epochs 1 File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python keras engine training.py line 728 in fit use multiprocessing use multiprocessing File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python keras engine training v2.py line 324 in fit total epochs epochs File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python keras engine training v2.py line 123 in run one epoch batch outs execution function iterator File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python keras engine training v2 utils.py line 86 in execution function distributed function input fn File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager def function.py line 457 in call result self. call args kwds File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager def function.py line 503 in call self. initialize args kwds add initializers to initializer map File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager def function.py line 408 in initialize args kwds File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager function.py line 1848 in get concrete function internal garbage collected graph function self. maybe define function args kwargs File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager function.py line 2150 in maybe define function graph function self. create graph function args kwargs File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager function.py line 2041 in create graph function capture by value self. capture by value File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python framework func graph.py line 915 in func graph from py func func outputs python func func args func kwargs File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager def function.py line 358 in wrapped fn return weak wrapped fn . wrapped args kwds File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python keras engine training v2 utils.py line 73 in distributed function per replica function args model x y sample weights File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python distribute distribute lib.py line 760 in experimental run v2 return self. extended.call for each replica fn args args kwargs kwargs File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python distribute distribute lib.py line 1787 in call for each replica return self. call for each replica fn args kwargs File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python distribute distribute lib.py line 2132 in call for each replica return fn args kwargs File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python autograph impl api.py line 292 in wrapper return func args kwargs File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python keras engine training v2 utils.py line 264 in train on batch output loss metrics model. output loss metrics File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python keras engine training eager.py line 311 in train on batch output loss metrics output loss metrics File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python keras engine training eager.py line 268 in process single batch grads tape.gradient scaled total loss trainable weights File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager backprop.py line 1014 in gradient unconnected gradients unconnected gradients File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager imperative grad.py line 76 in imperative grad compat.as str unconnected gradients.value File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager function.py line 738 in backward function return self. rewrite forward and call backward call op args File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager function.py line 661 in rewrite forward and call backward forward function backwards function self.forward backward len doutputs File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager function.py line 582 in forward backward forward backward self. construct forward backward num doutputs File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager function.py line 629 in construct forward backward func graph backwards graph File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python framework func graph.py line 915 in func graph from py func func outputs python func func args func kwargs File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager function.py line 619 in backprop function src graph self. func graph File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python ops gradients util.py line 679 in GradientsHelper lambda grad fn op out grads File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python ops gradients util.py line 350 in MaybeCompile return grad fn Exit early File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python ops gradients util.py line 679 in lambda lambda grad fn op out grads File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager function.py line 715 in registered grad fn return self. rewrite forward and call backward op doutputs File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager function.py line 661 in rewrite forward and call backward forward function backwards function self.forward backward len doutputs File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager function.py line 582 in forward backward forward backward self. construct forward backward num doutputs File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager function.py line 629 in construct forward backward func graph backwards graph File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python framework func graph.py line 915 in func graph from py func func outputs python func func args func kwargs File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager function.py line 619 in backprop function src graph self. func graph File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python ops gradients util.py line 679 in GradientsHelper lambda grad fn op out grads File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python ops gradients util.py line 350 in MaybeCompile return grad fn Exit early File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python ops gradients util.py line 679 in lambda lambda grad fn op out grads File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager function.py line 715 in registered grad fn return self. rewrite forward and call backward op doutputs File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager function.py line 661 in rewrite forward and call backward forward function backwards function self.forward backward len doutputs File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager function.py line 582 in forward backward forward backward self. construct forward backward num doutputs File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager function.py line 629 in construct forward backward func graph backwards graph File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python framework func graph.py line 915 in func graph from py func func outputs python func func args func kwargs File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python eager function.py line 619 in backprop function src graph self. func graph File Library Frameworks Python.framework Versions 3.7 lib python3.7 site packages tensorflow core python ops gradients util.py line 637 in GradientsHelper op.name op.type LookupError No gradient defined for operation while op type While , yanndupis I have tried on colab with TF version 2.0 and i am not seeing any error message. Please find the gist here https colab.sandbox.google.com gist ravikyram d301ea4c8d4f311e7047e2161df2576d untitled360.ipynb .Thanks Hello ravikyram thank you very much for your quick response. I should have been more precise. You will get the error if you load the model in a different script where the model hasn t been explicitly defined then start training it. In the gist the entire code is in the same Colab. You should be able to reproduce if you have two different Colabs. In the first Colab you run and in a different Colab you run Let me know if you have any question. Thank you I have tried on jupyter notebook with TF version 2.0 and was able to reproduce the issue.Please find the files in attachment.Thanks error.tar.gz https github.com tensorflow tensorflow files 3844965 error.tar.gz Hi yanndupis ravikyram It looks like we have the same problem with a similar error when saving a model and then loading it again e.g. for retrain. We save the model using tf.saved model.save and not Keras. We are running on Ubuntu 16.04 Python 3.6 TF2.0 We have just tried and the problem still exists with TF2.1.0 rc1 Is there any update on this Hitting the same error with a bidirectional GRU layer hitting a similar error with an LSTM model Hitting similar issue with simple LSTM. yanndupis I cannot reproduce the issue with recent tf nightly . Can you please check the gist here https colab.sandbox.google.com gist jvishnuvardhan f5204a88549ae0746a4b9d2287de8017 untitled848.ipynb . I ran the first cell to save the model then restarted colab s runtime then ran second cell to load the model and execute model.fit . Please close the issue if this was resolved for you. Thanks yanndupis Closing this as it was resolved in tf nightly . Please feel free to reopen if this was not resolved for you. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34211 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34211 No a Error still present in tf nightly . Ran the example given by jvishnuvardhan in two separate colab runtimes saving in one and loading in the other and the error is still present Gentatsu I ran the first part and then restarted the runtime and then ran the second part loading . I don t see any issue. Please note that I ran both the code in one colab but two runtimes. As this is an old issue Can you please open a new issue with standalone code to reproduce the issue. Thanks Facing the same error with tensorflow 2.3.1 and python 3.8. The model includes an LSTM layer. And I want to load the model and retrain it. I have been using the same script. I use tf.save model.save and tf.save model.load. ipython input 7 59ea268a147c in compute loss data model y truth inverse transform 1 def compute loss data model y truth inverse transform 2 predictions model data 3 predictions tf.matmul predictions inverse transform 4 loss tf.math.reduce mean predictions y truth 2 5 return loss anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python saved model load.py in call attribute instance args kwargs 507 508 def call attribute instance args kwargs 509 return instance. call args kwargs 510 511 anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python eager def function.py in call self args kwds 778 else 779 compiler nonXla 780 result self. call args kwds 781 782 new tracing count self. get tracing count anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python eager def function.py in call self args kwds 812 In this case we have not created variables on the first call. So we can 813 run the first trace but we should fail if variables are created. 814 results self. stateful fn args kwds 815 if self. created variables 816 raise ValueError Creating variables on a non first call to a function anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python eager function.py in call self args kwargs 2827 with self. lock 2828 graph function args kwargs self. maybe define function args kwargs 2829 return graph function. filtered call args kwargs pylint disable protected access 2830 2831 property anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python eager function.py in filtered call self args kwargs cancellation manager 1841 args and kwargs . 1842 1843 return self. call flat 1844 t for t in nest.flatten args kwargs expand composites True 1845 if isinstance t ops.Tensor anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python eager function.py in call flat self args captured inputs cancellation manager 1927 possible gradient type 1928 executing eagerly 1929 forward function args with tangents forward backward.forward 1930 if executing eagerly 1931 flat outputs forward function.call anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python eager function.py in forward self 1430 def forward self 1431 Builds or retrieves a forward function for this call. 1432 forward function self. functions.forward 1433 self. inference args self. input tangents 1434 return forward function self. inference args self. input tangents anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python eager function.py in forward self inference args input tangents 1187 self. forward self. forward graph self. backward 1188 self. forwardprop output indices self. num forwardprop outputs 1189 self. forward and backward functions inference args input tangents 1190 return self. forward 1191 anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python eager function.py in forward and backward functions self inference args input tangents 1386 while len outputs len self. func graph.outputs 1387 outputs list self. func graph.outputs 1388 self. build functions for outputs 1389 outputs inference args input tangents 1390 forward function forward graph anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python eager function.py in build functions for outputs self outputs inference args input tangents 893 graph placeholder gradient dtype gradient shape 894 with ops.device None 895 gradients wrt inputs gradients util. GradientsHelper pylint disable protected access 896 trainable outputs 897 self. func graph.inputs anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python ops gradients util.py in GradientsHelper ys xs grad ys name colocate gradients with ops gate gradients aggregation method stop gradients unconnected gradients src graph 666 If grad fn was found do not use SymbolicGradient even for 667 functions. 668 in grads MaybeCompile grad scope op func call 669 lambda grad fn op out grads 670 else anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python ops gradients util.py in MaybeCompile scope op func grad fn 334 xla scope op.get attr XlaScope .decode 335 except ValueError 336 return grad fn Exit early 337 338 if not xla compile anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python ops gradients util.py in lambda 667 functions. 668 in grads MaybeCompile grad scope op func call 669 lambda grad fn op out grads 670 else 671 For function call ops we add a SymbolicGradient anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python eager function.py in rewrite forward and call backward self op doutputs 710 def rewrite forward and call backward self op doutputs 711 Add outputs to the forward call and feed them to the grad function. 712 forward function backwards function self.forward backward len doutputs 713 if not backwards function.outputs 714 return backwards function.structured outputs anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python eager function.py in forward backward self num doutputs 619 if forward backward is not None 620 return forward backward 621 forward backward self. construct forward backward num doutputs 622 self. cached function pairs num doutputs forward backward 623 return forward backward anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python eager function.py in construct forward backward self num doutputs 662 backwards graph func graph module.FuncGraph 663 backward name self. func graph.name 664 func graph module.func graph from py func 665 name backwards graph.name 666 python func backprop function anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python framework func graph.py in func graph from py func name python func args kwargs signature func graph autograph autograph options add control dependencies arg names op return value collections capture by value override flat arg shapes 984 original func tf decorator.unwrap python func 985 986 func outputs python func func args func kwargs 987 988 invariant func outputs contains only Tensors CompositeTensors anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python eager function.py in backprop function grad ys 653 def backprop function grad ys 654 with ops.device None 655 return gradients util. GradientsHelper pylint disable protected access 656 trainable outputs 657 self. func graph.inputs anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python ops gradients util.py in GradientsHelper ys xs grad ys name colocate gradients with ops gate gradients aggregation method stop gradients unconnected gradients src graph 666 If grad fn was found do not use SymbolicGradient even for 667 functions. 668 in grads MaybeCompile grad scope op func call 669 lambda grad fn op out grads 670 else anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python ops gradients util.py in MaybeCompile scope op func grad fn 334 xla scope op.get attr XlaScope .decode 335 except ValueError 336 return grad fn Exit early 337 338 if not xla compile anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python ops gradients util.py in lambda 667 functions. 668 in grads MaybeCompile grad scope op func call 669 lambda grad fn op out grads 670 else 671 For function call ops we add a SymbolicGradient anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python eager function.py in rewrite forward and call backward self op doutputs 710 def rewrite forward and call backward self op doutputs 711 Add outputs to the forward call and feed them to the grad function. 712 forward function backwards function self.forward backward len doutputs 713 if not backwards function.outputs 714 return backwards function.structured outputs anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python eager function.py in forward backward self num doutputs 619 if forward backward is not None 620 return forward backward 621 forward backward self. construct forward backward num doutputs 622 self. cached function pairs num doutputs forward backward 623 return forward backward anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python eager function.py in construct forward backward self num doutputs 662 backwards graph func graph module.FuncGraph 663 backward name self. func graph.name 664 func graph module.func graph from py func 665 name backwards graph.name 666 python func backprop function anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python framework func graph.py in func graph from py func name python func args kwargs signature func graph autograph autograph options add control dependencies arg names op return value collections capture by value override flat arg shapes 984 original func tf decorator.unwrap python func 985 986 func outputs python func func args func kwargs 987 988 invariant func outputs contains only Tensors CompositeTensors anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python eager function.py in backprop function grad ys 653 def backprop function grad ys 654 with ops.device None 655 return gradients util. GradientsHelper pylint disable protected access 656 trainable outputs 657 self. func graph.inputs anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python ops gradients util.py in GradientsHelper ys xs grad ys name colocate gradients with ops gate gradients aggregation method stop gradients unconnected gradients src graph 666 If grad fn was found do not use SymbolicGradient even for 667 functions. 668 in grads MaybeCompile grad scope op func call 669 lambda grad fn op out grads 670 else anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python ops gradients util.py in MaybeCompile scope op func grad fn 334 xla scope op.get attr XlaScope .decode 335 except ValueError 336 return grad fn Exit early 337 338 if not xla compile anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python ops gradients util.py in lambda 667 functions. 668 in grads MaybeCompile grad scope op func call 669 lambda grad fn op out grads 670 else 671 For function call ops we add a SymbolicGradient anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python eager function.py in rewrite forward and call backward self op doutputs 710 def rewrite forward and call backward self op doutputs 711 Add outputs to the forward call and feed them to the grad function. 712 forward function backwards function self.forward backward len doutputs 713 if not backwards function.outputs 714 return backwards function.structured outputs anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python eager function.py in forward backward self num doutputs 619 if forward backward is not None 620 return forward backward 621 forward backward self. construct forward backward num doutputs 622 self. cached function pairs num doutputs forward backward 623 return forward backward anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python eager function.py in construct forward backward self num doutputs 662 backwards graph func graph module.FuncGraph 663 backward name self. func graph.name 664 func graph module.func graph from py func 665 name backwards graph.name 666 python func backprop function anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python framework func graph.py in func graph from py func name python func args kwargs signature func graph autograph autograph options add control dependencies arg names op return value collections capture by value override flat arg shapes 984 original func tf decorator.unwrap python func 985 986 func outputs python func func args func kwargs 987 988 invariant func outputs contains only Tensors CompositeTensors anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python eager function.py in backprop function grad ys 653 def backprop function grad ys 654 with ops.device None 655 return gradients util. GradientsHelper pylint disable protected access 656 trainable outputs 657 self. func graph.inputs anaconda3 envs tensorflow2 p38 lib python3.8 site packages tensorflow python ops gradients util.py in GradientsHelper ys xs grad ys name colocate gradients with ops gate gradients aggregation method stop gradients unconnected gradients src graph 619 grad fn func call.python grad func 620 else 621 raise LookupError 622 No gradient defined for operation s op type s 623 op.name op.type LookupError No gradient defined for operation while op type While 
34250,Collective AllGather Fails on Polymorphic Shapes, System information OS Platform and Distribution Ubuntu 18.04 TensorFlow installed from source or binary binary whl TensorFlow version use command below tensorflow gpu 2.0.0 Python version 3.6.8 CUDA cuDNN version 10.0 7.6.4 GPU model and memory GeForce GTX 1080 Ti Describe the current behavior In one session if one runs the above graph second time with the feed in a size different from the first time it will raise an error Inconsistent output shapes got 14 but expected is 16 . Describe the expected behavior The graph construction above sets the expected shapes of the placeholders as polymorphic None . However after the first session.run the collective op caches its output shape which is 16 in our case in tensorflow tensorflow core kernels collective ops.cc . But should it be expected that the collective op keeps its graph defined polymorphic behavior Specifically in our case should it allow a user to all gather two size 7 tensors into a size 14 Code to reproduce the issue See above ,Hi tensorflower I opened a pull request there 34295 for this issue. If someone would like to review it I d very appreciate it Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34250 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34250 No a 
34297,tf.function hangs on ragged tensor input, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 No TensorFlow installed from source or binary Colab TensorFlow version use command below 2.0.0 Python version 3.6 GPU model and memory None Describe the current behavior When using tf.function with a number of for loops on a RaggedTensor the function call hangs I stopped waiting after half hour . when running the function directly no tf.function decorator the function executes immediately when converting the ragged tensor to dense tensor the function executes immediately. I couldn t pinpoint the exact combination of operations that causes this Autograph behavior but I tried to reduce my code to the simplest combination that still causes this behavior. I struggled for hours with my own code trying to get tf.function to work until I figured it was due to the ragged tensor for loops tf.function hanging the kernel I observed similar behavior on my machine and a colab machine as well. Describe the expected behavior Should execute in a comparable time to a dense tensor. Code to reproduce the issue Other observations Also noted very large memory footprint as a result 9gb and growing ,Could reproduce the issue with TF Version 2.0. Here is the Gist https colab.sandbox.google.com gist rmothukuru 3a286f4b25e62d041d0030e80a4e1cb2 34297.ipynb . Thanks This bug is a combination of several problems that we ll need to resolve fopefully in 2.2. In the mean time a workaround is to avoid writing for loops over ragged tensors e.g. for x in ragged tensor . This means rewriting your code like so A few more details on the cause of the bug Autograph doesn t currently support iterating over RaggedTensor and it thinks they are normal Python objects. We ll fix that in autograph hopefully by TF 2.2. Now this should have normally generated an error something in the lines of cannot directly iterate over RaggedTensor in graph mode . But it seems that the RaggedTensor class is iterable and the iterator it returns hangs when consumed in graph mode. edloper I think this iterator should error out in tf.function instead Edit fixed the index in the example Thank you for the update and the details. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34297 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34297 No a The PR that just got merged should fix the issue in tf.function. As a side note we should still disallow RaggedTensor. iter in graph code or make it return a proper graph based iterator . 
34298,could not restore weights to model with same structure when enable eager,Sytem ubuntu18.04 TF version 2.0 Hardware gtx1080ti and gt 840m CUDA 10.0 CUDNN 7.6 the save weights code is the load weights code is When enable eager the weights saved under mode train could not be restored into model when mode test The error message is as follows the Shapes 20 is the shape of reg head the Shapes 100 is the shape of cla head Maybe the loading order is out of order When disable eager the weights saved under mode train could be restored into model when mode test with some Warnings , Stick To Tried reproducing your issue but encountered the error NameError name backone is not defined after giving some dummy values to batch size and lr . Please find the Gist https colab.sandbox.google.com gist rmothukuru 47956cd5c8b7742f2e313efc116a7fdc 34298.ipynb . Can you please help me to reproduce the error. Thanks rmothukuru It s done when I combine cla head and reg head as this the code could run well rmothukuru https colab.research.google.com gist Stick To e745c258bfa65bdda2d95844e1ce3f76 34298.ipynb Could reproduce the error with TF Version 2.0. Here is the Gist https colab.research.google.com gist Stick To e745c258bfa65bdda2d95844e1ce3f76 34298.ipynb . Thanks rmothukuru the reconstruction of it is done isn t it https colab.research.google.com gist Stick To e745c258bfa65bdda2d95844e1ce3f76 34298.ipynb Stick To Yes it is done and I have forwarded it to another Engineer. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34298 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34298 No a I know this issue has been closed but I ve also been struggling with this. Has this been resolved or if not any where I can follow progress stefanobranco Please create a new issue with details and a simple standalone code to reproduce the issue. Thanks 
34312,Error when loading SavedModel with models.load model and compile false , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 Ubuntu 18.04 TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 Python version 3.6.8 CUDA cuDNN version 10.0.130 7.6.3.30 GPU model and memory Tesla K80 with 11441MiB GeForce GTX 1080Ti with 11441MiB Describe the current behavior When loading a model saved using model.save . model save format tf without compiling it and executing an inference using model.predict the following exception is raised The problem is due to the load function setting the optimizer of the model with an instance of tensorflow.python.keras.optimizer v2.optimizer v2.RestoredOptimizer and therefore the model is tried to be compiled in https github.com tensorflow tensorflow blob 4ff2916c8ca78839cebcfa39e2ac939fa50eab80 tensorflow python keras engine training.py L2320 L2322 Raising the error as there is no loss metrics loaded previously. Describe the expected behavior The expected behaviour is that the model performs the inference without any need to be compiled and returns the result. In fact this behaviour is meet when the model is saved with the Keras format model.h5 and loaded. Code to reproduce the issue A executable notebook can be found here https colab.research.google.com drive 10ljGMdWthbZ3waalt 4vKguEyxbZzUbt ,Issue replicating with TF 2.0 and tf nightly . Hi k w w Can I take on this issue lsgrep Feel free to pick this up if you re interested adrigrillo Looks like this was resolved in recent tf nightly . I was not able to reproduce the issue. Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 3bd6555e2cbc31bfba51a7f04dc3b199 load model bug tf2 0.ipynb . Please close the issue if it was resolved for you. Thanks I can confirm that the latest nightly solves the problem. Thanks. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34312 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34312 No a 
34334,default installed version of tensorflow lite arduino library is pre compiled causing confusing error reports,hiya petewarden ive been re porting my demos hooray for Arduino boards https learn.adafruit.com tensorflow lite for edgebadge kit quickstart right now when folks install the TensorFlow library it defaults to the pre compiled version which causes very obscure errors about register arguments if they are not using the exact same processor. https learn.adafruit.com tensorflow lite for edgebadge kit quickstart troubleshooting uses vfp register arguments and libtensorflowlite dot a does not error 10 2 please make the default non pre compiled...Arduino IDE has a huge collection of supported boards and as is will confuse a lot of people ,Sorry I missed this one originally I will dig into this since we are keen to have widespread support. thank u I ve contacted the Arduino IDE team for help on this I hope to have more progress to report soon. hihi checkin in on this issue i saw also SFE bumped into the same issue https learn.sparkfun.com tutorials programming the sparkfun edge with arduino NOTE It is imperative that you install the non precompiled version of the library. Installing the pre compiled library will only lead to failure and sadness. The Arduino team now have a pending PR which they believe should fix this problem https github.com arduino arduino cli pull 512 Could you take a look and provide feedback Thanks for your patience super rad i will i published a guide this weekend https learn.adafruit.com tensorflow lite for circuit playground bluefruit quickstart I think this is now fixed and checked in so closing Let me know if I m wrong and I ll reopen thanks for your work on this. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34334 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34334 No a 
34348,save method shows buggy confusing behaviour, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Google Colab Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device NN TensorFlow installed from source or binary NN TensorFlow version use command below 2.0.0 Python version 3.6 Bazel version if compiling from source NN GCC Compiler version if compiling from source NN CUDA cuDNN version NN GPU model and memory NN Describe the current behavior tf.keras.Model.save shows confusing behavior with the save format argument. See gist https colab.research.google.com gist nikochiko 7a624ae90563b831d5229eb0ee5b0d41 tf model save buggy.ipynb . Even when save format is set as tf the model is saved as h5 if the filepath ends in suffix .h5 Also it defaults random string arguments to tf format. Describe the expected behavior The value of the save format argument should be the format of the saved file irrespective of the filepath. Or else there should be a boolean argument like save as h5 instead. Code to reproduce the issue https colab.research.google.com gist nikochiko 7a624ae90563b831d5229eb0ee5b0d41 tf model save buggy.ipynb scrollTo 1H73RxH5sTgl Other info logs Source code https github.com tensorflow tensorflow blob r2.0 tensorflow python keras engine network.py L923 L975 Outdated documentation https www.tensorflow.org api docs python tf keras Model save Updated docs for current behavior in PR https github.com tensorflow tensorflow pull 34347 files More details model.save weights handles it better see gist https colab.research.google.com gist nikochiko ff693562546dbda5d5868ec7e7d75bad tf save weights.ipynb ,Could reproduce the issue with TF Version 2.0. Here is the Gist https colab.sandbox.google.com gist rmothukuru 565a2bee7543a888a9677813b9e0447a tf model save buggy.ipynb . rmothukuru k w w I can make a fix for this in accordance with the save weights method. Shall I start working That would be great Reviewing the PR now Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34348 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34348 No a 
34433,r2.0 2.1 Python 3.8 AutoGraph could not transform bound method LinearRegressionTF.fit of tensorflow.python.eager.function.TfMethodTarget object at 0x7f7a5ccc1fa0 and will run it as is, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes attached file code warning py38.py.txt rename it as .py and execute it OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04 TensorFlow installed from source or binary source TensorFlow version use command below This problem occurs with both r2.0 built from head and r2.1rc0 Python version 3.8 virtual environment The problem does not occur with Python 3.7.3 Bazel version if compiling from source 0.26.1 GCC Compiler version if compiling from source 7.4.0 CUDA cuDNN version CUDA 10 cuDNN 7.6.5 GPU model and memory Nvidia Geforce RTX 2080 Ti Describe the current behavior export AUTOGRAPH VERBOSITY 10 Run the attached python script renamed as .py python code warning py38.py output.txt Read the output in the attached output.txt Describe the expected behavior There is no warning message when the script is run with Python 3.7.3 Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. See both files attached. code warning py38.py.txt https github.com tensorflow tensorflow files 3867410 code warning py38.py.txt output.txt https github.com tensorflow tensorflow files 3867412 output.txt , dbonner Thanks for reporting the issue. Please provide the input data file to reproduce the reported issue. Thanks gadagashwini cal house.json.gz https github.com tensorflow tensorflow files 3873037 cal house.json.gz Sorry I forgot to do this. Here it is. Regards Dan Works without warning or error with Tf 2.0 and Python 3.6. Please see the gist here https colab.sandbox.google.com gist gadagashwini 850e1f79b50a8847c2fff9e42aa2b4bf untitled274.ipynb . Thanks gadagashwini and jvishnuvardhan I expected Python 3.6 with TF2.0 to work without warning or error. I believe this problem only occurs with Python 3.8 with TF2.0 built from source using Python 3.8. All the best Daniel I m looking into this but am a bit swamped today. If you have the chance could you check the version of gast you have installed Hi Dan gast 0.2.2 Cheers On Thu 5 Dec 2019 at 01 59 Dan Moldovan notifications github.com wrote I m looking into this but am a bit swamped today. If you have the chance could you check the version of gast you have installed You are receiving this because you were mentioned. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 34433 email source notifications email token AAB26QXHEPMOUDV47WAHBJDQW7AUFA5CNFSM4JPNL272YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEF5JVBI issuecomment 561683077 or unsubscribe https github.com notifications unsubscribe auth AAB26QSSYU74LWBQPDBBHC3QW7AUFANCNFSM4JPNL27Q . Thanks that saved me a few roundtrips. I ran a build with 3.8 and it looks like the AST transformations need to be updated to work with Python 3.8 I suspect that s because of the grammar changes. I ll make the necessary fixes hopefully it won t take more than a week or two to finish them. Update after https github.com tensorflow tensorflow commit a396eb4a3a7dc08b859df42919fcc7bdf8236a01 3.8 syntax should be fully supported. I m re checking the script to see if anything else remains. Looks like astor is not compatible with 3.8 either so we ll need to replace that dependency. Fortunately it s fairly straightforward to swap in something else like astunparse. Progress update upgrade to astunparse is done awaiting new release of gast which fixes a bug with FunctionDef nodes in 3.8 see https github.com serge sans paille gast pull 43 . This should now be fixed at head. With the pip package from source running code warning py38.py should no longer print a warning. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34433 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34433 No a I can t test the resolution until I build tensorflow for python 3.8.0. Unfortunately my build is failing see issue 36170 I got the build to work. Yes this is fixed. 
34454,Layer model is not connected no input to return., System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 macos mohave Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary pip install tensorflow 2.0.0 TensorFlow version use command below 2.0.0 Python version Python 3.7.4 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Describe the current behavior Raises exception. When loading saved model through the tf.keras.Model we get tf 2.0.0 AttributeError Layer model is not connected no input to return. Describe the expected behavior It should pass. It pass when saving and loading model from .h5 format. Code to reproduce the issue Other info logs ,Issue replicating for given code in TF 2.0 kindly find the gist https colab.sandbox.google.com gist oanush edd8e140df86794bc17324b0b7cb2333 34454.ipynb of colab.Thanks I am not sure if this is working in 2.1rc because right now there is another bug 33870 which should be solved before. k w w Any update on this k w w Any update on this save your model in keras .h5 format instead of tensorflow checkpoint format seems to work. Tested again this issue can be solved by using .h5 format to save the model. It looks like checkpoint format isn t integrated with Keras well and some information is missing in the checkpoint format. It s working in tf nightly 2.2... Thank you k w w . You can close this yes this issue is fixed with latest tf nightly. please see the updated gist https colab.sandbox.google.com gist goldiegadde 9c8c6b3e324ca6fbe16e8e3c446fb9c0 34454.ipynb Updating your code to below should work. model loaded tf.keras.Model inputs model l2.input outputs model l2.get layer layer name .output for layer name in probs embeddings Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34454 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34454 No a I installed the latest tf nightly and I still have this issue. Here is my code that returns the error guillefix Since you are using Sequential API you may want to specify input shape argument in your first layer. If want to avoid setting input shape then other option will be to build the first by model.compile and model.fit Not sure if it is precisely the same issue but I got around my specific problem by overriding the get config method in my custom layer taking care to serialize the initializer arguments in particular 
34458,Using skip and ignore errors cause training hang,Using skip on a dataset that contains corrupted data and then applying ignore errors causes fit method to hang before the validation step. The fit method uses dataset for training and validation. This behavior was reported using Ubuntu 18.04 Tensorflow 2.0.0 installed using pip python 3.7.3 ,Issue is replicating with Tf 2.0. Please take a look at colab gist https colab.sandbox.google.com gist gadagashwini db3cd6c2d13e3b16b6cc3fedfac2b1ae untitled265.ipynb . Thanks grzjab This bug has been fixed in TF nightly. Please find my github gist here https colab.sandbox.google.com gist gowthamkpr 3beef897724021a730c2e8b0086068c7 copy of untitled265.ipynb . Thank you Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34458 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34458 No a 
34479,Got ValueError Unable to create group name already exists when saving a convolutional model, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below v2.0.0 rc2 26 g64c3d382ca Python version 3.7.3 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior I m currently building a YOLOv3 model the training was good but when I try to save the model to h5 format it throws out the ValueError Unable to create group name already exists . Describe the expected behavior It should finish the save process successfully. Code to reproduce the issue Other info logs The log , gekowa Please provide the complete standalone code to reproduce the reported issue.Thanks gekowa Similar issue 32672 https github.com tensorflow tensorflow issues 32672 . Thanks We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks try to use tf.keras Lambda if you do some op on Inputs. 
34528,External loss function raises error on incompatible types,On TensorFlow 2.0 OS Ubuntu 16.04 when I ve an external loss function the tf.keras.model fit function call raises an error of incompatible types whereas the same code on TensorFlow 1.15 does not raise any error and works fine. The function fit fails with The minimal code to reproduce is If I remove line 1 it works on TensorFlow 2.0.,I have tried on colab with TF version 1.15 2.0 2.1.0 dev20191124 and was able to reproduce the issue.Please find the gist here https colab.sandbox.google.com gist ravikyram a3a85b9e573c95299b5d248d662f8db7 untitled402.ipynb . Thanks andmax This was resolved in tf nightly pip install tf nightly . Please check the gist here https colab.sandbox.google.com gist ravikyram cc4d485120973b17d5616b85b81a61f0 untitled715.ipynb . You could use tf nightly for now and in the next couple of months new stable version will be released.Can you please confirm whether we can close this issue as this was resolved. Thanks It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34528 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34528 No a Looks like it has been resolved 
34565,Applying certain ImgAug augmenters inside of a tf.py function causes AutoGraph errors., em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOS 1.15.1 TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 Python version 3.7.5 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior I m currently using imgaug 0.3.0 to augment images as part of a map function applied to a tf.data.Dataset. I m using tf.py function to call the augmenter and it appears that Autograph is incorrectly generating ops for some of the augmenters causing the pipeline to error. Describe the expected behavior I m upgrading from tf.py func in TF 1.14 to tf.py function TF 2.0 and it was possible to map the tf.py func that called the augmentation onto the dataset. Code to reproduce the issue Here is a minimal example also runable in Colab https colab.research.google.com drive 1X6NiCnGxNFG9pAZ1Jzd7R1In8BNh9pts scrollTo Q scaVKrpKwJ Other info logs Full stack trace ,This appears to be fixed in tf nightly 2.1.0.dev20191124 but if anyone has a work around for 2.0 that would be great. jamesonthecrow Hello as the issue is fixed in latest tf nightly can you try using the same version Also looks like complete code is not given please provide the same. Thanks I just updated the comment with a working Colab notebook that reproduces the error. I also noticed that the kernel only dies when imgaug 0.3.0 is used. imgaug 0.2.9 throws an error but doesn t appear to actually kill the kernel. For the updated code issue replicating in both TF 2.0 and tf nightly .Kindly find the gist https colab.sandbox.google.com gist oanush 7d178d1e212fd596b1386ff3b9d1ffae tensorflow 2 0 py function bug.ipynb scrollTo XLjFY URu6RH of colab. It is likely there is some existing issue in your augment batch code that is not surfacing clearly due to autograph. Could you try annotating augment batch with tf.function autograph False and see if you get a clearer error message jamesonthecrow Can you please respond to jaingaurav s comment. Thanks Given that things work fine on TF 2.1 we re just going to go with that. I m closing this out. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34565 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34565 No a try something like this example is a single element of your dataset after you parse for example tf records then map the transformations with dataset.map process sample num parallel calls tf.data.experimental.AUTOTUNE 
34592,TensorFlow SavedModel export fails with AttributeError,I m following the tutorial exactly as it is here https www.tensorflow.org tutorials keras text classification with hub Finally if I want to export the trained model from this tutorial using model.save I get this error message What s going on Shouldn t it be possible to simply export this model to the SavedModel format I m trying with and without the save format tf parameter.,Here you go Was able to reproduce the issue. Please find the gist here https colab.sandbox.google.com gist gowthamkpr e7c994341d6d4f2b6757b03cc4359fd2 copy of text classification with hub.ipynb . I am having the same issue This issue is fixed in latest tf nightly. Close this for now. Feel free to reopen it if needed. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34592 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34592 No a 
34619,Wrong output printing when using fit generator,On TensorFlow 1.15 OS Ubuntu 16.04 when I m using generators to train a model via the tf.keras.model fit generator function call prints a wrong and duplicated output of Epoch 1 when starting the validation steps on every epoch. The function fit generator prints The minimal code to reproduce is If I run it on TensorFlow 2.0 the duplicated and wrong prints disappear.,Could replicate the issue with Tf 1.15 and its working fine with Tf 2.0. Please see the gist here https colab.sandbox.google.com gist gadagashwini aca26f3a33c9dabd1819dcce2d4af363 untitled275.ipynb . Thanks andmax Thanks for the issue This is fixed in the latest nightly unfortunately we can t backport changes like this to older version. Please try it out with pip install U tf nightly Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34619 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34619 No a 
34620,Batch size reset to 1 after conversion, System information OS Platform and Distribution Windows 10 and Ubuntu 64 bit TensorFlow installed from source or binary pip TensorFlow version or github SHA if from source 2.0.0 Command used to run the converter or code if you re using the Python API The output from the converter invocation Failure details Despite before conversion the batch size is explicitly set to 50 after conversion the batch size is always 1. The reason why I am trying to have batch size 1 is because the model has to be executed in the edge TPU and I want to exploit as best the parallelism offered by the chip thus performing the convolution on as many inputs as possible in parallel. If not supported by TF Lite any suggestion to achieve the same result is welcome and appreciated , antofara I ran your code in tf nightly and I see batch size set as 50. There were lots of improvements recently and team is continuously improving the converter. I have also added new experimental new converter as converter.experimental new converter True . Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan c7f8adcc2be802258e522540b0c61472 untitled686.ipynb . Please close the issue if it was resolved for you. Otherwise please share a colab gist with a standalone code to reproduce the issue. Thanks Thanks jvishnuvardhan Solved with the following fixes TF version 2.1.0 dev20191204 converter.experimental new converter True Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34620 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34620 No a 
