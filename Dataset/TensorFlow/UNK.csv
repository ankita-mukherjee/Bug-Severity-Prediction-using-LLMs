ID,title,summary,comments
3709,random uniform for int32 broken on GPU, tf.random uniform with dtype tf.int32 always produces 106199773 on GPU. Environment CentOS 7 TensorFlow 0.9.0 GTX TITAN X CUDA 7.5 NVIDIA 367.27 ,This is likely caused by conversion between float and int. That does seem like a bug but it will be hard for us to reproduce since we don t have CentOS machines and it doesn t seem to happen on what we do have. Would you be up for investigating further Yes I would like to. Have you had a chance to do any more experiments to try to isolate the problem A system reboot seems to fix the problem for a few days. I will make more investigation the next time I have a chance. Thanks for the update let us know if you find anything else. 
4162,contrib.layers TypeError optimize loss got an unexpected keyword argument summaries ,Trying to provide a custom list of summaries to tf.contrib.layers.optimize loss fails Tested on a fresh 0.10.0rc0 install as well as in the tensorflow tensorflow nightly docker image. Little mystified by how this is occuring since the it s correct in source. I suspect it has something to do with everything being brought into the layers namespace. ,Just checked with current version 0.11rc1 doesn t seem to have this issue. Please re open if it still persists on your side. 
7306,OutOfRangeError Early EOF on file read in Windows Server 2012,Hi folks I am finding that tensorflow code snippets which ran on Windows 10 are failing on Windows Server 2012 NTFS. In particular anytime I try to load a file with tf.gfile.Open tf.gfile.FastGFile or tf.contrib.slim.assign from checkpoint fn I encounter an out of range error. What related GitHub issues or StackOverflow threads have you found by searching the web for your problem I found Git issue 6791 Contrib support for Windows https github.com tensorflow tensorflow issues 6791 where it was suggested that the fault lies with contrib packages. However the code sample I provide below displays the problem without using any contrib imports. Environment info Operating System Windows Server 2012 R2 Installed version of CUDA and cuDNN N A please attach the output of ls l path to cuda lib libcud If installed from binary pip package provide 1. A link to the pip package you installed tensorflow 0.12.1 cp35 cp35m win amd64.whl https pypi.python.org packages 64 a3 0054a3329579de44d557f491adbcaf8127809a7992bc46af80f0a589e29b tensorflow 0.12.1 cp35 cp35m win amd64.whl 2. The output from python c import tensorflow print tensorflow. version . 0.12.1 If possible provide a minimal reproducible example We usually don t have time to read hundreds of lines of your code A similar error is encountered when reading a text file and a more complicated DataLossError occurs when trying to load a model from a checkpoint but the trace includes references to out of range errors so I assume the same underlying problem is responsible . The same code snippet gives no error and returns the image byte data when run on the same version of Tensorflow on Windows 10 also NTFS . What other attempted solutions have you tried I can workaround this problem for images and text files by using native Python to read the files. However I really need to use tensorflow built ins to e.g. read a trained model from a checkpoint. Logs or other output that would be helpful If logs are large please upload as attachment or provide link . See above for output of MWE. Thanks very much for your help , mrry Do we support Windows Server 2012 NTFS I doubt the issue is related to a specific release of Windows but it could be a bug in the file io.py library which AFAIK doesn t distinguish between text and binary files modes r versus rb and I think this can invalidate file length checks. rohan100jain Can you take a look at this mawah Can you share a minimal text file that reproduces this problem on Windows Sure thing mrry the following generates a text file and shows the error thrown Edit to clarify this throws an error on my Windows Server 2012 Azure N series GPU VM but not on my local Windows 10 computer. Did you manage to resolve this issue I am facing the same problem on Windows Server 2012. Sorry chadland no progress yet. Not sure if rohan100jain or mrry need more information to repro the problem Thanks for letting me know mawah Unfortunately I don t have easy access to a Windows Server 2012 box so I haven t been able to reproduce the problem you are seeing. There is no obvious platform dependence in the file io library that would explain different behavior on Windows Server 2012 and Windows 10. rohan100jain since you wrote that library can you suggest what steps could be useful to debug this further mawah based on my high level understanding of the file system code the problem stems from a mismatch between the length of the file as reported by file io.stat https github.com tensorflow tensorflow blob 1bbb52426bd4f8046400731100b11e9ca767d303 tensorflow python lib io file io.py L470 and the number of bytes that can be read from the file. When I ran your program labels.txt is 10 bytes on disk presumably because the n becomes r n and the string read back in has length 10 and contains the added r characters . Since you have access to both platforms can you try to identify where the mismatch creeps in Thanks mrry I also get a length of 10 using file io.stat on both Windows 10 and Windows Server 2012 with the code snippet below Evidently the problem lies elsewhere. I m not sure where to find the definition of pywrap tensorflow which I would have examined next...thought it would be defined under the site packages tensorflow python directory but I m having trouble locating it. Thanks for checking that. Does the file also have size 10 if you dir it at the command prompt Can you also try printing each individual character of that file using code like the following I d expect this to crash with an error when it eventually goes beyond the end of the file but it would be interesting to learn where this happens.... Thanks again mrry. This certainly cleared up something for me. The following fails regardless of whether the filepath is relative or absolute and I did double check that the file is still present ...which fully explains the origin of the OutOfRangeError on attempting to read from the file. Unsurprisingly I got that error again if I simply tried to read from f one character at a time. Any idea why tf.gfile.Open would return an object of class tensorflow.python.platform.gfile.GFile and read check passed set to true but still have read buf None Okay so one bug clearly is that the tell method https github.com tensorflow tensorflow blob master tensorflow python lib io file io.py L147 needs to do a preread check and not just check for read check passed. That should fix the problem with Tell . It should look like... def tell self Returns the current position in the file. self. preread check return self. read buf.Tell I ll send out that fix. This still doesn t solve the problem that you experienced earlier I think .. I agree with mrry that the file sizes somehow are the culprit... FYI the pywrap tensorflow method comes from file io.i SWIG wrapping the c interface . The C implementation is in core platform env.cc core platform file system.cc. Hopefully this might help you debug more if you want to dig into it further. When you read it one byte at a time.. where did it crash Hi rohan100jain and mrry I added a call to f. preread check . Even so looks like the for loop crashes while trying to read the first byte Output The same error occurs in window 7 64bit. Output mnist input data.read data sets . MNIST DATA one hot True File C Users LEE Documents Visual Studio 2015 Projects PythonApplication1 deepLearning input data.py line 196 in read data sets train images extract images local file File C Users LEE Documents Visual Studio 2015 Projects PythonApplication1 deepLearning input data.py line 64 in extract images buf bytestream.read rows cols num images File H Util Anaconda3 lib gzip.py line 274 in read return self. buffer.read size File H Util Anaconda3 lib compression.py line 68 in readinto data self.read len byte view File H Util Anaconda3 lib gzip.py line 467 in read buf self. fp.read io.DEFAULT BUFFER SIZE File H Util Anaconda3 lib gzip.py line 82 in read return self.file.read size File H Util Anaconda3 lib site packages tensorflow python lib io file io.py line 112 in read return pywrap tensorflow.ReadFromStream self. read buf length status File H Util Anaconda3 lib contextlib.py line 66 in exit next self.gen File H Util Anaconda3 lib site packages tensorflow python framework errors impl.py line 469 in raise exception on not ok status pywrap tensorflow.TF GetCode status tensorflow.python.framework.errors impl.OutOfRangeError reached end of file mrry and rohan100jain if limited access to a Windows environment prevents you from investigating this I could set up a low performance Azure VM with my monthly personal MSDN credits. Couldn t run it all month but certainly long enough for you to have a look. You can reach me at mawah at microsoft if interested. I remember seeing above gzip trace above back in October and there was some discussion about eof semantic in ReadFromStream . Just tried my old test app again and it fails but seem to work linux. I can debug this a little ... might take a day or 2 before I get to it spend some time on this smartdolphin I can reproduce the same exception on 0.12.1 but on 1.0 this is gone. Could you update to 1.0 pip install upgrade tensorflow and see if this works for you mawah I can t reproduce your problem. I ll send you some email if we can debug this in your setup. Hi folks the error was resolved by upgrading to Tensorflow 1.0.0 which has been released in the meantime . Closing this issue. Do we have a solution work around in Windows 10 Issue still exists in Windows 10 Tensorflow 1.0.1 GPU AttributeError Traceback most recent call last ipython input 6 415a7deab32c in module 1 with tf.gfile.GFile train.x.txt r as f 2 print f.tell I d expect this to print zero but who knows 3 for in range 10 4 print f.read n 1 c program files python35 lib site packages tensorflow python lib io file io.py in tell self 140 raise errors.PermissionDeniedError None None 141 File isn t open for reading 142 return self. read buf.Tell 143 144 def enter self AttributeError NoneType object has no attribute Tell oldmonk101 This bug tell raising an AttributeError if called before read or write was present in all versions of TensorFlow up to 1.0.1 but is fixed in 1.1. Upgrading to the 1.1 release candidate or nightly version will fix the problem. Tried Tensorflow 1.1.0 rc0 f.tell is giving right result 0 . f.size is still throwing OutOfRangeError 
7320,Tensorflow freezes for a small model in Windows 10, What related GitHub issues or StackOverflow threads have you found by searching the web for your problem I have not been able to find any related information. I did post a SO thread http stackoverflow.com questions 41889147 tensorflow execution freezes for a small cnn and got the suggestion that I may have encountered a bug. Environment info Operating System Windows 10 home edition Installed version of CUDA and cuDNN None. Was using the CPU version. please attach the output of ls l path to cuda lib libcud If installed from binary pip package provide 1. A link to the pip package you installed https storage.googleapis.com tensorflow windows cpu tensorflow 0.12.1 cp35 cp35m win amd64.whl 2. The output from python c import tensorflow print tensorflow. version . 0.12.1 If possible provide a minimal reproducible example We usually don t have time to read hundreds of lines of your code Attached as Issue example code.txt A brief explanation of the issue Training freezes as I slightly increase the model size. For the attached code using 50 50 for the two fully connected layers would work but going slightly larger e.g. to 100 50 would result in execution freezing after model initialization or a few rounds of training. It does not seem to be constrained by memory since when the freeze occurs the Python process takes around 50MB of memory as observed from task manager while when the training runs for smaller sizes it takes GB range of memory. What other attempted solutions have you tried None as I was unable to find related info by searching and I myself does not understand the possible cause of this issue. Logs or other output that would be helpful If logs are large please upload as attachment or provide link . Issue example code.txt https github.com tensorflow tensorflow files 756669 Issue example code.txt ,Can you get us a stack trace from where it freezes This isn t enough information to guess at a cause. A Python stack trace would be a starting point just hit ctrl c or whatever the windows equivalent is. girving Actually the ctrl c doesn t work in this case the console execution simply stays frozen. I had to forcibly close the console or kill the process to terminate the execution. Is there another good way to gather more info The ideal would be to run it through a debugger. You could also try adding print statements until you localize it but it ll be hard to get below the Python level with those. mrry Do you have suggestions for Windows debugging I ve mainly used Visual Studio for debugging on Windows. The free Community edition should be able to attach to your frozen process and gather a stack trace. I have stepped through my code in the IDLE debugger and gotten the following stack trace main . module line 87 l1 l2 predictions session.run optimizer loss loss reg train prediction feed dict feed dict tensorflow.python.client.session .run line 766 run metadata ptr tensorflow.python.client.session . run line 964 feed dict string options run metadata tensorflow.python.client.session . do run line 1014 target list options run metadata tensorflow.python.client.session . do call line 1021 return fn args tensorflow.python.client.session . run fn line 1003 status run metadata The execution then freezes at line 1001 when I tried to step into tf session.TF Run ... there. Should I go deeper pengd49 Yes unfortunately TF Run is where everything happens so it isn t specific enough yet. girving If I understand it correctly the TF Run is ultimately defined in the file pywrap tensorflow.pyd right I m sorry I am inexperienced in debugging a pyd file. From what I found online it seems I will need to try to compile and generate a version of the pyd with debug flag in order to debug it pengd49 TF Run is where essentially everything happens. All it means is that the error is somewhere in C code. Automatically closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks 
7896,Issue with while loop in tf.scan,Hello I have recently updated to the newest version of tensorflow v1.0 and am suddenly having some trouble when I use tf.scan. I have already tried setting shapes for all of the variables. Which didn t work Don t really know what else i could do since that is the solution that is recommended for the error in tf.while loop... Here is the code snippet h z tf.scan self.sample x initializer h0 tf.expand dims z0 1 Where h0 is 256 90 z0 is expanded to 256 1 and x is None 256 400 . It worked in the previous version so I am assuming the trouble is related to the tf.scan code. ValueError The shape for scan while Merge 2 0 is not an invariant for the loop. It enters the loop with shape 256 1 but has shape 256 after one iteration. Provide shape invariants using either the shape invariants argument of tf.while loop or set shape on the loop variables. ,I have fixed my problem by adding an additional expand dims and an additional squeeze at the end. However I find it curious that it still produced this error in the new version whereas it didn t in the previous one. yuanbyu any ideas I am facing the exact same error. Relevant code below The error i receive is ValueError The shape for scan while scan while Merge 1 0 is not an invariant for the loop. It enters the loop with shape 2 2 but has shape unknown after one iteration. Provide shape invariants using either the shape invariants argument of tf.while loop or set shape on the loop variables. RiaanZoetmulder could you please elaborate exactly where you added the additional expand dims and squeeze to resolve the error Thanks Bhuwan bdhingra RiaanZoetmulder I met the same question exactly as you described. Could you give me some advice about how to fix this problem 
8277,Different Code Path Taken in conv2d for Constant vs Variable Filter,It appears that a different code path is taken in conv2d for Constant vs Variable filters. On a box with GPU this works however this fails with Check failed data format FORMAT NHWC Generic conv implementation only supports NHWC tensor format for now. Both of these work the data format is supported and I don t see a reason for the Constant filter to take what I am guessing is a less efficient code path not using GPU op than the Variable Filter. To make things even weirder it seems like even though the GPU is not being used for the Constant there is still cuda memcpy . Run on 20 calls ,Is there any way to convert a pre trained NCHW weight into NHWC and load it into the same graph What do you mean Weights are always stored the same way. tfboyd Any ideas Hi cancan101 I ve a net graph with some conv2D and maxPool operations declared using NCHW format. I ve trained the net on GPUs and created some checkpoints. The graph and the model works well on GPUs when you test by restoring the checkpoints. Now I want to test the same code graph on CPU. The same net graph is not working because the conv2D and maxPool operations do not run since they were declared using NCHW. I m getting the following error So I ve modified the net so that the operations are declared using NHWC format. Is it possible to restore the same checkpoint weights with this new net graph without retraining the new graph that has NHWC ops finally I want to freeze the graph and have a standalone graph with weights to run test forward pass. I was not able to create a standalone graph using checkpoints I ve got when I trained me NCHW graph and use the standalone graph on CPU for testing running forward pass Is there a way to do it It has been 14 days with no activity and this issue has an assignee.Please update the label and or status accordingly. I tried with tensorflow gpu v1.4.0 and wasn t able to reproduce this issue. Close for now. Please feel free to reopen it if you think the issue still exists. 
8701,InvalidArgumentError using tf.learn and eval,See https github.com google seq2seq issues 103 for details and user logs. TLDR I m using tf.learn and for some people the evaluation fails with shape errors. This seems to be some kind of GPU memory sharing issue as subsequent runs seem to consistently increase the shape size Evaluation works independently when there is no training in progress. It also doesn t happen when using the CPU only. I personally have run into similar issues before then multiple processes were trying to share the GPU but that shouldn t be the case here., dennybritz can you identify which line of python caused this error My guess is this is related to the argument reorder on metrics... from the TensorFlow release notes. So go find those and insert explicit keyword argument labels appropriately and it should fix it. Let me know if that fixes you problem. Thanks I don t think that is the issue. All arguments are named and it only happens to some users I don t have this problem myself . Also if you read the original issue you can see that some users managed to fix it by disabling bucketing or reducing the dev data. It looks very much like some kind of GPU memory issue to me. I m sorry I missed that. I ve seen many examples of shape errors being caused by that change so it is worth a shot. Since this issue links to a incomplete summary and then 4 more issues which are themselves just blobs of erros it s pretty hard to follow what is going on. Let me ask a few clarifying questions since you ve spent a good deal of time looking at this it appears. 1. What versions of TensorFlow are people using compiled from source from the master branch is not useful since master changes all the time. 2. Did all of them manage to run it on CPU only successfully 3. What versions of CUDA are people using Maybe there is a pattern. 4. What is the command I can run to try this example Where is the code 1. 1.0.0 or 1.0.1 The code only works with that version. 2. The people I ve talked to yes. But I will try to ask more. 3. Not sure will try to ask. 4. The command in this tutorial https google.github.io seq2seq nmt So this seems to be related to train and evaluate in tf.learn see https github.com google seq2seq issues 103 issuecomment 293796457. It seems like the only solution right now is to monkeypatch the Experiment class This has the patch https github.com google seq2seq pull 173 Did you mean to reopen Yes. I patched it in my code but that doesn t seem right. I wonder what the official solution is. On Mon Apr 17 2017 at 3 59 PM drpngx notifications github.com wrote Did you mean to reopen You are receiving this because you modified the open close state. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 8701 issuecomment 294619545 or mute the thread https github.com notifications unsubscribe auth AAYmvTlOCPdaVSYW6UmaSTRNSCvMiV88ks5rw 63gaJpZM4MoqzO . OK I wonder if it s best to use the seq2seq thread or this one. sguada what do you think of this issue It appears that it is related to tf.learn . Automatically closing due to lack of recent activity. Since this issue is old at this point please reopen the issue if it still occurs when tried with the latest version of Tensorflow. Thank you. I meet the same problem. TF versio 1.3.1 python version 3.5.2 code https gist.github.com jinyu121 2c2e22f984a993a86f11f1be98f1d14c Sorry for the Chinese comments... It s just my note data the flower dataset used in tensorflow examples data convert create train and test TXT file with the format image path image label log log.log https github.com tensorflow tensorflow files 1351702 log.log 
9895,XLA Aborted core dumped ,OS Ubuntu Linux 16.04 TensorFlow Compiled from source TensorFlow Version r1.1 Bazel Version 0.4.5 CUDA CuDNN Versions 8.0 5.1 GPU Model Memory TitanX 12Gb After turning on XLA JIT compiling TF fails with a core dump. ,Thanks for reporting Do you have a simple script to repro CC tatatodd I am implementing the IRNN https arxiv.org pdf 1504.00941v2.pdf paper but the RNN layer runs really slow which prompted me to give XLA a try. I have done my best to simplify the code as much as possible. I hope this helps. Hey tatatodd could you take a look when you get a chance Thanks Ali Hi Guys Just checking in Any progress on this issue Cheers Tom I m also running into this problem. I m running into this problem too when I run the mnist softmax xla.py. I m also seeing this problem on Ubuntu 16.04 Linux ppc64le platform with tensorflow 1.1.0 built with XLA enabled. Any update on this It s an easily reproduced issue that makes using XLA impossible but there hasn t been anyone assigned to this ticket. Apologies on the long delay. I m taking a look at this now. BTW if anyone who s running into this problem has a smaller repro script that would be appreciated. I believe wj1066 reported that he s having the same issue with mnist softmax xla.py https github.com tensorflow tensorflow blob master tensorflow examples tutorials mnist mnist softmax xla.py . I tried to simplify the script I provided above but not sure I can make it any simpler. Thanks for looking into this issue. thomasquintana I replicated your issue on TF 1.1.0 build with XLA enabled using the python program you provided. FYI the CHECK failure indicates a logical error in the compiler I also tried the same thing on TF 1.2.0 rc2 and the failure goes away. Looking through the commit history of algebraic simplifier.cc there have been many changes to the code. I did not identify exactly which change fixed the problem. So the solution is to use TF 1.2.0 rc2 or later. As a reminder different TF releases can be found here https hub.docker.com r tensorflow tensorflow tags I m closing this out if you feel this doesn t resolve the issue just comment on this thread and I ll re open. Your CPU supports instructions that this TensorFlow binary was not compiled to use AVX2 FMA 2019 03 09 16 58 39.308046 W tensorflow compiler xla service platform util.cc 240 unable to create StreamExecutor for CUDA 0 failed initializing StreamExecutor for CUDA device ordinal 0 Internal failed call to cuDevicePrimaryCtxRetain CUDA ERROR OUT OF MEMORY out of memory total memory reported 11523260416 2019 03 09 16 58 39.308242 F tensorflow stream executor lib statusor.cc 34 Attempting to fetch value instead of handling error Internal no supported devices found for platform CUDA Aborted core dumped tensorflow deeplab deeplab X399 DESIGNARE EX Tracy FasterRCNN tf faster rcnn master Attempting to fetch value instead of handling error Internal no supported devices found for platform CUDA Attempting command not found Deprecated in favor of operator or tf.math.divide. 2019 07 11 12 42 06.524160 I tensorflow core platform cpu feature guard.cc 141 Your CPU supports instructions that this TensorFlow binary was not compiled to use AVX2 FMA 2019 07 11 12 42 11.226602 W tensorflow compiler xla service platform util.cc 240 unable to create StreamExecutor for CUDA 0 failed initializing StreamExecutor for CUDA device ordinal 0 Internal failed call to cuDevicePrimaryCtxRetain CUDA ERROR OUT OF MEMORY out of memory total memory reported 11523260416 2019 07 11 12 42 11.399046 F tensorflow stream executor lib statusor.cc 34 Attempting to fetch value instead of handling error Internal no supported devices found for platform CUDA Aborted core dumped this is the error appearing need help 2019 10 31 14 55 52.525459 I tensorflow core platform cpu feature guard.cc 141 Your CPU supports instructions that this TensorFlow binary was not compiled to use AVX2 FMA 2019 10 31 14 55 52.625544 W tensorflow compiler xla service platform util.cc 240 unable to create StreamExecutor for CUDA 0 failed initializing StreamExecutor for CUDA device ordinal 0 Internal failed call to cuDevicePrimaryCtxRetain CUDA ERROR OUT OF MEMORY out of memory total memory reported 4236312576 2019 10 31 14 55 52.625882 F tensorflow stream executor lib statusor.cc 34 Attempting to fetch value instead of handling error Internal no supported devices found for platform CUDA Aborted core dumped 
17130,Java SIGSEGV when Tensors.create ing from an uninitialized array, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 macOS 10.13.3 JRE version Java TM SE Runtime Environment 8.0 121 b13 build 1.8.0 121 b13 TensorFlow installed from source or binary maven TensorFlow version use command below 1.4.0 Python version N A Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Exact command to reproduce Describe the problem Using Tensors.create to get a Tensor String from an uninitialized 2D byte array byte results in a SIGSEGV from JNI. See full log below. Source code logs When running the above test case I get the following output. I m not familiar enough with JNI in general to know if I m expecting too much. I won t be surprised if you mark this wontfix I was just surprised to be able to crash the process with a segfault given some bad data. Easy enough to work around I will size init my arrays more carefully but thought you might want to know. Cheers Thanks so much for this library ,Thanks very much for the report Yes this is indeed a bug it should not be possible to cause the JVM to segfault when using the Java API . Will send a fix shortly. 
23413,Models cannot be loaded, System information What is the top level directory of the model you are using Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes written a simple MWE script OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04.1 TensorFlow installed from source or binary Compiled from source TensorFlow version use command below 1.11.0 Python version 3.6.6 Bazel version if compiling from source 0.16.1 GCC Compiler version 7.3.0 CUDA cuDNN version Cuda 9.2.148 cuDNN 9.2 v7.3.1.20 GPU model and memory GeForce GTX1080Ti 11GB Exact command to reproduce Minimal working example code Describe the problem Returns the following output error included Same is true for all other models that are being trained Source code logs Output of tf env attached tf env.txt https github.com tensorflow models files 2535750 tf env.txt ,After some testing the problem can be resolved by replacing the model with But it seems strange that replacing that replacing the ReLU solves the issue and the error is quite unclear . fchollet PTAL Hi all I am experiencing the same problem. However simply replacing layers.RelU with layers.Activation relu does not yield the same training result in my case. Of course I checked if I randomized the data in a non deterministic way but in all usages of randomization I explicitely set the seed to the same value. Is there a systematic difference between ReLU and Activation relu which I fail to see Thanks for your help I was able to execute your code snippet successfully using TF 1.13.0 rc0 Thanks 
26922,AttributeError tensorflow.python.framework.ops.EagerTensor object has no attribute swapaxes , using keras API in tensorflow 2.0,How do you import keras are you importing from tensorflow Have you tried import tf.keras yes i have used tf.keras UnknownError Traceback most recent call last ipython input 14 c90c3b2c3d99 in module 3 earlystopper tf.keras.callbacks.EarlyStopping patience 5 verbose 1 4 checkpointer tf.keras.callbacks.ModelCheckpoint test 1.h5 verbose 1 save best only True 5 results model.fit X train Y train validation split 0.1 batch size 16 epochs 50 shuffle True steps per epoch None anaconda3 lib python3.7 site packages tensorflow python keras engine training.py in fit self x y batch size epochs verbose callbacks validation split validation data shuffle class weight sample weight initial epoch steps per epoch validation steps validation freq max queue size workers use multiprocessing kwargs 871 validation steps validation steps 872 validation freq validation freq 873 steps name steps per epoch 874 875 def evaluate self anaconda3 lib python3.7 site packages tensorflow python keras engine training arrays.py in model iteration model inputs targets sample weights batch size epochs verbose callbacks val inputs val targets val sample weights shuffle initial epoch steps per epoch validation steps validation freq mode validation in fit prepared feed values from dataset steps name kwargs 350 351 Get outputs. 352 batch outs f ins batch 353 if not isinstance batch outs list 354 batch outs batch outs anaconda3 lib python3.7 site packages tensorflow python keras backend.py in call self inputs 3237 value math ops.cast value tensor.dtype 3238 converted inputs.append value 3239 outputs self. graph fn converted inputs 3240 return nest.pack sequence as self. outputs structure 3241 x.numpy for x in outputs anaconda3 lib python3.7 site packages tensorflow python eager function.py in call self args kwargs 538 raise TypeError Keyword arguments unknown. Expected . .format 539 list kwargs.keys list self. arg keywords 540 return self. call flat args 541 542 def filtered call self args kwargs anaconda3 lib python3.7 site packages tensorflow python eager function.py in call flat self args 607 Only need to override the gradient in graph mode and when we have outputs. 608 if context.executing eagerly or not self.outputs 609 outputs self. inference function.call ctx args 610 else 611 self. register gradient anaconda3 lib python3.7 site packages tensorflow python eager function.py in call self ctx args 414 attrs executor type executor type 415 config proto config 416 ctx ctx 417 Replace empty list with None 418 outputs outputs or None anaconda3 lib python3.7 site packages tensorflow python eager execute.py in quick execute op name num outputs inputs attrs ctx name 64 else 65 message e.message 66 six.raise from core. status to exception e.code message None 67 except TypeError as e 68 if any ops. is keras symbolic tensor x for x in inputs anaconda3 lib python3.7 site packages six.py in raise from value from value UnknownError AttributeError tensorflow.python.framework.ops.EagerTensor object has no attribute swapaxes Traceback most recent call last File home adventum anaconda3 lib python3.7 site packages tensorflow python ops script ops.py line 205 in call return func device token args File home adventum anaconda3 lib python3.7 site packages tensorflow python ops script ops.py line 107 in call ret self. func args File ipython input 12 1bbcb510438c line 58 in iou metric batch value iou metric y true in batch y pred in batch File ipython input 12 1bbcb510438c line 2 in iou metric labels label y true in 0.5 File home adventum anaconda3 lib python3.7 site packages skimage measure label.py line 93 in label return clabel input neighbors background return num connectivity File skimage measure ccomp.pyx line 351 in skimage.measure. ccomp.label cython File skimage measure ccomp.pyx line 325 in skimage.measure. ccomp.reshape array File skimage measure ccomp.pyx line 302 in skimage.measure. ccomp. apply swaps AttributeError tensorflow.python.framework.ops.EagerTensor object has no attribute swapaxes node metrics 1 my iou metric EagerPyFunc Op inference keras scratch graph 47213 shashanka300 Could you fill the issues template here https github.com tensorflow tensorflow issues new template 00 bug performance issue.md . Were you able to install TF2.0 without any issues What were the commands used to install TF Other than keras were you able to run any TF code Please provide more details to find root cause of the issue. Thanks yes i was able to get a glean installation for GPU with cuda 10.1 and cundd 7.5 after trying a few time standard commands provided on website haven t explored much yet the issue occurs whenever i try to use meaniou accuracy metric which has been changed in the current version even after changing it to adapt to the current version using the source code. shashanka300 1. Could you provide a code to reproduce the bug 2. Was there any issue when you ran any tf.keras tutorials on the TF website Please provide as many details along with the code as possible to find root cause of the issue. Thanks 1.I can reveal the exact on but https www.kaggle.com aglotero another iou metric this code and https www.kaggle.com keegil keras u net starter lb 0 277 if ported to tensorflow 2.0 gives the same error when using mea iou as accuracy metric. 2. no I get same error in using tensorflow with eager execution For example cross entropy y hat y OUT array 2.30258509 0.69314718 If I run cross entropy y hat y I will get correct result but if I use An error occurred I think it may be that tensorflow with eager execution has made a mistake. Is there any update on this I am also having the same issue. just check the functions if they are available in 2.0 Hi just ran into same issue. Looks like operator overloading for eager tensors is the issue. Here is a simple code to reproduce error. this works Below code does not work gives same error. tensorflow 2.0 CUDA 10 CuDNN 7.6 ERROR I m closing this issue as it looks like something like 3 or 4 completely unrelated issues. I ll address them separately. harrylyx your cross entropy function uses numpy methods like ndarray.tolist those are not supported on tf tensors which is why you re seeing this error. Use tf methods and then eager mode and gradients will work. hegman12 your code is creating a new tf.Variable every time you call l x which I m pretty sure is not what you intended. Please reopen specific issues derived from this one with clear instructions to reproduce. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 26922 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 26922 No a I m also having an issue with using pyfunction in dataset... tensorflow 2.0b CUDA 10.0 my code def read npy file item data np.load item.decode return data.astype uint8 dataset tf.data.Dataset.from tensor slices false list 3 dataset dataset.map lambda item tuple tf.py function read npy file item tf.uint8 for file in dataset.take 3 error print file pass Please file a separate issue and follow the issue template. On Tue Jun 18 2019 at 11 09 AM Alon Samuel notifications github.com wrote I m also having an issue with using pyfunction in dataset... tensorflow 2.0b CUDA 10.0 my code def read npy file item data np.load item.decode return data.astype uint8 dataset tf.data.Dataset.from tensor slices false list 3 dataset dataset.map lambda item tuple tf.py function read npy file item tf.uint8 for file in dataset.take 3 error print file pass You are receiving this because you modified the open close state. Reply to this email directly view it on GitHub https github.com tensorflow tensorflow issues 26922 email source notifications email token AAABHRJGATVLC6N24LKX5ZDP3EQHBA5CNFSM4G72RZI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODX7O6XY issuecomment 503246687 or mute the thread https github.com notifications unsubscribe auth AAABHRITOXN5O2UZAXDPN3DP3EQHBANCNFSM4G72RZIQ . Alex 
27181, TF 2.0 alpha Tutorial Using TFRecords and tf.Example can t run on GPU, em Please make sure that this is a documentation issue. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag doc template em System information TensorFlow version Tensorflow 2.0 alpha Doc Link Using TFRecords and tf.Example https www.tensorflow.org alpha tutorials load data tf records Describe the documentation issue I don t know why this tutorial only works for tensorflow CPU version. If I run on GPU I will have an error in the cell tf serialize example f0 f1 f2 f3 Does it have a specific thing only for CPU We welcome contributions by users. Will you be able to update submit a PR use the doc style guide https www.tensorflow.org community documentation to fix the doc Issue , shaolinkhoa The tutorial works fine when executed on cpu as well as gpu. Can you please try using google colab https colab.sandbox.google.com notebooks welcome.ipynb recent true and confirm Thanks ymodak Yes I already tested on the google colab. Currently the tutorial is using tensorflow for cpu as you can see in the code install tensorflow 2.0.0 alpha0 so google colab runs perfectly on cpu. If I change to pip install tensorflow gpu 2.0.0 alpha0 I also change the setting to GPU then I get the error at this line tf serialize example f0 f1 f2 f3 shaolinkhoa Apologies for the delay in response. Thanks for trying TF 2.0 alpha. I was able to reproduce reported behavior using TF 2.0 alpha gpu in google colab gpu environment. Ayush517 Can you please take a look Thanks ymodak I was also able to reproduce the issue. I got almost the same error but the number of bytes to be copied was different for the same code. According to shaolinkhoa s error there are 15 bytes to be copied whereas my error shows 11 bytes to be copied. Mine showed this RuntimeError Error copying tensor to device CPU 0. Can t copy 11 bytes of a tensor into another with 8 bytes buffer. Looking into it. In the meantime I found another one like this 26951 It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue tensorflowbutler I just tested on google colab with tensorflow gpu. It works fine. When did you guys fix this Do you know what is the problem Closing this issue since its resolved. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27181 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27181 No a I also have the same problem in tensorflow gpu 2.0.0 beta UnknownError Traceback most recent call last ipython input 24 0d21f525f9e6 in module 1 tf serialize example f0 f1 f2 f3 2 tf. version ipython input 20 674875c41404 in tf serialize example f0 f1 f2 f3 2 tf string tf.py function serialize example 3 f0 f1 f2 f3 pass these args to the above function. 4 tf.string the return type is tf.string . 5 return tf.reshape tf string The result is a scalar 6 tf20 lib python3.5 site packages tensorflow python ops script ops.py in eager py func func inp Tout name 390 if func returns None. 391 392 return internal py func func func inp inp Tout Tout eager True name name 393 394 tf20 lib python3.5 site packages tensorflow python ops script ops.py in internal py func func inp Tout stateful eager is grad func name 279 if eager 280 result gen script ops.eager py func 281 input inp token token Tout Tout name name 282 else 283 if stateful tf20 lib python3.5 site packages tensorflow python ops gen script ops.py in eager py func input token Tout name 63 else 64 message e.message 65 six.raise from core. status to exception e.code message None 66 Add nodes to the TensorFlow graph. 67 token execute.make str token token usr lib python3 dist packages six.py in raise from value from value UnknownError RuntimeError Error copying tensor to device CPU 0. Can t copy 11 bytes of a tensor into another with 8 bytes buffer. Traceback most recent call last File home jeonghwan tf20 lib python3.5 site packages tensorflow python ops script ops.py line 207 in call return func device token args File home jeonghwan tf20 lib python3.5 site packages tensorflow python ops script ops.py line 121 in call self. convert ret dtype self. out dtypes 0 File usr lib python3.5 contextlib.py line 77 in exit self.gen.throw type value traceback File home jeonghwan tf20 lib python3.5 site packages tensorflow python eager context.py line 586 in mode yield File home jeonghwan tf20 lib python3.5 site packages tensorflow python ops script ops.py line 109 in call ret self. func args File ipython input 18 aca852a2de23 line 10 in serialize example feature2 bytes feature feature2 File ipython input 15 99e749525cc0 line 4 in bytes feature value value.numpy BytesList won t unpack a string from an EagerTensor. File home jeonghwan tf20 lib python3.5 site packages tensorflow python framework ops.py line 769 in numpy maybe arr self. cpu nograd . numpy pylint disable protected access File home jeonghwan tf20 lib python3.5 site packages tensorflow python framework ops.py line 952 in cpu nograd return self. copy nograd context.context CPU 0 File home jeonghwan tf20 lib python3.5 site packages tensorflow python framework ops.py line 895 in copy nograd new tensor self. copy to device context ctx. handle device device name RuntimeError Error copying tensor to device CPU 0. Can t copy 11 bytes of a tensor into another with 8 bytes buffer. Op EagerPyFunc 
27428,Tensorflow 2.0.0 multiple GPU output zero for non root GPU, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device No. TensorFlow installed from source or binary Source. TensorFlow version use command below v2.0.0 alpha0 0 g2c319fb415 2.0.0 alpha0 Python version 3.7 Bazel version if compiling from source 0.23.0 GCC Compiler version if compiling from source GCC 4.8 CUDA cuDNN version CUDA 10.0 cuDNN 7 GPU model and memory A simple model can reproduce. You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION When I run the tf env collect.sh script tensorflow just hangs there forever. I couldn t even kill it. What the hell is going on Describe the current behavior Run the test code below. For GPU 0 the behavior is normal but for GPU 1 the output becomes zero. Describe the expected behavior The output of GPU 1 should be the same as GPU 0. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. run python test.py . Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. My running log of test.py python sess tf.Session with sess.as default tensor tf.range 10 print op tf.print tensor with tf.control dependencies print op out tf.add tensor tensor sess.run out As for my GPUs the driver version is 410.104 installed according to official instructions. Hope it helps. Could anybody help me ASAP I am almost crazy with TF s multiple GPU. Thank you very much ,why are you trying to use v2.0.0 alpha0 0 g2c319fb415 2.0.0 alpha0. if your code seems to be TF 1 I am writing code like this because I want to see what the math operation is as clearly as possible. As for using TF2 it is just following up with the latest version. I can confirm you re not using the version of TF you claim to be using. The current 1.x nightly shows v1.12.0 11729 g98c3cfbf74 1.14.1 dev20190404 as the version string while you have the one from the 2.x alpha. It also looks like your cuda installation is borked somehow. Have you tried using an nvidia provided docker image I have tried nightly build and it seems to work Check forward Check GPU 0 2019 04 04 17 40 19.590663 W tensorflow compiler jit mark for compilation pass.cc 1284 One time warning Not using XLA CPU for cluster because envvar TF XLA FLAGS tf xla cpu global jit was not set. If you want XLA CPU either set that envvar or use experimental jit scope to enable XLA CPU. To confirm that XLA is active pass vmodule xla compilation cache 1 as a proper command line flag not via TF XLA FLAGS or set the envvar XLA FLAGS xla hlo profile. 2019 04 04 17 40 19.591831 I tensorflow stream executor platform default dso loader.cc 42 Successfully opened dynamic library libcublas.so.10.0 fc1 weight bias 8.02037048 0 fc1 2.59844 fc1 weight bias 8.02037048 0 fc2 weight bias 2.84391737 0 fc2 0.13365 Check GPU 1 fc1 weight bias 8.02037048 0 fc1 2.59844 fc1 weight bias 8.02037048 0 fc2 weight bias 2.84391737 0 fc2 0.13365 Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27428 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27428 No a 
27429,ValueError Invalid tensors Mul were found.,This template is for miscellaneous issues not covered by the other issue categories. For questions on how to work with TensorFlow or support for problems that are not verified bugs in TensorFlow please go to StackOverflow https stackoverflow.com questions tagged tensorflow . If you are reporting a vulnerability please use the dedicated reporting process https github.com tensorflow tensorflow blob master SECURITY.md . For high level discussions about TensorFlow please post to discuss tensorflow.org for questions about the development or internal workings of TensorFlow or if you would like to know how to contribute to TensorFlow please post to developers tensorflow.org. Python 2.7 tensorflow 1.9 python script input arrays Mul output arrays final result converter tf.contrib.lite.TocoConverter.from frozen graph graph def file input arrays output arrays tflite model converter.convert open model.tflite wb .write tflite model It was working fine a month before.Now it shows the following error File home ubuntu .local lib python2.7 site packages tensorflow contrib lite python convert saved model.py line 205 in get tensors from tensor names .join invalid tensors ValueError Invalid tensors Mul were found. , jennings1716 You mentioned it was working well a month before. What was the change you made recently Did you upgrade tensorflow or python or any other dependencies Could you provide a code to reproduce bug Could you try it in Google colab https colab.sandbox.google.com to verify whether it is working or not. Can you also try to upgrade your code to latest TF. Thanks Hi jvishnuvardhan It was working fine... When I use the same input and output tensor names for the tflite conversion i got the error... Then I changed the tensor names to input array lambda 1 Mul output array model 2 model 1 dense 2 Softmax It works with above names... I wonder why... jennings1716 Could you check with TF1.13.1 and see whether the bug persists with latest version. You could also try TF2.0. Thanks Hi jvishnuvardhan It is working for python 3.5 Mul final result But for Python 2.7 there remains the bug Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27429 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27429 No a 
27802,Autograph fails after restoring from the checkpoint, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow no TensorFlow installed from source or binary TensorFlow version use command below Python version 3.7.1 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version not relevent GPU model and memory not relevent Describe the current behavior Running the following code for the first time show no warning. However running it the second time results in this Warning message Describe the expected behavior Autograph should works just find even after restoring from a checkpoint. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,I could reproduce the issue with TF2.0.0 alpha0. Thanks Thanks for reporting this bug The error looks familiar and might be fixed at head. I ll verify. Sorry for the delay. This is fixed in tf nightly. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27802 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27802 No a 
27856,get error message when use tf.summary.scalar with TF 2.0, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below tf nightly gpu 2.0 preview 2.0.0 dev20190413 Python version 3.6.7 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version 10.0 GPU model and memory Yes 11GB You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with python c import tensorflow as tf print tf.GIT VERSION tf.VERSION Describe the current behavior I get the following error message when I use tensorflow in GPU mode. When I use tensorflow in CPU mode such error message doesn t occur. AttributeError module tensorboard.summary. tf.summary has no attribute summary scope Describe the expected behavior I expect no error when I call tf.summary.scalar in TF2.0 in both GPU and CPU mode. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,Just in case it might help I got past this issue today by running pip3 install upgrade force reinstall tb nightly thx. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 27856 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 27856 No a 
28144,pb file cannot run on my mobile devices with android demo but it pass with python scripts,here is my log No OpKernel was registered to support Op Range with these attrs. Registered devices CPU Registered kernels device CPU Tidx in DT INT32 device CPU Tidx in DT FLOAT node unpool range Range Tidx DT INT64 unpool range start unpool range limit unpool range delta I don t know why the Range operation s data type is int64 how can I solve this probelm If this is the cause why my pb files can run pass with python on pc , sunzhe09 Could you provide more details about the issue and context Also it would be great if you can provide any commands you followed. Please provide as many details as possible to resolve the issue faster. Thanks sorry for my lately here is my test model.pb https github.com Joker316701882 Salient Object Detection I convert it the input node is Placeholder output node is sigmoid you can freeze it sunzhe09 any updates or progress made im stuck on a similar error. azilaazman No I just change my tensorflow to 1.12.2 hope it helps Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28144 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28144 No a 
28711,Tensorflow lite demo app gives wrong result in GPU delegate Honor Play Android 9.0 GPU Turbo , System information OS Platform and Distribution Android 9.0 API 28 Mobile device Huawei Honor Play 9.0 GPU Mali G72 MP12 GPU Turbo TensorFlow installed from tensorflow lite 0.0.0 gpu experimental Describe the current behavior The tensorflow lite gpu delegate demo application gives incorrect results for image classification when it is run on GPU whereas the same float model gives correct results when it is run on CPU.Even the quantized model in the demo application gave correct inference results in this application.We even tried the official deeplab model for semantic segmentation with the same phone but even in this scenario it gave wrong results square stripes instead of correct masks. The same model is running in other phones with Adreno GPU and also in some phones with Mali GPU eg Samsung A8 Android 9.0 . Describe the expected behavior The tensorflow lite gpu inference should give same results in cpu and gpu in all the android phones. Other info logs It looks like the phone uses a new feature called GPU Turbo .Initially the model was working correctly with android 8.1 stock os .But after the 9.0 upgrade the tflite models are giving wrong results. ,It is working fine in tensorflow lite 0.0.1 gpu experimental and latest nightly version ... Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28711 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28711 No a 
28831,tf nightly gpu 2.0 very slow on tape.gradient ,While migrating to tensorflow 2.0 I found a big performance issue. tensorflow gpu 1.13.1 takes 0.7 seconds tensorflow gpu 2.0 nightly takes 2.4 seconds removing gradients tape.gradient loss params and ggn tf.linalg.global norm gradients takes 0.3 seconds That means gradients tape.gradient loss params is consuming most of the time ,I also noticed that tensorflow2.0a is much faster than the latest nightly build with keras when updating the weights. Can you add full instructions to reproduce This feels like a serious problem but it could be caused by many different things and so having working code we can debug would be very helpful. Or if you don t want to share your code if you re willing to run a git bisect and share the results with us that d also be great. alextp my full code is in this notebook https github.com alew3 huia experience blob master 02 train train huia poses keras tf2.ipynb I timed this snippet of training as an example it is running 3 times slower in the nightly. Visually the slow down seems to happen when it starts updating the weights. start datetime.now model.fit train dataset epochs epochs steps per epoch steps per epoch verbose 1 validation data test dataset callbacks callbacks end datetime.now print f total time of end start for epochs epochs tensorflow version tf. version and run twice the code for each version of tensorflow TF2 Alpha total time of 0 01 00.676783 for 5 epochs tensorflow version 2.0.0 alpha0 total time of 0 01 02.069855 for 5 epochs tensorflow version 2.0.0 alpha0 TF2 Nightly total time of 0 03 20.682476 for 5 epochs tensorflow version 2.0.0 dev20190521 total time of 0 03 21.514782 for 5 epochs tensorflow version 2.0.0 dev20190521 Can you give me a shorter example to reproduce I can t quite run your notebook as I don t have the training data or the filesystem setup in the way you do. alextp I ve cut down the code and included the training data in two different collabs to make things easier to reproduce with the only change being the version of Tensorflow 2 used. Tensorflow Nightly is 3x slower. TensorFlow 2 Alpha GPU https colab.research.google.com drive 1AOhKl18zI W1YZ2BPTHO e0zezUgw6Jh scrollTo OaNzmAVbosq0 run 1. total time of 0 05 39.198438 for 5 epochs tensorflow version 2.0.0 alpha0 run 2. total time of 0 05 30.281252 for 5 epochs tensorflow version 2.0.0 alpha0 Tensorflow 2 Nightly GPU 2.0.0 dev20190522 https colab.research.google.com drive 1Z9M ovwlUjHUiswW3a29aS67CsaOHjWo scrollTo 3cVLAo9Mospi run 1. total time of 0 15 31.170586 for 5 epochs tensorflow version 2.0.0 dev20190522 Enabling device logging during model.fit I see that it is mostly running on the GPU. But I m not sure what Executing op ExperimentalDatasetCardinality in device job localhost replica 0 task 0 device CPU 0 or Executing op inference keras scratch graph 264441 in device unspecified do but they happen when things start to slow down. Executing op CloseSummaryWriter in device job localhost replica 0 task 0 device CPU 0 Executing op DestroyResourceOp in device job localhost replica 0 task 0 device CPU 0 Epoch 1 20 Executing op ExpandDims in device job localhost replica 0 task 0 device GPU 0 W0522 20 40 51.144862 140192919144256 deprecation.py 323 From home ale anaconda3 envs tensor20 lib python3.7 site packages tensorflow python ops math grad.py 1250 add dispatch support. locals .wrapper from tensorflow.python.ops.array ops is deprecated and will be removed in a future version. Instructions for updating Use tf.where in 2.0 which has the same broadcast rule as np.where Executing op VarHandleOp in device job localhost replica 0 task 0 device GPU 0 Executing op AssignVariableOp in device job localhost replica 0 task 0 device GPU 0 Executing op Fill in device job localhost replica 0 task 0 device GPU 0 Executing op VarHandleOp in device job localhost replica 0 task 0 device GPU 0 Executing op LogicalNot in device job localhost replica 0 task 0 device GPU 0 Executing op Assert in device job localhost replica 0 task 0 device GPU 0 Executing op VarHandleOp in device job localhost replica 0 task 0 device GPU 0 Executing op VarHandleOp in device job localhost replica 0 task 0 device GPU 0 Executing op VarHandleOp in device job localhost replica 0 task 0 device GPU 0 Executing op VarHandleOp in device job localhost replica 0 task 0 device GPU 0 Executing op VarHandleOp in device job localhost replica 0 task 0 device GPU 0 Executing op VarHandleOp in device job localhost replica 0 task 0 device GPU 0 Executing op Cast in device job localhost replica 0 task 0 device GPU 0 Executing op inference keras scratch graph 13159 in device unspecified 68 70 . ETA 0s loss 0.8524 accuracy 0.7154 Executing op ExperimentalDatasetCardinality in device job localhost replica 0 task 0 device CPU 0 Executing op inference keras scratch graph 14812 in device unspecified Executing op WriteScalarSummary in device job localhost replica 0 task 0 device CPU 0 Executing op WriteScalarSummary in device job localhost replica 0 task 0 device CPU 0 Executing op WriteHistogramSummary in device job localhost replica 0 task 0 device CPU 0 70 70 68s 973ms step loss 0.8384 accuracy 0.7196 val loss 1.9080 val accuracy 0.3720 iganichev could this be related to placer changes Unlikely. The new placer should have the same placement decisions safe for some rare corner cases for graphs without functions outputting resources on multiple devices. It is unlikely that keras produces such graphs. Also Dataset ops have not been integrated into the recursive placement framework. It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue The issue persists. Very slow updating gradients on the nightly and beta compared to the alpha. The issue persists. robieta the notebook which is slow seems to be using model.fit. Do you know what part of it changed between alpha and nightly that could cause this alew3 this is still a very large example to reproduce and the notebook you shared doesn t overlap at all in terms of code with the thing starting this post. It s quite hard to understand what the actual problem is. alextp sorry I tried to cut down the code to the essential. It is just a simple image classification training via transfer learning using mobilenet with keras and tf.data.dataset as recommended by Tensorflow guides. But the slow down is very noticeable on the newer versions of TF2 when it updates the weights. It is so slow that makes it impracticable for real work. I can t guarantee it is the same reason as the original poster s problem. For example can you reproduce the slowdown with just a constant size numpy array Can you reproduce with a smaller keras model If you can reproduce with a numpy array with random numbers instead of a real dataset this gets much easier for me to reproduce. Similarly the smallest the model that is slow the easier it is for me to know what is going on. I would echo alextp s point about synthetic data. It s often a source of much complexity in a model and if an issue reproduces with a synthetic pipeline it drastically narrows down the possible causes. And a similar line of reasoning goes for models. Complex topologies are hard to debug. Other than that however I thought it was a very high quality repro and much appreciated. I ve tweaked it to use a synthetic pipeline https colab.research.google.com gist robieta 0c8a007cf1ec2181872511b1367809e7 tf issue 28831 repro.ipynb The issue is this bit here There are 4191 examples hence 20 4191 838 . However at this time the dataset is already batched so at a batch size of 48 that actually becomes 838 batches 48 838 examples 9.6 epochs . So you are actually telling the dataset to process nearly 10 epochs before starting validation. The results of skip are not used by test dataset they are simply discarded This of course also has the unfortunate effect that it does not preserve the train test split. Generally I would recommend making the split in the dataset as early as possible as it tends to be both more performant and less error prone. The reason that this is showing up now is that there have been some changes to how Model.fit interacts with datasets so the dataset is making a new iterator each time. As opposed to the alpha code where the inefficiency was only in the first epoch. tomerk Is iterator reuse across epochs via the steps per epoch argument on your radar That said I think the original issue reported by nicoosokhan seems to be different from the behavior observed by alew3. nicoosokhan if you can provide a minimal repro colab we would be very interested to see what s going on with GradientTape. qlzh727 For the steps per epoch iterator reuse behavior as well. robieta thanks for the feedback I also tried running my code on multiple gpus with mirror distribution strategy and was running out of data so that s probably related I m going to go ahead and close this out since it seems to be resolved. Feel free to reopen if I missed something. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28831 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28831 No a 
28846,TF 2.0 crossed column on Windows fails with SystemError built in function TFE Py FastPathExecute returned a result with an error set, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Windows 10 Home Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below v1.12.0 9492 g2c319fb415 2.0.0 alpha0 Python version 3.6.8 Anaconda Inc. default Feb 21 2019 18 30 04 MSC v.1916 64 bit AMD64 Bazel version if compiling from source NA GCC Compiler version if compiling from source NA CUDA cuDNN version NA GPU model and memory NA You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior The code snippets works fine on Colab but gives the following error on Windows Executing gives 9223372036854775807 Describe the expected behavior Same output as running on Colab image https user images.githubusercontent.com 2398765 57988793 d5c2c180 7ac4 11e9 8b33 02f3cdb22abc.png Code to reproduce the issue Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. , hsm207 I tried reproducing the issue on my system but the code executed without any error. Can you try once again and let us know if that still gives error. Thanks gadagashwini yes I still get the same error. Is there additional information I can provide to help you diagnose this issue I have the same issue following the same Classify structured data tutorial. Same characteristics as the original poster only I m running tf on GPU. Same here. Same here on 2.0 beta version I get the same error about the Built in function ... returned a result with an error set but refers to the function TFE Py TapeWatch . I find that all functions with TFE are defined in pywrap tensorflow internal. So I try to import it on tape but the result is the same as before. Version tensorflow gpu 2.1 beta Platform Google Colab I get the same error about the Built in function ... returned a result with an error set but refers to the function TFE Py TapeWatch . I find that all functions with TFE are defined in pywrap tensorflow internal. So I try to import it on tape but the result is the same as before. Version tensorflow gpu 2.1 beta Platform Google Colab I am having the same error... any solutions I am also having the same error in the Classify structured data guide for tf 2.0. version 2.0.0 beta1 Platform Jupyter Lab Windows. I also had a similar issue on another model and both are listing gen sparse ops.py as the last script in the traceback. I tried on Colab as well on local system with Tensorflow 2.0.0.rc0. It is working as expected can you please try with latest TF version and check. PTAL colab gist here https colab.sandbox.google.com gist gadagashwini fadf44ecddae701e7f051fe01c049c04 untitled146.ipynb and Jupyter notebook gist here 28846.ipynb.tar.gz https github.com tensorflow tensorflow files 3608766 28846.ipynb.tar.gz .Let us know how it progresses. Thanks I tried running the code again after upgrading to v2.0.0 rc0 101 gd2d2566eef 2.0.0 rc1 and now the error is gone. Thanks gadagashwini Glad it is working. Closing the issue. Please feel free to reopen if still issue persists. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28846 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28846 No a hsm207 I tried to upgrade with pip install upgrade tensorflow 2.0.0rc1 but problem existed stilly. Could you tell me how to upgrade to v2.0.0 rc0 101 gd2d2566eef 2.0.0 rc1 . Thanks hsm207 I tried to upgrade with pip install upgrade tensorflow 2.0.0rc1 but problem existed stilly. Could you tell me how to upgrade to v2.0.0 rc0 101 gd2d2566eef 2.0.0 rc1. Thanks JerichoHy I did not do an upgrade. I created a new conda environment and ran pip install tensorflow 2.0.0 rc1 hsm207 I tried to upgrade with pip install upgrade tensorflow 2.0.0rc1 but problem existed stilly. Could you tell me how to upgrade to v2.0.0 rc0 101 gd2d2566eef 2.0.0 rc1. Thanks JerichoHy I did not do an upgrade. I created a new conda environment and ran pip install tensorflow 2.0.0 rc1 Thanks. JerichoHy I created a new conda environment with tensorflow 2.0.0 rc1 installed but still get the same error. OverflowError Python int too large to convert to C long followed by traceback down to Miniconda3 lib site packages tensorflow core python ops gen sparse ops.py in sparse cross indices values shapes dense inputs hashed output num buckets hash key out type internal type name 1147 shapes dense inputs hashed output hashed output num buckets 1148 num buckets hash key hash key out type out type 1149 internal type internal type 1150 result SparseCrossOutput. make result 1151 return result SystemError built in function TFE Py FastPathExecute returned a result with an error set Happy to provide more info. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 28846 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 28846 No a Akoopie Please post a new issue describing your problem and provide all relevant information from the template. Thanks ymodak The issue is OverflowError Python int too large to convert to C long error after running this line of feature columns.ipynb https github.com tensorflow docs blob master site en tutorials structured data feature columns.ipynb codes on the windows OS. demo feature column.indicator column crossed feature This error is closed but it still happening in windows 10. import tensorflow as tf tf. version 2.1.0 from tensorflow import feature column from tensorflow.keras import layers data marks 55 21 63 88 74 54 95 41 84 52 grade average poor average good good average good average good average point c f c b b c a d b c def demo feature column feature layer layers.DenseFeatures feature column print feature layer data .numpy marks feature column.numeric column marks marks buckets feature column.bucketized column marks boundaries 30 40 50 60 70 80 90 grade feature column.categorical column with vocabulary list grade poor average good crossed feature feature column.crossed column marks buckets grade hash bucket size 10 demo feature column.indicator column crossed feature OverflowError Traceback most recent call last OverflowError Python int too large to convert to C long The above exception was the direct cause of the following exception SystemError Traceback most recent call last ipython input 33 d48208d43111 in module 21 22 crossed feature feature column.crossed column marks buckets grade hash bucket size 10 23 demo feature column.indicator column crossed feature ipython input 33 d48208d43111 in demo feature column 11 def demo feature column 12 feature layer layers.DenseFeatures feature column 13 print feature layer data .numpy 14 15 marks feature column.numeric column marks C Anaconda3 envs tensorflow lib site packages tensorflow core python keras engine base layer.py in call self inputs args kwargs 820 with base layer utils.autocast context manager 821 self. compute dtype 822 outputs self.call cast inputs args kwargs 823 self. handle activity regularization inputs outputs 824 self. set mask metadata inputs outputs input masks C Anaconda3 envs tensorflow lib site packages tensorflow core python feature column dense features.py in call self features cols to output tensors 133 with ops.name scope column.name 134 tensor column.get dense tensor transformation cache 135 self. state manager 136 processed tensors self. process dense tensor column tensor 137 if cols to output tensors is not None C Anaconda3 envs tensorflow lib site packages tensorflow core python feature column feature column v2.py in get dense tensor self transformation cache state manager 4349 Feature has been already transformed. Return the intermediate 4350 representation created by transform feature. 4351 return transformation cache.get self state manager 4352 4353 deprecation.deprecated FEATURE COLUMN DEPRECATION DATE C Anaconda3 envs tensorflow lib site packages tensorflow core python feature column feature column v2.py in get self key state manager 2613 column key 2614 logging.debug Transforming feature column s. column 2615 transformed column.transform feature self state manager 2616 if transformed is None 2617 raise ValueError Column is not supported. .format column.name C Anaconda3 envs tensorflow lib site packages tensorflow core python feature column feature column v2.py in transform feature self transformation cache state manager 4288 4289 id weight pair self.categorical column.get sparse tensors 4290 transformation cache state manager 4291 return self. transform id weight pair id weight pair 4292 C Anaconda3 envs tensorflow lib site packages tensorflow core python feature column feature column v2.py in get sparse tensors self transformation cache state manager 4140 See CategoricalColumn base class. 4141 return CategoricalColumn.IdWeightPair 4142 transformation cache.get self state manager None 4143 4144 deprecation.deprecated FEATURE COLUMN DEPRECATION DATE C Anaconda3 envs tensorflow lib site packages tensorflow core python feature column feature column v2.py in get self key state manager 2613 column key 2614 logging.debug Transforming feature column s. column 2615 transformed column.transform feature self state manager 2616 if transformed is None 2617 raise ValueError Column is not supported. .format column.name C Anaconda3 envs tensorflow lib site packages tensorflow core python feature column feature column v2.py in transform feature self transformation cache state manager 4101 inputs feature tensors 4102 num buckets self.hash bucket size 4103 hash key self.hash key 4104 4105 deprecation.deprecated FEATURE COLUMN DEPRECATION DATE C Anaconda3 envs tensorflow lib site packages tensorflow core python ops sparse ops.py in sparse cross hashed inputs num buckets hash key name 596 num buckets num buckets 597 hash key hash key 598 name name 599 600 C Anaconda3 envs tensorflow lib site packages tensorflow core python ops sparse ops.py in sparse cross internal inputs hashed output num buckets hash key name 649 out type out type 650 internal type internal type 651 name name 652 653 return sparse tensor.SparseTensor indices out values out shape out C Anaconda3 envs tensorflow lib site packages tensorflow core python ops gen sparse ops.py in sparse cross indices values shapes dense inputs hashed output num buckets hash key out type internal type name 1046 hashed output hashed output num buckets num buckets 1047 hash key hash key out type out type internal type 1048 internal type 1049 result SparseCrossOutput. make result 1050 return result SystemError built in function TFE Py FastPathExecute returned a result with an error set import tensorflow as tf tf. version 1.13.1 run my code on https www.katacoda.com courses tensorflow playground from tensorflow import feature column sess tf.Session appended data marks 55 21 63 88 74 54 95 41 84 52 grade average poor average good good average good average good average point c f c b b c a d b c marks feature column.numeric column marks marks buckets feature column.bucketized column marks boundaries 30 40 50 60 70 80 90 grade feature column.categorical column with vocabulary list grade poor average good crossed feature feature column.crossed column marks buckets grade hash bucket size 10 inputs tf.feature column.input layer data feature column.indicator column crossed feature init tf.global variables initializer init tf.global variables initializer sess.run tf.tables initializer sess.run init outputs sess.run inputs print outputs image https user images.githubusercontent.com 36342491 90229569 215a3d00 dde6 11ea 8992 60ce8d00963e.png I just changed my code for running on tensorflow version 1.13.1 from https www.katacoda.com courses tensorflow playground I m getting the same error on Windows OS 10 tensorflow version 2.1.0 on gpu in conda environment using jupyter lab. final error message built in function TFE Py FastPathExecute returned a result with an error set . Is there any fix workaround for it yet 
28928, TF2.0 steps per epoch parameter not working when input data passed as dictionary, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary pip TensorFlow version use command below v1.12.0 9492 g2c319fb415 2.0.0 alpha0 Python version 3.6.8 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Describe the current behavior If using a dictionary to pass input to a keras.Model with named inputs the steps per epoch argument appears to have no effect. Instead as many steps are performed as needed for a full epoch. Describe the expected behavior I would expect only as many iterations as specified by steps per epoch in each iteration. While it s possible I have misunderstood something this is the behaviour in TensorFlow 1 when I run analogous code. Code to reproduce the issue with this setup I observe but on the other hand if I pass the input as a dictionary I observe Strangely if I then run the first .fit command again it also performs a full pass over the data rather than doing the specified number of steps as originally. In TensorFlow 1 with the same code as above but a TensorFlow 1 friendly model compilation step I do not see the same issue the number of steps is as specified whether data is passed in a dictionary or not. Other info logs This could be related to 28710 Though the author there was using TensorFlow 1 and reported that data input as NumPy arrays did not cause problems so this seemed different enough for its own issue. Happy for them to be consolidated if they are too similar. , tcbegley I tried reproducing the issue through colab with Tf 2.0.0 alpha0 but the code executed as expected. Can you try once again and let us know if that still gives weird output. Thanks gadagashwini thanks for getting back to me. I was able to replicate my issue in Google Colab see this notebook https colab.research.google.com drive 1pjpBakrkgd0pQKhGhnQW2 PNUUXKI28G . As in the screenshot below which is taken from that notebook the first fit argument performs two steps of training the second cell appears to do a full pass over the data ignoring the steps per epoch argument. As I stated in my original post this is different to behaviour in TensorFlow 1 and also different to my interpretation of the docs so I think it s a bug. image https user images.githubusercontent.com 15220906 58250963 72ea5800 7d5a 11e9 852c abb6916d9686.png I was able to reproduce the mentioned output. tcbegley The other possibly related issue is fixed with latest tf nightly build. Is this still an issue with tf 2.0 nightly build pip install tf nightly 2.0 preview ymodak I tried out the nightly build and everything worked correctly so it seems likely this issue has also been resolved. Great. Thanks for confirming. I will close this issue now. Feel free to reopen if still have problems. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28928 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28928 No a 
28929,TF2.0 Problem making prediction from a checkpoint if model has an embedding column, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Google Colab Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary NA TensorFlow version use command below v1.12.0 9492 g2c319fb415 2.0.0 alpha0 Python version 3.6.7 default Oct 22 2018 11 32 17 n GCC 8.2.0 Bazel version if compiling from source NA GCC Compiler version if compiling from source NA CUDA cuDNN version NA GPU model and memory NA You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior I have a keras model that has an embedding column as a feature. I can train and save the model s weights just fine. Making predictions immediately after training works too. The problem is when I recreate the model and reload the weights. Making a prediction from that model gives a shape error. Describe the expected behavior Same output as making predictions immediately after training. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Please refer to this gist https gist.github.com hsm207 305d068c982edf9e1df7db77446df50c . Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,I was able to reproduce the mentioned output on Colab with TensorFlow version 2.0.0 alpha0. hsm207 I could not reproduce the issue as it was resolved in tf nightly 2.0 preview . I can see exactly same results with model 1 original and model 2 restored model . Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan a6a1ca212893e68b4b2448bca31bfa05 tf2 bug embedding column.ipynb . I am closing the issue as it was resolved in tf nightly 2.0 preview. Please feel free to open it if the issue persists again. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 28929 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 28929 No a 
29161,tf.keras predict stuck with Sequence when using multi processing, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04.2 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below 1.13.1 Python version 3.6.8 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 10.0 GPU model and memory TITAN You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior Hi When using tf.keras with a custom Sequence the program hangs during predict with multi processing . I was able to reproduce the issue with a simple NN that contains a single Dense layer. This happens after setting the weights of the layer and running predict with multi processing. When commenting the set weights line or running with multi threading the program does not hang. Issue exists also in 1.14.0 rc0 Same code works OK with tensorflow 1.12.0 and 2.0.0a0. Code to reproduce the issue Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,I was able to reproduce the reported issue on google colab with Tf 1.13.1. Thanks Hi Any update Thanks. mwin76 I think it was resolved sometime ago. I have tested in nightly builds 1.15 and 2.0b1 and don t see any issue. Please check the gist with TF2.0 https colab.sandbox.google.com gist jvishnuvardhan b466a0758214055628c16026d68125d8 tf 29161 tf2p0b1.ipynb and tf nightly https colab.sandbox.google.com gist jvishnuvardhan 220c6d9555b467f171b56ebbe36c6b18 tf 29161 tf1p15.ipynb . I am closing this issue as it was resolved. Please reopen if the issue persists again. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29161 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29161 No a Hi I tested it with tf nightly gpu and it is stuck. Thanks Mattan. mwin76 Can you check my gists or provide a gist screenshot showing the issue Thanks Hi Tried this in colab and it is stuck. the code was the same as the code in your gist just replaced the pip install tf nightly with pip install tf nightly gpu When killing the job I get the following stack running predict with multiprocessing True E0728 05 34 49.872965 140443983652736 ultratb.py 152 Internal Python error in the inspect module. Below is the traceback from this internal error. Traceback most recent call last File usr local lib python3.6 dist packages IPython core interactiveshell.py line 2882 in run code exec code obj self.user global ns self.user ns File ipython input 2 5127600acadc line 41 in module res model.predict seq workers workers use multiprocessing multiprocessing steps NUM OF BATCHES File usr local lib python3.6 dist packages tensorflow core python keras engine training.py line 920 in predict use multiprocessing use multiprocessing File usr local lib python3.6 dist packages tensorflow core python keras engine training generator.py line 648 in predict use multiprocessing use multiprocessing File usr local lib python3.6 dist packages tensorflow core python keras engine training generator.py line 221 in model iteration batch data get next batch generator File usr local lib python3.6 dist packages tensorflow core python keras engine training generator.py line 363 in get next batch generator output next generator File usr local lib python3.6 dist packages tensorflow core python keras utils data utils.py line 779 in get inputs self.queue.get block True .get File usr lib python3.6 multiprocessing pool.py line 638 in get self.wait timeout File usr lib python3.6 multiprocessing pool.py line 635 in wait self. event.wait timeout File usr lib python3.6 threading.py line 551 in wait signaled self. cond.wait timeout File usr lib python3.6 threading.py line 295 in wait waiter.acquire KeyboardInterrupt Thanks Mattan. Hi jvishnuvardhan. I tested your gist with 1.15.0 dev20190728 and it still doesn t work. Let me know if you need more information. Thanks mwin76 I could reproduce the issue with tf nightly gpu but TF2.0 has no issues. We will try to find the root cause of the issue in TF1.x nightly. Thanks Is there any progress on the issue Thanks I faced the same issue and It seem to resolve it with adding this It seem to be unrelated problem but it s works. Hi Thanks. Using config.gpu options.allow growth works however if you clear the session and reload the model it gets stuck test predict model.predict generator test generator max queue size 64 workers 4 use multiprocessing True Exception in thread Thread 24 Traceback most recent call last File D Anaconda envs tfgpu lib threading.py line 926 in bootstrap inner self.run File D Anaconda envs tfgpu lib threading.py line 870 in run self. target self. args self. kwargs File D Anaconda envs tfgpu lib site packages tensorflow python keras utils data utils.py line 619 in run with closing self.executor fn SHARED SEQUENCES as executor File D Anaconda envs tfgpu lib site packages tensorflow python keras utils data utils.py line 600 in pool fn workers initializer init pool generator initargs seqs None File D Anaconda envs tfgpu lib multiprocessing context.py line 119 in Pool context self.get context File D Anaconda envs tfgpu lib multiprocessing pool.py line 176 in init self. repopulate pool File D Anaconda envs tfgpu lib multiprocessing pool.py line 241 in repopulate pool w.start File D Anaconda envs tfgpu lib multiprocessing process.py line 112 in start self. popen self. Popen self File D Anaconda envs tfgpu lib multiprocessing context.py line 322 in Popen return Popen process obj File D Anaconda envs tfgpu lib multiprocessing popen spawn win32.py line 89 in init reduction.dump process obj to child File D Anaconda envs tfgpu lib multiprocessing reduction.py line 60 in dump ForkingPickler file protocol .dump obj TypeError can t pickle thread.lock objects mwin76 Thanks for the issue This looks like it is fixed in the latest tf nightly please give it a try pip install U tf nightly Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 29161 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 29161 No a omalleyt12 I m facing the exact same issue on v2.1.0. I plan on giving tf nightly a try. But will it impact my previously trained models if I upgrade Hi I am facing the exact same issue with tf nightly 2.2.0.dev20200324. A sequential multiprocessing model woudn t start re training. Any news I m facing the same problem here. Any updates on this I am also facing a similar issue while trying to do model.predict with multi processing. I made sure that the tensorflow graph and tensorflow session are unique for each process. My multi process case I can perfectly run the same code on my Windows machine however the same code cannot work on Ubuntu. It seems like it is caused by the differences in creating new processes between Windows and Linux. In Linux fork is called to create a new process by default and you can manually change to spawn which will create a new process from zero instead of sharing some environmental variables between processes. You can try adding the following codes to the head which will force Linux to use the spawn method Reference https stackoverflow.com questions 40615795 pathos enforce spawning on linux It hangs on TF 1.15.2 or 1.15.5. works OK in TF 2.1.0 2.3.0. Python 3.6.9. When I commented out the dense layer.set weights call in the original code in the first comment it works otherwise it hangs. That piece of code runs a TF session. It seems that the initializer of the Pool the init pool generator function is not called. Thus it waits for the results indefinitely. I tried to add logging from the multiprocessing module It stops after I also tried to change from fork to swawn . Then it still fails but the worker get created again and again in a loop Running on MacOS with Python 3.7.10 tensorflow cpu 1.15.2 When using the spawn method and guarding the code beyond definition of the sequence with if name main the code works. So it looks like that after works Tensorflow tries to join on some thread from the main process which got copied but is not available in the child anymore. Adding the allow growth option to Session helped even in the fork case. 
29571,tflite Slicing isn t compatible with quantisation , System information OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 TensorFlow installed from source or binary Source TensorFlow version use command below v1.12.1 3185 g1a4a0aee1f 1.13.1 Python version 3.6.7 CUDA cuDNN version GPU model and memory I m trying to implement shufflenet v2 in Tensorflow 2.0 with tflite which requires slicing. This works fine with float precision however when I turn on quantization I get an error. If I turn off either use slice or quantise it works fine however with both on I get the following error RuntimeError tensorflow lite kernels dequantize.cc 67 op context.input type kTfLiteUInt8 op context.input type kTfLiteInt8 op context.input type kTfLiteFloat16 was not true.Node number 3 DEQUANTIZE failed to prepare. ,Any updates on this Please let me know if there s any more debug info required. This looks to have been fixed just gave this a retest with the latest tf 2.0 nightly Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29571 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29571 No a 
29636,Can t call tf.contrib.layers.group norm due to supplying a DeferredTensor instead of Eager Tensor during eager execution., System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution Linux Ubuntu 16.04 TensorFlow installed from source or binary binary TensorFlow version use command below 1.12 Python version 3.5 CUDA cuDNN version V8.0.61 GPU model and memory Tesla P100 PCIE 12193MiB Describe the current behavior I run the following snippet of code in eager execution However the call to tf.contrib.layers.group norm tensor gives me the following error Also I am not able to find any documentation on DeferredTensor s whatsoever. Can anyone link me to some Describe the expected behavior A possible conversion from the DeferredTensor to a Tensor or EagerTensor or alternatively to perform group normalization in another way. Code to reproduce the issue ,me too. It looks like that this op can run on CPU but not GPU for the recent version no matter in eager mode or not. MaeThird What version are you running EmielBoss Please help us to reproduce the issue we see some undefined entities like conv block identity block. Thanks EmielBoss tf 1.13.1 installed from binary with python 3.6 gadagashwini Apologies I forgot about those and they ve been added. I didn t think it would matter much since I would assume tf.keras.layers.Conv2D s output would always be of the same type but that s probably incorrect. Try out this snippets with log out like this TypeError Failed to convert object of type class list to Tensor. Contents None 112 112 4 8 . Consider casting elements to a supported type . EmielBoss I tried reproducing issue with above code but i got this NameError name SCALED HEIGHT is not defined. Please help us to reproduce the issue. Thanks Try out this snippets with log out like this TypeError Failed to convert object of type class list to Tensor. Contents None 112 112 4 8 . Consider casting elements to a supported type . MaeThird Can you please post a new issue by providing the information asked by the template The reason for this is we can focus on your specific configuration and problem since the root cause can be unrelated even though the error messages are similar. Thanks gadagashwini issues 29961 gadagashwini Gee that s embarrassing. I checked if I could run that code in isolation without any other issues but of course I was still importing my own utilities ... A thousand apologies The code should now work out of the box. EmielBoss Now I am able to reproduce the issue on colab with Tf 1.12.0 Apologies for the delay in response. This executes successfully with TF 1.15. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 29636 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 29636 No a 
29674,Cannot find the placeholder op that is an input to ReadVariableOp in tf lite conversion., System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes. OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below 2 beta Python version 3.6.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version Cuda 10 GPU model and memory Geforce GTX 1060 Describe the current behavior I have a pretty complicated model with three different inputs and I can save and load it with custom objects as a keras model with model.save and model.load methods. I want to convert it to tf lite and I get this error ValueError Cannot find the Placeholder op that is an input to the ReadVariableOp Code to reproduce the issue Other info logs ,Can you rerun your code with TensorFlow 2.0.0 beta1 There was an issue with beta0 that was fixed in beta1. If that doesn t work can you provide a reproducible example I have tried using TensorFlow 2.0.0 beta1 and see the same issue. But I get the error only when trying to convert tf.keras.layers.GRU or tf.keras.layers.LSTM layers to TFLite. Is this an issue with tf.lite.TFLiteConverter.from saved model I see the same error when using tfjs converter tf saved model tfjs graph model . There is currently limited support for LSTMs in TFLite. The documented path is available here https github.com tensorflow tensorflow blob master tensorflow lite experimental examples lstm g3doc README.md . We are working on improving our support of control flow based operations and models. siavash khodadadeh Is this still an issue Can you check with TF2.0 and let us know whether the issue persists with latest TF version. Can you please provide a simple standalone code to reproduce the issue Thanks I will try it and let you know. It might take a couple of days since I do not have access to that code now. siavash khodadadeh Is this still an issue Can you check with TF2.0 and let us know whether the issue persists with latest TF version. Can you please provide a simple standalone code to reproduce the issue Thanks I am using a model with LSTM After getting the Cannot find placeholder op issue with TF2.0b1 I updated to TF2.0 I get the following error Here is now the model looks like Is this issue specific to keras models with LSTMs Is there a workaround for this Yuvaraj8blr Please check the comment here https github.com tensorflow tensorflow issues 29674 issuecomment 510966279 . Thanks I m using the TF 2.0 gpu version and still get this error with trt.TrtGraphConverterV2 PolinaDemochkina Please provide a simple standalone code to reproduce the issue. Please check the link https github.com tensorflow tensorflow issues 29674 issuecomment 541231768 before providing standalone code. Thanks jvishnuvardhan I am not using TFLite but I guess the issue has to do with the LSTM layer in my model. Tried to replace this layer with GRU however I got the same mistake. Does the optimizer not support any kind of recurrent layers Here s the code to reproduce the issue PolinaDemochkina Your issues is different from original issue is with TF lite . Can you please post a new issue with details platform TF version and standalone code as above . Thanks Closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 29674 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 29674 No a Run on tensorflow 2.1.0 This produces the same error Can you reopen this issue jvishnuvardhan Can you reopen this issue jakesabathia2 and srikris Can you please open a new issue with more details on the issue. It will be easy for others to follow learn for your issue. Thanks https github.com tensorflow tensorflow issues 36391 I have similar issue with the model NOT using LSTMs I posted it here https github.com tensorflow tensorflow issues 35987 with latest tf nightly and experimental converter turned on File ... miniconda3 envs tf nightly py3 lib python3.7 site packages tensorflow core python framework convert to constants.py line 526 in convert variables to constants v2 impl raise ValueError Cannot find the Placeholder op that is an input ValueError Cannot find the Placeholder op that is an input to the ReadVariableOp. 
29909,tf.meshgrid high variance in computational time, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary Binary TensorFlow version use command below 1.10 Python version 3.5 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version CUDA 9.0 Cudnn 7.1 GPU model and memory GTX 1080 ti 10GB Describe the current behavior Running tf.meshgrid tf.range A tf.range B tf.range C tf.range D indexing ij with the same amount of resulting elements can have high variations for example sometimes 1 second sometimes up to 8 seconds . Describe the expected behavior I expect this operation to be quicker as AFAIK it is needed for any complex operations involving tf.scatter nd and other element gathering ops. Code to reproduce the issue Run this code a s and b s are set to always multiply to 1000. I see performance going from 0.9 seconds up to 45 seconds on this test even for the same values. Other info logs Here s an example log extract from the program above the first value is the time the second and third the values of A and B respectively I m using this in a large system based on the Transformer where this is used together with tf.scatter nd. I ve attached the corresponding timeline the meshgrid op is called output rec output prob meshgrid timeline.trace.tar.gz https github.com tensorflow tensorflow files 3300969 timeline.trace.tar.gz , nikita68 Can you upgrade to recent version of TF 1.14. I have attached a gist here https colab.sandbox.google.com gist jvishnuvardhan f732825a2c757d61bf41fb92b2733b73 tf29909 runtime.ipynb . It shows that it is consistently taking 0.43 sec after warming up takes 0.6 0.8 sec . Thanks It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue Automatically closing this out since I understand it to be resolved but please let me know if I m mistaken.Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 29909 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 29909 No a 
30453, TF 2.0 categorical column with vocabulary list not usable in custom training loop, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOS Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary Source TensorFlow version use command below v2.0.0 beta0 16 g1d91213fe7 2.0.0 beta1 Python version 3.6.8 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory Describe the current behavior Outside of fit e.g in a custom training loop categorical column with vocabulary list results in an error. I have provided a modified version of Classifying Structured data https www.tensorflow.org beta tutorials keras feature columns which demonstrates this. The error is ValueError Column dtype and SparseTensors dtype must be compatible. key thal column dtype dtype string tensor dtype dtype int32 Describe the expected behavior Code runs without causing an error Code to reproduce the issue It should be directly copy paste able If you flip the CUSTOM TRAINING variable between True and False line 72 you ll see what I mean. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. 1 The complete relevant stacktrace is 2 Placing a debugger on line 50 leads me to feature column v2.py specifically transform input tensor The input tensor arg to transform input tensor is which seems strange It s like it forgot that it had transformed those variables ,Reproduced the error with TF Version 2.0.beta1 Thanks 1 Does anyone have an intuition as to why this happens in a custom loop but not in fit 2 Does anyone have a quick fix Adding rohan100jain who is the expert on feature columns. Is this still an open issue facing same issue. ValueError Column dtype and SparseTensors dtype must be compatible. key opt categories column dtype dtype int64 tensor dtype dtype float32 I am also facing the same issue. Any help... This got resolved for me. The datatype of the vocabulary list passed to the method the data in the column of the record and the data type that we specify in the model have to be the same. Was able to find the issue it got resolved. same issue with tensorflow 1.14 ValueError Column dtype and SparseTensors dtype must be compatible. key adt sev flag column dtype dtype int64 tensor dtype dtype float64 This got resolved for me. The datatype of the vocabulary list passed to the method the data in the column of the record and the data type that we specify in the model have to be the same. Was able to find the issue it got resolved. how did you solved it The have you checked if the datatype of the data in the vocab list the datatype of the input data and the data type of the feature column declaration are same. In my case the vocab list that I passed had infered to a different datatype and I passed it into the categorical column with vocabulary list layer incorrectly. I have tried this example in tf nightly which seems to have been resolved. It s probably a Keras issue instead of FeatureColumn. Can anyone help to verify Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 30453 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 30453 No a 
30577,TensorflowLiteSwift fails pod validation, System information OS Platform and Distribution MacOS 10.14.5 TensorflowLiteSwift Pod version 0.2.0 Commit 477447155b Cocoapods version 1.7.3 Xcode version 10.2.1 Describe the problem When running the command pod spec lint tests fail because i386 architecture cannot be found. This happens not only on my machine but also on our private pod system. Failing validation means we cannot use the pod for our apps without turning off the tests. Provide the exact sequence of commands steps that you executed before running into the problem 1. cd to TensorflowLiteSwift directory 2. run pod spec lint verbose Any other info logs here s the output https github.com tensorflow tensorflow files 3378927 log.txt I was able to resolve this issue by upping the the iOS deployment target in the podspec from 9.0 to 11.0. I believe pod spec lint wants to compile a 32 bit version of TensorFlowLiteC for iOS 9 and 10 but the TensorFlowLiteC binary only contains x86 64 and no i386 . A more robust solution might be to compile for i386 . Or it may be possible to configure the podspec to only execute tests on arm and x86 64 . ,Is this still a problem for you When I run pod spec lint verbose allow warnings TensorFlowLiteSwift.podspec in the tensorflow lite experimental swift directory it succeeds on my side. A few more things to note. The i386 target is now included in the TensorFlowLiteC nightly binaries as of 29e6eafad33725f3c1b8f0dae54c702df804e03d . The allow warnings flag will be needed due to a swift version related warning. If the missing i386 still causing you a problem you could also use skip import validation to pass the lint although it s not ideal. Please let me know. We turned off the tests for this pod as a short term workaround. Also since we only support the last 2 versions of iOS since iOS 13 is out we ll drop support for iOS 10 which should also fix the problem. Sounds good. Let me close this bug for now. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 30577 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 30577 No a 
30835,Keras Adam optimizer unsupported by GPU, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Colab Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary preinstalled in colab TensorFlow version use command below 1.14.0 Python version 3.6.8 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory GPU on Colab Describe the current behavior I run a Keras Adam optimizer with a CNN network. The code works fine with CPU. If I turn on GPU in the notebook and rerun the same code I get an exception. Describe the expected behavior No exception Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Activate GPU. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. , ssable I tried executing the given code on Colab with Tensorflow 1.14.0 with GPU activate. I didn t get any error. Can you check once and let us know is still an issue. Thanks Thank you for your feedback. I tried again and indeed the code works fine. Sorry for the confusion. I modified the example above to add MirroredStrategy which is what is causing the issue. You should be able to reproduce the bug this time. ssable I tried reproducing the issue on Colab with the updated code and i didn t see any error. Let us know the expected behavior. Thanks I just tried creating a new notebook activating GPU putting the code above in the notebook and running and... I still get the exception above. I am not sure what can be the difference between our 2 environments. Here is the full callstack I could able to reproduce the issue on Colab with TF 1.14.0. Look at the gist here https colab.research.google.com drive 1ryWl00ykTXykIO01Ne45eyjHW4W1XaOb . Thanks Hi this has been fixed already so if you use the latest versions of TF you should not see this issue. I just tested with latest tf nightly https pypi.org project tf nightly gpu and it works fine. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30835 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30835 No a 
30990,LSTM is not working with ModelCheckpoint callback, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 MacOS 10.14.5 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary pip TensorFlow version use command below v2.0.0 beta0 16 g1d91213fe7 2.0.0 beta1 Python version 3.6.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior While training a model with tf.keras.layers.LSTM and having tf.keras.callbacks.ModelCheckpoint in callbacks model.fit stops with an error message at end of last epoch and no model weights is saved as ModelCheckpoint should do. Describe the expected behavior model.fit should train the model and model weights should be saved in desired files. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Here is an example which reproduces this error Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. Here is error message after model.fit with the precedent code snippet The same error occurs on a Linux machine with tf nightly 2.0.0 dev20190723 . Thanks for help ,I am experiencing a similar issue. print tf.version.GIT VERSION tf.version.VERSION 2.2.4 tf OS Ubuntu Distributor ID Ubuntu Description Ubuntu 18.04.2 LTS Release 18.04 Codename bionic running on CPUs Architecture x86 64 CPU op mode s 32 bit 64 bit Byte Order Little Endian CPU s 20 On line CPU s list 0 19 Thread s per core 2 Intel R Xeon R W 2155 CPU 3.30GHz Data Generated using a class which inherits from Sequence from tensorflow.python.keras.utils import Sequence this works fine model.fit generator generator gent validation data genv epochs 10 callbacks logger this causes an error callback ModelCheckpoint checkpoint file verbose 1 model.fit generator generator gent validation data genv epochs 10 callbacks callback Error Epoch 1 10 7 8 .... ETA 1s loss 1.5245 acc 0.2857 Epoch 00001 saving model to ... checkpoints weights improvement 01.hdf5 Traceback most recent call last File media iwona Optane Project BugLoc DeepTracePy venv model base baseline.py line 380 in module model.fit generator generator gent validation data genv epochs 10 callbacks callback use multiprocessing True workers 20 File media iwona Optane Project BugLoc DeepTracePy venv lib python3.6 site packages tensorflow python keras engine training.py line 1433 in fit generator steps name steps per epoch File media iwona Optane Project BugLoc DeepTracePy venv lib python3.6 site packages tensorflow python keras engine training generator.py line 331 in model iteration callbacks.on epoch end epoch epoch logs File media iwona Optane Project BugLoc DeepTracePy venv lib python3.6 site packages tensorflow python keras callbacks.py line 311 in on epoch end callback.on epoch end epoch logs File media iwona Optane Project BugLoc DeepTracePy venv lib python3.6 site packages tensorflow python keras callbacks.py line 969 in on epoch end self. save model epoch epoch logs logs File media iwona Optane Project BugLoc DeepTracePy venv lib python3.6 site packages tensorflow python keras callbacks.py line 1018 in save model self.model.save filepath overwrite True File media iwona Optane Project BugLoc DeepTracePy venv lib python3.6 site packages tensorflow python keras engine network.py line 1211 in save saving.save model self filepath overwrite include optimizer save format File media iwona Optane Project BugLoc DeepTracePy venv lib python3.6 site packages tensorflow python keras saving save.py line 113 in save model model filepath overwrite include optimizer File media iwona Optane Project BugLoc DeepTracePy venv lib python3.6 site packages tensorflow python keras saving hdf5 format.py line 99 in save model to hdf5 config model.get config File media iwona Optane Project BugLoc DeepTracePy venv lib python3.6 site packages tensorflow python keras engine network.py line 940 in get config layer config layer.get config File media iwona Optane Project BugLoc DeepTracePy venv lib python3.6 site packages tensorflow python keras engine network.py line 919 in get config raise NotImplementedError NotImplementedError Process finished with exit code 1 Please help. model base baseline.txt https github.com tensorflow tensorflow files 3429426 model base baseline.txt nn data generator.txt https github.com tensorflow tensorflow files 3429429 nn data generator.txt I am able to reproduce the issue on Colab with Tensorflow version 2.0.0.beta1. Please take a look at gist https colab.research.google.com drive 1 WNfJK6G7ahlrTyouiStGNOhCFxVuK R of Colab. Thanks I could reproduce the issue with pip install tf nightly gpu 2.0 preview 2.0.0.dev20190724 . Here is the gist https colab.sandbox.google.com gist jvishnuvardhan 5ee731bc2b38fc03604f66e964b9aff3 untitled323.ipynb . Thanks Thanks for reporting the issue let me take a look. I wasn t be able to reproduce issue with latest nightly in tf nightly gpu 2.0 preview 2.0.0.dev20190731 somehow the issue was fixed recently. Can u have a try again Thanks. I am closing the issue. I can confirm that the issue wasn t reproducible with tf nightly gpu 2.0 preview 2.0.0.dev20190731 . Here is the gist https colab.sandbox.google.com gist jvishnuvardhan ba9fbfba12a51072cb59b0ee4b1e5962 untitled323.ipynb for your reference. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 30990 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 30990 No a 
31054,I0725 19 06 30.700653 7708 tf logging.py 115 Saver not created because there are no variables in the graph to restore When Training the tf estimator model , INFO tensorflow Calling model fn. I0725 19 06 29.207903 7708 tf logging.py 115 Calling model fn. INFO tensorflow Saver not created because there are no variables in the graph to restore I0725 19 06 30.700653 7708 tf logging.py 115 Saver not created because there are no variables in the graph to restore INFO tensorflow Saver not created because there are no variables in the graph to restore I0725 19 06 31.963634 7708 tf logging.py 115 Saver not created because there are no variables in the graph to restore ValueError Feature descriptions is not in features dictionary. originally defined at File C Users AppData Local Continuum anaconda3 lib site packages tensorflow python estimator canned dnn.py line 108 in dnn logit fn name dnn File C Users AppData Local Continuum anaconda3 lib site packages tensorflow python estimator canned dnn.py line 143 in init create scope now False File C Users AppData Local Continuum anaconda3 lib site packages tensorflow python feature column feature column.py line 323 in init self. name internal input layer create scope now create scope now File C Users AppData Local Continuum anaconda3 lib site packages tensorflow python ops template.py line 154 in make template kwargs , Neeraj0011 Please provide details about TensorFlow version. Also did you compile from source or install a binary In order to expedite the trouble shooting process please provide a complete code to reproduce the issue reported here. Thanks gadagashwini TF Version 1.12.0 Compiled from Source Neeraj0011 Could you please provide the sample data set to reproduce the issue. Thanks gadagashwini Please find the dataset link https drive.google.com file d 1PB7xnodZpT7EFcKq8d6vBmIR9EuczqAm view usp sharing Neeraj0011 Sorry for the delay in my response. I think this was resolved in TF1.14.0 and tf nightly . I could not reproduce the issue. Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 9fe1b0df17e5135a36021a797edf3cc2 tf 31054.ipynb . I am closing the issue as it was resolved. Please feel free to open it if the issue persists again. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31054 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31054 No a 
31282, TypeError expected str bytes or os.PathLike object not io.BytesIO when trying to restore a model from a byte stream in tensorflow 2.0.0 beta0,in tensorflow version 2.0.0 alpha0 I was able to serialize and deserialize a model in memory by using the following code from tensorflow 2.0.0 beta0 instead the restoreModel function throws the error Did you remove the support to byte stream or is this a bug Thanks System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 all Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device don t know TensorFlow installed from source or binary binary TensorFlow version use command below 2.0.0 beta0 Python version 3.6 , helloIAmPau Will it possible to provide the full code to reproduce the reported issue. Thanks gadagashwini sure helloIAmPau Thanks for reproducible code. I am able to reproduce the issue on Colab with Tensorflow 2.0.0.beta1. Please take a look at gist here https colab.research.google.com drive 1uI3 BwH HZ94j6LT8n xwKNtp Y2wDh9 . Thanks I tried running the colab with tensorflow 2.0.0 alpha0 but the model was still unable to be loaded. load model s docstring states that it only supports Can you try passing the h5 File object to load model instead helloIAmPau Did you try k w w s comment. Please close the issue if it was already resolved for you. Thanks we changed the approach in our application. we don t need the fix anymore thanks. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 31282 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 31282 No a 
31359,tflite output different result with pbfile when using only one convolutional layer , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes. OS Platform and Distribution e.g. Linux Ubuntu 16.04 In a Ubuntu18.04 docker container. Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary pip install tensorflow 1.14 TensorFlow version use command below tf cpu 1.14.0 Python version python3.6.8 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version No. GPU model and memory No. Describe the current behavior tflite output different result with pbfile when using only one convolutional layer Describe the expected behavior tflite should output the same result with pbfile when using only one convolutional layer. Code to reproduce the issue I use the code above to generate a pb file with only one convolutional layer and convert it to a tflite file. And the output of the pb and tflite files are different as following. I wonder why , rmothukuru karimnosseir aselle Do you know what s the problem thank you I found that when installing tensorflow using pip install tensorflow 1.14.0 the problem occurs when installing by building from source and pip install tensorflow 1.14.1 cp36 cp36m linux x86 64.whl the problem disappear. I wonder this is a problem in tf 1.14.0 and has been fixed in tf 1.14.1 I found another question Adding one layer to the network makes the result different between pb file and lite file... tensorflowbutler aselle I found that using tensorflow 1.13.2 the output of tensorflow with multi layer cnn is the same with the output of tflite using the .lite file converted from .pb file. However tensorflow 1.14.0 and tensorflow 1.14.1 both output wrong results. And in tensorflow 1.13.2 the output of .pb file is not the same as that of .lite file when using dilation no matter whether the w and h is the same in the dilation rate . The diff is loss in accuracy. For example it is passing with print np.allclose target output 1e 5 1e 7 Will look when this small diff started showing. karimnosseir I wonder why converting from pb to tflite would introduce loss in accuracy And I found the loss is small in the first several layers but is huge in the last several layers and causing the model with tflie output wrong result I think that converting pb to tflite both with float32 data type should not introduce error or loss in accuracy. jiarenyf This might be an error in the C code. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31359 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31359 No a 
31550,Error Check failed dims sizes.size 5 vs. 4 when using CPU and MKL instead of Eigen or GPU., System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 macOS 10.13.6 High Sierra Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below 1.14 Python version 3.7 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version N A GPU model and memory N A Describe the current behavior Running my code on cpu with tensorflow 1.14 using mkl from anaconda throws the following error 2019 08 12 17 42 33.158451 F . tensorflow core util mkl util.h 636 Check failed dims sizes.size 5 vs. 4 Abort trap 6 The error trace gives me no hint on how to localize the problem see below . The issue does not occur when installing a tensorflow build using eigen. Describe the expected behavior The code should work using both MKL or Eigen Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. I was not able to localize the exact issue. It occurs when running ndnet.py from https github.com sziem deconv unet 2d3d Other info logs Full output python ndnet.py WARNING Logging before flag parsing goes to stderr. W0812 18 24 24.249456 140736187437952 deprecation.py 323 From Users Soenke anaconda envs tensorflow lib python3.7 site packages tensorflow python compat v2 compat.py 61 disable resource variables from tensorflow.python.ops.variable scope is deprecated and will be removed in a future version. Instructions for updating non resource variables are not supported in the long term testing training 2019 08 12 18 24 26.506695 2019 08 12 18 24 26.621728 I tensorflow core platform cpu feature guard.cc 145 This TensorFlow binary is optimized with Intel R MKL DNN to use the following CPU instructions in performance critical operations SSE4.1 SSE4.2 AVX To enable them in non MKL DNN operations rebuild TensorFlow with the appropriate compiler flags. 2019 08 12 18 24 26.691965 I tensorflow core common runtime process util.cc 115 Creating new thread pool with default inter op setting 4. Tune using inter op parallelism threads for best performance. W0812 18 24 26.750053 140736187437952 deprecation.py 323 From Users Soenke anaconda envs tensorflow lib python3.7 site packages tensorflow python data util random seed.py 58 add dispatch support. locals .wrapper from tensorflow.python.ops.array ops is deprecated and will be removed in a future version. Instructions for updating Use tf.where in 2.0 which has the same broadcast rule as np.where W0812 18 24 26.794230 140736187437952 deprecation.py 323 From Users Soenke Documents Uni Master Masterarbeit gits code public unet deconv dataset handlers tfdata dataset handlers.py 332 py func from tensorflow.python.ops.script ops is deprecated and will be removed in a future version. Instructions for updating tf.py func is deprecated in TF V2. Instead there are two options available in V2. tf.py function takes a python function which manipulates tf eager tensors instead of numpy arrays. It s easy to convert a tf eager tensor to an ndarray just call tensor.numpy but having access to eager tensors means tf.py function s can use accelerators such as GPUs as well as being differentiable using a gradient tape. tf.numpy function maintains the semantics of the deprecated tf.py func it is not differentiable and manipulates numpy arrays . It drops the stateful argument making all functions stateful. Cropping to nearest allowed input image size. Cropping to nearest allowed input image size. W0812 18 24 27.274060 140736187437952 deprecation.py 323 From Users Soenke Documents Uni Master Masterarbeit gits code public unet deconv dataset handlers tfdata dataset handlers.py 139 DatasetV1.make one shot iterator from tensorflow.python.data.ops.dataset ops is deprecated and will be removed in a future version. Instructions for updating Use for ... in dataset to iterate over a dataset. If using tf.estimator return the Dataset object directly from your input function. As a last resort you can use tf.compat.v1.data.make one shot iterator dataset . input shape 400 100 100 1 building Unet v3 for training net input shape 398 98 98 1 W0812 18 24 27.390805 140736187437952 deprecation.py 323 From Users Soenke Documents Uni Master Masterarbeit gits code public unet deconv network architectures ops.py 173 conv3d from tensorflow.python.layers.convolutional is deprecated and will be removed in a future version. Instructions for updating Use tf.keras.layers.Conv3D instead. input block1 2 396 96 96 2 W0812 18 24 27.957751 140736187437952 deprecation.py 323 From Users Soenke Documents Uni Master Masterarbeit gits code public unet deconv network architectures ops.py 116 dropout from tensorflow.python.layers.core is deprecated and will be removed in a future version. Instructions for updating Use keras.layers.dropout instead. W0812 18 24 28.167316 140736187437952 deprecation.py 323 From Users Soenke Documents Uni Master Masterarbeit gits code public unet deconv network architectures ops.py 232 average pooling3d from tensorflow.python.layers.pooling is deprecated and will be removed in a future version. Instructions for updating Use keras.layers.AveragePooling3D instead. down block2 4 196 46 46 4 bottom block4 4 192 42 42 4 up block4 2 380 80 80 2 output block2 1 378 78 78 1 net output shape 378 78 78 1 output shape 378 78 78 1 loss is l2loss determining number of trainable vars except batch norm for regularization... done 2183 saving new ckpt and logs in models unetv3 small valid fp0 pp0 bn00 chlast poisson n1000 wl520 seed1 bs1 do0.0 loss l2loss0 weightreg 0.001l2 loss datareg 1e 08None example run8 Saving 2 logs per epoch by default. Saving checkpoint every 2 epochs by default. starting training with start step 0 epoch 1 2 saving 0 summarizing 0 2019 08 12 18 24 44.145176 F . tensorflow core util mkl util.h 636 Check failed dims sizes.size 5 vs. 4 2019 08 12 18 24 44.145177 F . tensorflow core util mkl util.h 636 Check failed dims sizes.size 5 vs. 4 Abort trap 6,I got the exact same issue at the moment PS print keras. version 2.2.4 print tf. version 1.14.0 We submitted a few fixes to the 1.15 branch could you please test your use case in 1.15 branch Great I was able to run the code without error after compiling the 1.15 branch with mkl. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31550 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31550 No a And if we can t use the 1.15 branch 
31609, Init node weights Assign doesn t exist in graph happens when use convert in tflite, System information OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 18.04 TensorFlow installed from source or binary TensorFlow version Tensorflow nightly Python version 3.6 Installed using virtualenv pip conda pip CUDA cuDNN version 7 10 Describe the problem When I tried to convert a TensorFlow GraphDef into a TensorFlow Lite FlatBuffer from a tf.Session object a error happend such like this 2019 08 14 16 01 23.946453 I tensorflow core grappler clusters single machine.cc 356 Starting new session 2019 08 14 16 01 23.947157 E tensorflow core grappler grappler item builder.cc 656 Init node weights Assign doesn t exist in graph and my code all showed below def main def loss function weight logits labels labels tf.one hot labels 4 labels tf.cast labels tf.float32 first tf.reduce sum tf.multiply labels logits 1 second 0 tf.add tf.exp logits 0 tf.exp logits 1 second 1 tf.add tf.exp logits 2 tf.exp logits 3 log tf.log tf.add second 1 second 0 weight tf.transpose tf.reduce sum tf.multiply labels weight 1 output tf.multiply weight tf.add first log return output def normalize stft stft 1 numpy.empty stft.shape 0 128 128 stft 2 numpy.empty stft 1.shape 0 stft 1.shape 1 stft 1.shape 2 1 for i in range stft 1.shape 0 image Image.fromarray stft i image image.resize 128 128 stft 1 i numpy.array image min numpy.min stft 1 i max numpy.max stft 1 i stft 1 i stft 1 i min max min stft 2 i stft 1 i .reshape stft 1.shape 1 stft 1.shape 2 1 return stft 2 Get the data. stft training mfcc training labels training joblib.load open FLAGS.input mode rb stft test mfcc test labels test joblib.load open FLAGS.test mode rb stft test numpy.array stft test mfcc test numpy.array mfcc test labels test numpy.array labels test stft test normalize stft test mfcc test normalize mfcc test stft training numpy.array stft training mfcc training numpy.array mfcc training labels training numpy.array labels training stft training normalize stft training mfcc training normalize mfcc training stft shape stft training.shape stft shape None stft shape 1 stft shape 2 1 mfcc shape mfcc training.shape mfcc shape None mfcc shape 1 mfcc shape 2 1 labels shape labels training.shape labels shape None stft placeholder tf.placeholder stft training.dtype stft shape labels placeholder tf.placeholder labels training.dtype labels shape mfcc placeholder tf.placeholder mfcc training.dtype mfcc shape dataset training tf.data.Dataset.from tensor slices stft placeholder mfcc placeholder labels placeholder dataset training dataset training.apply tf.data.experimental.shuffle and repeat len stft training None dataset training dataset training.batch BATCH SIZE dataset training dataset training.prefetch 1 iterator training dataset training.make initializable iterator next element training iterator training.get next num epochs FLAGS.epochs train size labels training.shape 0 with tf.name scope input stft tf.placeholder name stft dtype data type shape BATCH SIZE IMAGE HEIGHT IMAGE WEITH NUM CHANNELS mfcc tf.placeholder name mfcc dtype data type shape BATCH SIZE IMAGE HEIGHT IMAGE WEITH NUM CHANNELS labels tf.placeholder tf.int64 shape BATCH SIZE with tf.name scope test input stft t tf.placeholder data type shape EVAL BATCH SIZE IMAGE HEIGHT IMAGE WEITH NUM CHANNELS mfcc t tf.placeholder data type shape EVAL BATCH SIZE IMAGE HEIGHT IMAGE WEITH NUM CHANNELS model BRN logits model.forward stft mfcc logits tf.add 0. logits name logits try scalar summary tf.scalar summary SummaryWrite tf.train.SummaryWrite merge summary tf.merge summary except scalar summary tf.summary.scalar SummaryWrite tf.summary.FileWriter merge summary tf.summary.merge with tf.name scope loss weights 1.0 1.7 4.1 5.7 mid loss function weights logits logits labels labels loss tf.reduce sum mid loss summary scalar summary loss loss regularizers tf.nn.l2 loss model.conv1 weights tf.nn.l2 loss model.conv2 weights tf.nn.l2 loss model.fc weights tf.nn.l2 loss model.fc biases batch tf.Variable 0 dtype data type with tf.name scope train optimizer tf.train.AdamOptimizer 0.001 .minimize loss train prediction tf.nn.softmax logits eval prediction tf.nn.softmax model.forward stft t mfcc t start time time.time def eval in batches stft data mfcc data sess type size stft data.shape 0 if size EVAL BATCH SIZE raise ValueError batch size for evals larger than dataset d size predictions numpy.ndarray shape size NUM LABELS dtype numpy.float32 for begin in xrange 0 size EVAL BATCH SIZE end begin EVAL BATCH SIZE if end size if type train predictions begin end sess.run train prediction feed dict stft stft data begin end ... mfcc mfcc data begin end ... else predictions begin end sess.run eval prediction feed dict stft t stft data begin end ... mfcc t mfcc data begin end ... else if type train batch predictions sess.run train prediction feed dict stft stft data EVAL BATCH SIZE ... mfcc mfcc data EVAL BATCH SIZE ... else batch predictions sess.run eval prediction feed dict stft t stft data EVAL BATCH SIZE ... mfcc t mfcc data EVAL BATCH SIZE ... predictions begin batch predictions begin size return predictions config tf.ConfigProto config.gpu options.allow growth True with tf.Session config config as sess tf.global variables initializer .run merged tf.summary.merge all writer SummaryWrite FLAGS.logs train sess.graph sess.run iterator training.initializer feed dict stft placeholder stft training mfcc placeholder mfcc training labels placeholder labels training for step in xrange int num epochs train size BATCH SIZE batch stft batch mfcc batch labels sess.run next element training feed dict stft batch stft mfcc batch mfcc labels batch labels sess.run optimizer feed dict feed dict if step EVAL FREQUENCY 0 summary l sess.run merged loss feed dict feed dict writer.add summary summary step elapsed time time.time start time start time time.time rate acc error rate eval in batches stft training mfcc training sess train labels training acc summary scalar summary accuracy acc print Step d epoch .2f Minibatch loss .3f Minibatch error .1f Accuracy .4f step float step BATCH SIZE train size l rate acc sys.stdout.flush test error test acc error rate eval in batches stft test mfcc test sess test labels test print Testset error .1f Accuracy .4f test error test acc converter tf.lite.TFLiteConverter.from session sess stft mfcc logits tflite model converter.convert open BRN.tflite wb .write tflite model writer.close When I run the official demo of converting a TensorFlow GraphDef into a TensorFlow Lite FlatBuffer from a tf.Session object the error also happens. Does that ok I mean can I use the weight trained in TensorFlow Lite or the file doesn t save the weight , mmmmayi Please provide the complete code to reproduce the reported issue. Thanks mmmmayi Please provide the complete code to reproduce the reported issue. Thanks i have edited it and add up the whole useful code mmmmayi I tried replicating the issue looks like some entities are not defined NameError name joblib is not defined . Please help us reproduce the issue. Thanks mmmmayi I tried replicating the issue looks like some entities are not defined NameError name joblib is not defined . Please help us reproduce the issue. Thanks I m so sorry I misunderstand what you need then you can find the whole code and database wavelet stft.p and wavelet stft test.p in my google cloud https drive.google.com open id 1DfV7WPJymj66jJ13ds6javg3GSLPhwyr but for your convenience you can also use the official demo it caused the same error in my condition mmmmayi Thanks for providing the code. I tried reproducing the issue on Colab with official demo code but i didn t receive any error. Please take a look at gist here https colab.research.google.com drive 1GYSh6qXCGzz4pOWU3Ht0a raXYOfToKp . let us know. Thanks mmmmayi Thanks for providing the code. I tried reproducing the issue on Colab with official demo code but i didn t receive any error. Please take a look at gist here https colab.research.google.com drive 1GYSh6qXCGzz4pOWU3Ht0a raXYOfToKp . let us know. Thanks Thanks for your reply but when I try the error still exist I don t know why. Here are the whole log mmmmayi I couldn t reproduce the issue. Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan ed91d93d0fa43a8a210881da87443b62 tf 31609 tflite.ipynb . 1. Based on the error trace you are running the code with tensorflow gpu right Can you try with tensorflow cpu only to see whether the issue caused due to any cuda related file. 2. Can you run the gist provided here and see what you get as output Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 31609 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 31609 No a I think the problem is about get variable because if I use tf.placeholder to get var in the demo code there isn t error otherwise the tf.get variable leads to it mmmmayi I couldn t reproduce the issue. Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan ed91d93d0fa43a8a210881da87443b62 tf 31609 tflite.ipynb . 1. Based on the error trace you are running the code with tensorflow gpu right Can you try with tensorflow cpu only to see whether the issue caused due to any cuda related file. 2. Can you run the gist provided here and see what you get as output Thanks it still exists when I use CPU and you can get the output of it from the link named converted model3.tflite https drive.google.com drive folders 1Ds8ihFsz9K5ZKPCS0FPQ2 qZBUOn3to3 usp sharing mmmmayi 1. Did you ran the gist I created Do you see any error 2. Are you using tf nightly right Just a confirmation. Thanks mmmmayi I am currently facing the same issue have you found a solution Thanks mmmmayi I am currently facing the same issue have you found a solution Thanks I just fixed this problem by using tf 1.13.1 but not tf 1.14... DragonX081mk2 Can you try TF1.15.0rc3 and let us know whether the issue persists with the latest version. As TF1.14 was not built properly there were lot of issues when using it. Hence TF team released new version TF1.15 . Thanks DragonX081mk2 Can you try TF1.15.0rc3 and let us know whether the issue persists with the latest version. As TF1.14 was not built properly there were lot of issues when using it. Hence TF team released new version TF1.15 . Thanks Before trying 1.13.1 I have tried the 1.15.0 rc3 this problem existed too.. mmmmayi I am currently facing the same issue have you found a solution Thanks hi actually I didn t figure it out but it seems have no effect to the result because I can still get the tflite model even with this error mmmmayi Can we close this issue I don t see any error when I used TF1.15.0 . Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan bc515f885097fc6fd01076a3859e7e5b tf 31609 tflite.ipynb . Thanks Automatically closing this out since I understand it to be resolved but please let me know if I m mistaken.Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 31609 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 31609 No a 
31943, TF 2.0.0 rc0 Run model.evaluate let notebook crash Chrome ., System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution Mac TensorFlow installed from source or binary pip TensorFlow version use command below Tensorflow 2.0.0 rc0 Python version 3.6.5 print tf.version.GIT VERSION tf.version.VERSION v2.0.0 beta1 5101 gc75bb66a99 2.0.0 rc0 Describe the current behavior When I run result model.evaluate x train y train my jupyter notebook crashed. Describe the expected behavior I will be run another cell in jupyter notebook but I can t. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. I use sample code in Tensorflow 2.0.0 RC Classify images Other info logs ,I found crash only happens in Chrome browser. In Firefox as below Screenshot from 2019 08 26 13 53 56 https user images.githubusercontent.com 20853096 63668267 315dc300 c809 11e9 812f 6b665c13ab47.png kaka lin I tried in Google colab and Jupyter notebook with Google chrome and able to execute the code successfully.Can you please upgrade your google chrome version and see if the problem still persists.Thanks Hi ravikyram I upgrade my google chrome version Version 76.0.3809.132 Official Build 64 bit But the problem is still persistent. I record a demo video pls check it thanks. TF2 beta1 https drive.google.com file d 1P5s ROhQJq4 a6WyrPEdCQqUGBarYLAf view usp sharing TF2 rc0 https drive.google.com file d 1G7MFn53t YMFgcFNbZXZk06P921 Rid0 view usp sharing I have the same problem. kaka lin As per my understanding in Google chrome with TF2 beta1 version you are able to execute the code successfully .But with Google chrome using TF2 rc0 you are facing the issue. Am i correct Thanks ravikyram Yes I tried reproducing the issue using Google chrome in Jupyter notebook and was able to execute the code with TF2 beta1.But i am facing the issue with TF2 rc0.Please find the screenshot below.Thanks error https user images.githubusercontent.com 51902062 63920602 dc1aef00 ca5e 11e9 93bd 18c6dd457d30.png kaka lin I think this was resolved in the recent TF versions. I checked with TF2.0 and tf nightly and I cannot reproduce the issue. I am closing the issue. Please feel free to reopen the issue if it persists for you. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 31943 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 31943 No a 
32281,The TF function for the TRT segment could not be empty, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow YES OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary when I convert pb to tensorrt I use tf nightly gpu 1.15 from pip install. When I do infer I use tf 1.14.0 from source. TensorFlow version use command below tf 1.14.0 Python version 3.5 Bazel version if compiling from source 0.24.1 GCC Compiler version if compiling from source 5.4 CUDA cuDNN version cuda 10 cudnn 17 GPU model and memory 1060Ti Describe the current problem Hi I successfully used trt convert to my pb model to tensorRT plan and I want to use c to infer my model. So I use bazel complie tf with tensorrt together . In my code pb model can run successfully. I use the same code and TensorRT model can load successfully but as session running tf give me the following error 2019 09 06 06 56 37.455586 E tensorflow core common runtime executor.cc 642 Executor failed to create kernel. Invalid argument The TF function for the TRT segment could not be empty node fa layer4 c0 TRTEngineOp 108 When I convert pb to tensorrt it is shown as following 2019 09 06 06 25 59.150320 W tensorflow compiler tf2tensorrt utils trt logger.cc 37 DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output. 2019 09 06 06 25 59.183609 I tensorflow compiler tf2tensorrt convert convert graph.cc 831 TensorRT node fa layer4 TRTEngineOp 107 added for segment 107 consisting of 3 nodes succeeded. 2019 09 06 06 25 59.189007 I tensorflow compiler tf2tensorrt convert convert graph.cc 831 TensorRT node fa layer4 c0 TRTEngineOp 108 added for segment 108 consisting of 2 nodes succeeded. 2019 09 06 06 25 59.189378 W tensorflow compiler tf2tensorrt utils trt logger.cc 37 DefaultLogger Tensor DataType is determined at build time for tensors not marked as input or output. 2019 09 06 06 25 59.189403 E tensorflow compiler tf2tensorrt utils trt logger.cc 41 DefaultLogger Network must have at least one output 2019 09 06 06 25 59.189426 W tensorflow compiler tf2tensorrt convert convert graph.cc 834 TensorRT node fa layer4 c0 conv0 bn cond 1 TRTEngineOp 109 added for segment 109 consisting of 4 nodes failed Internal Failed to build TensorRT engine. Fallback to TF... Additionally I can use the tensorrt model in python code to infer. python installed from pip Anyone could provide some ideas how to solve it ,I met this problem too have you solved this problem I tried convert savedmodel to trt savedmodel successfully with below command line And deploy the trt savedmodel with tf serving no error found. But when send grpc request to tf serving. error occurs below anybody who can explain this thanks in advance double344931987 superhg2012 could you provide a detailed repro E.g. which model you re using and how you deploy run the model with TF serving Thanks. aaroey I am ok now. I updated tensorflow version to 1.14.0 and tensorRT from 5.0.26 to 5.1.5.0. Thanks superhg2012. double344931987 I m closing this feel free to reopen with a repro model script if you re still experiencing this issue. Thanks. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32281 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32281 No a 
32412, TF 2.0.0 rc0 Keras Model subclassing dynamic True throws Error. Possible Regression bug of rc0, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mac OSX 10.13.6 intel i7 MacBook Pro Retina 13 inch Early 2015 Intel Iris Graphics 6100 1536 MB Google Colab Both CPU and GPU Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device No TensorFlow installed from source or binary Binary pip TensorFlow version use command below 2.0.0 rc0 pip install tensorflow 2.0.0 rc0 Python version 3.7.3 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version NA GPU model and memory Intel Iris Graphics 6100 1536 MB Stock Intel i7 CPU bundled Describe the current behavior In 2.0.0 rc0 While using Model Subclassing of tf keras.Model and passing dynamic True . Then when I call model.fit it throws AttributeError NoneType object has no attribute dtype Describe the expected behavior model.fit should not throw any error. Note this was working fine in 2.0.0 beta1 Code to reproduce the issue Other info logs Run code in 2.0.0 beta1 it works. in rc0 fails. Colab Notebook with same issue https colab.research.google.com drive 1dZaIwlgKRk 8FSKkdyQlwXsT3auqV6VX ,I have tried on colab with TF version 2.0.0 rc0 and was able to reproduce the issue.However i am not seeing any issue with TF 2.0.0 beta1 .Please find the gist here https colab.sandbox.google.com gist ravikyram c1083c26816bb17ae69785f8d5a111cc untitled174.ipynb . Thanks this issue is no longer seen with 2.2.0 rc0. faizanahemad could you please check and close if the issue if it is resolved. Closing this now faizanahemad please feel free to re open if you still continue to see the issue. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32412 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32412 No a 
32573,TFTRT CUDA ERROR ILLEGAL ADDRESS, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary binary TensorFlow version use command below 1.14 Python version 2.7 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 10.1 GPU model and memory RTX2080 You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior Seeing the following error Describe the expected behavior Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. ,Since TF 1.14 prebuilt binaries support cuda 10.0 can you try by switching to cuda 10.0 instead. Thanks I see. Using the nvcr container with TF 1.14 . Which has Cuda 10.1. sgambient Please provide the minimal standalone code to reproduce the reported issue. Thanks That is an easy ask but very hard to do. I do not think that is the right way to handle large software. My recommendation is to build in more detailed logs and metrics so as not to have to rely on largess of random developers. Is this still an issue after switching to cuda 10.0 Automatically closing due to lack of recent activity. Please update the issue when new information becomes available and we will reopen the issue. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 32573 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 32573 No a 
32685,tf.function problem when slicing tensor with variable, System information Have I written custom code Yes OS Platform and Distribution Windows 10 TensorFlow installed from binary TensorFlow version v2.0.0 rc0 101 gd2d2566eef 2.0.0 rc1 Python version 3.7.4 CUDA cuDNN version N A GPU model and memory N A Describe the behavior Slicing tensors using slices indexed by a tf.Variable does not work with tf.function . The problem does not occur when executing eagerly or when slicing with tensors which are not variables. Code to reproduce the issue Executing the code import tensorflow as tf pos tf.Variable 0 dtype tf.int32 def ok return tf.zeros 5 pos 3 tf.function def also ok return tf.zeros 5 pos 0 3 tf.function def not ok return tf.zeros 5 pos 3 tf.print ok tf.print also ok tf.print not ok produces the output 0 0 0 0 0 0 StagingError Detailed traceback StagingError Traceback most recent call last ipython input 29 6470bfb94cb0 in module 12 13 tf.print ok 14 tf.print not ok .conda envs tf2 lib site packages tensorflow core python eager def function.py in call self args kwds 455 456 tracing count self. get tracing count 457 result self. call args kwds 458 if tracing count self. get tracing count 459 self. call counter.called without tracing .conda envs tf2 lib site packages tensorflow core python eager def function.py in call self args kwds 501 This is the first call of call so we have to initialize. 502 initializer map object identity.ObjectIdentityDictionary 503 self. initialize args kwds add initializers to initializer map 504 finally 505 At this point we know that the initialization is complete or less .conda envs tf2 lib site packages tensorflow core python eager def function.py in initialize self args kwds add initializers to 406 self. concrete stateful fn 407 self. stateful fn. get concrete function internal garbage collected pylint disable protected access 408 args kwds 409 410 def invalid creator scope unused args unused kwds .conda envs tf2 lib site packages tensorflow core python eager function.py in get concrete function internal garbage collected self args kwargs 1846 if self.input signature 1847 args kwargs None None 1848 graph function self. maybe define function args kwargs 1849 return graph function 1850 .conda envs tf2 lib site packages tensorflow core python eager function.py in maybe define function self args kwargs 2148 graph function self. function cache.primary.get cache key None 2149 if graph function is None 2150 graph function self. create graph function args kwargs 2151 self. function cache.primary cache key graph function 2152 return graph function args kwargs .conda envs tf2 lib site packages tensorflow core python eager function.py in create graph function self args kwargs override flat arg shapes 2039 arg names arg names 2040 override flat arg shapes override flat arg shapes 2041 capture by value self. capture by value 2042 self. function attributes 2043 Tell the ConcreteFunction to clean up its graph once it goes out of .conda envs tf2 lib site packages tensorflow core python framework func graph.py in func graph from py func name python func args kwargs signature func graph autograph autograph options add control dependencies arg names op return value collections capture by value override flat arg shapes 913 converted func 914 915 func outputs python func func args func kwargs 916 917 invariant func outputs contains only Tensors CompositeTensors .conda envs tf2 lib site packages tensorflow core python eager def function.py in wrapped fn args kwds 356 wrapped allows AutoGraph to swap in a converted function. We give 357 the function a weak reference to itself to avoid a reference cycle. 358 return weak wrapped fn . wrapped args kwds 359 weak wrapped fn weakref.ref wrapped fn 360 .conda envs tf2 lib site packages tensorflow core python framework func graph.py in wrapper args kwargs 903 except Exception as e pylint disable broad except 904 if hasattr e ag error metadata 905 raise e.ag error metadata.to exception e 906 else 907 raise StagingError in converted code ipython input 20 6173bf43c1fe 11 not ok return tf.zeros 5 pos 3 C Users Daniel .conda envs tf2 lib site packages tensorflow core python ops array ops.py 748 slice helper s.start sys.maxsize C Users Daniel .conda envs tf2 lib site packages tensorflow core python ops variables.py 1111 ne return gen math ops.not equal self other C Users Daniel .conda envs tf2 lib site packages tensorflow core python ops gen math ops.py 7012 not equal name name C Users Daniel .conda envs tf2 lib site packages tensorflow core python framework op def library.py 527 apply op helper preferred dtype default dtype C Users Daniel .conda envs tf2 lib site packages tensorflow core python framework ops.py 1296 internal convert to tensor ret conversion func value dtype dtype name name as ref as ref C Users Daniel .conda envs tf2 lib site packages tensorflow core python framework tensor conversion registry.py 52 default conversion function return constant op.constant value dtype name name C Users Daniel .conda envs tf2 lib site packages tensorflow core python framework constant op.py 227 constant allow broadcast True C Users Daniel .conda envs tf2 lib site packages tensorflow core python framework constant op.py 265 constant impl allow broadcast allow broadcast C Users Daniel .conda envs tf2 lib site packages tensorflow core python framework tensor util.py 450 make tensor proto nparray np.array values dtype np dt OverflowError Python int too large to convert to C long ,Can you please try with pip install tf nightly 2.0 preview 2.0.0.dev20190919 and see if the problem still persists. I am not seeing any error message.Please find the gist here https colab.sandbox.google.com gist ravikyram 8a0f645e34569d088493c78613b72da4 untitled208.ipynb . Thanks Thanks That fixed it. But also had to downgrade to Python 3.6.8 since nightly builds are not available for 3.7. I am closing this issue since the query is been resolved.Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 32685 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 32685 No a I have this same issue I have installed tf nightly 2.9.0.dev20211227 without any solution. I run tfp.mcmc chain that has to call a function p t . tf.function def p t pos tf.raw ops.Bucketize t t obs precip t Prec.values pos 1 pos return precip t 0 
32725,run interpreter.invoke just show Aborted core dumped , em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information yes Have I written custom code as opposed to using a stock example script provided in TensorFlow Linux Ubuntu 16.04 OS Platform and Distribution e.g. Linux Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device conda TensorFlow installed from source or binary 1.13.1 and 1.14 TensorFlow version use command below 3.6.8 Python version Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version GPU model and memory You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior When I use tf version 1.13.1 to convert pb to tflite it shows dim not matched error but when I use 1.14 to convert it succeeds to save the tflite file. But when I use the test code in the tf doc it just shows core dumped. Describe the expected behavior i want to use the tflite file in android however i even can t use it in python Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. code generated the tflite file code to test in python from doc Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. converted with 1.13.1 ture guard.cc 141 Your CPU supports instructions that this TensorFlow binary was not compiled to use AVX2 FMA 2019 09 23 14 33 07.069746 I tensorflow core platform profile utils cpu utils.cc 94 CPU Frequency 3696000000 Hz 2019 09 23 14 33 07.072588 I tensorflow compiler xla service service.cc 150 XLA service 0x564ecd24a790 executing computations on platform Host. Devices 2019 09 23 14 33 07.072658 I tensorflow compiler xla service service.cc 158 StreamExecutor device 0 undefined undefined Ignore tcmalloc large alloc warnings. Traceback most recent call last File pb to tflite.py line 9 in module tflite model converter.convert File home dm anaconda3 envs s t3 lib python3.6 site packages tensorflow lite python lite.py line 455 in convert converter kwargs File home dm anaconda3 envs s t3 lib python3.6 site packages tensorflow lite python convert.py line 442 in toco convert impl input data.SerializeToString File home dm anaconda3 envs s t3 lib python3.6 site packages tensorflow lite python convert.py line 205 in toco convert protos TOCO failed. See console for info. n s n s n stdout stderr tensorflow.lite.python.convert.ConverterError TOCO failed. See console for info. 2019 09 23 14 33 21.269591 I tensorflow lite toco graph transformations graph transformations.cc 39 Before Removing unused ops 529 operators 854 arrays 0 quantized 2019 09 23 14 33 21.275653 I tensorflow lite toco graph transformations graph transformations.cc 39 After Removing unused ops pass 1 527 operators 849 arrays 0 quantized 2019 09 23 14 33 21.283483 I tensorflow lite toco graph transformations graph transformations.cc 39 Before general graph transformations 527 operators 849 arrays 0 quantized 2019 09 23 14 33 21.330461 F tensorflow lite toco graph transformations propagate fixed sizes.cc 117 Check failed dim x dim y 512 vs. 10 Dimensions must match Aborted core dumped converted by 1.14 2019 09 23 14 38 45.285133 I tensorflow core grappler optimizers meta optimizer.cc 716 Optimization results for grappler item graph to optimize 2019 09 23 14 38 45.285519 I tensorflow core grappler optimizers meta optimizer.cc 718 constant folding Graph size after 440 nodes 112 631 edges 112 time 213.324ms. 2019 09 23 14 38 45.285580 I tensorflow core grappler optimizers meta optimizer.cc 718 constant folding Graph size after 440 nodes 0 631 edges 0 time 54.184ms. and when I use the code above to test it just shows name input index 95 shape array 1 257 400 1 dtype int32 dtype class numpy.float32 quantization 0.0 0 name lambda 1 l2 normalize index 96 shape array 1 512 dtype int32 dtype class numpy.float32 quantization 0.0 0 1 257 400 1 Aborted core dumped and I could use netron to open the generated tflite file the network structure it shows 1 here is the code of this model https github.com WeidiXie VGG Speaker Recognition could somebody give me some help 1 https i.stack.imgur.com zlSAn.png ,by selecting different graph node as the output i found maybe is the sub node making this error i defined my own trainable variable in keras and sub them to calculate the residual is there any way to fixed this problem image https user images.githubusercontent.com 31615877 65477728 c9f36b80 deb9 11e9 949c 849de9df7cd3.png i try to avoid to use broadcast but still not work and i also try to use tf.get variable to replace keras add weight in self define layer but not work again I also have the same issue i found the reason of my problem is because when two tensors subtract using broadcast. but if the tensor dim is smaller than 5 this problem will not occur. Once the tensor dim is larger than 4 it can t use broadcast or will be broken. But when i try to use tf.tile to avoid using broadcast the tflite will move this tile node from the graph may be it s where the reason from. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 32725 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 32725 No a the same issue is there any solution 
33012,tf.function fails with tf.ragged.boolean mask, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow YES OS Platform and Distribution Windows 10 TensorFlow version v2.0.0 rc2 26 g64c3d382ca 2.0.0 Python version 3.7.4 CUDA cuDNN version None GPU model and memory NO GPU Describe the current behavior A function containing tf.ragged.boolean mask and decorated with tf.function works on first execution but fails when executed with different inputs. Setting experimental relax shapes True does not help. Code to reproduce the issue Output of the code ,I have tried on colab with TF version 2.0.0 rc2 2.0.0 dev20191002 and was able to reproduce the issue.Please find the gist here https colab.sandbox.google.com gist ravikyram 2229ff515d40f06a322a1df614c564e7 untitled246.ipynb . Thanks PistaSaki Looks like this was resolved in tf nightly . I was not able to reproduce the issue with tf nightly . Please check the gist here https colab.sandbox.google.com gist jvishnuvardhan 129fd452f492f59d3789779155dbbc6d untitled246.ipynb . Thanks I am closing this issue. Please feel free to reopen if the issue persists again. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33012 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33012 No a 
33095,Tensorflow 2.0 stop gradient cause ValueError has None for gradient for tf.Optimizers.Adam, em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04.2 LTS Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary pip TensorFlow version use command below v2.0.0 rc2 26 g64c3d38 2.0.0 1.14.0 Python version 3.7.4 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 7.6.0 GPU model and memory Describe the current behavior For Tensorflow 2.0 stop gradient doesn t seem to work properly. For example the code below update one model s weight using output from another model. In Tensorflow 2.0 it will try to compute gradient for the stop gradient model. Describe the expected behavior In Tensorflow 1.14.0 and 1.13.2 it works correctly. The model only updates for model1 without any checking for model2. Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs ValueError Traceback most recent call last ipython input 2 bb2983e16d6f in module 26 model1.compile optimizer loss None 27 28 model1.fit input1 x label y None epochs 100 miniconda3 envs dl ehr rl lib python3.7 site packages tensorflow core python keras engine training.py in fit self x y batch size epochs verbose callbacks validation split validation data shuffle class weight sample weight initial epoch steps per epoch validation steps validation freq max queue size workers use multiprocessing kwargs 726 max queue size max queue size 727 workers workers 728 use multiprocessing use multiprocessing 729 730 def evaluate self miniconda3 envs dl ehr rl lib python3.7 site packages tensorflow core python keras engine training arrays.py in fit self model x y batch size epochs verbose callbacks validation split validation data shuffle class weight sample weight initial epoch steps per epoch validation steps validation freq kwargs 672 validation steps validation steps 673 validation freq validation freq 674 steps name steps per epoch 675 676 def evaluate self miniconda3 envs dl ehr rl lib python3.7 site packages tensorflow core python keras engine training arrays.py in model iteration model inputs targets sample weights batch size epochs verbose callbacks val inputs val targets val sample weights shuffle initial epoch steps per epoch validation steps validation freq mode validation in fit prepared feed values from dataset steps name kwargs 187 function we recompile the metrics based on the updated 188 sample weight mode value. 189 f make execution function model mode 190 191 Prepare validation data. Hold references to the iterator and the input list miniconda3 envs dl ehr rl lib python3.7 site packages tensorflow core python keras engine training arrays.py in make execution function model mode 563 if model. distribution strategy 564 return distributed training utils. make execution function model mode 565 return model. make execution function mode 566 567 miniconda3 envs dl ehr rl lib python3.7 site packages tensorflow core python keras engine training.py in make execution function self mode 2182 def make execution function self mode 2183 if mode ModeKeys.TRAIN 2184 self. make train function 2185 return self.train function 2186 if mode ModeKeys.TEST miniconda3 envs dl ehr rl lib python3.7 site packages tensorflow core python keras engine training.py in make train function self 2114 Training updates 2115 updates self.optimizer.get updates 2116 params self. collected trainable weights loss self.total loss 2117 Unconditional updates 2118 updates self.get updates for None miniconda3 envs dl ehr rl lib python3.7 site packages tensorflow core python keras optimizer v2 optimizer v2.py in get updates self loss params 498 499 def get updates self loss params 500 grads self.get gradients loss params 501 grads and vars list zip grads params 502 self. assert valid dtypes miniconda3 envs dl ehr rl lib python3.7 site packages tensorflow core python keras optimizer v2 optimizer v2.py in get gradients self loss params 396 gradient defined i.e. are differentiable . 397 Common ops without gradient 398 K.argmax K.round K.eval. .format param 399 if hasattr self clipnorm 400 grads clip ops.clip by norm g self.clipnorm for g in grads ValueError Variable tf.Variable d21 kernel 0 shape 1 12 dtype float32 has None for gradient. Please make sure that all of your ops have a gradient defined i.e. are differentiable . Common ops without gradient K.argmax K.round K.eval. , Nephalen Can you please check this link https github.com tensorflow model optimization issues 3 issuecomment 497151388 and let us know if it helps. Thanks Nephalen Can you please check this link https github.com tensorflow model optimization issues 3 issuecomment 497151388 and let us know if it helps. Thanks rmothukuru I don t think that problem is the same as mine. The sample code that produces error in Tensorflow 2.0 runs on single machine. In addition there is no part that can produce additional layers. I also don t think it s the problem of keras customized loss function. If I remove stop gradient the code runs fine in Tensorflow 2.0. Upon further testing the sample code works in Tensorflow 1.14 and 1.13.2 However it doesn t work in Tensorflow 1.15.0rc3 and 2.0.0. To me it looks like stop gradient is not working as intended since optimizer still tries to compute the gradient of part of graph that is blocked by stop gradient . Could reproduce the issue with Tensorflow Version 2.0. Here is the Gist https colab.sandbox.google.com gist rmothukuru 5b15b64917818c18b24797b962e7f452 33095.ipynb . Thanks Nephalen I think the original issue has been addressed by this comment https github.com tensorflow tensorflow issues 26557 issuecomment 487133316 right As mentioned stop gradient does not work well and please follow the suggestion mentioned in the comment. Nephalen I think the original issue has been addressed by this comment https github.com tensorflow tensorflow issues 26557 issuecomment 487133316 right As mentioned stop gradient does not work well and please follow the suggestion mentioned in the comment. gowthamkpr I m afraid the issue is not the same. If you look at the network structure of 26557 https github.com tensorflow tensorflow issues 26557 issue 419269658 closely you will find the model is completely disconnected from input because of stop gradient . In this case TF 1.14 will also produce a ValueError saying that no gradient has been provided. However in the sample code I provided the stop gradient only blocks the gradient of the second model which is not the model to be trained. In TF 1.14 it works correctly. It is possible that it has something similar to do with 33471 https github.com tensorflow tensorflow issues 33471 issue 508537917 . Because it is clear that weight of any additional model added in add loss function is incorrectly treated as update target in TF2.0 1.15. In this case it actually makes sense that the code will raise ValueError in TF2.0 1.15 because there is no gradient for model2 completely. But this is only my guess. Nephalen Thanks for the issue This looks fixed in the latest tf nightly w eager execution enabled . This code works for me Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33095 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33095 No a 
33814,Profile contains events with truncated names, System information Have I written custom code yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 RHEL 7.4 TensorFlow installed from source or binary Docker container TensorFlow version use command below v2.0.0 rc2 26 g64c3d38 2.0.0 Python version 3.6.8 CUDA cuDNN version 10.0 GPU model and memory K80 Describe the current behavior When gathering a trace via the supposedly only supported in TF2 method the resulting trace file shows incomplete GPU op names. Validated in TensorBoard and deserializing the Protobuf message. Examples Describe the expected behavior Trace shows complete function names at least and signatures optional or even better Layers to which those activities events belong. Code to reproduce the issue Other info logs Traces generated train.zip https github.com tensorflow tensorflow files 3782526 train.zip ,I think the issue has already been fixed. The trace was collected on Oct 29th. Can you give us a trace at head and we can investigate if the issue persists. I tried but can t test it currently. I just tried with the docker containers but neither tensorflow devel gpu py3 nor tensorflow devel py3 seem to actually contain tensorflow. In usr local lib python3.6 dist packages I see some Keras stuff but nothing with tensorflow. pip freeze doesn t list it either. Using pip to install the 2.1.0rc has upgraded the CUDA requirement to 10.1 under which the driver requires admin rights or unrestricted mode to enable tracing. I passed it on to our HPC admins but it might take a bit. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33814 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33814 No a We are closing this issue for now due to lack of activity. Please comment if this is still an issue for you. Thanks Still checking if this is an issue tensorflowbutler please reopen 
33848,CUDNN STATUS INTERNAL ERROR when running with CUDA 10.0 tensorflow gpu on RTX2080 ubuntu18,I have Geforce 2080 running latest nvidia drivers Cuda 10.0 matching cudnn libs and I m getting CUDNN STATUS INTERNAL ERROR when running with tensorflow gpu. System information NVIDIA drivers NVIDIA SMI 430.50 Driver Version 430.50 CUDA Version 10.1 Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution Linux Ubuntu 18.04 TensorFlow installed from source or binary using pip install tensorflow TensorFlow version use command below v2.0.0 rc2 26 g64c3d38 Python version 3.7.4 GCC Compiler version if compiling from source 7.4 CUDA 10.0 cuDNN version 7.6.4.38 GPU model and memory You can collect some of this information using our environment capture script https github.com tensorflow tensorflow tree master tools tf env collect.sh You can also obtain the TensorFlow version with 1. TF 1.0 python c import tensorflow as tf print tf.GIT VERSION tf.VERSION 2. TF 2.0 python c import tensorflow as tf print tf.version.GIT VERSION tf.version.VERSION Describe the current behavior Getting CUDNN STATUS INTERNAL ERROR when trying to run my tensorflow program. Describe the expected behavior It should not fail with this error Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. export LD LIBRARY PATH LD LIBRARY PATH usr local cuda 10.0 lib64 python classify.py image file someimage.jpg classify.py ,I found to also be failing in the same way using docker gpu image I have shortened the code above that still replicates the problem Also I should say the same nvidia driver cuda cudnn seems to work fine for my pytorch code tombburnell Dude just move to Pytorch Tensorflow is the biggest crap i have seen in my life. tombburnell Similar issue 24496 https github.com tensorflow tensorflow issues 24496 . Apologies for the delay in response. Is this still an issue You may try gpu memory resources management by allowing gpu memory growth. To know more see https www.tensorflow.org guide gpu limiting gpu memory growth You can try Allowing GPU memory growth with Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33848 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33848 No a The solution provided by ymodak worked for me. I am using tensorflow 1.15 cudnn 7.6.5 on conda 4.8.2 Ubuntu 20.04. Thanks for the help. Apologies for the delay in response. Is this still an issue You may try gpu memory resources management by allowing gpu memory growth. To know more see https www.tensorflow.org guide gpu limiting gpu memory growth You can try Allowing GPU memory growth with Thanks. This solution worked for me. I had the same problem and I solved it with this code import tensorflow as tf config tf.compat.v1.ConfigProto config.gpu options.allow growth True sess tf.compat.v1.InteractiveSession config config 
33868,Distribution Strategy running variables NCCL error crashes CUDA , System information Have I written custom code as opposed to using a stock example script provided in TensorFlow OS Platform and Distribution e.g. Linux Ubuntu 16.04 Linux Ubuntu 16.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device TensorFlow installed from source or binary TensorFlow version use command below 2.0.0 Python version 3.7.4 Bazel version if compiling from source GCC Compiler version if compiling from source CUDA cuDNN version 10.0 7.6.2 GPU model and memory Titan XP 12 gb x 8 Describe the current behavior When using distribution strategy the following code runs in graph mode but fails in eager mode resulting in an NCCL error. Since I need persistent Variables to normalize my loss the loss function was implemented as a callable keras layer instantiated as a property of the keras model for which is compute the loss. If I change the assign add call to the model runs in eager mode but yields the following error in graph mode Is there a preferred way to have persistent variables within your model that are aggregated across multiple GPUs Describe the expected behavior Code to reproduce the issue Provide a reproducible test case that is the bare minimum necessary to generate the problem. Other info logs NCCL debug info ,After some debugging I think this might have to do with attempting to aggregate the int64 Hi can you provide the entire model training code that you re using above That will help us reproduce and debug the issue. The all reduce is most likely triggered from the .assign add which will try to aggregate the delta before applying to the mirrored variables. But it s not clear to me why it fails. because similar things happen for gradient aggregation as well and presumably those are not failing for you When you eager vs graph mode in TF 2.0 what do you mean perhaps sharing your training code for either case would help. thanks mjlbach Can you please provide the details guptapriya is asking Please close the issue If the issue was already resolved. Thanks I think it was resolved. Closing due to lack of recent activity. Please open new ticket if you see similar issue. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 33868 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 33868 No a 
34090,ResourceScatterNdUpdate bug in graph mode and tape.gradient , em Please make sure that this is a bug. As per our GitHub Policy https github.com tensorflow tensorflow blob master ISSUES.md we only address code doc bugs performance issues feature requests and build installation issues on GitHub. tag bug template em System information Have I written custom code as opposed to using a stock example script provided in TensorFlow No OS Platform and Distribution e.g. Linux Ubuntu 16.04 Ubuntu 18.04 Mobile device e.g. iPhone 8 Pixel 2 Samsung Galaxy if the issue happens on mobile device N A TensorFlow installed from source or binary binary TensorFlow version use command below v2.0.0 rc2 26 g64c3d38 2.0.0 Python version 3.6 Bazel version if compiling from source N A GCC Compiler version if compiling from source N A CUDA cuDNN version 10.0.130 GPU model and memory GeForce RTX 2080 Ti Describe the current behavior Running the code gets the following error There is no error if running the code without tf.function decoration or without using while loop or without using tape.gradient Describe the expected behavior Code should run without error Code to reproduce the issue Other info logs Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks please include the full traceback. Large logs and files should be attached. Full error message , emailweixu Can you try running the code in latest tf nightly 2.1.0.dev20191110 version Issue seemed to be fixed kindly find the gist https colab.sandbox.google.com gist oanush 188e307b70eaf4ddbb43fd26e8be1c87 34090.ipynb of colab for the same.Thanks Yes it works with tf nightly emailweixu Can you please try using the nightly version as the issue is fixed in the latest version kindly close the issue if its resolved Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34090 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34090 No a 
34127,Impossible to use tf.keras.callbacks.ModelCheckpoint in distributed training, System information Have I written custom code as opposed to using a stock example script provided in TensorFlow Yes OS Platform and Distribution e.g. Linux Ubuntu 16.04 Google Colab TensorFlow version use command below 2.0.0 Describe the current behavior It is not possible to use tf.keras.callbacks.ModelCheckpoint in distributed training RuntimeError add update was called in a cross replica context. This is not expected. If you require this feature please file an issue. Code to reproduce the issue See this Colab notebook https colab.research.google.com drive 11CmG16 x9z MMKoSeKaEKZ0zrnBdeMMV .,Note no error if we use .h5 instead of TF checkpoint format. However just a few hours ago karmel and jvishnuvardhan recommended me to use TF format https github.com tensorflow tensorflow issues 34016 because .h5 can cause issues okay fine . So which one should I use finally None is working I am lost... Hello netw0rkf10w sorry about the confusion. I was trying to repro locally with your example but unfortunately there was some issue downloading the dataset. When I tried some other dataset I have the program could finish just fine. Would you mind providing a minimal repro with some dummy data so I can quickly repro Thank you. Hi rchao. Could you tell me what kind of issues did you have I have just tested again and there was no issue with the download of data. Note that in the notebook I created the dataset and fit the model in the same code cell so maybe the error you got was from the training part and not from the dataset download part. For clarity I have split them into different cells. Please try it again. Thanks. Hello netw0rkf10w sorry for the late response. I made a copy of your updated colab and used nightly instead and it could work fine without the problem. Can you take a look if that fixes your issue The TF version I verified to work at is 2.1.0 dev20191202. Closing for lack of activity. Please reopen if the issue still exists. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34127 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34127 No a 
34262,train with muliple gpu get error Out of range End of sequence,I am training with tensorflow2.0 with multiple GPU. It got the following errors. But if I use only one GPU it ran without any error. My tensorflow version is tensorflow gpu 2.0.0 and this is my code ,Can you please provide the full stack trace so we can debug this further How many GPUs did you run this with Also can you test with the latest TF 2 nightly For reference I tried with the latest TF and not able to repro I ran with 4 GPUs. This thefull stack trace I will try tf nightly later. guptapriya I can not repro for tf nightly gpu. But for tensorflow gpu 2.0 I reproduce the error every time. Is this a bug of tensorflow2.0 honeytidy that s possible we ve fixed a number of issues since TF 2.0 branch was cut. Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34262 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34262 No a 
34518, Failed to get convolution algorithm. This is probably because cuDNN failed to initialize so try looking to see if a warning log message was printed above.,I researched that issue but unfortunnately all i was able to find was a compatibility problems. i made sure to install exactly what was specified in the tensorflow gpu installation procedure. The only thing that is different is that i m running the 430 nvidia driver which was installed by asking to install the 418 System information https github.com tensorflow models blob master research object detection object detection tutorial.ipynb Ubuntu 18.04 Tensorflow gpu installed using pip Tensorflow gpu 2.0.0 Python version 3.6.8 CUDA cuDNN version cuda 10.1 cuDNN 7.6.5 GPU model and memory NVIDIA GeForce GTX 1050 Mobile 2Gb Describe the current behavior Describe the expected behavior Expecting to show the image with rectangle for detected objects Code to reproduce the issue https github.com tensorflow models blob master research object detection object detection tutorial.ipynb I used the provided code and i m getting this error while computing the block Other info logs , Adding those lines just after importing the modules fixed the precedent issue. But i still cannot run on jupyther the kernel die Does anybody have a workarround this The code is working as expected with CPU version of Tensorflow. Please find the colab gist here https colab.sandbox.google.com gist gadagashwini c7ce19638ffb638e5b62374c28faa0e6 untitled270.ipynb . Thanks I haven t tried yet on the CPU as i would like to be able use my GPU for training faster. Would it be possible to have a fix for it Thank you in advance i have the same problem with the same error too Still having the issue even so i upgraded to latest cudnn package. Does anybody have a fix Hope this helps. I was facing the similar issue with tensorlfow 2.0 CUDA 10.0 cuDNN 7.6.5 I downgraded to cuDNN 7.6.2 and my model is training now. I tried cuDNN 7.5.1 before cuDNN 7.6.2 but that complained that source was built with 7.6 Also you are on CUDA 10.1. I started with CUDA 10.2 but eventually decided to downgrade to CUDA 10.0 and it was working fine for smaller models until I hit the above error message with more robust CNNs. well i tried to downgrade to cudnn 7.6.2 and CUDA 10.0 but the issue persist ... What configuration are you using to get this to work I ve the same issue. Latest tensorflow built from source HEAD 55119aadc69d394047e5f75d514fb6488cd4adb4 Cuda 10.0 cudnn 7.6.5 menatte Can you please downgrade to downgrade to cudnn 7.6.2 and let me know if it works. Thanks I was able to get this to work by downgrading cudnn 7.6.2.24 1 cuda10.0 Driver Version 430.50 Tensorflow gpu 1.14 That unfortunnately works for tensorflow only the same error appear while using keras with the same backend niccle27 Thanks It works by using tensorflow gpu 1.14 niccle27 I am closing this issue as it has been resolved. If you have any more concerns please create an other issue and we can take a look at it. Thanks Are you satisfied with the resolution of your issue a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 Yes entry.2137816233 https github.com tensorflow tensorflow issues 34518 Yes a a href https docs.google.com forms d e 1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1 qMdv3gc6bEP90vY1ew viewform entry.85265664 No entry.2137816233 https github.com tensorflow tensorflow issues 34518 No a i need to use two class id s for this is it possible 